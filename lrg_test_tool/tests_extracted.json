[
  {
    "test_name": "tsag4kcolumn.tsc",
    "setup": "srdbmsini",
    "flags": {
      "log_file": "tsag4kcolumn.log"
    },
    "description": "tsag4kcolumn.tsc - Smart Scan tests on 4k-columns table\n\nSmart Scan tests on 4k-columns table:\n\n       1) cc1 and cc2\n       2) SI",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsag4kflashgd.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "asm_ausize": "1048576"
    },
    "description": "tsag4kflashgd.tsc - test for 4k sector size for flash grid disks",
    "platform": null
  },
  {
    "test_name": "tsag4pd1lun_setup.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag4pd1lun_setup.tsc - setup one lun four physical disk environment\n\nThis test sets up dbnode on fake hardware, creates physical disks and LUNs\n     and lists those disks using debugCli and dbmCli.\n\nThis test sets up dbnode on fake hardware, creates physical disks and LUNs\n     and lists those disks using debugCli and dbmCli.",
    "platform": null
  },
  {
    "test_name": "tsag8rack_count_cdfd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag8rack_count_cdfd.tsc - checks the number of celldisks and flashdisks\n                                are half in number after setting eighthRack to true\n\nThis is a real hardware test and will run on X5 cell\n\nchecks the number of celldisks and flashdisks are half in number after setting eighthRack to true",
    "platform": null
  },
  {
    "test_name": "tsag8rack_cpu_usage.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag8rack_cpu_usage.tsc - checks the number of CPUS are half in number after setting eighthRack to true\n\nThis is a real hardware test and will run on X5 cell\n\nchecks the number of CPUS are half in number after setting eighthRack to true",
    "platform": null
  },
  {
    "test_name": "tsag8rack_cpu_usage_x3dbnode.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag8rack_cpu_usage_x3dbnode.tsc - enables/disables half of the CPU resources\n                                        on X3 db node using resourcecontrol script\n\nThis test runs on real hardware. It enables/disables half of the CPU resources on X3 db node\n     using resourcecontrol script. An X3 compute node has 8 active CPU cores per socket and a total\n     number of 16 active cores. Half of them can be disabled and then 4 cores per socket and total\n     8 cores will be functioning.\n\nThis is a real hardware test and at the time this test has been written eighthRack attribute\n     has not been implemented for compute nodes therefore CPU cores are enabled/disabled by using\n     resourcecontrol script. resourcecontrol is tool found in /opt/oracele.SupportedTools/\n     directory on a cell/compute node.",
    "platform": null
  },
  {
    "test_name": "tsag8rack_cpu_usage_x42dbnode.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag8rack_cpu_usage_x42dbnode.tsc - enables/disables half of the CPU resources\n                                        on X42 db node using resourcecontrol script\n\nThis test runs on real hardware. It enables/disables half of the CPU resources on X42 db node\n     using resourcecontrol script. An X42 compute node has 12 active CPU cores per socket and a total\n     number of 24 active cores, half of them can be disabled. After  having disabled half of the CPU\n     cores 6 cores per socket and total 12 cores will be functioning.\n\nThis is a real hardware test and at the time this test has been written eighthRack attribute\n     has not been implemented for compute nodes, therefore CPU cores are enabled/disabled by using\n     resourcecontrol script. Script resourcecontrol is a tool found in /opt/oracele.SupportedTools/\n     directory on a cell/compute node.",
    "platform": null
  },
  {
    "test_name": "tsag8rack_cpu_usage_x7dbnode.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag8rack_cpu_usage_x7dbnode.tsc - enables/disables half of the CPU resources\n                                        on X7 db node using resourcecontrol script\n\nThis test runs on real hardware. It enables/disables half of the CPU resources on X7 db node\n     using resourcecontrol script. An X7 compute node has 24 active CPU cores per socket and a total\n     number of 48 active cores. Half of them can be disabled and then 12 cores per socket and total\n     24 cores will be functioning.\n\nThis is a real hardware test and at the time this test has been written eighthRack attribute\n     has not been implemented for compute nodes therefore CPU cores are enabled/disabled by using\n     resourcecontrol script. resourcecontrol is tool found in /opt/oracele.SupportedTools/\n     directory on a cell/compute node.",
    "platform": null
  },
  {
    "test_name": "tsag8rack_drop_recreate_cell.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag8rack_drop_recreate_cell.tsc - see if eighthRack configuration remains even after dropping the cell\n\nThis scripts set eighthRack attribute to true and then drop and recreate the cell to see that eighthRack\n     attribute still remains to true. By positive behavior it should be.\n\nThis scripts set eighthRack attribute to true and then drop and recreate the cell to see that eighthRack\n     attribute still remains to true. By positive behavior it should be.",
    "platform": null
  },
  {
    "test_name": "tsag8rack_rem_celldiskconfig.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag8rack_rem_celldiskconfig.tsc - removes cell_disk_config.xml and see if half of the resources are used\n\nThis script tests if cell_disk_config.xml gets removed along with its backup yet\n     the eighthRack attribute should remain treu and only half of the resources should be used.\n\nThis script tests if cell_disk_config.xml gets removed along with its backup yet\n     the eighthRack attribute should remain treu and only half of the resources should be used.",
    "platform": null
  },
  {
    "test_name": "tsag8rack_tst_1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag8rack_tst_1.tsc - Test 1: Eight Rack can only be enabled on X3 h/w\n     unless the underscore parameter _cell_eighth_rack_feature_enabled is\n     set to true in cellinit.ora\n\ntsag8rack_tst_1.tsc - Test 1: Eight Rack can only be enabled on X3 h/w\n     unless the underscore parameter _cell_eighth_rack_feature_enabled is\n     set to true in cellinit.ora",
    "platform": null
  },
  {
    "test_name": "tsag8rack_tst_1_asm.tsc",
    "setup": null,
    "flags": {
      "tst_num": "1"
    },
    "description": "tsag8rack_tst_1_asm.tsc - disk recreation test with ASM setup\n\ntests recreation of celldisks, griddisks, flashcache, and flashlog on\n     spare disks in case disk(s) meet to any failure and when eighthRack feature is ON.\n\nASM setup should be there to execute this test, tsag8rack_tst_asm_setup.tsc is\n     included in beginning of the test to setup ASM.",
    "platform": null
  },
  {
    "test_name": "tsag8rack_tst_2_5.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag8rack_tst_2_5.tsc - executes test 2, 3, 4, 5\n\nTest 2: Ensure that eighth rack cannot be enabled on a cell\n             with existing cell disks.\n     Test 3: After dropping all celldisks, eighth rack should be\n             successfully enabled.\n     Test 4: After enabling eighth rack only 6 hard cell disks and\n             8 flash cell disks can be created.\n     Test 5: Creating more cell disks after having 6 hard cell disks\n             and 8 flash cell disks created should throw an error.",
    "platform": null
  },
  {
    "test_name": "tsag8rack_tst_2_asm.tsc",
    "setup": null,
    "flags": {
      "tst_num": "2"
    },
    "description": "tsag8rack_tst_2_asm.tsc - disk recreation test with ASM setup\n\ntests recreation of celldisks, griddisks, flashcache, and flashlog on\n     spare disks in case disk(s) meet to any failure and when eighthRack feature is ON.\n     This test restarts cellSrv service during disk recreation and see disk are yet being recreated\n     successfully.\n\nASM setup should be there to execute this test, tsag8rack_tst_asm_setup.tsc is\n     included in beginning of the test to setup ASM.",
    "platform": null
  },
  {
    "test_name": "tsag8rack_tst_3_asm.tsc",
    "setup": null,
    "flags": {
      "tst_num": "3"
    },
    "description": "tsag8rack_tst_3_asm.tsc - - disk recreation test with ASM setup\n\ntests recreation of celldisks, griddisks, flashcache, and flashlog on\n     spare disks in case disk(s) meet to any failure and when eighthRack feature is ON.\n     This test restarts MS service during disk recreation and see disk are yet being recreated\n     successfully.\n\nASM setup should be there to execute this test, tsag8rack_tst_asm_setup.tsc is\n     included in beginning of the test to setup ASM.",
    "platform": null
  },
  {
    "test_name": "tsag8rack_tst_6.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag8rack_tst_6.tsc - executes test 6\n\nTest 6: Simulate a failure on a disk. The cell disk (cd)/grid disks (gd)/flash\n             cache (fc)/flash log (fl) on it (if any) will be automatically recreated\n             on a spare disk. Make a note of the cd/gd/fc/fl info before the failure\n             simulation, and compare it with the recreated ones after the simulation.\n             Make sure cell disk deviceName, devicePartition, id, and physicalDisk\n             attributes are all different. The status should all be normal. FC and FL\n             remain normal with the same size.",
    "platform": null
  },
  {
    "test_name": "tsag8rack_tst_7.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag8rack_tst_7.tsc - Ensure that cell disks can be created on all disks after having eighthRack set to FALSE.\n\nIf eighthRack is set to TRUE then we cannot create more than 6 hard cell disks and 8 flash cell disks.\n     After setting eighthRack set to FALSE we will be able to create 12 hard cell disks and 16 flash disks.",
    "platform": null
  },
  {
    "test_name": "tsag8rack_tst_8.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag8rack_tst_8.tsc - If the cell_disk_config.xml and all its backups are\n                           lost, then users need to use the alter cell command\n                           to change the cell to an eighth rack.\n\nIf the cell_disk_config.xml and all its backups are lost, then users need\n     to use the alter cell command to change the cell to an eighth rack.",
    "platform": null
  },
  {
    "test_name": "tsag8rack_tst_asm_setup.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "normal    ## two cell setup",
      "oss_failgroup": "failalldbdg  ## mirror controlfile and flint diskgroups too",
      "asmdisks_created": "2",
      "nflint": "1",
      "tst_num": "0"
    },
    "description": "tsag8rack_tst_asm_setup.tsc - setup asm on a cell which runs in eighthRack mode\n                                   where 6 hard cell disks, and 8 flash cell disks can be created.\n\ntsag8rack_tst_asm_setup.tsc  creates 6 hard cell disks and 8 flash cell disks\n     as base setup and then sets up asm above that to execute eighth rack feature tests\n\ntsag8rack_tst_asm_setup.tsc  creates 6 hard cell disks and 8 flash cell disks\n     as base setup and then sets up asm above that to execute eighth rack feature tests",
    "platform": null
  },
  {
    "test_name": "tsag8rack_tst_setup.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagrackdef"
    },
    "description": "tsag8rack_tst_setup.tsc - creates 12 hard cell disks and 16 flash cell disks\n\ntsag8rack_tst_setup.tsc  creates 12 hard cell disks and 16 flash cell disks\n     as base setup to execute eighth rack feature tests\n\ntsag8rack_tst_setup.tsc  creates 12 hard cell disks and 16 flash cell disks\n     as base setup to execute eighth rack feature tests",
    "platform": null
  },
  {
    "test_name": "tsag_add_qourum_fg.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_add_qourum_fg.tsc - the script for avoiding PST issues.\n\nAdds 2 quorum disk fail groups for normal/high redundancy DG.\n     This script can be included after diskgroup creation, incase there is PST issue.",
    "platform": null
  },
  {
    "test_name": "tsag_asm_pow_limit_adj_scripts.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_asm_pow_limit_adj_scripts.tsc - Holds the helper scripts user for power limit adjustment test.\n\nMore details on this test case is described in comfluence page.\n     Link : https://confluence.oraclecorp.com/confluence/display/~lansing.chen@oracle.com/%5BTest+Doc%5D+ASM+power+limit+adjustment+via+CLI+Commands#id-[TestDoc]ASMpowerlimitadjustmentviaCLICommands\n     Contact person from dev team : Lansing Chen from Ryan's team",
    "platform": null
  },
  {
    "test_name": "tsag_auto_asm_pow_lim_adj.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_failgroup": "failalldbdg",
      "OSS_AUTO_MANAGE_DISKS": "true"
    },
    "description": "tsag_auto_asm_pow_lim_adj.tsc - Test cases for Auto ASM power limit adjustment\n\nTest case 1 . Syntax tests for rebalancePowerLimitPolicy\n    Test case 2 . Version Migration Test\n    Test case 3 . To verify that power limit adjustment is not happenning when there is no active rebalance.\n    Test case 4 . Verifies the power limit adjustments during rebalance. The policy having highest priority\n                  will be effective when there are multiple policies set.\n    Test case 6 . Tests underscore parameters.\n\nMore details on this test case is described in comfluence page.\n     Link : https://confluence.oraclecorp.com/confluence/display/~lansing.chen@oracle.com/%5BTest+Doc%5D+ASM+power+limit+adjustment+via+CLI+Commands#id-[TestDoc]ASMpowerlimitadjustmentviaCLICommands\n     Contact person from dev team : Lansing Chen from Ryan's team",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsag_b8292943.tsc",
    "setup": "tsagnini",
    "flags": {
      "oss_testing": "1"
    },
    "description": "tsag_b8292943.tsc - Test for bug8292943 which introduces new\n                         parameter kcfis_block_dump_level to dump\n                         smart I/O blocks on different table formats\n\nkcfis_block_dump_level when enabled, this test would verify if the\n     blocks are being dumped with out any errors on basic table, compressed table\n     OLTP compressed table, HCC table, non-HCC table, encrypted table.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsag_bug_35969849.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_bug_35969849.tsc - Test Case to verify Bug-35081089\n   UPDATE TO RU## LOCATION IN SWITCH PORT DESCRIPTION IN GOLDEN CONFIG\n\nVerify the switch patch carries the golden config files\n     with RU## instead of adm##/celadm##, under\n     patch_switch_<version>/roce_switch_templates directory\n\nThis test is added in lrgsaexacldescli6.",
    "platform": null
  },
  {
    "test_name": "tsag_celofl_smrt_scn.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cellconnstr": "root@^CELL^"
    },
    "description": "tsag_celofl_smrt_scn.tsc - OSS CELL OFFLOAD TEST\n\nCELL OFFLOAD PACKAGE SMART SCAN TESTS\n\nvalid only for 12.1 and above",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsag_db_acc1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_db_acc1.tsc - Exadata multi-node db access file\n\nAccesses the RDBMS instance and creates table and workload",
    "platform": null
  },
  {
    "test_name": "tsag_db_asm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_db_asm.tsc - Exadata multi-node db access file\n\nAccesses the RDBMS instance and creates table and workload",
    "platform": null
  },
  {
    "test_name": "tsag_db_chk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_db_chk.tsc - Exadata multi-node db access file\n\nAccesses the RDBMS instances and runs sql which checks for correct number of rows",
    "platform": null
  },
  {
    "test_name": "tsag_db_spc_rclmn.tsc",
    "setup": null,
    "flags": {
      "crashfileloc": "/u01/crashfiles",
      "crashfiledir": "/u01"
    },
    "description": "tsag_db_spc_rclmn.tsc - Test for OSS DBMS Space Reclamation\n\nTest for crashfile compression and space reclamation on dbnode",
    "platform": null
  },
  {
    "test_name": "tsag_dbms_pow_ctrl.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_dbms_pow_ctrl.tsc - MS restart test",
    "platform": null
  },
  {
    "test_name": "tsag_dbms_upgrd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_dbms_upgrd.tsc - DBMS_UPGRADE_TEST\n\nExadata-dbmmgmt upgrade-downgrade test",
    "platform": null
  },
  {
    "test_name": "tsag_dbservice_shutdown.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_dbservice_shutdown.tsc - test for DB service shutdown\n\nTest if DB services are shut down after rpm installation, but started up before the upgrade finishes.",
    "platform": null
  },
  {
    "test_name": "tsag_domu_check.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_domu_check.tsc - Script to run verifications on Exadata VMs",
    "platform": null
  },
  {
    "test_name": "tsag_ers_restfuzz.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsag_ers_restfuzz.tsc - <restfuzz test for ers>\n\n<this test implements restfuzz for ers>",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsag_ilom_sp_reset.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsag_ilom_sp_reset.tsc - ILOM SP RESET TEST\n\n1. Remove celldiskkconfig files\n\t2. Add _sp_reset_interval in cellinit.ora\n      3. Restart MS and verify alerts in ms-odl.trc",
    "platform": null
  },
  {
    "test_name": "tsag_ilomfail_dbnode.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_ilomfail_dbnode.tsc - ILOM Failure Simulation Tests for DB Node\n\nThe test simulates ILOM failure using CFS to test ILOM resetusing KCS, LAN and crash_reset for DB Node.\n\nruns in lrgrh2sadbmcli1",
    "platform": null
  },
  {
    "test_name": "tsag_oedacli_altercluster.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_oedacli_altercluster.tsc - OEDACLI test coverage for command 'alter cluster'\n\nThis wrapper file will include all the oedacli commands related to 'alter cluster'\n     Test cases added:\n      1. VCPU argument positive/negative validation\n      2. VMEM argument positive/negative validation",
    "platform": null
  },
  {
    "test_name": "tsag_oedacli_altermachine.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_oedacli_altermachine.tsc - OEDACLI test coverage for 'alter machine'\n\nThis wrapper file will include all the oedacli commands\n     related to 'alter machine' to run on fake LRG\n     Test cases added:\n      1. ACTION=ATTACHDISK HOSTNAME argument positive/negative validation\n      2. ACTION=DETACHDISK HOSTNAME argument positive/negative validation\n      3. ACTION=SETVDISK VDISK argument positive/negative validation",
    "platform": null
  },
  {
    "test_name": "tsag_oedacli_altermachines.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_oedacli_altermachines.tsc - OEDACLI test coverage for command 'alter machines'\n\nThis wrapper file will include the oedacli commands related to 'alter machines'\n     Test cases added:\n        1. ORADISKSIZE argument positive/negative validation\n        2. ORADISKPATH argument positive/negative validation\n        3. IMAGEVERSION argument positive/negative validation\n        4. STORAGETYPE argument positive/negative validation",
    "platform": null
  },
  {
    "test_name": "tsag_oedaclicoverage_main.tsc",
    "setup": null,
    "flags": null,
    "description": "tsag_oedaclicoverage_main.tsc - Main file to add oedacli test coverage\n\nCreate separate wrapper file for each of the oedacli command and call from here",
    "platform": null
  },
  {
    "test_name": "tsag_oss_em_interface.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsag_oss_em_interface.tsc - Test for OSS EM Interface\n\nTest verifies for the cellcli commands supported by EM.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsag_pvt_pkg_ofld.tsc",
    "setup": null,
    "flags": {
      "cellconnstr": "root@^cell_name^"
    },
    "description": "tsag_pvt_pkg_ofld.tsc - Private Offload Package Test\n\nThe test adds a script for Bug 16928903: REAL HARDWARE LRG TESTS NEEDED FOR PRIVATE OFFLOAD PACKAGE INSTALL/UNINSTALL",
    "platform": null
  },
  {
    "test_name": "tsag_spc_rclmn.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsag_spc_rclmn.tsc - space reclamation test\n\nTest for BUG 17044163 -  X4: CRASHFILE GOT REMOVED BY SPACE RECLAIMATION AFTER CELL BOOT UP\n              BUG 17272263 - MS RAISES AN ALERT ON NEWLY PATCHED CELLS ABOUT OLD KERNEL CRASHES\n              BUG 17257895 - MS COMMAND TIMEOUT REPEATEDLY RESET ILOM.\n\nTest for space reclamation through crashfiles compression in parallel to MS startup",
    "platform": null
  },
  {
    "test_name": "tsagabortsnapdel.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true",
      "multiple_bst": "true"
    },
    "description": "tsagabortsnapdel.tsc - Verify that  snap deletion job is aborted when VIP is moving to new worker\n\nAbort snap deletion job if VIP is moving to new worker,\n     so that it won't be blocked since snap file deletion can drag long.\n     To be more specific, VIP will be released earlier and let it move\n     instead of waiting there and hit the ORA since VIP cannot drain the running job.\n     The job will keep running and end the job when file deletion is completed.\n\n     Test Steps:\n1.  Start with iscsi mode  and multiple BSWs\n2.  create 3 VIPs, 3 5g volumes, 3 snapshots\n3.  kill two BSWs\n4.  Set snap file deletion delay simulation\n5.  remove 3 snapshots - eventually the requests will be timed out or get Error code: 20018\n6.  Bring up one BSW\n7.  wait 5 mins. Should not see ORA \"fail to drain running jobs \"\n8.  turn off simulation\n      9.  check vips and snapshots\n      10. wait 5 mins more. Should not see ORA \"fail to drain running jobs \"\n11.  check VIPs - 3 VIPs should all be state CREATED instead of MOVING\n12. check snapshots - 3 snapshots are removed\n\nThis test case was added in transaction kukuchen_abort.snap.del.job .",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagabortwhenhitkeeplimit.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagabortwhenhitkeeplimit.tsc - Check keep ioctl rejects\n\nTest to verify that when flashcache is already over keep threshold,\n     new coming population request should be rejected\n\n   Notes\n    Run srdbmsini to setup database and cell, this script created a 1.5GB\n    cellcli -e list flashcache detail\n    Set cellsrv flashcache layer trace to mid level.\n    Create a 1.5GB table testtb1\n    Sqlplus> alter table testtb1 storage (cell_flash_cache keep); //This should full fill the flashcache with populated data of testtb1\n    Create a smaller table(maybe 500MB) testtb2\n    Sqlplus> alter table testtb1 storage (cell_flash_cache keep);//Populate request should be rejected by flashcache since flashcache is over keep threshold\n    Grep cell trace for âfcPopRanges over keep limitâ, you should see it print out flashcache is over keep limit.",
    "platform": null
  },
  {
    "test_name": "tsagacfssetup.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagacfssetup.tsc - setup acfs file system and create files",
    "platform": null
  },
  {
    "test_name": "tsagactdsk1.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagaudef",
      "nflint": "1",
      "oss_testing": "2",
      "SAGE_MIRROR_MODE": "normal"
    },
    "description": "tsagactdsk1.tsc - Tests for Exadata Active Disk project\n\nTest case:\n     1. qos failure test\n     2. Normal harddisk confinement test\n     3. Harddisk confinement-rebalance test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdsk10.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'",
      "asm_ausize": "1048576"
    },
    "description": "tsagactdsk10.tsc - Exadata Active disks tests\n\nExadata Active disks tests for slow flash disk along with dead & predfail disk",
    "platform": null
  },
  {
    "test_name": "tsagactdsk11.tsc",
    "setup": null,
    "flags": {
      "oss_testing": "2",
      "SAGE_MIRROR_MODE": "normal",
      "conn1": "'sys/knl_test7 as sysasm'",
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagactdsk11.tsc - Tests for Exadata Active Disk project\n\nFlash disks into confine active state",
    "platform": null
  },
  {
    "test_name": "tsagactdsk12.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "nflint": "1"
    },
    "description": "tsagactdsk12.tsc - Tests for Exadata Active Disk project\n\nHard disks into confine active state",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdsk13.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "nflint": "1"
    },
    "description": "tsagactdsk13.tsc - Tests for Exadata Active Disk project\n\nSimulate slow hard disk across ASM instance reboot",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdsk14.tsc",
    "setup": "srdbmsini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "nflint": "1",
      "oss_asm_sec": "true",
      "disk": "datafile1"
    },
    "description": "tsagactdsk1.tsc - Tests for Exadata Active Disk project\n\nSimulate slow hard disk for a single disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdsk15.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagr2def2",
      "nflint": "0"
    },
    "description": "tsagactdsk15.tsc - Tests for Exadata Active Disk Project\n\nSimulate high latency I/Os on a hard disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdsk16.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagr2def2",
      "nflint": "0"
    },
    "description": "tsagactdsk16.tsc - Tests for Exadata Active Disk Project\n\nSimulate high latency I/Os on a hard disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdsk17.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "nflint": "1",
      "disk": "datafile1"
    },
    "description": "tsagactdsk1.tsc - Tests for Exadata Active Disk project\n\nrepeatedly put a disk into confine state.\n      3rd attempt should go into proactive fail.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdsk2.tsc",
    "setup": "tsagnini",
    "flags": {
      "oss_testing": "2",
      "SAGE_MIRROR_MODE": "normal",
      "creatdev_file": "tsagaudef",
      "nflint": "1",
      "disk": "datafile0"
    },
    "description": "tsagactdsk2.tsc - Tests for Exadata Active Disk project\n\nSimulate slow hard disk for multiple disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdsk3.tsc",
    "setup": null,
    "flags": {
      "oss_testing": "2",
      "SAGE_MIRROR_MODE": "normal",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'",
      "disk": "FLASH0"
    },
    "description": "tsagactdsk3.tsc - Exadata Active disks tests\n\nExadata Active disks tests for slow flash disks\n     Test case:\n     1. Normal flashdisk confinement [confinement test should pass]",
    "platform": null
  },
  {
    "test_name": "tsagactdsk4.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'",
      "client_pmemcache_rdma_crc_check_mode": "unsafe",
      "disk": "FLASHa0"
    },
    "description": "tsagactdsk4.tsc - Tests for Exadata Active Disk project\n\nSimulate slow flash disk for multiple disk",
    "platform": null
  },
  {
    "test_name": "tsagactdsk5.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "nflint": "1",
      "disk": "FLASH0"
    },
    "description": "tsagactdsk5.tsc - Tests for Exadata Active Disk project\n\nSimulate slow flash disk for a single disk - check flashcache status",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdsk6.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "nflint": "1",
      "disk": "FLASH0"
    },
    "description": "tsagactdsk6.tsc - Tests for Exadata Active Disk project\n\nSimulate slow flash disk for a single disk - check flashcache status",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdsk7.tsc",
    "setup": "tsagnini",
    "flags": {
      "disable_multims": "true",
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "nflint": "1",
      "disk": "datafile0"
    },
    "description": "tsagactdsk7.tsc - Tests for Exadata Active Disk project\n\nRestart Cellsrv + MS across slow hard disk or flash disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdsk7a.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "nflint": "1",
      "disk": "datafile0"
    },
    "description": "tsagactdsk7a.tsc - Tests for Exadata Active Disk project\n\nRestart Cellsrv + MS across slow hard disk or flash disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdsk8.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "nflint": "1"
    },
    "description": "tsagactdsk8.tsc - Tests for Exadata Active Disk project\n\nSimulate a slow hard disk with some dead hard disks",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdsk9.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "nflint": "1"
    },
    "description": "tsagactdsk9.tsc - Tests for Exadata Active Disk project\n\nSimulate a slow hard disk with some pred fail hard disks",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagactdskchk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagactdskchk.tsc - check status of various cell objects\n\ncheck status of lun ,physicaldisk ,grididsk ,ASM disk ,etc",
    "platform": null
  },
  {
    "test_name": "tsagactdskchk2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagactdskchk.tsc - check status of various cell objects\n\ncheck status of lun ,physicaldisk ,grididsk ,celldisk",
    "platform": null
  },
  {
    "test_name": "tsagactdskchk3.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagactdskchk3.tsc - check status of various cell objects\n\ncheck status of lun ,physicaldisk ,grididsk ,etc",
    "platform": null
  },
  {
    "test_name": "tsagactdsuit.tsc",
    "setup": null,
    "flags": {
      "keep_pmemlog_disabled_exc": "true"
    },
    "description": "tsagactdsuit.tsc - Exadata Active Disks Test suit\n\nExadata Active Disks Test suit",
    "platform": null
  },
  {
    "test_name": "tsagactondskevnt.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "oss_auto_manage_disks": "true",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagactondskevnt.tsc - Exadata Act on disk Event Test\n\n#   Test verifies MS is shutdown and restarted during disk failure, removal and replacement ##\n1. set MS_TESTING env\n2. add _cell_actupon_disk_event_reboot_ms=true in cellinit.ora and restart MS\n3. simulate  disk failure, list PD should fail with CELL-01514\n4. MS status should be running after a little wait\n5. Remove _cell_actupon_disk_event_reboot_ms, Restart MS\n6. List PD status (should be failed) and alert for failure should be generated\n\nRepeat the steps 2-6 for simulating disk removal\n\nRepeat the steps 2-6 for disk replacement",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagactreq.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagccdef",
      "asm_ausize": "65536",
      "undo_filename": "^asmprefix^^asm_dtfile_dg^/^asm_dbs_dir^/^dbs1_undofile_0_1_name^"
    },
    "description": "tsagactreq.tsc - SAGE active request test\n\nTests cellcli active request while IO's are pending\n\n- IO's are held\n     - Run IO workload (RDBMS LOB insert)\n     - When all the OSS queues are filled check using CELLCLI",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagadd_cell.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagfcgdef",
      "cell_to_add": "3",
      "ossport": "oss_port^cell_to_add^",
      "tmp_path": "raw_path^cell_to_add^"
    },
    "description": "tsagadd_cell.tsc - add a cell to the existing EXADATA setup\n\ncreates lu,celldisk, griddisk and add it to the existing DATAFILE diskgroup as  a new\n#      failgroup",
    "platform": null
  },
  {
    "test_name": "tsagadd_pd_to_lun.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagadd_pd_to_lun.tsc - creates LUN on four PDs\n                           - delete a PD from the LUN\n                           - add a PD to the LUN\n\nThis test creates a LUN on four PDs, then removes on PD from the LUN,\n     then again add a PD to the LUN.\n\nThis test creates a LUN on four PDs, then removes on PD from the LUN,\n     then again add a PD to the LUN.",
    "platform": null
  },
  {
    "test_name": "tsagaddcell.tsc",
    "setup": null,
    "flags": {
      "ctrl1": "'--ctrl localhost:'^nginx_https_port^",
      "ctrl2": "'--ctrl localhost:'^nginx_https_port2^",
      "ctrl": "^ctrl2^",
      "ossport": "oss_port^cell_to_add^",
      "instcell": "^cell_to_add^",
      "creatdev_file": "^devfilei^",
      "devfilei": "^creatdev_file^^cell_to_add^.log",
      "tmp_oconf": "OSSCONF^cell_to_add^",
      "curinst": "2",
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagaddcell.tsc - Adds a cell in the Exascale deployment\n\nThis script is used to add a new cell to Exascale deployment\n     run tsagaddcell cell_to_add=<cell number> reconfig=[true]\n         cell_to_add -> cell instance number to add\n         reconfig=true -> if true, it will reconfig the storagepool",
    "platform": null
  },
  {
    "test_name": "tsagadddrop_fail.tsc",
    "setup": "srdbmsini",
    "flags": {
      "nflint": "1",
      "creatdev_file": "tsagrddef3",
      "disable_multims": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsagadddrop_fail.tsc - add and drop a cell from existing exadata setup",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagadddropcell.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line"
    },
    "description": "tsagadddropcell.tsc - drop/add cell reconfig storage pool test\n\nFor Non active client IOV:\n1) In a 4 cell env, Run IOV workload on a vault/file and let it finish\n2) drop all VESGDs on cell1\n3) Reconfigure Storage pool - using chstoragepool --reconfig\n4) create VESGDs on cell1\n5) Reconfigure SP again so the disks are added back to the cluster\n6) md5sum on all mirrors of the IOV test files and they should be the same.\n# Jump back to 1 and run another IOV xml.\n\ntest to be added in  lrgsaexacldvesdropadd1",
    "platform": null
  },
  {
    "test_name": "tsagadddropcellactv.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "egsopidoffset": "-10000",
      "ves_sim": "4"
    },
    "description": "tsagadddropcellactv.tsc - drop/add cell reconfig storage pool test with active client\n\n#  For Active client:\n# 1) In a 4 cell env, start IOV in the background\n# 2) while IOV is running, drop all VESGDs on cell1\n# 3) reconfigure SP - using chstoragepool --reconfig\n# 4) create VESGDs on cell1\n 5) Reconfigure SP again so the disks are added back to the cluster\n 6) md5sum on all mirrors of the IOV test files and they should be the same.\n Jump back to 1 and run another IOV xml.\n\ntest to be added in lrgsaexacldvesdropadd2",
    "platform": null
  },
  {
    "test_name": "tsagaddquorum_flex.tsc",
    "setup": null,
    "flags": {
      "asm_diskstring": "^asm_diskstring^,''^T_WORK^/exadata_quorum/'*'''"
    },
    "description": "tsagaddquorum_flex.tsc - the script for avoiding PST issues.\n\nAdds 2 quorum disk fail groups for normal/high redundancy DG.\n     This script can be included after diskgroup creation, incase there is PST issue.\n     we will be using this script in few tests where we are using flex\n#     redundancy for +DATAFILE",
    "platform": null
  },
  {
    "test_name": "tsagaddredo.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagaddredo.tsc - test file for checking redolog workload completed perfectly\n\nInvocation: runtest tsagaddredo.tsc redolog_file=xyz.log\n     Here redolog_file is log file whice is used with fork command, when running redolog workload in background.",
    "platform": null
  },
  {
    "test_name": "tsagaddrepegs.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "vault_db": "DATA",
      "cdb": "true",
      "dbusrwallet": "^ORACLE_BASE^/admin/^ORACLE_SID^/eswallet"
    },
    "description": "tsagaddrepegs.tsc - test to be added in lrgsaexacldegstrust\n\n# test to add a new egs server on top of the 3 existing egs server,\n# and see if the new server is allowed in the quorum.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagaddrepegs_kill.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "dbusrwallet": "^ORACLE_BASE^/admin/^ORACLE_SID^/eswallet",
      "vault_db": "DB^ORA_SID_UPPER^",
      "cdb": "true"
    },
    "description": "tsagaddrepegs_kill.tsc - test to be added in lrgsaexacldegstrust\n\nreplacing an existing egs server for a new one, and see\n  if the new server is allowed in the quorum.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagaddsparsegd.tsc",
    "setup": null,
    "flags": {
      "sparse_disk_per_cell": "5",
      "dg_compat": "^scompatible.asm^",
      "sparsedg_redund": "external",
      "creatdev_file": "tsagrddef",
      "spdisk_index": "^sparse_disk_per_cell^",
      "redund": "external",
      "sparse_cells": "1   ##1 cell",
      "raw": "^ossconf1^/raw",
      "raw_path1": "^raw_path^",
      "oss_devdir": "^ossconf1^/raw"
    },
    "description": "tsagaddsparsegd.tsc -Adds SPARSE Griddisks",
    "platform": null
  },
  {
    "test_name": "tsagaepunittest.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagaepunittest.tsc - AEP unit test run\n\ntsagaepunittest.sh run for unit testing AEP",
    "platform": null
  },
  {
    "test_name": "tsagaffinity.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagaffinity.tsc - Tests node->IP address affinity code in ossnet.c\n\nOn Numa machines, some IP address are closer to some nodes than\n     others.  On such machines it is more efficient for a given node\n     to use a particular IP address for message traffic in and out than\n     other IP address.  This test, test the code which makes this\n     selection in ossnet.c.\n\n     In addition to that, having more than 4 IPs (e.g. 8) in the system\n     requires the libcell to select a subset of 4 IPs that would be used\n     to initialize the SKGXP context.\n\n     See http://dbdev.us.oracle.com/twiki/bin/view/Sage/DBNodeSupportFivePlusHCAs\n\nThere are two ways to simulate NUMA-aware machine:\n\n      - Using \"SIM_NUMA\" environment variable, which simulates the NUMA-aware\n        machine at the skgsn level. We're using this method to test the\n        oss_check_sage_config() which happens to have no diag context.\n\n      - Using the 'libcell.cellclnt_ossnet_trc' diag event. We're using this\n        method to simulate a particular node (not possible via the skgsn\n        simulation).\n\n     tsagaffinity.c - the C test case that drives the load",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagaidealert.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagaidealert.tsc - Test for AIDE alert check mechanism\n\nTest for AIDE alert check mechanism on storage cell",
    "platform": null
  },
  {
    "test_name": "tsagalrt.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagalrt.tsc - It tests the various alerts generated on the cell.\n\nIt tests the various alerts generated on the cell.",
    "platform": null
  },
  {
    "test_name": "tsagalrt_cl.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagalrt_cl.tsc - alert test for exacli\n\nAlert test similar to tsagalrt.tsc fo rexacli",
    "platform": null
  },
  {
    "test_name": "tsagalrt_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagalrt_nls.tsc - alerthistory, alertdefinition command of CellCLI",
    "platform": null
  },
  {
    "test_name": "tsagalrtrds.tsc",
    "setup": null,
    "flags": {
      "cell1_f": "^cell^.us.oracle.com"
    },
    "description": "tsagalrtrds.tsc - Alert configuration helper script for RDS lrgs",
    "platform": null
  },
  {
    "test_name": "tsagaltcach.tsc",
    "setup": null,
    "flags": {
      "asm_ausize": "4194304",
      "db_file_multiblock_read_count": "128",
      "mostlyOltp": "'$kvalue{FC_OLTP_used_size}>.8*$kvalue{FC_used_size}"
    },
    "description": "tsagaltcach.tsc - alter cell_flash_cache between default, none, keep\n                     - dump cache stats",
    "platform": null
  },
  {
    "test_name": "tsagarbauton.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagarbauton.tsc - one-line expansion of the name",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagarbcov.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "dsk_def"
    },
    "description": "tsagarbcov.tsc - Arbiter test for griddisk layer coverage\n\nGriddisks can be physcially fragmented - but for the sage lrgs we run,\n     the griddisks that get created have one contiguous fragment, due to\n     which that portion of the code which deals with fragmented griddisk\n     does not get exercised. We have added a parameter\n     '_cell_force_split_gdisk' which creates a situation such that griddisk\n     gets fragmented. NOTE this parameter is ONLY used for coverage and\n     should never be tampered with in other tests.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagarbiterpmem.tsc",
    "setup": "tsaginit",
    "flags": {
      "ENABLE_LW_CACHING": "true",
      "creatdev_file": "tsagrddef",
      "cell_with_flash_cache": "all",
      "cell_with_pmem_cache": "true",
      "cell_with_write_cache": "0"
    },
    "description": "tsagarbiterpmem.tsc - rebalance with pmemcache\n\nTest to simulates an ASM rebalance with pmemcache using Arbiter\n There is basically a matrix of 6 scenarios that should be tested\n with Arbiter. The test matrix consists of 2 store clause modes\n (default and keep) multiplied by 3 cache write modes\n (pmem wt flash wt, pmem wt flash wb, pmem wb flash wb).\n\ntest to be added in lrgsanvcache1",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagarbitr.tsc",
    "setup": "tsaginit",
    "flags": {
      "host_domain": "^hostname^",
      "hoststr": "^hostipadd^';'^hostipadd^';'^hostipadd^';'^hostipadd^"
    },
    "description": "tsagarbitr.tsc - Simple Arbiter test\n\nAn Arbiter test which initiates number of READ and WRITES in OSS\n     server. OSS should be up.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagarccomp.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "tsagcln": "3",
      "asm_ausize": "4194304",
      "compatible": "^max_compatibility^",
      "level1": "'compress for query low'",
      "level2": "'compress for query high'",
      "level3": "'compress for archive'",
      "tmp_lvl": "level^lvl^",
      "level": "^^tmp_lvl^"
    },
    "description": "tsagarccomp.tsc - Exadata Columnar Compression test\n\nSimple Columnar compression test\n\nThis test is a clone of a part of tktgadv.tsc",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagarcomps.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "compatible": "11.2.0.0",
      "sys_group_name": "^sys_group_name^",
      "celloflalert": "^ADR_BASE^/diag/asm/cell/^sys_group_name^/trace/"
    },
    "description": "tsagarcomps.tsc - Archive compression with failure simulation events",
    "platform": null
  },
  {
    "test_name": "tsagarpe1.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sage_mirror_mode": "high",
      "bug37094407": "open"
    },
    "description": "tsagarpe1.tsc - Sanity test for Auto Rebal Level Engine\n\nTests the parameters of the level engine.\n       1) Cases for enable/disable engine\n       2) Cases for diskgroup priority order\n       3) Cases for max vote value for rebalance\n\nConfluence link of test descriptions :\n     https://confluence.oraclecorp.com/confluence/display/~yan.zhao@oracle.com/Functional+Test+Spec+of+Auto+Rebalance+Policy+Engine",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagarpejarhc.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cellconnstr": "root@^cell_name^.us.oracle.com"
    },
    "description": "tsagarpejarhc.tsc - Exploratory Unit Test for Auto Rebalance Power engine on HC\n\nThe tool - autoRebalTest.jar is used for simulating rebalance. It was developed by Yan Zhao.\n     It requires 2 arguments : I/O workload configuration and ASM rebalance configuration.\n     Following scenarios are run in the test using the tool:\n     1) Small Read Only\n     2) Large Read Only\n     3) Small Write Only\n     4) Large Write Only\n     5) Small Log Write Only\n     6) Large Log Write Only\n     Each of the above scenarios are simulated by using appropriate IO workload configurations.\n\n     After running each of these scenarios, the test validates whether the power vote values\n     followed the trend, the pattern should be like increase -> decrease -> increase.\n\n     This test case is run on both High Capacity and Extreme Flash hardwares.\n     \"ef_test\" flag is used for picking right config files, to run the test on EF hardware.\n     If this flag is not set, the config files appropriate for HC hardware will be picked.\n\nConfluence page : https://confluence.oraclecorp.com/confluence/display/~yan.zhao@oracle.com/Exploratory+Unit+Test",
    "platform": null
  },
  {
    "test_name": "tsagasm.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagddef"
    },
    "description": "tsagasm.tsc - ASM short regression test.\n\nShorter version of tkfgsuit",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagasmalrtini.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagasmalrtini.tsc - prepare scripts for asm alert tests\n\nprepare commonly used scripts for asm alert tests",
    "platform": null
  },
  {
    "test_name": "tsagasmcluster.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagasmcluster.tsc - Test for create, list, alter and drop asm cluster.\n\nThis adds support for ASMCLUSTER objects in MS and CellCLI.\n     This is part of the \"Exadata Cloud Support for Unique DB Names\" project.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagasmonedvsuite.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "redund": "external",
      "compatible": "^scompatible.rdbms^"
    },
    "description": "tsagasmonedvsuite.tsc\n\nTest suite file to verify 19c DB on ASM-on-EDV\n\nThis suite file identifies LRG of format : lrgcmainr19000sa<basetestname>_asmonedv",
    "platform": null
  },
  {
    "test_name": "tsagasmorion.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback"
    },
    "description": "Orion tests with ASM",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagasmpath.tsc",
    "setup": null,
    "flags": {
      "tmp_nowarn": "^tst_nowarn^"
    },
    "description": "tsagasmpath.tsc - sets path variables for use by tkfginit, tkfgpath, ...\n\nSpecify devdir if the disks are in a different dir then raw<port> or raw.\n      If you are running the test against a remote oss, ^devdir^ should already\n      contain disks needed by the test.\n      For example, if the test calls for a database on SAGE, ^devdir^ should have\n      disks named datafile[0-4], logfile[0-1], controlfile0, and standby, which\n      may be symbolic links to real partitions.",
    "platform": null
  },
  {
    "test_name": "tsagasmsecconfig.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagasmsecconfig.tsc - this is a ASM-scoped security configuration file to setup ASM key on CellSrv level for the IOV test only",
    "platform": null
  },
  {
    "test_name": "tsagasmworkload.tsc",
    "setup": null,
    "flags": {
      "maxtime": "7200"
    },
    "description": "tsagasmworkload.tsc - Sets up and start workload\n\nUsed to setup and start DB workload in a hybrid Exascale ASM setup",
    "platform": null
  },
  {
    "test_name": "tsagasr_cell.tsc",
    "setup": "tsaginit",
    "flags": {
      "snmp_port": "^free_port_number^",
      "cell_passwd": "welcome1"
    },
    "description": "tsagasr_cell.tsc - ASR test for cell\n\nThis runs on v2 exadata hardware",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagasr_comp_node.tsc",
    "setup": null,
    "flags": {
      "SCRIPT1": "^OSS_SCRIPTS_HOME^/unix/hwadapter/agentadp",
      "SCRIPT2": "^ADE_VIEW_ROOT^/oss/util"
    },
    "description": "tsagasr_comp_node.tsc - ASR test for compute node",
    "platform": null
  },
  {
    "test_name": "tsagasrcell.tsc",
    "setup": "tsaginit",
    "flags": {
      "snmp_port": "^free_port_number^"
    },
    "description": "tsagasrcell.tsc - Simulation of disk failure on Cell\n\nThis test simulates disk failure and predictive failure\n      on Cell physicaldisks (system, non system and flash). and verifies\n      that ASR is created for each failure.\n\nCurrently, there is no automated way to receive emails to user registerd\n      email ids. So, we have to check the asruat mailbox",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagasrcomp.tsc",
    "setup": null,
    "flags": {
      "compnode": "dscgigdb05",
      "SCRIPT1": "^OSS_SCRIPTS_HOME^/unix/hwadapter/agentadp",
      "SCRIPT2": "^ADE_VIEW_ROOT^/oss/util",
      "connstr": "root@^compnode^"
    },
    "description": "tsagasrcomp.tsc - ASR Client interface test for compute node",
    "platform": null
  },
  {
    "test_name": "tsagasxdb.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagasxdb.tsc - generate tsagasxdb.sql\n\nGenerates Exadata version of tkxmsxdb.sql.\n\nAdapted from tkxmasxdb.tsc",
    "platform": null
  },
  {
    "test_name": "tsagasync_vault_deletion.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagasync_vault_deletion.tsc - Test for EDS async vault deletion\n\nPlease see below\n\nAdded in lrgsaexacldeds",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagatdsk.tsc",
    "setup": null,
    "flags": {
      "tst_nowarn_tmp": "^tst_nowarn^",
      "scratch_path": "^t_work^^s^scratch^s^",
      "asm_diskstring": "^sage_diskstring^,^scratch_path^'*'"
    },
    "description": "tsagatdsk.tsc - create some Exadata and non-Exadata disks\n\ntsagattr.tsc and tsagblkrd.tsc invoke this to create Exadata and\n     non-Exadata disks.",
    "platform": null
  },
  {
    "test_name": "tsagatp.tsc",
    "setup": null,
    "flags": {
      "dbconnstr": "^dbnodeconnstr^",
      "OSSCONF": "/opt/oracle/dbserver/dbms/deploy/config/"
    },
    "description": "tsagatp.tsc - Test to trigger some of the events for ATP monitor.",
    "platform": null
  },
  {
    "test_name": "tsagatp_cell.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true"
    },
    "description": "tsagatp_cell.tsc - Test to trigger some of the events for ATP monitor.",
    "platform": null
  },
  {
    "test_name": "tsagatp_ioreplay.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagatp_ioreplay.tsc - Test for iostat replay\n\nThis is a test for iostat replay (an iostat simulation using the\n     diskstats data from Netsuite)",
    "platform": null
  },
  {
    "test_name": "tsagatp_ioreplay_generic.tsc",
    "setup": null,
    "flags": {
      "replay_factor": "0.01",
      "dbconnstr": "^dbnodeconnstr^",
      "OSSCONF": "/opt/oracle/dbserver/dbms/deploy/config/",
      "ATP_TEST_DIR": "/u01/work"
    },
    "description": "tsagatp_ioreplay_generic.tsc - IOStat replay\n\nThis is a test for iostat replay (an iostat simulation using the\n     diskstats data from Netsuite)\n\nNeeds a .zip file to get passsed in as dumpfile argument",
    "platform": null
  },
  {
    "test_name": "tsagatphugepages.tsc",
    "setup": "srdbmsini",
    "flags": {
      "ORIG_JAVA_HOME": "^JAVA_HOME^",
      "JAVA_HOME": "^ADE_VIEW_ROOT^/jdk17"
    },
    "description": "tsagatphugepages.tsc - Test to check atp alerts for memory hugepages",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagatpmetric.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1"
    },
    "description": "tsagatpmetric.tsc - Test to veriy ATP metrics for streaming\n\nTest for metric streaming of ATP metrics (which are only available on\n     real hardware)",
    "platform": null
  },
  {
    "test_name": "tsagatpsim.tsc",
    "setup": null,
    "flags": {
      "ATP_TEST_DIR": "^T_WORK^",
      "ORIG_JAVA_HOME": "^JAVA_HOME^",
      "JAVA_HOME": "^ADE_VIEW_ROOT^/jdk17"
    },
    "description": "tsagatpsim.tsc - Test to check for ATP alerts with SIMulation\n\nThis includes ATP tests that rely on simulation, specifically\n     . port up/down\n\nSimulation relies on the ATP_TEST_DIR parameter which will contain\n     the \"fake\" data to generate the alerts",
    "platform": null
  },
  {
    "test_name": "tsagattachlimit.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagattachlimit.tsc - Tests that maximum of 256 attachments could be made per initiator id.\n\nTest case to check the maximum number of attachments\n   that could be attached to a single initiator id. Its expected\n   that 64 attachments to single initiator id will be successfull\n   and 65th attachment to that initiator id will fail.\n\n   Test cases covered:\n    1) Attaches 260 volume attachments to 260 different initiator ids. All should succeed.\n    2) Attaches 260 volume attachments, 5 attachment per initiator id. All should succeed.\n    3) Attaches 64 volume attachments to single initiator id. They should succeed and 65th attachment to same initiator id will fail.\n\nThe same test cases is covered for volume snapshot attachments.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagattr.tsc",
    "setup": "tsaginit",
    "flags": {
      "differentszfg": "1"
    },
    "description": "tsagattr.tsc - create smart_scan_capable and non smart_scan_capable diskgroup\n\nTest creation of smart_scan_capable and non smart_scan_capable diskgroup",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagaucached.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sage_mirror_mode": "normal",
      "errbasename": "^tst_tscname^tmp.log"
    },
    "description": "tsagaucached.tsc - check that primary extent is cached after relo\n\nCreate a table which takes up less than 1 AU.\n     Relocate the primary and 2ndary extent to different failgroups.\n     Check that old primary AU is not cached but new one is.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagauto_iop1.tsc",
    "setup": null,
    "flags": {
      "asmdisks_created": "2"
    },
    "description": "tsagauto_iop1.tsc - auto online test for compatibility testing",
    "platform": null
  },
  {
    "test_name": "tsagautofileencrypt.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "egs_deplmode": "onPrem"
    },
    "description": "tsagautofileencrypt.tsc\n\nAllow autoFileEncryption to be enabled or disabled under onPrem deployment mode.\n\nRuns in lrgsaexacldescli10",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagautologout.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagautologout.tsc - ExaCLI test for auto logout\n\nExaCLI test for auto logout",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagauton.tsc",
    "setup": null,
    "flags": {
      "cell": "^numoss^",
      "downtime": "10",
      "test_dg": "datafile",
      "autontst": "1",
      "msg": "'deactivate 2 disks, pull out and put back 1 disk, replace the other one'",
      "tsagaurm": "tsagaurm2.sh",
      "tsagaurp": "tsagaurp2b.sh",
      "tmpalert": "asmalert^i^",
      "sysdba": "'sys/knl_test7 as sysdba'",
      "file_dest": "'+'^test_dg^"
    },
    "description": "tsagauton.tsc - temporarily remove a disk during file create\n\nWhile writing to ASM disks, pull out disks and check that they are\n     automatically altered online after they are put back or replaced.\n\n     oratst tsagauton [cell=n] [autontst=m] [tsagautontst=subtest] [downtime=s]\n                      [asm_disk_repair_time=x]\n     n - cell number: 2 or 3\n     m - test suite #:\n       1 ,2, or 3 (just rm, and put back, as run by lrgsauton)\n       11, 12, 13, 14, or 15 (plus cell shutdown, restart, as run by lrgsautoc)\n       21, 22, or 23 (plus deactivate, activate, as run by lrgsautod)\n     subtest - single subtest: tsagauton[a-h]\n     s - number of seconds to wait before putting disk back",
    "platform": null
  },
  {
    "test_name": "tsagauton2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagauton2.tsc - remove disks from 2 cells and put them back\n\nRemove disks from 2 cells and put them back at the same time\n     This is test for bug 12433293.",
    "platform": null
  },
  {
    "test_name": "tsagauton_iop1.tsc",
    "setup": null,
    "flags": {
      "ossinst": "^cell^",
      "downtime": "10",
      "msg": "'rm a disk, shutdown cell, put disk back, and restart cell'",
      "tsagaurm": "tsagaudn2.sh",
      "tsagaurp": "tsagauup2.sh",
      "tmpalert": "asmalert^i^",
      "tmp_nowarn": "^tst_nowarn^"
    },
    "description": "tsagauton_iop1.tsc - auto online test for compatibility test",
    "platform": null
  },
  {
    "test_name": "tsagautona.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagautona.tsc - test auto online disk\n\nPull a disk out then put it back.\n     Disk should be automatically online.",
    "platform": null
  },
  {
    "test_name": "tsagautonb.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagautonb.tsc - test auto online disk\n\nOffline disk, pull it out then put it back.\n     Disk should be automatically online.",
    "platform": null
  },
  {
    "test_name": "tsagautonc.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagautonc.tsc - test auto online disk\n\nDrop a disk from diskgroup, pull disk out then put it back.\n     Disk should not be automatically added back.",
    "platform": null
  },
  {
    "test_name": "tsagautond.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagautond.tsc - test auto online disk\n\nPull a disk out, drop it from diskgroup with force, then put it back.\n     Disk should be automatically online and added back.",
    "platform": null
  },
  {
    "test_name": "tsagautone.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagautone.tsc - test auto online disk\n\nDismount diskgroup, pull a disk out then put it back.\n     Should be able to mount diskgroup like normal afterwards.",
    "platform": null
  },
  {
    "test_name": "tsagautonf.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagautonf.tsc - test auto online disk\n\nDismount diskgroup, pull a disk out then mount diskgroup force.\n     Then put disk back.\n     Disk should be automatically online.",
    "platform": null
  },
  {
    "test_name": "tsagautong.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagautong.tsc - test auto online disk\n\nPull a disk out, dismount diskgroup force then put disk back.\n     Should be able to mount diskgroup like normal afterwards.",
    "platform": null
  },
  {
    "test_name": "tsagautonh.tsc",
    "setup": null,
    "flags": null,
    "description": "Pull a disk out, dismount diskgroup and remount force,\n     then put disk back.\n     Disk should be automatically online afterwards.",
    "platform": null
  },
  {
    "test_name": "tsagautoni.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagautoni.tsc - test for bug 8686542\n\nCheck that after cell is not available, discovery code does not\n     try to access it.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagautonini.tsc",
    "setup": "tsagnini",
    "flags": {
      "oss_testing": "2",
      "tmp_nowarn": "^tst_nowarn^",
      "asm_ifile": "initrepair.ora",
      "tmpsid": "_asmsid^i^"
    },
    "description": "tsagautonini.tsc - initializations for tsagauton.tsc",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagautonini_iop1.tsc",
    "setup": null,
    "flags": {
      "tmp_nowarn": "^tst_nowarn^",
      "asm_ifile": "initrepair.ora",
      "num_failgroup": "2",
      "tmpsid": "_asmsid^i^"
    },
    "description": "tsagautonini_iop1.tsc - tsagautonini for compatibility testsing\n\nInitializes auto online tests for compatibility testing",
    "platform": null
  },
  {
    "test_name": "tsagautonj.tsc",
    "setup": null,
    "flags": {
      "test_dg": "datafile"
    },
    "description": "tsagautonj.tsc - dismount DG in both ASM instances after pulling disk\n\nSimilar to tsagautong.tsc with RAC except DG is dismounted in both\n     ASM instances.\n     - have 2 ASM instances\n     - have DG mounted in both ASM instances\n     - drop datafile1~4 from diskgroup.\n     - pull a disk and verify that the disk goes offline in ASM\n     - dismount DG in both ASM instances\n     - push the disk back\n     - verify that the disk does not get onlined\n     - mount DG in one ASM instance\n     - verify that the disk now gets onlined\n     - add disks back in diskgroup",
    "platform": null
  },
  {
    "test_name": "tsagautonl.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagautonl.tsc - auto-online test with double jeopardy\n\nBounce cell. While disks are being brought online,\n     kill background process XDMG/XDWK, remove 2 disks, put one back\n     as is and replace the other one.",
    "platform": null
  },
  {
    "test_name": "tsagautonm.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagrddef",
      "sysdba": "'sys/knl_test7 as sysdba'",
      "tsagsysstr": "'sys/knl_test7 as sysasm'"
    },
    "description": "tsagautonm.tsc - Test for bug 25925076\n\n- simulate predicative failure on one disk, it should be dropped on both cell and asm side\n     - replace a disk, and it should be added back automatically on asm side.\n     Running above steps in a loop for 20 times",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagautoprovresources.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true",
      "diskSizeinMB": "500000"
    },
    "description": "tsagautoprovresources.tsc - Test case for auto provisioning of resources\n\nThe end user provides a size for the resource,\n     and the service computes iops automatically.\n     Flash and xrmem cache are also allocated automatically based on capacity.\n\n     That is, if autoProvisionResources is on\n     The iops,flash and xrmem are calculated automatically according to the\n     size of vault that is supplied by end user. Similar is the case for volumes.\n\n     The test enables autoProvisionResources and checks\n     creation of vaults and volume.\n     Tries to alter the iops/flash/xrmem",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagautovault.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "cdb": "true",
      "auto_local_undo": "true",
      "skip_set_file_dest": "true"
    },
    "description": "tsagautovault.tsc - AUto Generate Vault Names\n\nStarts EGS in auto generate mode",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagawrdbtimezone.tsc",
    "setup": null,
    "flags": {
      "nflint": "0"
    },
    "description": "tsagawrdbtimezone.tsc - test case for timestamps to use the DB timezone.\n\nExadata fixed tables will use macros that translate timzeone to DB timezone.",
    "platform": null
  },
  {
    "test_name": "tsagawrrpt.tsc",
    "setup": null,
    "flags": {
      "report_name": "'tsagrpt_'^awrsuffix^",
      "awr_constr": "'connect sys/knl_test7 as sysdba'"
    },
    "description": "tsagawrrpt.tsc - Generate ADDM, AWR, ASH report\n\n* snap_interval - setting affects how often(mins) snapshots are automatically captured.\n   * snap_topnsql  - The number of SQL captured for each Top criteria.\n\n   Usage: runtest tsagawrrpt  mode=<BEGIN|END|> \\\n                           [awr_beginsnap=<awr_beginsnap>] \\\n                           [awr_endsnap=<awr_endsnap>] \\\n                           [report_name=<report_name>]\n\n      1. If you know the snapshot ids and want to genereate reports between begin snapshot\n          and end snapshot, then you can do:\n          runtest tsagawrrpt awr_beginsnap=bein_snap awr_endsnap=end_sanp\n      Note: a) Ensure bein_snap is less than end_sanp\n            b) There isn't db shutdown event between bein_snap and end_sanp\n\n      2. If you don't know snapshot ids but want to monitor oratst blocks, please follow\n           the following example:\n\n              set snap_interval 10 - The default value is changed to 30 mins\n              set snap_topnsql 1000 - The default value is 100\n              runtest tsagawrrpt mode=BEGIN\n              {runutl tkrmopti -con ^s1^ -DropTables -pctlimit 90 >>> ^ofile^^r^.lst\n               ....}\n              runtest tsagawrrpt mode=END",
    "platform": null
  },
  {
    "test_name": "tsagawsclean.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagawsclean.tsc - Cleanup the env. of Exadata on AWS\n\nCleanup the env. of Exadata on AWS\n\nCleanup the env. of Exadata on AWS",
    "platform": null
  },
  {
    "test_name": "tsagawsinit.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagawsinit.tsc - Init Exadata on AWS\n\nInit Exadata on AWS\n\nInit Exadata on AWS",
    "platform": null
  },
  {
    "test_name": "tsagawsrunscript.tsc",
    "setup": null,
    "flags": {
      "callername": "^tst_current_name^",
      "tscname": "u^callername^_^rndm^^tsagawsrunscriptsuffix^",
      "append_prefix": "'>'",
      "output_list": "'> '^log^",
      "ALL_ARG_LIST": "^ALL_ARG_LIST^' '^ARGVALUE^'='^^ARGVALUE^",
      "NXTARG": "0",
      "ARGNAME": "ARG^NXTARG^",
      "ARGVALUE": "^^ARGNAME^",
      "value": "^^ARGVALUE^",
      "aws_cell": "^value^",
      "aws_compute": "^value^",
      "aws_compute_asm": "true",
      "cellcli_prefix": "'@'",
      "aws_node": "^cur_compute^",
      "aws_node_twork": "^aws_compute_twork^",
      "fileprefix": "'.sql'",
      "script_file": "^cmdfile^",
      "outfilenm": "^tscname^.log",
      "macro_debug_file": "^tscname^_debug.log"
    },
    "description": "tsagawsrunscript.tsc - The wrapper of invoking sqlplus/perl/sh/cellcli\n         to run on AWS cell and compute nodes\n\nThe wrapper of invoking sqlplus/perl/sh/cellcli to run on AWS cell and compute nodes\n   it requires cell or compute parameter\n     cell=[1-3], mean run on cell[1-3] node\n     compute=[1-3], mean run on compute[1-3] node\n\n   usage: how to run sql/perl/shell/cellcli\n\n     echo > cell.dat\n          > set echo on\n          > list cell detail\n     endecho\n     tcellcli cell.dat cell=1 > ^outbasename^\n     log append\n\n     echo > checkimage.sh\n          > imageinfo\n          > echo \"test value $1\"\n     endecho\n     tsh checkimage.sh test1 cell=2 > ^outbasename^\n     log append\n\n     echo > checkimage.pl\n          > system(\"imageinfo\");\n     endecho\n     tperl checkimage.pl cell=2 > ^outbasename^\n     log append\n\n     echo > tsagcrttb.sql\n          > set verify off\n          > connect sys/knl_test7 as sysdba\n          > set echo on\n          > set trimspool on\n          > set linesize 160\n          > drop table t1;\n          > 'create table &1 (c0 number, c1 date, c2 varchar2(2000), c3 nvarchar2(2000), c4 timestamp,'\n          >       c5 timestamp with time zone, c6 timestamp with local time zone, c7 float(20), c8 number,  c9 number);\n          > declare\n          >  i     NUMBER := 0;\n          >  k     timestamp:= systimestamp;\n          >  l     nvarchar2(30):= ''stage22299'';\n          > begin\n          >  for i in 1..80000 loop\n          > '  insert into &1 values (i,sysdate,''storage'',l,k+i,k+i,systimestamp,9999.99999,2,i); '\n          >  end loop;\n          >  commit;\n          > end ;\n          > /\n          > 'select count(*) from &1 ;'\n     endecho\n     sesql tsagcrttb t1 compute=1 > ^outbasename^\n     log append\n\nThe wrapper of invoking sqlplus/perl/sh/cellcli",
    "platform": null
  },
  {
    "test_name": "tsagawssesqlfp.tsc",
    "setup": null,
    "flags": {
      "sesqlf_name": "^arg1^",
      "log": "^sesqlf_basename^",
      "target_db": "'compute='^compute^",
      "NXTARG": "1",
      "ARGNAME": "ARG^NXTARG^",
      "ARGVALUE": "^^ARGNAME^",
      "sql_args": "^sql_args^' '^ARGVALUE^",
      "reflog": "^compare^",
      "maskf": "mask"
    },
    "description": "tsagawssesqlfp.tsc - SESQLF with Parameter passing on AWS and real hw lrgs\n\nUse sqlplus to execute a sql script and optionally compare results.\n     See sesqlf for a general description.  This file is the same as\n     sesqlf except that it adds parameter parsing similar to sesql, so that\n     parameters can be passed to the sql script being executed.\n\nUsage:\n       fork sesqlf script [compute=1-x] [compare[=refname]] [mask] [log=logname.log] \\\n                   [sqlargs...]\n\n     Example:\n       fork sesqlf tkfoo 3.14 2.718 compute=1 compare log=tkfoo01.log\n                         ^    ^\n                         \\----|--- Arguments for tkfoo.sql",
    "platform": null
  },
  {
    "test_name": "tsagawssetup.tsc",
    "setup": null,
    "flags": {
      "deploy_oeda": "1",
      "aws_terminate_instance": "y",
      "aws_exadata_group": "FUN"
    },
    "description": "tsagawssetup.tsc - Setup Exadata on AWS\n\nSetup Exadata on AWS\n\n     You can just run tsagawssetup.tsc to setup Exadata on AWS, below is the parameters\n     that you can set with your speical setting:\n\n       1) deploy_oeda : true|false, run oeda to deploy GI and DB, default is false\n       2) aws_account_id : your aws account id, default is 548323806279\n       3) aws_access_key_id: your aws access key id\n       4) aws_secret_access_key: your aws secret access key\n       5) aws_region_name: the region of your aws EC2 instances, default is us-west-2\n       6) aws_exadata_group: Exadata group, default is FUN\n             DEV - Development\n             FUN - Functional Testing\n             MAA - MAA/XTeam\n             PRF - Performance Testing\n             SUS - Sustaining Eng Team\n             STR - Stress Testing\n       7) cluster_id: cluster id, should be 1-47, default is 47\n       8) aws_terminate_instance: y|n whether terminate instances at the end of test\n                                  default is y\n\n     For local run, you can just run:\n       oratst tsagawssetup.tsc key=value\n\nSetup Exadata on AWS",
    "platform": null
  },
  {
    "test_name": "tsagbadfdata.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "normal",
      "file_dest": "'+DATAFILE'",
      "creatdev_file": "tsagrddef",
      "nflint": "1",
      "asm_ausize": "4194304"
    },
    "description": "tsagbadfdata.tsc - Test for bug 9800393",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbasedb_ehcc.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbasedb_ehcc.tsc\n\nThis test verifies EHCC table creation on BaseDB on ASM on EDV.\n\n4 types of table EHCC creation are verified in this test :\n\n     1. compress for archive high\n     2. compress for archive low\n     3. compress for query high\n     4. compress for query low",
    "platform": null
  },
  {
    "test_name": "tsagbatdb.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagbattst.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbattst.tsc - Runs one of the battery tests.\n\nBattery tests comprises of following\n\n o LUN Test - Analyses the effect of the availability of the physical disks on Writeback and Writethrough cache mode of the LUNs.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbattst_fakehw.tsc",
    "setup": "tsagnini",
    "flags": {
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagbattst_fakehw.tsc\n\nTo resolve intermittent diffs occuring in lrgrh2sabat1 moving Test2 from\n     tsagbattst.tsc to run on fake hardware.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbdqmcli.tsc",
    "setup": null,
    "flags": {
      "qplan1": "SYSTEM"
    },
    "description": "tsagbdqmcli.tsc - Run Basic bdscli quarantine command test cases\n\nFollowing command not support yet:\n       1) alter bdsql restart services all ignore redundancy\n 2) alter bdsql events = \"immediate bdsqlsrv.bdsqlsrv_testaction(test_timeout, 30, 0, 0)\"\n 3) create quarantinePlan getnameplan comment=\"This is a test\"\n 4) help alter quarantine\n\n    Known Bug: #19488870 DBUNIQUENAME NOT RECOGNIZED BY BDSQL WHEN CREATE QUARANTINE\n so cannot create quarantine with specific dbUniqueName\n\nquarantineType,dbUniqueName,planLineID,fineGrainControl,fineGrainValue are not modifiable using 'alter quarantine'",
    "platform": null
  },
  {
    "test_name": "tsagbdsqlini.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbdsqlini.tsc - Init some related scripts for bdsql commands\n\n< DESCRIPTION >",
    "platform": null
  },
  {
    "test_name": "tsagbdsqlsi.tsc",
    "setup": null,
    "flags": {
      "log_file": "tsagbdsqlsi.log"
    },
    "description": "tsagbdsqlsi.tsc - Basic function tests on Storage Index for bdsqlsrv",
    "platform": null
  },
  {
    "test_name": "tsagbdsqlsiqm.tsc",
    "setup": null,
    "flags": {
      "log_file": "tsagbdssiqm.log"
    },
    "description": "tsagbdsqlsiqm.tsc - Storage Index + Quarantine(sqlplan+database) test",
    "platform": null
  },
  {
    "test_name": "tsagbigdatacell.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbigdatacell.tsc - Starts exadoop cells, clusterware, rdbmsini without ASM\n\nStarts clusterware (diskmon), rdbms and exadoopcells\n\nDefault is 2 cells",
    "platform": null
  },
  {
    "test_name": "tsagbkpconcurrency.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbkpconcurrency.tsc - This test performs conflicting operations and\n     observes if proper return codes are observed.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbkpobjabsent.tsc",
    "setup": "xblockini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagbkpobjabsent.tsc - Test backup deletion and restore operations when object is absent.\n\nTest for kukuchen_allow.delete.bkp.if.bkp.obj.absent .\n     Now the code allows user to delete backup or complete restore with backup in the situation that the object is absent.\n     This is because we have a routine mechanism to delete bucket every 7 days. When the bucket is gone, backup object will\n     be gone as well. In this case, instead of letting the server down or making the restore operation hang, will allow the\n     operation to go ahead and complete the job.\n\nTests:\n\n        1.  Delete the bucket and try deleting all backups starting from some intermediate backup.\n            (We will hit soft ORA such as \"absentBkpObj\" but it shouldn't block the operation)\n        2.  Delete the bucket. Create a new backup (this will likely recreate the bucket with same name,\n            but old objects would be gone). Then try and restore from some older backup whose objects are all gone.\n            Then delete all the backups starting from some intermediate one first.\n            (Delete might hit the soft ORA. Restore should complete without hang, might also hit the soft ORA.)\n        3.  Delete the bucket while restore, backup is going on.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbkpopabort1.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "full_run": "true",
      "setup_blockstore": "true"
    },
    "description": "tsagbkpopabort1.tsc - Backup abort test\n\nThe test creates root, intermediate and leaf backups\n     by simulating aborts at all the job states of\n     following job class:\n     1.  BswCreateBkpJob : states 5 - max\n     2.  BswBkpOCIObjUploadJob : states 0 - max\n     3.  BswBkpReadFromCellJob : states 0 - max",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbkpopabort2.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagbkpopabort2.tsc - Test backup aborts during volume backup restore\n\nThe test restores root, intermediate and leaf backups\n     by simulating aborts at the job states of\n     following code files:\n      1. BswRestoreBkpJob.h: states 3 - max\n2. BswRestoreIncBkpJob.h: states 0 - max",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbkpopabort3.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagbkpopabort3.tsc - Test backup aborts during volume backup deletion.\n\nThe test deletes root, intermediate and leaf backups\n     by simulating aborts at the job states of\n     following job classes:\n1.\tBswDeleteBkpJob : states 5 - 28\n2.\tBswBkpReadFromOCIJob : states 0 - max\n3.\tBswBkpOCIObjUploadJob : states 0 - 26",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbkpopsuspend1.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "full_run": "true",
      "setup_blockstore": "true"
    },
    "description": "tsagbkpopsuspend1.tsc - suspending the create bkp operations when we are unable to\n                             connect to object store, and then resuming them later.\n\nThe test creates root, intermediate and leaf backups\n     by simulating suspends at all the job states of\n     following job classes:\n     1.  BswCreateBkpJob.h: states 17 - 35\n     2.  BswBkpOCIObjUpload\n     3.  BswBkpReadFromCell\n     4.  BswIncBkpUploadDiffJob",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbkpopsuspend2.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "full_run": "true",
      "setup_blockstore": "true"
    },
    "description": "tsagbkpopsuspend2.tsc - Test backup suspends during volume backup restore\n\nThe test restores root, intermediate and leaf backups\n     by simulating suspends at the job states of\n     following code files:\n      1. BswRestoreBkpJob.h\n      2. BswRestoreIncBkpJob.h",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbkpopsuspend3.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "backoff_inc_vol_bkp": "true",
      "full_run": "true",
      "setup_blockstore": "true"
    },
    "description": "tsagbkpopsuspend3.tsc - Test backup suspends during volume backup deletion.\n\nThe test deletes root, intermediate and leaf backups\n     by simulating suspends at the job states of\n     following code files:\n      1.      BswDeleteBkpJob.h: states 11 - max\n      2.      BswReadIncBkpPieceJob.h: states 0 - max\n      3.      ObsDownloadObjectJob.h: states 3 - max",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbldoneoff.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbldoneoff.tsc - test Makefile target 'oneoff'\n\nThis testcase is to test Makefile target 'oneoff' which\n     builds and packages oneoff deliverable.",
    "platform": null
  },
  {
    "test_name": "tsagblkcache.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagcc2def",
      "log_file": "^tst_tscname^",
      "tst_tbl": "temp"
    },
    "description": "tsagblkcache.tsc -  this is for block cache population test\n\nTest for Block Cache Population for non-EHCC tables in the foreground\n      a. Created an uncompressed tabled\n      b. manually delayed background population by 60 sec\n      c. scanned the table and checked the data on the flash cache\n      (list flashcachecontent in cellcli)\n      d. query table again, also saw the total_smart_scan_ios_completed_from_flash to be non zero",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagblkrd.tsc",
    "setup": "tsaginit",
    "flags": {
      "shared_pool_size": "120M",
      "auto_undo_management": "true",
      "_rdbms_internal_fplib_enabled": "false",
      "compatible": "^def_compatibility^"
    },
    "description": "tsagblkrd.tsc - Test for Bug 7324432\n\nCreate a table on a SAGE diskgroup and one on a non-SAGE diskgroup\n     and check for \"Cell single block\" and \"multi block read\"\n     and \"db file sequential read\" and \"scattered read\" wait events.\n     This test was split out from tsagattr.tsc.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagblkrprsim.tsc",
    "setup": "tsagnini",
    "flags": {
      "skip_chkms": "1",
      "creatdev_file": "tkfgrddef",
      "compatible": "11.2.0.0.0"
    },
    "description": "tsagblkrpr.tsc - SAGE Fault Simulation for Block Repair\n\nSimulates fualt for PRED_READ and PRED_BKF to test block repair initiated during\n     Smart I/O\n\nFind the description of testcases (1 and 2) below.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagblockstore.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagblockstore.tsc - Blockstore test cases\n\nAdditional: Test case to  catch any data corruption in the block device. And also ACL restriction - user1 and user2 are not able to access each other's volumes.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagblockstorevip.tsc",
    "setup": "xblockini",
    "flags": {
      "bug31836685": "^TST_RETURN^"
    },
    "description": "tsagblockstorevip.tsc - Test blockstore Vips\n\nAdditional: test for blockstore VIPs",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbloom.tsc",
    "setup": "tsaginit",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbloom.tsc - Exadata Bloom Filter misc tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbrnout.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbrnout.tsc - brownout cell tests",
    "platform": null
  },
  {
    "test_name": "tsagbrnout1.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbrnout1.tsc - Test for verifying reduced brownout time on cellsrv restart\n\nTest 1 ## Running a workload on a table created on a normal redundancy\n     ## diskgroup, and cellsrv is crashed. Verify the cell brownout time\n    Test 2 ## Running a workload on a table created on a normal redundancy\n     ## diskgroup and cellsrv is restarted.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbrnout2.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbrnout2.tsc - Cell brownout reduction Test\n\nThis script tests the cell brownout reduction in case of cell failure\n    Test:1 ## Run a workload on a table created on a normal redundancy\n           ## diskgroup, and reboot one of the cell.\n    Test:2 ## Run a workload on a table created on a normal redundancy\n           ## diskgroup, and reset one of the cell.\n \tCheck the cell brownout time in both the cases.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbrnout3.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbrnout3.tsc - Cell brownout time when cell is power reset\n\nTEST:1 ## Run a workload on a table created on a normal redundancy\n     \t      ## diskgroup, and power cycle one of the cell.\n  \t\tCheck the cell brownout time after a cell failure.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbsapisecurity.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbsapisecurity.py - This script does few blockstore api security testing to ensure rest api security is intact.\n\nThis script does few blockstore api security testing to ensure rest api security is intact.\n     Firstly the api is tested and then 4 security test is done on those api. These are:\n       1. Invalid url test by modifying actual url.\n       2. Unauthorized access test by giving wrong username and then giving wrong password.\n       3. Test for unparsed json body\n       4. Test for unexpected data in json body",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbscellfail.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "iscsi": "true",
      "oss_testing": "3",
      "setup_blockstore": "true"
    },
    "description": "tsagbscellfail.tsc - test to try out cell failures during blockstore operations\n\ntest to try out cell failures during blockstore operations\n     1. run blockstore workload in background\n     2. shutdown cellsrv on cell2 and cell3\n     3. startup cellsrv on cell2 and cell3\n     4. check for success of blockstore operations",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbscellfail2.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "oss_testing": "3",
      "setup_blockstore": "true"
    },
    "description": "tsagbscellfail2.tsc - test to try out cell failure during blockstore operations\n\ntest to try out cell failures during blockstore operations\n     1. run blockstore workload in background\n     2. shutdown all services on cell2 and then start them all up.\n     3. shutdown all services on cell3 and then start them all up.\n     4. check for success of blockstore operations",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbscellfail3.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "iscsi": "true",
      "oss_testing": "3",
      "setup_blockstore": "true"
    },
    "description": "tsagbscellfail3.tsc - test to try out disk failure during blockstore operations\n\ntest to try out cell failures during blockstore operations\n     1. run blockstore workload in background\n     2. initiate two disks failure and wait for alerts\n     4. check for success of blockstore operations",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbscellfail4.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "iscsi": "true",
      "oss_testing": "3",
      "setup_blockstore": "true"
    },
    "description": "tsagbscellfail4.tsc - test to try out cellsrv simevent BLOCKIO_ALL_HANG failures during blockstore operations\n\ntest to try out cell failures during blockstore operations\n     1. run blockstore workload in background\n     2. initaite cellsrv simevent BLOCKIO_ALL_HANG\n     3. check for success of blockstore operations\n     4. cancel cellsrv simevent BLOCKIO_ALL_HANG",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbscellfail5.tsc",
    "setup": null,
    "flags": {
      "setup_blockstore": "true",
      "media_type": "all",
      "media_type_vault": "all",
      "sage_mirror_mode": "high",
      "mixed_workload": "true",
      "maxpdb": "2"
    },
    "description": "tsagbscellfail5.tsc - Test to try out cell failures during blockstore operations\n\nTest to try out cell failures during blockstore operations\n     1. Run blockstore workload in background\n     2. Shutdown a service on both cell 2 and cell 3 (can be same or different)\n        The test is added for following services: BSM,BSW,SYSEDS,USREDS,CELLSRV\n     3. Wait for some time and then start them again\n     4. Check for success of blockstore operations\n\nTest plan is added here: https://confluence.oraclecorp.com/confluence/display/EXC/Test+Plan+for+Cell+Failure+in+Blockstore",
    "platform": null
  },
  {
    "test_name": "tsagbscellfail5_2.tsc",
    "setup": null,
    "flags": {
      "setup_blockstore": "true",
      "media_type": "all",
      "media_type_vault": "all",
      "sage_mirror_mode": "high",
      "mixed_workload": "true",
      "maxpdb": "2"
    },
    "description": "tsagbscellfail5_2.tsc - Test to try out cell failures during blockstore operations\n\nTest to try out cell failures during blockstore operations\n     1. Run blockstore workload in background\n     2. Shutdown a service on both cell [1,2,3] and cell [1,2,3] (can be same or different)\n        The test is added for following services: BSM,BSW,SYSEDS,USREDS,CELLSRV\n     3. Wait for some time and then start them again\n     4. Check for success of blockstore operations\n\nTest plan is added here: https://confluence.oraclecorp.com/confluence/display/EXC/Test+Plan+for+Cell+Failure+in+Blockstore",
    "platform": null
  },
  {
    "test_name": "tsagbsconcurrency1.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbsconcurrency1.tsc - Simulate delay in a blockstore operation and run all other blockstore commands in parallel with delayed command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbscsitest.tsc",
    "setup": "tsagnini",
    "flags": {
      "BIGSCN": "1"
    },
    "description": "tsagbscsitest.tsc - SI test with BIG SCN\n\nSI test with BIG SCN",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbsmbswcrash.tsc",
    "setup": null,
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagbsmbswcrash.tsc - Simulates bsm and bsw failure through cellcli events.\n\nIn this test, BSM and BSW are crashed during control operation through\n     cellcli events. The cellcli events are:\n     Event 1 : EBS_IOCTL_ERR_BEFORE_CBK - Service is crashed while control\n               operation is in progress.\n     Event 2 : EBS_SRV_ERR_BEFORE_IOCTL_REPLY - Service is crashed when control\n               operation is finshed but escli has not been informed about it.\n\n     There are a total of 4 scenarios that can be simulated:\n     1. Crashing BSM with Event 1 and BSW with Event 1.\n     2. Crashing BSM with Event 1 and BSW with Event 2.\n     3. Crashing BSM with Event 2 and BSW with Event 1.\n     4. Crashing BSM with Event 2 and BSW with Event 2.",
    "platform": null
  },
  {
    "test_name": "tsagbsmbswfail.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "multiple_bst": "true # Inorder to setup 2 BSM + 3 BSW instances",
      "setup_blockstore": "true"
    },
    "description": "tsagbsmbswfail.tsc - Failure test for bsm+bsw\n\nAdditional: Failure tests for BSM & BSW on a single cell by creating multiple BSM & BSW instances.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbsmcrash.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true",
      "bug32488441": "OPEN",
      "bug32221112": "CLOSED",
      "bug32286012": "CLOSED"
    },
    "description": "tsagbsmcrash.tsc - Simulates bsm failure through cellcli events.\n\nhttps://confluence.oraclecorp.com/confluence/display/EXC/Testing",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbsmediatype.tsc",
    "setup": null,
    "flags": {
      "setup_blockstore": "true",
      "media_type": "all",
      "media_type_vault": "all",
      "sage_mirror_mode": "high",
      "mixed_workload": "true",
      "maxpdb": "2"
    },
    "description": "tsagbsmediatype.tsc - Blockstore operations on volumes of different media types (HC,EF,XT)\n\nAdditional: Blockstore media type tests",
    "platform": null
  },
  {
    "test_name": "tsagbsmfail.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "multiple_bst": "true # Inorder to setup 2 BSM + 3 BSW instances",
      "setup_blockstore": "true"
    },
    "description": "tsagbsmfail.tsc - Failure test case for BSM\n\nSets up environment with 2 BSM  + 3 BSW\n     Create 2 users\n     Each user creates block devics and login to them\n     Kill BSM leader and wait for 2nd bsm to become leader\n     Logout from blockdevices and remove them for both users\n     Each user creates block devices and login to them\n     Kill new BSM leader . Now all bsm got killed\n     All blockstore operations will fail\n     Startup 2 bsm s and one among them will become leader\n     Logout from blockdevices and remove them for both users",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbsstability1.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagbsstability1.tsc - check stability across BSM crashes\n\nSimulates bsm/bsw crash at different job states during\n     the execution of blockstore control operation",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbsstability2.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagbsstability2.tsc - bsm/bsw crash tests for asynchronus operations\n\nThe test simulates bsm ans bsw crashes during asynchronus operations.\n     That is, when chvolume, rmvolume and rmvolumesnapshot operations are ran with\n     async attribute set.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbstestframework.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbstestframework.tsc - A blockstore test framework using REST API.\n\nA python script to write multiple escli test cases for sanity testing,\n     reproducing bug, reusing as a helper function and experimenting with\n     different escli REST API",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbswbkpdiscard.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbswbkpdiscard.tsc - This test script fixes one issue seen in stress test environment\n       where a volume backup restoration is not able to find its backing object in OCI\n       objectstorage\n\nThis test script fixes one issue seen in stress test environment where a volume backup\n     restoration is not able to find its backing object in OCI objectstorage.\n        Here are the test steps:\n          1. create a 100G volume.\n          2. write 10MB to the volume from offset 0.\n          3. Make volume snapshot then backup.\n          4. Restore the volume, before the previous transaction the operation will not be able to complete.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbswcrash.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true",
      "bug32487661": "OPEN"
    },
    "description": "tsagbswcrash.tsc - Simulates BSW failure through cellcli events\n\nThis test checks that blockstore operations can recover from bsw crashes simulated using the following events:\n       Event 1 - EBS_IOCTL_ERR_BEFORE_CBK - BSW is crashed when control operation is in progress\n       Event 2 - EBS_SRV_ERR_BEFORE_IOCTL_REPLY - BSW is crashed when control operation is finished but escli hasn't been informed about it\n\n     For more details: https://confluence.oraclecorp.com/confluence/display/EXC/Testing",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbswfail.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "multiple_bst": "true # Inorder to setup 2 BSM + 3 BSW instances",
      "setup_blockstore": "true"
    },
    "description": "tsagbswfail.tsc - Failure test case for bsw\n\nSets up 2 bsm + 3 bsw env, create vips\n     Create 2 users\n     Each users create block devices and login before each kil\n     kill bsw1 / bsw2 / bsw3 as 3 different cases\n     After each case logout and delete block devices\n     Then test fail back of each bsw as above",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbswnetworkmon.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbswnetworkmon.tsc - Test for adding Network Monitoring functionality to BSW\n\nTest steps:\n       1) create two interfaces bsw1 and bsw2.\n       2) update $OSSCONF/bsw/excloudinit.ora to include these two interfaces (both for client as well as storage)\n       3) create vips and verify that they gets distributed among interfaces\n       4) Fail one interface and ensure that IP address moves to the remaining\n       5) Bring back the failed interface to ensure IPs get distributed nicely again\n       6) Fail both the interfaces and make sure that BSW detects this and goes down.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbufcache.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagcache.tsc - SAGE version of buffer cache short regress\n\nSAGE version of buffer cache short regress\n\nNot coded to run with multiple oss servers",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug1.tsc",
    "setup": "srdbmsini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "creatdev_file": "tsagcaudef",
      "nflint": "0",
      "conn_str": "'sys/knl_test7@inst1 as sysdba'"
    },
    "description": "tsagbug1.tsc - Explain Plan on Temporary Tables\n\nTest Explain Plan behavior with temp tablespaces in Exadata. Bug 7656633",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug10136473.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagccdef",
      "compatible": "11.2.0.0",
      "file_dest": "'+DATAFILE'"
    },
    "description": "tsagbug10136473.tsc - Alter table move partition for EHCC table\n\nAlter table move partition for EHCC table",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug10196117.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagr3def"
    },
    "description": "tsagbug10196117.tsc - tests for _cell_smartio_passthru_enabled\n\ntests for the parameter _cell_smartio_passthru_enabled",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug10374290.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "high",
      "disable_multims": "true",
      "creatdev_file": "tsagccdef",
      "cell_offload_processing": "true",
      "cell_offload_plan_display": "AUTO",
      "file_dest": "'+DATAFILE'",
      "compatible": "^max_compatibility^",
      "loop_string": "'organization hybrid columnar compress for query high tablespace bugtest'"
    },
    "description": "tsagbug10374290.tsc - <Simulate segv event>\n\n<Simulate segv event for compressed and uncompressed tables>",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug10386742.tsc",
    "setup": "tsagnini",
    "flags": {
      "numiter": "15"
    },
    "description": "tsagbug10386742.tsc - memory corruption after deactivate\n\nRun deactivate/activate griddisks in a loop while there is a DB\n     workload running.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug11791183.tsc",
    "setup": "tsaginit",
    "flags": {
      "log_file": "tsagbug11791183.log"
    },
    "description": "tsagbug11791183.tsc - verify the exit code of the cellsrv process\n\nverify the exit code of cellsrv when issue a SEGV signal",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug11791413.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug11791413.tsc - test for bug 11791413\n\nkcfis auto memory management tests",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug11840879.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug11840879.tsc - Test for BUG 11840879\n\nTest for BUG 11840879. Invalid Disk replacement\n\nTest for various entites staus check when physicaldisk is invalid.",
    "platform": null
  },
  {
    "test_name": "tsagbug12343467.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagccdef",
      "file_dest": "'+'datafile"
    },
    "description": "tsagbug12343467.tsc - Bug 12343467: Smart scan corruption handling\n\nTest case for customer bug 11906405.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug12731362.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug12731362.tsc\n\nThis test simulates errors in cellip.ora and there by checks, if\n     it is not crashing ASM instance trying to mount the disks.",
    "platform": null
  },
  {
    "test_name": "tsagbug12885292.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1"
    },
    "description": "tsagbug12885292.tsc - Tests for bug 12885292\n\nTests for Bug 12885292 : ADD A DEBUG EVENT TO SIMULATE CELL-02628",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug12887257.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef",
      "dgattr_forceset": "true",
      "log_file": "tsagbug12887257.log"
    },
    "description": "tsagbug12887257.tsc - Add test case for bug12887257(CELLSRV IS ABSENT)\n\nUNCAUGHT EXCEPTION ON CELLSRV DUE TO NLS GLOBAL CORRUPTION",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug12959688.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "creatdev_file": "my_disks",
      "file_dest": "'+DATAFILE'",
      "nflint": "1",
      "asm_ausize": "4194304"
    },
    "description": "tsagbug12959688.tsc - New Testcases for Cell Server Passthru mode testing\n\nthe test script has new test cases for cell server passthru mode testing",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug12959688_2cell.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "sage_mirror_mode": "high",
      "creatdev_file": "tsagrddef",
      "logfile1": "^tst_tscname^.log"
    },
    "description": "tsagbug12959688_2cell.tsc - Tests for Bug 12959688\n\nTests for Bug 12959688 for cases having 2 cells.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug13018550.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "adminwallet": "^t_work^/esadmin_wallet",
      "creatdev_file": "tsagrddef",
      "logfile1": "^tst_tscname^.log"
    },
    "description": "tsagbug13018550.tsc - new test case added to reproduce Bug 13018550\n\nALTER GRIDDISK ALL INACTIVE GENERATES IO ERRORS IN DB ALERT LOG",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug13114222.tsc",
    "setup": null,
    "flags": {
      "compatible": "11.2.0.0"
    },
    "description": "tsagbug13114222.tsc - Check, if rowsrc and rowsrc SI stats are updated for table scan.\n\nKCFIS layer (smart scan) should consider adding row source statistics such as\n     storage index savings, number of flash cache hits, a bitmap of different type\n     of errors encountered, etc. Such row source statistics will eventually be part of\n     sqlmon and will help in isolating performance regressions by comparing row source\n     statistics between the good case and the bad case.",
    "platform": null
  },
  {
    "test_name": "tsagbug13495012.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug13495012.tsc - Testcase for bug13495012\n\nsnmpSubscriber definition test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug13534084.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagprdef"
    },
    "description": "tsagbug13534084.tsc - Negative test case for bug 13534084, which involves shrinking the griddisk size\n\t             into the data area. The test will be changed once the bug is fixed.\n\nThe test can be outlined as follows :\n  * Bring up DB on two cells\n  * Shrink the griddisks to 1/4 of its original size, without shrinking them at ASM first.\n        * Try restarting ASM and mount all the diskgroup. This operation should fail.\n  * Any DB workload intiated at the DB instance should fail as well, as some of the data may\n    lie outside the valid offset of the griddisk\n  * Now, revert all the changes made to the griddisk, restart ASM and mount all the griddisks. This operation should work.\n  * Resizing the griddisk back to its original size might have corrupted some griddisk. So, restarting DB could lead to\n          ambiguous behavior as its possible that griddisk related to controlfile might be corrupted. Hence, we dont restart DB in this test case.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug13578074.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "Bug#13578074 Network Performance logging change test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug13587920.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug13587920.tsc - test-case for OpenDisk async fencing check\n\nThis test aims to exercise the asynchronous implicit fencing check\n     introduced as the fix for bug 13587920.\n\n     Testcase overview:\n\n      1. Set the lag-simulation event to inject 15 seconds delay the first time\n         the FenceMaster is writing the metadata to the disk.\n      2. Bring the CELLSRV down.\n      3. Remove FenceMaster's on-disk metadata ($OSSCONF/cell_bootstrap.ora).\n      4. Bring the CELLSRV up.\n      5. Do a simple query to ensure the RDBMS had reconnected. Upon re-connect,\n         the CELLSRV will have to update the FenceMaster metadata, which will step\n         on the injected 15 seconds lag, causing the reconnects from other\n         processes to be deferred.\n      6. Validate with the phase-stats that we have indeed covered the async\n         fencing check path (i.e. the OpenDisk's Fenceleader_Wait_to_Process phase\n         has a non-zero 'phase_traverse_count').",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug13624159.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug13624159.tsc - Test for BUG 13624159\n\nThis test file verfies the bug 13624159 fix.\n\nThe test corrupt the metadata of celldisk and then performs various operations on celldisk,griddisk,flashcache,flashlog.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug13807139.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_TESTING": "1",
      "pattern1": "'RS-700 \\[Missing IP netmask in Exadata config file\\]'",
      "pattern2": "'RS-700 \\[Incorrect IP netmask in Exadata config file\\]'",
      "pattern3": "'RS-700 \\[Bad IP found in Exadata config file\\]'"
    },
    "description": "tsagbug13807139.tsc - testcase for cellsrv startup failures with invalid IPs\n\ntestcase for cellsrv startup failures with invalid IPs in cellinit.ora",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug13836473.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug13836473.tsc -  Test for Bug 13836473\n\nTest for Bug 13836473 - MS NEEDS TO MONITOR AND ALERT ON USB HEALTH\n\nFake Hardware test using DEBUGCLI",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug13842658_1.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true",
      "SAGE_MIRROR_MODE": "normal"
    },
    "description": "tsagbug13842658_1.tsc - Test for bug 13842658",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug13842658_2.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true",
      "SAGE_MIRROR_MODE": "high"
    },
    "description": "tsagbug13842658_2.tsc - Test for bug 13842658",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug13967474.tsc",
    "setup": null,
    "flags": {
      "connect_string": "celladmin@^MACH_NAME^",
      "MACH_PASSWD": "welcome"
    },
    "description": "tsagbug13967474.tsc - Test for bug 13967474.\n\nTest to create a package using the adrci ips pack command from celladmin user.",
    "platform": null
  },
  {
    "test_name": "tsagbug14045900.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug14045900.tsc - test case for Bug No 14045900\n\nBug 14045900 - SUMMARY TEXT FOR CALIBRATE COMMAND DOES NOT REPORT ERROR",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14097127.tsc",
    "setup": null,
    "flags": {
      "compatible": "^def_compatibility^",
      "tkdg_compatible": "^def_compatibility^",
      "sga_target": "250m",
      "connect_as_sysdba": "'connect sys/knl_test7 as sysdba'",
      "cdb": "true",
      "outsize": "255",
      "TKDG_SETUP_WALLET": "ON"
    },
    "description": "tsagbug14097127.tsc - Test for Bug 14097127 fix.\n\nThe test use tkdgsums.tsc for ADG setup.\n\nThe test implements various testcases for object storage ( KEEP/DEFAULT) checks. It verify the behaviour on standby db.",
    "platform": null
  },
  {
    "test_name": "tsagbug14142723.tsc",
    "setup": "tsagnini",
    "flags": {
      "oss_auto_manage_disks": "true",
      "creatdev_file": "tsagdebugclidef"
    },
    "description": "tsagbug14142723.tsc - Test cases for Bug No. 14142723\n\nThis script has various test cases for Bug 14142723",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14148652.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug14148652.tsc - Test for BUG 14148652\n\nTest to check BUG 14148652\n\n     The test verifies the particular error message returned by the cellcli\n     when the MS startup fails.\n\n     The test uses the nc(1) (netcat) utility to create a socket on the MS\n     listener's port number, which prevents the MS from successfully coming\n     up. Then we attempt to start the MS and verify the error message.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14154033.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug14154033.tsc - cell_concat_traces.pl test\n\ntest cell_concat_traces.pl for test directory",
    "platform": null
  },
  {
    "test_name": "tsagbug14183159.tsc",
    "setup": "tsaginit",
    "flags": {
      "log_file": "tsagbug14183159.log"
    },
    "description": "tsagbug14183159.tsc - correct mkdir error\n\nBug14183159 test mkdir failure should not cause ORA-600",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14192799.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "creatdev_file": "tsagccdef",
      "nflint": "1",
      "redund": "external",
      "oss_testing": "1"
    },
    "description": "tsagbug14192799.tsc - TESTS FOR BUG 14192799\n\nBUFFER RESERVATION FAILURE ON EXADATA STORAGE SERVER\n\nBUG - 14192799",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14212228.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug14212228.tsc - Test case for Bug 14174174 - PQ SLAVE STUCK IN DIRECT PATH WRITE AFTER OSSNET TIMEOUT\n\na single Exadata cell configuration\n   HOLD I/O on cellsrv to simulate Hung I/Os\n# Fork of a sql which does inserts in a loop and periodically commits.\n# Sleep for 8 mins. (skgxp vrpc timeout of 5mins + delta 3mins)\n Upon control returning from sleep unset the simulation event. This should\n     allow the I/O's to go through.\n   Verify that we see \"Reconnect\" messages in the trace file.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14217377.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_auto_manage_disks": "true",
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug14217377.tsc - Test for Bug 14188828\n\nThis is an additional test for Bug 14188828 - to verify that if drop is\n     in-memory only (ie. disk pull), the resilvering tables of\n     child griddisks are NOT dropped.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14223356.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug14223356.tsc - Test for bug 14223356 fix.\n\nBUG 14223356 - NEW TEST REQUESTS FOR BUG 13971834\n     BUG 13971834 - WRB:FLASH POPULATION DUMPS EVERY 100 SECONDS IF CELLSRV NOT UP",
    "platform": null
  },
  {
    "test_name": "tsagbug14261634.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug14261634.tsc - Test for bug 14261634\n\nTest for bug 14261634 - CELLSRV MUST OVERRIDE USER REQUEST FOR WRITETHROUGH IF THERE IS DIRTY DATA\n     Test for Bug 16064753 - ALTER FLASHCACHE ALL WITH A FLASHCACHE ATTRIBUTE REPORTS SUCCESS\n\nThe test runs on fake hardware",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14263653.tsc",
    "setup": "tsagnini",
    "flags": {
      "auto_undo_management": "true",
      "asm_ausize": "4194304",
      "creatdev_file": "tkfgrddef",
      "redund1": "external",
      "nflint": "1"
    },
    "description": "tsagbug14263653.tsc - Test case for Bug No: 14263653\n\nBUG 14263653 - NEED PERMANENT FIX FOR 14263651",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14284319.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug14284319.tsc - script for BUG 14284319\n\nNEW TEST REQUEST FOR THE CASE OF SWITCHING WRITEBACK TO WRITETHROUGH MODE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14327362.tsc",
    "setup": "tsaginit",
    "flags": {
      "kxdrs_sim_evt": "true",
      "64m_extent": "true",
      "asm_ausize": "1048576",
      "disable_multims": "true",
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug14327362.tsc - Test for bug 14327362\n\nUses simulation event kxdrs_sim with 64MB extents",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14356484.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug14356484.tsc - Test for Bug 14356484\n\nTest for fix for bug - IMPROVE ASMMIRRORSTATUS TO CHECK FOR DEGRADED DISK PARTNERS",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14356484_2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug14356484_2.tsc - Test for Bug 14356484\n\nThis file checks the test case for bug 14356484 with 2 failgroups with 9 disks each.",
    "platform": null
  },
  {
    "test_name": "tsagbug14356702.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug14356702.tsc - run celllist list command and dismount/mount dg in a loop",
    "platform": null
  },
  {
    "test_name": "tsagbug14367949.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug14367949.tsc - BUG 14367949 - NEW REAL HW TEST REQUESTS FOR BUG 14311852\n\nBUG 14311852 - PRINT REASON FOR CELL REBOOT IN ALERT LOG\n\nFinds out the reasons of cell reboot",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14378866.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug14378866.tsc - Test for Bug 14378866\n\nAdds Multiple flash failure test case and verifies that Resilvering request is sent once for a Griddisk.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14464028.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug14464028.tsc\n\nTest for BUG 14464028 - Celldisk Invalid Error When Flashcache Created With Not Present Status CD",
    "platform": null
  },
  {
    "test_name": "tsagbug14484940.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug14484940.tsc - Test for bug 14484940\n\nTest for bug 14484940 - BETTER ERROR REPORTING IS NEEDED WHEN CD CREATION FAILS\n\nThe test corrupt the metadata of celldisk and then performs various operations on celldisk,griddisk,flashcache,flashlog.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14510276.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_no_asmdb": "true"
    },
    "description": "tsagbug14510276.tsc - Test cases for tracking bug - 14510276\n\nThis script has test cases for Bug 14088526 and Bug 14312216",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14555001.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug14555001.tsc - Test for Bug 14555001\n\nTest for Bug 14555001 - Create Flascache All Creates Flashcache of size 128M with not normal celldisks",
    "platform": null
  },
  {
    "test_name": "tsagbug14596891.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug14596891.tsc - Test for Bug 14596891\n\nTest for BUG 14596891 - Alter Flashcache Command Failures with Failed Flash Disks",
    "platform": null
  },
  {
    "test_name": "tsagbug14621505.tsc",
    "setup": "tsagnini",
    "flags": {
      "pdf5": "^T_WORK^/raw/datafile5",
      "pdf4": "^T_WORK^/raw/datafile4",
      "pdf3": "^T_WORK^/raw/datafile3"
    },
    "description": "tsagbug14621505.tsc - bug14621505 auto grid disk creation test\n\nGridDisk category bugs\n     14621505 auto grid disk creation for fragmented grid disks\n     17463667 drop griddisk with prefix option fails with a presently failed physicaldisk\n     20231493 errMediaCount attributes is not defined as number type\n     20852223 GRIDDISK STATUS PROACTIVE FAILURE - SCRUB RESILVERING AFTER A DISK FAILURE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14635090.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug14635090.tsc - Test for BUG 14635090\n\nTest for Bug 14635090 fix. (  CONFINED OFFLINE CELLDISK NOT ADDED AS DEGRADED CELLDISK IN FLASHCACHE CREATION  )",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14637801.tsc",
    "setup": null,
    "flags": {
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagbug14637801.tsc - Test cases for Bug No. 14637801\n\nThis script has various test cases involving swapping of disks and genera\n     ting appropriate alerts when they are inserted in wrong slots.",
    "platform": null
  },
  {
    "test_name": "tsagbug14674875.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_testing": "2",
      "oss_auto_manage_disks": "true",
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug14674875.tsc - BUG 14674875 - ASMDEACTIVATIONOUTCOME RETURNS YES WHEN PARTNER DISK IS IN SYNC STATE\n\nBUG 14674875 - ASMDEACTIVATIONOUTCOME RETURNS YES WHEN PARTNER DISK IS IN SYNC STATE\n\nBUG 14674875 - ASMDEACTIVATIONOUTCOME RETURNS YES WHEN PARTNER DISK IS IN SYNC STATE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14768528.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug14768528.tsc - test cases for Bug No. 14768528\n\nBug 14768528 - CELL GUIDANCE FOR IOPS, MBPS AND I/O LOAD",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14769368.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug14769368.tsc - allow \"\" for cell attributes to define default\n\nsetting 0 length string allows recovring default value.\n     CELLCLI> alter cell traceLevel=\"\"\n     This sets default traceLevel even any other value was defined.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14803349_1.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagbug14803349_1.tsc - Test case for bug 14803349\n\nThe test can be outlined as follows :\n * Bring up DB with normal redundancy\n * Stop caching unmirrored disks in cell 1\n * Shutdown cell services on cell 2 and  set events to run confinement tests on c9flash0,\n   this should flush data on this disk.\n * Set events to pause the flush trigger in the previous step.\n * The disk c9flash0 shouldnt go in confined offline state, as there is some data still to be flushed and\n   its partner disk is still unavailable.\n * Now, bring up cell services on cell 2. The disk c9flash0 should now go in confinedOffLine state.\n       * Sleep for some time and make sure that the MS onlines the disk c9flash0 after running the confinement tests.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14803349_2.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagbug14803349_2.tsc - Test case for bug 14803349\n\nThe test can be outlined as follows :\n       * Bring up DB with normal redundancy\n       * Stop caching unmirrored disks in cell 1\n       * Set events to trigger confinement tests on c9flash0, this should trigger flushing of data on this disk.\n       * After the disk goes to confinedOffline state, sleep for some time to let MS run confinement tests on the disk.\n   The disk is brought online after the tests are done runing on it.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14804030_1.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug14804030_1.tsc - MS logging and tracing too much causing file rollover",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14822881.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug14822881.tsc - Test for MS looping thread issue\n\nThis is a test for MS looping thread issue specified in Bug # 14822881",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug14849049.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug14849049.tsc - Test for Bug 14849049.\n\nTest for Bug 14849049 - CELL UNDERSCORE PARAMETERS TO REMOVED ONCE THEY ARE NO LONGER NEEDED\n\nThe test outline:\n      Adds underscore cell parameter before upgrade.\n      Verify that the cell underscore params should be removed after upgrade.",
    "platform": null
  },
  {
    "test_name": "tsagbug15852451.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug15852451.tsc - Test for Bug 15852451\n\nPerformance Test for Harddisk Population",
    "platform": null
  },
  {
    "test_name": "tsagbug15869228.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_testing": "1"
    },
    "description": "tsagbug15869228.tsc - Tests for Bug 15869228\n\nTests for Bug 15869228",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug15882436.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug15882436.tsc - smart scan test\n\nEnable smart scan for call->plsql->sql call",
    "platform": null
  },
  {
    "test_name": "tsagbug15963552.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "sage_mirror_mode": "high",
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "1",
      "creatdev_file": "tsagaudef",
      "disk": "gddisk0"
    },
    "description": "tsagbug15963552.tsc - Exadata riddisk Status Test for bug15963552\n\nTo do the followings:\n     1. create a grid disk that is not added to any ASM diskgroup.\n     2. simulate predictive failure on it.\n     3. check alert history to see if the hard disk can be replaced alert appears for this griddisk.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16001442.tsc",
    "setup": "tsagnini",
    "flags": {
      "redund": "external",
      "oss_no_asmdb": "true",
      "num_flash_per_cell": "16"
    },
    "description": "tsagbug16001442.tsc - Test for Bug 16001442\n\nBUG 16001442 - FAILED CELLCLI COMMANDS HAVE ZERO EXIT STATUS",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16024576.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug16024576.tsc - Test for bug 16024576\n\nTest for BUG 16024576 - STATEFUL ALERT MISSING A PARAMETER",
    "platform": null
  },
  {
    "test_name": "tsagbug16074182.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line"
    },
    "description": "tsagbug16074182.tsc - Testcase for cellcli command 'list database'\n\nCreates 2 databases, and runs the cellcli command to list them.",
    "platform": null
  },
  {
    "test_name": "tsagbug16092831.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagaudef",
      "file_dest": "'+DATAFILE'"
    },
    "description": "tsagbug16092831.tsc - tests for bug 16092831\n\ntests for cellsrv crash during smart scan when a query scans data from\n      multiple tablespaces using different blocksizes.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16213900.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug16213900.tsc - Test for BUG 16213900\n\nTest for BUG 16213900 - LIST LUN LUN_NAME COMMAND FAILS WHEN CELLSRV IS STOPPED\n\nTest runs on fake hardware",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16232745.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug16232745.tsc - Test for Bug 16232745\n\nTest for Bug 16232745 - NEW TEST REQUESTS FOR IBPORTS POPULATION ON MS",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16278024.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagbug16278024.tsc - Test for bug 16278024, which validates whether force drop of WBFC celldisk and griddisk checks\n\t\t     for ASM redundancy.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16288997.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": null,
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16298128.tsc",
    "setup": "srdbmsini",
    "flags": {
      "asm_ausize": "4194304"
    },
    "description": "tsagbug16298128.tsc - test cases for Bug No. 16298128\n\nBug No. 16298128 - CELLSRV NEEDS TO RE-INITIATE THE FLUSHING ONCE\n                      UNDERNEATH DATA DISK RECREATED.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16406334.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug16406334.tsc - BUG16406334- CREATE CELL FAILS WITH MSG \"AN ERROR WAS ENCOUNTERED WHILE POPULATING THE DISK\"",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16413066.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug16413066.tsc - bug16413066 test\n\nbug16413066,bug16065180 and 16973508 test\n\n     BUG16413066 - LIST CELL DOES NOT DISPLAY ALL ATTRIBUTES ON RE-CREATING CELL\n\n     BUG16065180 - FLASHCACHEMODE SET TO WRITETHROUGH FOR WRB FLASHCACHE\n\n     Bug16973508 - WFC: FLASHCACHE SIZE WAS ROUNDED TO MULTIPLE OF 16MB\n\nversion, kernelVersion attribute is removed on drop cell\n\n     cell version, linux release version should be displayed after recreating cell\n\n     FlashCacheMode should not to be set to default after drop/create cell\n\n     Flashcache creation specified size 500M resulted in 256M.\n     ms log should show actual size",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16417426.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagbug16417426.tsc -  Test for Bug 16417426\n\nThis test case is to test if a pd which has a foreign config is inserted (by replacing some disk which is either failed or normal state), MS should import the lun successfully (which is currently working) and also MS should not try to create a lun on this pd.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16448015.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug16448015.tsc - Tests for CellCLI ORDER BY & LIMIT options\n\nTests for CellCLI order by & limit options",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16478172.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_auto_manage_disks": "true",
      "creatdev_file": "tsagrddef",
      "oss_devdir": "^T_WORK^/raw"
    },
    "description": "tsagbug16478172.tsc - test script for Bug. 16478172\n\nBUG 16478172 - FLASHDISK FAILURE WITH D21Y - FIXED BY REBOOT",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16495149.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug16495149.tsc - add for bug16495149\n\nrun four queries concurrently and check results are correct",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16525645.tsc",
    "setup": "srdbmsini",
    "flags": {
      "dbnodestr": "root@^has_hostname^",
      "cellconnstr2": "root@^CELL2^",
      "MACH_PASSWD": "welcome1"
    },
    "description": "tsagbug16525645.tsc\n    LOT OF LGWR I/O OUTLIERS WHEN BACK UP OR\n    SOME NETWORK INTENSIVE OPERATION RUNS\n\nWhen backup runs it ends up using a significant part of the IB network\n     bandwidth. As a results I/O's belonging to LGWR which are extremely\n     latency sensitive can experience large wait times. This results in OLTP\n     workload experiencing stalls. This scripts will run on real hardware.\n\nWhen backup runs it ends up using a significant part of the IB network\n     bandwidth. As a results I/O's belonging to LGWR which are extremely\n     latency sensitive can experience large wait times. This results in OLTP\n     workload experiencing stalls.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16536827.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_testing": "1"
    },
    "description": "tsagbug16536827.tsc - Test for Bug 16536827\n\nTest for Bug 16536827 : run orion with -is_lgwr option\n      on cell with flashlog.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16536876.tsc",
    "setup": "tsagnini",
    "flags": {
      "log_file": "tsagbug16536876.log"
    },
    "description": "tsagbug16536876.tsc - Test case for Bug16536876\n\nTest Flow:\n     1. have one spare griddisk B not added in any asm diskgroup and one test\n        griddisk  A added in a diskgroup.\n     2. drop asm disk A with force\n     3. add asm disk with same name that A had, but with path of griddisk B\n        to same diskgroup\n     4. drop griddisk A with force option\n     5. ensure that griddisk B does not get dropped by XDWK #",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16576415.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug16576415.tsc - Test for BUG 16576415\n\nTest for BUG 16576415 - SINGLE FW UPDATE FAILS IN MULTIPLE DISK INSERTIONS FAILS CREATE LUN\n\nTest runs on real hardware",
    "platform": null
  },
  {
    "test_name": "tsagbug16576415_1.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug16576415_1.tsc - Test for Bug 16576415\n\nTest for BUG 16576415 - SINGLE FW UPDATE FAILS IN MULTIPLE DISK INSERTIONS FAILS CREATE LUN\n\nTest uses DEBUGCLI on fake hardware.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16591877.tsc",
    "setup": "tsagnini",
    "flags": {
      "dgattr": "'attribute '''compatible.asm'''='''19.0''','''compatible.rdbms'''='''^scompatible.rdbms^''','''cell.smart_scan_capable'''='''true''"
    },
    "description": "tsagbug16591877.tsc - refresh GD asmDiskGroupName attributes\n\nBug16591877 ASMDISKGROUPNAME, ASMDISKNAME IS NOT POPULATED CORRECTLY\n     AFTER ASM DG RECREATION",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16636175.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug16636175.tsc - Test for BUG 16636175\n\nTest for BUG 16636175 - OUTPUT MIXED UP BETWEEN SIMULTANEOUS CELLCLI COMMANDS\n\nThis test generates a script and run simultaneously on a cell.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16660059.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug16660059.tsc - Test case for bug 16660059.\n\nThis test does the following :\n1) Create a tbs of size 10G.\n2) Run a workload, which forks out two processes which does the following:\n          a. Extend the datafile of tbs 1 in increments of 5G\n          b. Create another tablespace of size 10G\n          c. Do some workload on both the tbs to make sure that they are usable.",
    "platform": null
  },
  {
    "test_name": "tsagbug16681259.tsc",
    "setup": "tsagnini",
    "flags": {
      "compatible": "11.2.0.0"
    },
    "description": "tsagbug16681259.tsc - test case for kcfiss unkeep session caching\n\nThis case is from bug16681259:\n     (1) Do create/drop table 10 times;\n     (2) Do a \"desc user_tables\" to kick off some cleanup action;\n     (3) Oradump\n     (4) Check stat in trace file\n\nNone",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16704019.tsc",
    "setup": "tsagnini",
    "flags": {
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagbug16704019.tsc - test case for Bug 16704019\n\nBUG 16704019 - A \"DISK REMOVED\" ALERT IS SENT WHEN A DISK ACTUALLY FAILS",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16746814.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug16746814.tsc - Test for Bug 16746814\n\nTest for Bug 16746814 - MULTIPLE SIGINT/CTRL-C DURING SOSREPORT CAUSES SERVER TO HANG OR CRASH\n\nUse sosreport utility and issue multiple sigint to verify that server should not crash",
    "platform": null
  },
  {
    "test_name": "tsagbug16769943.tsc",
    "setup": "srdbmsini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true   # for XDMG top startup",
      "oss_port2use": "5042"
    },
    "description": "tsagbug16769943.tsc\n\nThis test simulates failure to open first cell in cellip.ora.\n     The diskgroup should mount with the first invalid IP, but\n     second IP good. See details in bug: 16769943.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16778466.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug16778466.tsc - Test for Bug 16778466\n\nTest for Bug 16778466 - FC AND FL NOT CREATED WHEN DATA PARTITIONS ARE DELETED DURING RESCUE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16802118.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug16802118.tsc - Test for BUG 16802118",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16802118_2.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug16802118_2.tsc - Test#2 for BUG 16802118",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16807611.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug16807611.tsc - Test for Bug 16807611\n\nTest for Bug 16807611 - MS FILE DELETION LOGGING AND ROLLING RENAME WRAP ISSUES",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16858835.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug16858835.tsc - Test for BUG 16858835\n\nTest for BUG 16858835 - OUTOFMEMORYERROR RS-7445 [SERV MS NOT RESPONDING]\n\nMemory leak test",
    "platform": null
  },
  {
    "test_name": "tsagbug16864784.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug16864784.tsc - Test for Bug 16864784\n\nTest for Bug 16864784 - TEST NETWORK STATUS",
    "platform": null
  },
  {
    "test_name": "tsagbug16878140.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagdebugclidef"
    },
    "description": "tsagbug16878140.tsc - Test for Bug 16878140\n\nTest for Bug 16878140 - ADD CELLCLI SUPPORT FOR MORE DISK FAILURE SIMULATION TYPES\n\nThe test outline is following:\n      a. Simulate predictive failure on harddisk\n      b. Simulate predisctive failure on flashdisk\n      c. Simulate wtcaching failure on flashdisk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16917915.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "conn1": "'sys/knl_test7 as sysasm'",
      "creatdev_file": "tsagccdef",
      "event": "PRED_DA_NULL_BUF",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback"
    },
    "description": "tsagbug16917915.tsc - Test case for bug 16917915\n\nTests file creation with IOSUBMIT_DEFERRED_NOBUF and PRED_DA_NULL_BUF events set",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16917915_1.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "conn1": "'sys/knl_test7 as sysasm'",
      "creatdev_file": "tsagcaudef",
      "compatible": "11.2.0.0",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback"
    },
    "description": "tsagbug16917915_1.tsc - Test case for bug 16917915\n\nTests file creation with IOSUBMIT_DEFERRED_NOBUF and PRED_DA_NULL_BUF events set",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug16928903.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug16928903.tsc - Test for Bug 16928903 - REAL HARDWARE LRG TESTS NEEDED FOR PRIVATE OFFLOAD PACKAGE INSTALL/UNINSTALL\n\nRuns in lrgrhx5install",
    "platform": null
  },
  {
    "test_name": "tsagbug16935701.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug16935701.tsc - Test for Bug 16935701\n\nTests that datafiles are dropped when snapshot is dropped\n       including datafiles\n\ntest",
    "platform": null
  },
  {
    "test_name": "tsagbug16973227.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true                            # for XDMG to startup"
    },
    "description": "tsagbug16973227.tsc\n\nThis tests the mismatch configuration between cell ip on the\n     compute node and cellinit.ora.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17018717.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug17018717.tsc - BUG 17018717 - Error message for importing importForceRequired cell disk is not clear\n\nIf a user tries to import cell disks with importForceRequired status, the\n     current error message says\n\n     CellCLI> import celldisk all;\n     No cell disks qualified for this import operation\n\n     We should specifically say that there are importForceRequired cell disks but\n     these disks are not imported because they are in import force required\n     instead of import required.\n\n     This test ensures if correct messages are displayed.\n\nSEE DESCRIPTION",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17039439.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug17039439.tsc - add java option on ms startup\n\nThis test do various MS hang simulation\n       Hang simulation by _cellrsms_simulate_hang\n           0 -  9999 Thread hang simulation\n       10001 - 19999 HB check failure simulation\n       20001 - 29999 Port open failure simulation\n       Last digit means the max hang count simulation in hb loop\n\n6 java option can be added for oc4j java option by adding\n     following parameter in cellinit.ora\n       _cellrsms_java_opt\n       _cellrsms_java_opt1,2,3,4,5\n     stdout of ms is looged at oc4j<pid>.trc at the $LOG_HOME",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17222097.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug17222097.tsc - Test for bug 17222097\n\nTest for BUG 17222097 - DROP DISK FOR REPLACEMENT SHOULD OFFLINE THE DISK BEFORE TRYING TO FLUSH\n\nThe test objective is:\n    a) flushing inactive GD works\n    b) flushing flashcache to inactive GD works\n    c) flushing inactive GD but missing flash cache ID is not allowed\n    d) flushing flashcache to inactive but missing flashcache ID is allowed",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17279024.tsc",
    "setup": null,
    "flags": {
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagbug17279024.tsc - Test for Bug 17279024 - MS:MULTIPLE CELLDISKS FOUND ON THE SAME LUN\n\nThe test checks for the hard disk. The flash disk tests will be added later.\n\nThe same test script can be used for testing the flash disk paths once the required setup is ready.",
    "platform": null
  },
  {
    "test_name": "tsagbug17313438.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug17313438.tsc - <test cases to verify bug 17313438 fix>\n\ntest cases to verify bug 17313438 fix\n\ntest cases to verify bug 17313438 fix",
    "platform": null
  },
  {
    "test_name": "tsagbug17418908.tsc",
    "setup": null,
    "flags": {
      "oss_auto_manage_disks": "true",
      "oss_devdir1": "^T_WORK^/raw"
    },
    "description": "tsagbug17418908.tsc - test case for Bug 17418908\n\nBUG 17418908 - NON SYSTEM CELL DISKS SHOULD NOT BE CREATED ON PHYSICAL\n                   DISKS AT SLOT 0 OR 1",
    "platform": null
  },
  {
    "test_name": "tsagbug17432161.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug17432161.tsc - Test cases for Bug No. 17432161\n\nThe script contains test for BUG 17432161 :  LIST METRICCURRENT XML DISPLAY STALLS WHEN NUMBER OF METRICS IS HIGH",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17470654.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug17470654.tsc - Test for Bug 17470654\n\nTest for Bug 17470654 - FLASH DISK POPULATION HIT AN ERROR WHEN ALL FOUR FLASH CARDS ARE GONE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17478228.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug17478228.tsc - Test for Bug 17478228\n\nMS NEEDS A SEPARATE SKGXP RPC TIMEOUT UNDERSCORE PARAMETER",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17479172.tsc",
    "setup": "tsagnini",
    "flags": {
      "log_file": "tsagbug17479172.log"
    },
    "description": "tsagbug17479172.tsc - TEST BUG FOR COMPLETING TEST FOR OFLOADSRV TRACE PURGE FUNCTIONALITY\n\nIt includes below tests:\n          Test#1: Basic check for the purge functionality.\n          Test#2: Remove the purge file if the group got created again\n          Test#3: Check for ADR version number",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17479181.tsc",
    "setup": "srdbmsini",
    "flags": {
      "log_file": "tsagbug17479181.log"
    },
    "description": "tsagbug17479181.tsc - TRACKING BUG FOR CELLSRV SIGSGEGV SIGNAL HANDLING\n\nAdd test to cover bug 17479181 which tests for cellsrv SIGNAL handling:\n\n        1) Kill cellsrv to simulate SIGSEGV and ORA-600 and ORA-7445\n        2) When cellsrv gets the SIGSEGV, each thread of cellsrv should complete a\n           system state dump.\n        3) Check for start and end of state dump msgs in cell alert log and trace files under diag",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17484387.tsc",
    "setup": null,
    "flags": {
      "oss_auto_manage_disks": "true",
      "oss_devdir1": "^T_WORK^/raw"
    },
    "description": "tsagbug17484387.tsc - Test cases for Bug No. 17484387\n\nBUG 17484387 - DBMX28 REPLACED SYSTEM DISK SHOWN AS REJECTED DUE TO WRONG SLOT, LUN NOT CREATED",
    "platform": null
  },
  {
    "test_name": "tsagbug17505310.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug17505310.tsc - Test for bug 17505310\n\nSets the parameter _cell_state_dump_timeout_in_sec in cellinit.ora",
    "platform": null
  },
  {
    "test_name": "tsagbug17538280.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug17538280.tsc - Test for Bug 17538280\n\nTest for Bug 17538280 - OFFLOAD SERVER TRACES DON'T GET RECREATED AFTER REMOVAL",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17540144.tsc",
    "setup": null,
    "flags": {
      "logmsg": "'Create tablespaces, tables and insert data'"
    },
    "description": "tsagbug17540144.tsc - Test Case for Bug No. 17540144\n\nBUG 17540144 - HANDLE EXCEPTION AND QUARANTINE PROCESSING FOR SI MIN/MAX",
    "platform": null
  },
  {
    "test_name": "tsagbug17546682.tsc",
    "setup": "tsagnini",
    "flags": {
      "datafilelist": "^rtest^",
      "tsnum": "0",
      "bugtesti": "bugtest^tsnum^",
      "log_file": "tsagbug17546682.log"
    },
    "description": "tsagbug17546682.tsc - NEW TEST REQUESTS FOR BUG 17513555\n\nAdd a new test case for bug 17513555, after fix of bug 17513555,\n       scrubbing job would not kickoff back to back within 10min margin.\n     Main Steps:\n       1. Setup fake env with ASM instance\n       2. Kickoff scrubbing:\n          cellcli -e alter harddiskscrubstarttime='now'\n       3. Check for alert.log that scrubbing was kicked off:\n           look for messages like:\n           Begin scrubbing CellDisk:c9standby0.\n           Begin scrubbing CellDisk:c9datafile4.\n       4. Wait for 5min, check scrubbing should not be kicked off again:\n           Check alert.log, there should no duplicate \"Begin scrubbing <celldisk>\"\n           message for one celldisk.\n       5. issue \"cellcli -e alter harddiskscrubstarttime='now'\" again, and check\n           alert.log, scrubbing should kick off again.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17547471.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug17547471.tsc - Testcase for bug 17547471\n\nBug 17547471 - X2-8: CELL 1550 CELLSRV NOT RESPONSIVE WHEN CLUSTERWARE SHUTDOWN ABNORMALLY\n\nThis test ensures the subscriber (XDMG process in this case) is tracked aggressively for\n    it's inactivity. Once the threshold of being idle is reached by any subscriber, that will\n    be marked to be fenced automatically. This will avoid hangs on cellsrv when cluster services\n    are brought down abnormally and events published by cellsrv to subscriber(s) are not\n    responded\n\n    Workflow:\n   a) Bring up exadata stack with auto_management of disks enabled\n   b) Kill cssd process\n   c) sleep for 5 seconds\n   d) Publish an event to subscriber (XDMG) - e.g: make a griddisk inactive\n   e) The above event should wait for 120 seconds (threshold is 120sec in LRG and 10 min in prod)\n      and then on no response from XDMG it marks XDMG's RSP as FENCED\n   f) Grep for corresponding fence message in trace file.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17555854.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug17555854.tsc - test case for bug 17555854\n\ntest for SI population serialization problem",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17583993.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_auto_manage_disks": "true",
      "oss_devdir": "^T_WORK^/raw"
    },
    "description": "tsagbug17583993.tsc - Test cases for Bug No. 17583993\n\nBug No. 17583993 - SG_INQ ERROR SHOULD BE TOLERATED MULTIPLE TIMES BEFORE A DISK IS MARKED FAILED",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17599935.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug17599935.tsc - Test for Bug - 17599935\n\nTest for Bug 17599935 - MS VIRTUAL SLOT ASSIGNMENT FALSELY OCCUPIED IF CELLDISK IMPORT FORCE FAILED",
    "platform": null
  },
  {
    "test_name": "tsagbug17614227.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug17614227.tsc -  test for bug  17614227 - XDMG DOESN'T WAIT FOR RESYNC PREVENTING PROMPT RESILVER COMPLETION\n\nenable exadata auto mgmt\n      - fail flash and trigger RT creation\n      - before resilvering is completed, shutdown cellsrv and wait for all disks from that cell to go offline\n      - once all disks are offline, restart cellsrv and wait for all disks to go online\n      - verify that resilver is automatically started after resync is completed for all disks\n      - verify that resilver is completed",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17627583.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug17627583.tsc - Test cover bug#17627583\n\nBug 17627583:Use dcli to disable/enable switches port, will change the ib0/ib1 address\n\nshould run on cell node with active/active InfiniBand network config",
    "platform": null
  },
  {
    "test_name": "tsagbug17627847.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug17627847.tsc - Test for BUG - 17627847\n\nBUG-17627847 OBSOLETE FC ENABLED ON A NEW GD CAUSING ORA-600\n\nTo verify that after flash or hard disk failure, if any griddisk or flashcache\n     or flashlog part is dropped, and then if the disk is reinstated . MS should not\n     automatically reinstate the previously dropped part.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17636467.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug17636467.tsc - Test for Bug 17636467\n\nTest for Bug 17636467 - CLEANUP FAILS WHEN OLD /ROOT/DOSTEP.SH EXISTS\n\nThe test copies dostep.sh from t_data to the cell and then start the patching.\n    The oss/test/tsage/data/dostep.sh is stored from 11.2.2.2.4.2 release.",
    "platform": null
  },
  {
    "test_name": "tsagbug17673325.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug17673325.tsc - _CL_DEBUG test\n\nsubTime metric persistance test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17744898.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_view1": "^user^_^host^_^view2_name^",
      "log_file": "tsagbug17744898.log"
    },
    "description": "tsagbug17744898.tsc - Test for bug 17744898\n\n< DESCRIPTION >",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17757257.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "sgrcel2",
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagbug17757257.tsc - Test for Bug 17757257\n\nCREATE GRIDDISK FROM FLASH FAILS WITH CELL-2845 BUT RESIZE WORKS",
    "platform": null
  },
  {
    "test_name": "tsagbug17792594.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug17792594.tsc - ALERT NOT EMAILED IF INFO ALERT PRECEDES IT\n\nALERT NOT EMAILED IF INFO ALERT PRECEDES IT\n\nALERT NOT EMAILED IF INFO ALERT PRECEDES IT",
    "platform": null
  },
  {
    "test_name": "tsagbug17797504.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug17797504.tsc - Test cover bug#17797504\n\nUse ipconf to change bondib0 address, after ipconfig finished,\n     bondib0 in ifconfig's result and ipaddress1 in list cell details' result should be the same value\n     Restore ipaddress at the end of test\n\nshould run on cell node with bonding InfiniBand network config",
    "platform": null
  },
  {
    "test_name": "tsagbug17876146.tsc",
    "setup": "tsagnini",
    "flags": {
      "diskmon_pipe": "_^host^_dskm_pipe",
      "diskmon_pid": "^proc_pid^",
      "log_file": "tsagbug17876146.log"
    },
    "description": "tsagbug17876146.tsc - TEST CASE REQUIRED FOR BUG-17415315\n\nWe wanted to make sure diskmon process (pertaining to hang diskmon_pid)\n     cleans up by DiskmonAgent with following cases:-\n\n     Case 1: When diskmon hang (kill -STOP <diskmon_pid>) OR take only below case.\n\n     Case 2: When Diskmon hang (kill -STOP <diskmon_pid>) and at the same time if\n     orarootagent restarted due to some reason (kill -9 <diskmonAgent_pid>).\n\n     After sometime (say 90 seconds), we should not see the diskmon_pid running in\n     the machine.\n\n     This test might be verified on the production env since lrg may have multiple\n     diskmons running the same machine. Otherwise, we might need to check the\n     diskmon id properly on the lrg env. i.e., below command should return\n     nothing:-\n\n     (on lrg env)\n     ps -aef | grep $ORACLE_SID | grep -i \"_dskm_pipe\" |grep -i <diskmon_pid>\n\n     where diskmon_pid refers to the hang diskmon.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17898597.tsc",
    "setup": "srdbmsini",
    "flags": {
      "testdesc": "'Kick off workload, set event again and turn it off'"
    },
    "description": "tsagbug17898597.tsc - test for bug 17898597\n\nSet simevent 2 times, each with count=1.\n     Event should be triggered 2nd time even when count is not increased.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17906571.tsc",
    "setup": "tsaginit",
    "flags": {
      "num_flash_per_cell": "16",
      "flash_griddisk_size": "512",
      "flash_size": "1024",
      "SAGE_MIRROR_MODE": "normal",
      "oss_devdir": "^T_WORK^/raw^oss_port^"
    },
    "description": "tsagbug17906571.tsc - Test for BUG 17906571\n\nTest for BUG 17906571 - MULTIPLE FDOM SHOWN POOR PERFORMANCE PEER FAILURE CANNOT BE ENABLED PROPERLY\n\nThe test uses the variable _cell_allow_reenable_predfail in cellinit.ora",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug17907771.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_testing": "2",
      "oss_auto_manage_disks": "true",
      "creatdev_file": "tsagrddef",
      "uniq_dsknames": "all",
      "oss_devdir1": "^T_WORK^/raw^oss_port^",
      "oss_devdir2": "^T_WORK^/raw^oss_port2^",
      "asm_ausize": "4194304"
    },
    "description": "tsagbug17907771.tsc - TC for Bug No. 17907771\n\nBUG NO. 17907771 - WBFC FLUSH ON A PEER-FAILED FLASH FAILED DUE TO INACTIVE DEVICE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18064052.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug18064052.tsc - Test for BUG 18064052\n\nTest for BUG 18064052 - ENABLE CHECKCONFIG FUNCTIONALITY ON DB SERVER",
    "platform": null
  },
  {
    "test_name": "tsagbug18135075.tsc",
    "setup": "srdbmsini",
    "flags": {
      "db_block_size": "8192"
    },
    "description": "tsagbug18135075.tsc - Test case for bug #18135075\n\nA brief description of the test case,\n        *) Setup the database and initiate an RDBMS workload as exadb\n        *) Setup a passwordless ssh to the root of compute node and the cell and copy all the infinicheck utilities\n        *) Initiate an infinicheck workload from the compute node and run it for 30 minutes\n        *) At the end of 30 minutes make sure that there is no kworker hang messages in \"/var/log/messages\"",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18182904.tsc",
    "setup": null,
    "flags": {
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagbug18182904.tsc - Test for 18182904 - MS RESET CONFINMENT WHILE CELLSRV HAD CD IN BAD OFFLINE STATUS\n\nTest for 18182904 - MS RESET CONFINMENT WHILE CELLSRV HAD CD IN BAD OFFLINE STATUS\n\nTest for 18182904 - MS RESET CONFINMENT WHILE CELLSRV HAD CD IN BAD OFFLINE STATUS",
    "platform": null
  },
  {
    "test_name": "tsagbug18334192.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug18334192.tsc - Test for BUG-18334192\n\nBUG-18334192:CELLSRV HIT ORA-700[MERGEGDHASH_1]",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18347818.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug18347818.tsc - Test for Bug 18347818\n\nBug 18347818 - X4-2: ROLLING UPGRADE INACTIVE GRIDDISKS TOO EARLY LEAD TO DISKGROUP DISMOUNT",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18355759.tsc",
    "setup": "tsaginit",
    "flags": {
      "num_flash_per_cell": "16",
      "flash_griddisk_size": "512",
      "flash_size": "1024",
      "SAGE_MIRROR_MODE": "normal",
      "oss_devdir": "^T_WORK^/raw^oss_port^"
    },
    "description": "tsagbug18355759.tsc - Test for BUG 18355759\n\nTest for BUG 18355759 - CELL-02772: CANNOT CREATE OR ALTER FLASH CACHE DUE TO UNEQUAL CELL DISK SIZES",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18414843.tsc",
    "setup": null,
    "flags": {
      "asm_ausize": "4194304"
    },
    "description": "tsagbug18414843.tsc - test for bug",
    "platform": null
  },
  {
    "test_name": "tsagbug18442272.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug18442272.tsc - Test for Bug 18442272\n\n18442272 - NOT TO ALLOW DOWNGRADE FROM 12.1.2.1.0 IF THERE EXISTS ANY SPARSE GD",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18463782.tsc",
    "setup": null,
    "flags": {
      "log_file": "tsagbug18463782_^db_num^.log"
    },
    "description": "tsagbug18463782.tsc - mdb bug 18463782",
    "platform": null
  },
  {
    "test_name": "tsagbug18545180.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug18545180.tsc - Tests old fashioned clones\n\nCreates a clone of a non CDB Oracle DB",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18546882.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug18546882.tsc - Test for bug 18546882\n\nBug 18546882 - ALLOW EMAIL SUBSCRIBERS TO FILTER THE ALERTS THEY WANT TO RECEIVE\n\nVerify the cell attribute 'emailSubscriber'",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18553965.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug18553965.tsc - BUG 18553965 - MS SHOULD RESCAN THE DEVICE WHEN THE DEVICE GETS REENUMERATED\n\nThis is some rare case where the device name of\n     a disk changes while the system is still up.\n     We want to make sure that we recognize this change\n     and trigger CellSrvs to rescan the device.\n     This will be a DebugCLI test.\n\nSEE DESCRIPTION",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18648820.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug18648820.tsc - Tests for Cell attribute IOTIMEOUTTHRESHOLD\n\nTests for Cell attribute IOTIMEOUTTHRESHOLD",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18648820_dumpchk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug18648820_dumpchk.tsc - Checks the cdpolicy dump for test tsagbug18648820.tsc\n\nChecks the cdpolicy dump for test tsagbug18648820.tsc",
    "platform": null
  },
  {
    "test_name": "tsagbug18649810.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug18649810.tsc - script for Bug 18649810.\n\nBUG 18649810 ALL FLASH DISKS ARE IN FAILING STATE AFTER DROP FLASHCACHE ALL",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18684802.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug18684802.tsc - Test case for Bug 18684802\n\nTest case for bug 18684802",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18698141.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug18698141.tsc - Test for BUG 18698141\n\nBUG 18698141 - MS SHOULD HAVE A COMMAND TO LIST DISK MAPPINGS",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18722441.tsc",
    "setup": null,
    "flags": {
      "sga_target": "512M",
      "use_self_tune_sga": "true"
    },
    "description": "tsagbug18722441.tsc - Test for bug #18722441",
    "platform": null
  },
  {
    "test_name": "tsagbug18735650.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug18763626.tsc - BUG 18735650 - 12.1.1.1.1 OSS EXADATA CELL, \"ATTEMPTED TO CHANGE VIRTUAL SLOT\" MESSAGES\n\nBUG 18735650 - 12.1.1.1.1 OSS EXADATA CELL, \"ATTEMPTED TO CHANGE VIRTUAL SLOT\" MESSAGES\n\nBUG 18735650 - 12.1.1.1.1 OSS EXADATA CELL, \"ATTEMPTED TO CHANGE VIRTUAL SLOT\" MESSAGES",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18916813.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug18916813.tsc - Test for BUG 18916813\n\nTest for BUG 18916813 - CELL-02669: NO SLOTS ARE AVAILABLE FOR ILOM SNMP SUBSCRIBERS EVEN WHEN REMOVING\n\nibhosts is used with grep slcc13 for x2 machines. In case machine\n      sched_dir is changed, need to change the grep statement accordingly",
    "platform": null
  },
  {
    "test_name": "tsagbug18918942.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug189189422.tsc - Flashdisk sg_inq failure and recovery test\n\nFlashdisk sg_inq failure and recovery test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18961018.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug18961018.tsc - new bbu state polling\n\nSIGNIFICANT DELAY FOR BBUSTATUS TO BECOME NORMAL AFTER REPLACEMENT",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug18978746.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagbug18978746.tsc - Check allflash configuration on Cell\n\nEnables allflash configuration and ensures no flashcache is created by\n       \"create cell\"",
    "platform": null
  },
  {
    "test_name": "tsagbug18978925.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug18978925.tsc - ibport lid change test\n\nBug18978925.tsc - CELL-2623 PERFQUERY RETURNS ERROR CODE 255 AND LIST IBPORT REPORTS DOWN\n                     + CELL-02020: INFINIBAND PORT HCA-1:2 DOES NOT EXIST. -- WHILE EXECUTING COMMAND\n     Bug19820562 - X4-2 EXADATA STORAGE SOFTWARE DIDN'T DETECT INFINIBAND PORT LINKDOWN",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug19060805.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug19060805.tsc - OSS Test for bug 19060805\n\nTest for BUG 19060805 - IN HALRT FAULT TRAP TELEMETRY SLOT VALUE IS MISSING IN ASR DATA\n\nNOTE that the HALRT alert codes (HALRT-02014/02015/02016 etc.) can change\n       when the test is run on a different type of real HW.",
    "platform": null
  },
  {
    "test_name": "tsagbug19148107.tsc",
    "setup": "tsagnini",
    "flags": {
      "kxdbio_ut_ctl": "4",
      "cluster_database": "true",
      "max_instance": "2",
      "maxinstances": "^max_instance^     # number of asm instances",
      "SAGE_MIRROR_MODE": "high",
      "nflint": "1",
      "sga_target": "512M",
      "use_self_tune_sga": "true"
    },
    "description": "tsagbug19148107.tsc - Test case for Bug 19148107\n\nTest case for Bug 19148107",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug19190475.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug19190475.tsc - test case cover 19190475\n\ntest case for  _cell_oflgrp_in_debug_mode param\n     Test step:\n     1. set _cell_oflgrp_in_debug_mode in <user offloadgroup>/celloffloadinit.ora, restart cell services\n     2. kill user offloadgroup and sys offloadgroup\n     3. sleep for serveral seconds\n     4. see sys offloadgroup pid come back, but user offloadgroup didn't\n     5. set _cell_oflgrp_in_debug_mode in cellcli with offloadgroupEvents to sys offloadgroup\n     6. kill user offloadgroup and sys offloadgroup\n     7. sleep for serveral seconds\n     8. see neither user offloadgroup nor sys offloadgroup pid come back\n     9. shutdown cell services, remove  _cell_oflgrp_in_debug_mode in <user offloadgroup>/celloffloadinit.ora\n     10. restsart cell services, kill  user offloadgroup and sys offloadgroup\n     11. sleep for serveral seconds\n     12. see both user offloadgroup and sys offloadgroup pid come back",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug19461174.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug19461174.tsc - Test for BUG 19461174\n\nBUG 19461174 - No slots are available for ilom snmp subscribers since numalertdests is 0",
    "platform": null
  },
  {
    "test_name": "tsagbug19492277.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug19492277.tsc - Test for Bug 19492277\n\nTEST FOR 128K CLIENT SUPPORT AND BALANCE ACROSS ACTIVE/ACTIVE IPS",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug19519560.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug19519560.tsc - DB node specific bug tests",
    "platform": null
  },
  {
    "test_name": "tsagbug19524144.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "log_name": "^tst_tscname^.log"
    },
    "description": "tsagbug19524144.tsc - Add test for bug 19524144\n\nTest to capture error for list command when used with prefix",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug19571142.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "Testing user and role feature of cellcli",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug19695225.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_TESTING": "1"
    },
    "description": "tsagbug19695225.tsc - Testcase for bug 19695225\n\nThis test case covers the case where when create/alter griddisk are\n     run in specific order, it asserts.\n\nWe make sure alter griddisk operation creates a new sector to fill\n     segment map records and the opeartions following that create/alter\n     will assert as per the bug. With this fix the test should complete\n     this sequence of operations successfully.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug19721903.tsc",
    "setup": "tsagnini",
    "flags": {
      "numdisks": "31",
      "siz": "^TST_EXE_RESULT^",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug19721903.tsc - Test for bug 19721903. Make sure GD drop/create\n                         -  in a loop does not cause GD creation failure with\n                         - 'CELL-02620: An unmapped CELLSRV error has occurred.'\n                         - But this is a shorter version of the test to make sure\n                         - numRecords is computed correctly and not leaking.\n     Bug 20473718 test   - Ensure griddisk.owners is not corrupted after cell downgrade",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug19727038.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_testing": "2",
      "oss_auto_manage_disks": "true",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug19727038.tsc - Bug 19727038 test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug19798068.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug19798068.tsc - Test for bug 19798068\n\nBUG 19798068 : MS SHOULD NOT FOLLOW LINKS AS IT CLEANS UP FILES",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug19817732.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2"
    },
    "description": "tsagbug19817732.tsc - test for bug 19817732\n\nTest for Bug 19817732 -\n     FlashLog should not allow client I/Os until Recovery has completed",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug19897796.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug19897796.tsc - Test for FPLIB Support for P-N Row Pieces\n\nCreates an OSS_TESTING environment and runs tsagbug19897796.sql\n     Enhancement introduced in RDBMS 12.2 to support P-N chained row pieces\n     on the storage server side, mostly filtering and projection limited\n     to the output block size.\n\nThis test is part of tsagbugsuit.tsc - lrgsabug27\n     Please check tsagbug19897796.sql for more details",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug19982670_x3.tsc",
    "setup": null,
    "flags": {
      "cell": "slcm01cel12",
      "cellconnstr": "root@^cell^",
      "oss_hw_testing": "1"
    },
    "description": "tsagbug19982670_x3.tsc - Tests ASR traps on X3\n\nDISK failure simulation done and ASR traps received\n\nSystem Lun:\n        Failed -          HALRT-02001\n        PRED failed -     HALRT-02002\n        Poor Perf -       HALRT-02009\n    Non System HD:\n        Failed -          HALRT-02004\n        PRED failed -     HALRT-02003\n        Poor Perf -       HALRT-02010\n    FLASH Disks:\n        Failed -          HALRT-02014\n        PRED failed/WT caching -     HALRT-02015\n        Poor Perf -       HALRT-02016\n    USB failure\n        Not present -     HALRT-02017",
    "platform": null
  },
  {
    "test_name": "tsagbug19982670_x5.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug19982670_x5.tsc - Tests for ASR on X5\n\nTESTS ASR traps received after disk failures on X5 - EF cells\n\nX5 - EF:\n       Disk pred fail  -   HALRT_02021\n       Disk WTCaching  -   HALRT_02021\n       Disk fail       -   HALRT_02020\n       Poor Perf       -   HALRT_02022",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug2.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "high",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug2.tsc - Test for bug 9208843 and 9208779 (as described in\n                  - bug 9225235). Make sure query doesn't fail after system\n                  - runs out of 1mb buffers.\n\nThis tsc file tests the following scenarios:\n           - When a cellsv has a shortage of memory (1MB buffers), cellsrv return\n             an out-of-memory error.  kcfis will then issue block IO requests instead\n             of smart scans. For block IOs, we have a reserved pool of buffers on cellsrv.\n           - Kcfis will use block IOs only for those cells which return an out-of-mem\n             error.  For other cells, we use smart scans.  Hence, this test uses\n             multiple cells and with resource configured to a small value in some cells.\n             (not all the cells).",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug20064817.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug20064817.tsc - Test case for Bug 20064817\n\nTest case for 20064817. Makes sure VmRSS and VmSwap in $T_WORK/<mspid>/status\n     adds up to what cellcli 'list metriccurrent' reports",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug20072201.tsc",
    "setup": null,
    "flags": {
      "x7_ef_setup": "true"
    },
    "description": "tsagbug20072201.tsc - NVMe admin ioctl failure test\n\n<nvmecli error injection test on x5 setup>",
    "platform": null
  },
  {
    "test_name": "tsagbug20184512.tsc",
    "setup": "tsaginit",
    "flags": {
      "OSS_TESTING": "1"
    },
    "description": "tsagbug20184512.tsc - Add test for bug 20184512\n\nTest to throw more descriptive message when ib ports are down",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug20205080.tsc",
    "setup": "srdbmsini",
    "flags": {
      "processes": "250",
      "creatdev_file": "tsagbug20205080def"
    },
    "description": "tsagbug20205080.tsc - test for bug 20205080\n\nocl_si_ridx_histogram_update: out-of-sync time",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug20313295.tsc",
    "setup": "tsaginit",
    "flags": {
      "testname": "^lrg_name^",
      "dbcprefix1": "'cmain'",
      "sage_mirror_mode": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug20313295.tsc - Resilvering test\n\n- set asm_power_limit = 0\n      - fail flash on one cell and verify that asm deactivation outcome on the 2nd cell shows 'no' due to resilvering\n      - shutdown cellsrv on the 2nd cell 'ignore redundancy'\n      - verify that asm diskgroup got force dismounted",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug20326593.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug20326593.tsc - STORAGE INDEX EVICTION SHOULD CONSIDER AND CLEAN UP INVALIDATED RIDX\n\nTest for bug 20326593",
    "platform": null
  },
  {
    "test_name": "tsagbug20389051.tsc",
    "setup": null,
    "flags": {
      "x5_af_setup": "true"
    },
    "description": "tsagbug20389051.tsc - Test for BUG 20389051\n\nBUG 20389051 - MS NEEDS TO SUPPORT SINGLE NVME EXPANSION",
    "platform": null
  },
  {
    "test_name": "tsagbug20395833.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "scas15celadm04",
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagbug20395833.tsc - Test for BUG 20395833\n\nBUG 20395833 - ASR EVENT DOES NOT PROPAGATE THE PART NUMBER.",
    "platform": null
  },
  {
    "test_name": "tsagbug20441236.tsc",
    "setup": "tsagnini",
    "flags": {
      "oss_auto_manage_disks": "true",
      "oss_devdir": "^T_WORK^/raw"
    },
    "description": "tsagbug20441236.tsc - Test for BUG 20441236\n\nTest for BUG 20441236 : BUG BAD CONTEXT DRIVES ARE REPORTED PROPERLY BY MS",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug20483785.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug20483785.tsc - test for sparsedb 9lrgdbcsasparse20)\n\noptimization for regular file creation on sparse diskgroup",
    "platform": null
  },
  {
    "test_name": "tsagbug20510426.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug20510426.tsc - test for bug 20510426",
    "platform": null
  },
  {
    "test_name": "tsagbug20596195.tsc",
    "setup": "srdbmsini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "asm_ausize": "1048576",
      "kxdrs_tracing": "false"
    },
    "description": "tsagbug20596195.tsc - Test for Bug 20596195\n\nEXADATA SCRUB RESILVERING NEEDS TO REPAIR UNALLOCATED AUs",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug20598619.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug20598619.tsc - Bug test 20598619 in oss_safault4.tsc\n\nIORM HARD LIMIT WHEN REACHED, FLASHLOG TO BE BYPASSED\n\n- Set low IORM hard limits at database level\n     - Soon more IOs result in ending up in IORM queue as IORM throttles\n       the IOs seeing them hitting low hard limits\n     - This builds up IO queue which are pending to be written to disk\n     - While disk IOs are pending, if they are log writes, all are\n       rescued by FL and DB runs happily.\n     - In cellsrv, since the disk IOs are not complete and the buffers are\n       still not released, any new incoming IOs, cellsrv will not be able\n       to pick up as buffers exhausted and runs into hang.",
    "platform": null
  },
  {
    "test_name": "tsagbug20602836.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug20602836.tsc - Test for bug 20602836\n\nMake sure (non) columnar fc query have same uncompresed byte",
    "platform": null
  },
  {
    "test_name": "tsagbug20649137.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug20649137.tsc - Test case for bug 20649137\n\nTest case for bug 20649137",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug20656015.tsc",
    "setup": null,
    "flags": {
      "x7_ef_setup": "true",
      "sage_mirror_mode": "normal",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "cellname": "secondcell",
      "asmdisks_created": "2",
      "nflint": "1",
      "numlogfiles": "2",
      "do_not_set_numlogfile": "true"
    },
    "description": "tsagbug20656015.tsc - Test for Bug 20656015\n\nREBALANCE ALERTS NOT GENERATED AFTER SIMULATED FAILED FLASH ON EF CELL\n     Test Summary:\n      1) A normal redundancy setup with 2 cells\n      2) Both cells have 12 EF disks\n      3) All devices have flashcache and griddisk created off them\n      4) fail a device and unsimulate the failure\n      5) Disk alerts related to Rebalance operation should be generated",
    "platform": null
  },
  {
    "test_name": "tsagbug20766737.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug20766737.tsc - Test for bug 20766737\n\nBug 20766737 - ILOM SNMP NOTIFICATION TARGETS ARE NOT PROPERLY SET BY MS",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug20766737_db.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug20766737_db.tsc - Test for bug 20766737 on DB Node\n\nBug 20766737 - ILOM SNMP NOTIFICATION TARGETS ARE NOT PROPERLY SET BY MS",
    "platform": null
  },
  {
    "test_name": "tsagbug20899376.tsc",
    "setup": "tsaginit",
    "flags": {
      "cdb": "true",
      "oracle_cdb_run": "true",
      "creatdev_file": "tsagcaudef"
    },
    "description": "tsagbug20899376.tsc - Bug 20899376 test to ensure in a CDB\n                         - pluggable database file creation uses smart\n                         - file creation code path. I.e., DB shouldn't run into\n                         - \"ORA-27626: Exadata error: 212 (Offload library\n                         - initialization failed)\" during file creation.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug20992894.tsc",
    "setup": null,
    "flags": {
      "oss_auto_manage_disks": "true",
      "log_file": "tsagbug20992894.log"
    },
    "description": "tsagbug20992894.tsc - Test case for bug 20992894",
    "platform": null
  },
  {
    "test_name": "tsagbug20996934.tsc",
    "setup": null,
    "flags": {
      "x5_af_setup": "true",
      "num_af_dev": "12",
      "nvme_size": "4608",
      "sage_mirror_mode": "normal",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "cellname": "secondcell",
      "asmdisks_created": "2",
      "nflint": "1",
      "numlogfiles": "2",
      "do_not_set_numlogfile": "true"
    },
    "description": "tsagbug20996934.tsc - Test for Bug 20996934\n\nBUG 20996934 - CD STATUS SHOWS AS \"NOT PRESENT\" AFTER DISK REPLACEMENT\n\nRequires EF cell environment",
    "platform": null
  },
  {
    "test_name": "tsagbug21299486.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagccdef",
      "lgfileenv": "^tst_tscname^env.log"
    },
    "description": "tsagbug21299486.tsc - Bug 21299486 - EXECUTION OF LIST ACTIVEREQUEST LEAD TO IO HUNGS ON THE DATABASE\n\nBug 21299486 - EXECUTION OF LIST ACTIVEREQUEST LEAD TO IO HUNGS ON THE DATABASE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug21313699.tsc",
    "setup": null,
    "flags": {
      "x5_af_setup": "true",
      "num_af_dev": "12",
      "nvme_size": "512"
    },
    "description": "tsagbug21313699.tsc - Test for Bug 21313699\n\nREPEAT NVMECLI IDENTIFY CHECKS FOR UP TO 100S BEFORE FAILING THE DRIVE",
    "platform": null
  },
  {
    "test_name": "tsagbug21453718.tsc",
    "setup": "srdbmsini",
    "flags": {
      "log_file": "tsagbug21453718.log"
    },
    "description": "tsagbug21453718.tsc - Test for bug#21453718\n\n< DESCRIPTION >",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug21454474.tsc",
    "setup": "srdbmsini",
    "flags": {
      "SAGE_MIRROR_MODE": "high",
      "oss_testing": "3",
      "differentszfg": "1"
    },
    "description": "tsagbug21454474.tsc- test for bug 21454474 OK2RM LIGHT GOES OFF BEFORE REBALANCE COMPLETES\n\nsetup: high version cellsrv (support asm disk alert),\n      low version xdmg (does not support asm disk alert)\n      A disk enters predictive failure state on cell side.\n      In the test case, we need to check ASM rebalance is indeed completed before receiving the\n      'can be replaced' disk alert.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug21460529.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug21460529.tsc - bug test to check SVC and OK2RM in case of failed disk\n\nbug test to check SVC and OK2RM in case of failed disk",
    "platform": null
  },
  {
    "test_name": "tsagbug21498710.tsc",
    "setup": null,
    "flags": {
      "sleep_for_iov": "30",
      "errbasename": "^tst_tscname^o.log"
    },
    "description": "tsagbug21498710.tsc - test case for bug 21498710\n\nTest case for bug 21498710.",
    "platform": null
  },
  {
    "test_name": "tsagbug21503210.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_auto_manage_disks": "true  # for XDMG to startup"
    },
    "description": "tsagbug21503210.tsc - Bug 21503210 - CELLSRV SHUTDOWN NEEDS TO ENSURE ASM DISK OFFLINE IS FULLY COMPLETE\n\nBug 21503210 - CELLSRV SHUTDOWN NEEDS TO ENSURE ASM DISK OFFLINE IS FULLY COMPLETE",
    "platform": null
  },
  {
    "test_name": "tsagbug21533170.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug21533170.tsc - Test case for bug 21533170\n\nMake sure RS does not try to restart a bdsqlsrv which is being held. It will ask to reeboot host",
    "platform": null
  },
  {
    "test_name": "tsagbug21543039.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cluster_database": "true",
      "maxinstances": "2",
      "oss_auto_manage_disks": "true",
      "sage_mirror_mode": "normal"
    },
    "description": "tsagbug21543039.tsc - Test for bug 21459225\n\nThis txn checks that when ASM in rolloing state we should not be able\n     to deactivate ASM disks. The state of disks must reflect\n     \"Cannot deactivate because ASM is in rolling upgrade mode\".\n\n     This txn runs in interop mode with latest OSS_MAIN and RDBMS_MAIN labels.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug21563297.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug21563297.tsc - SYNC MD DEVICES IF THEY NEED IT",
    "platform": null
  },
  {
    "test_name": "tsagbug21609358.tsc",
    "setup": "tsagnini",
    "flags": {
      "format_long_identifier": "true"
    },
    "description": "tsagbug21609358.tsc - Add test case for bug 21609358\n\ntest for celldisk metadata scrubbing\n\ncorrupt celldisk metadata and it should work fine",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug21614011.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "scas15celadm04"
    },
    "description": "tsagbug21614011.tsc - Test for Bug 21614011\n\ntests that resourcecontrol -show output matches with number of celldisks",
    "platform": null
  },
  {
    "test_name": "tsagbug21665955.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "maxinstances": "2",
      "cluster_database": "true",
      "auto_undo_management": "true",
      "compatible": "12.2.0.0",
      "creatdev_file": "tsagbug21665955_creatdev_file",
      "logckdisk": "^tst_tscname^_ckdisk"
    },
    "description": "tsagbug21665955.tsc - Test for bug 21665955 - LNX64-12.2-ASM, KILL ONE DB INSTANCE LMON, ALL DB INSTANCES FENCE\n\nBug 21665955 - LNX64-12.2-ASM, KILL ONE DB INSTANCE LMON, ALL DB INSTANCES FENCE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug21756228.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef",
      "celluser": "user1",
      "cellpwd": "Welcome12345",
      "log_file": "tsagbug21756228.log"
    },
    "description": "tsagbug21756228.tsc - Test for bug 21756228\n\n< DESCRIPTION >",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug21861668.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrh2def"
    },
    "description": "tsagbug21861668.tsc - Test for bug 21861668\n\nBUG 21861668 - ORA-600 [SAFEFILE_OPEN1: CAN NOT OPEN MORE THAN ONE SAFEFILE IN A THREAD]\n\n     1.) start IOV on a griddisk to populate flash cache with dirty data.\n     2.) fail a flash that was caching for the griddisk.\n     3.) Set the following simulation events to simulate celldisk metadata IO errors:\n       cellcli -e alter cell events='\"cellsrv_simevent[CDMETA_READ_ERR] err_type=internal, frequency=1, count=2, evarg1=2201\"'\n       cellcli -e alter cell events='\"cellsrv_simevent[CDMETA_WRITE_ERR] err_type=internal, frequency=1, count=2, evarg1=2201\"'\n\n        The evarg1 parameter controls what errorcode to simulate. 2201 is\n        OSS_ERRCODE_IO_CANCELLED, which should NOT lead to celldisk confinement.\n        The only errorcodes which should lead to confinement are 201 (OSS_ERRCODE_IOERROR) and\n         2203 (OSS_ERRCODE_IO_CANCELLED_DUE_TO_DISK_FAILURE).\n\n      4.) wait for resilvering to complete\n      5.) When resilvering completes, it will attempt to write some celldisk metadata.\n         Due to the events set in step 3, this IO will fail, and depending on the evarg1 parameter,\n         will either lead to confinement of the celldisk or not. This also tests bug-21861668,\n        since we should not see an assert at this step with the fix.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug21868040.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug21868040.tsc - Test for bug 21868040\n\nBUG 21868040 - NEED TO ACCOUNT FOR FAILGROUP REPAIR TIMER FOR GD ATTRIBUTE ASMDISKREPAIRTIMER",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug21891473.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cell_with_pmem_cache": "1",
      "cell_with_xrmem_cache": "0"
    },
    "description": "tsagbug21891473.tsc - test wrong result scripts\n\ntest wrong result scripts",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug21911678.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true    # for XDMG to startup",
      "oss_testing": "2",
      "sage_mirror_mode": "normal",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "oss_no_asmdb": "true",
      "num_flash_per_cell": "2"
    },
    "description": "tsagbug21911678.tsc - TEST REQUEST FOR BUG 16212896\n\nTEST FOR BUG 16212896 - CELL-02637 HIT ON ROLLING UPGRADE IF GRID DISK STATUS IS CACHECONTENTLOST\n     Test procedure:\n     Code txn: yiljin_bug-21792958\n     1. Set up normal redundancy fake hw with asm, xdmg up.\n     2. Set flashcahemode to writeback\n     3. Look at griddisk cachedby attribute, make sure all gds have non-empty cached by after some time.\n\n     On one cell:\n     4. Shutdown ms and cellsrv\n     5. mv one flash disk to somewhere else.\n     6. Startup cellsrv and ms.\n     7. List griddisk, some of them should be in flashcontentlost status.\n     8. Do 'alter griddisk <those_gds> active' or 'alter griddisk <those_gds> inactive', it should complain:\n        The operation is not permitted on this cell disk.\n     9. Test end.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug22069208.tsc",
    "setup": null,
    "flags": {
      "cell": "slcm01cel13",
      "cellconnstr": "root@^cell^",
      "oss_hw_testing": "1"
    },
    "description": "tsagbug22069208.tsc - Test for bug 22069208\n\nThe test checks whether snmp alerts are generated for disk failures\n     and cancellation of disk failures.",
    "platform": null
  },
  {
    "test_name": "tsagbug22104716.tsc",
    "setup": "tsagnini",
    "flags": {
      "celltrcdir": "^t_diag^^s^diag^s^asm^s^'cell'^s^^hostname^^s^trace^s^"
    },
    "description": "tsagbug22104716.tsc - Test for Bug 22104716\n\nBUG 22104716 - OLD EMAIL ALERTS SENT OUT DURING CELL UPGRADE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug22107319.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug22107319.tsc - test for bug 22107319\n\nThis test confirms that on a cell, simultaneous installation of cell.bin\n     must not be allowed. Only first installation must succeed.",
    "platform": null
  },
  {
    "test_name": "tsagbug22114596.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cellconnstr": "root@^cell_node^"
    },
    "description": "tsagbug22114596.tsc - TEST FOR BUG 22114596 - VERIFY THE FABRIC FOR RDS FAILOVER DELAYS\n\nCreate fake rds-ping and ibtracert, to return 1 and simulate an error, then just\n     start up cellsrv and in the cellcli we should have error 'CELL-01562: Cross ip\n     connectivity failure' and in cellsrv alert log a soft assert triggered: ORA-00700:\n     soft internal error, arguments: [main_6d], [23], [Cross check ip connectivity failed]\n\nNone",
    "platform": null
  },
  {
    "test_name": "tsagbug22176025.tsc",
    "setup": "tsaginit",
    "flags": {
      "logsuf": "_excld"
    },
    "description": "tsagbug22176025.tsc - BUG 22176025 - TRACKING BUG TO IMPROVE CELLSRV HANDLING OF LOSS OF FLASHCACHE.LST\n\nIf flashcache.lst is lost, cellsrv may bootstrap a flash disk that contains\n    known stale data.\n\n    Potential fix is to check that griddisks are being cached by a flash disk\n    while bootstrapping the flash disk, and to abort bootstrap if cacheline is\n    for a griddisk that is not being cached by the flash disk.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug22216153.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug22216153.tsc - diagpack timezone test\n\ndiagpack test at different timezone",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug22220568.tsc",
    "setup": "srdbmsini",
    "flags": {
      "log_file": "tsagbug22220568.log"
    },
    "description": "tsagbug22220568.tsc - Diskmon bug#22220568 test\n\n1. Setup Exadata with below settings:\n         setenv RECON_CELL_FREQ_IN_SEC 2\n         setenv RECON_CELL_ATTEMPT_COUNT 5\n         setenv SAGE_MIRROR_MODE high\n      2. Shutdown DB\n      3. Set event to diskmon:\n         $ADE_VIEW_ROOT/oracle/bin/diskmon -m \"libcell.diskmon_sim_ops[sleep_diskmon_thrd]\n           dskmsimloc='sleep_at_dskm_proc_get_ossid',dskmsimstime=15000\"\n      4. Stop RS to prevent restart cellsrv on cell 2\n      5. Startup DB (at the same time)   kill -9 <cellsrv_ospid>\n      6. Unset the event:\n         $ADE_VIEW_ROOT/oracle/bin/diskmon -m\n           \"libcell.diskmon_sim_ops[sleep_diskmon_thrd] dskmsimloc='',dskmsimstime=20000\"",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug22236130.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "raw_path": "^T_WORK^/raw"
    },
    "description": "tsagbug22236130.tsc - Test for BUG 222361630\n\n1. Create WB FC with 2 CD (tsagnini)\n      2. list gd details - CachedBy non-empty, cachingPolicy default\n      3. drop PDs\n      4. list gd Details - CachedBy non-empty, CachingPolicy default\n      5. reenable PDs\n      6. list gd details - CachedBy non-empty, cachingPolicy default\n      Repeat for WBFC with 1 CD and alter it to use 2 CDs for 3rd case\n\nDue to the code change in bug 31439012, ALTER PHYSICALDISK DROP FOR\n   REPLACEMENT will not flush Flash Cache on HDD. Thus, the CachedBy attribute\n   will not be empty after the DROP command and is expected to remain the same\n   value.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug22257471.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug22257471.tsc - Test for bug 22257471\n\nBUG 22257471 - AN ALERT INDICATING PD-CD MISMATCH AFTER REMOVING A DRIVE AND RESTARTING SERVICE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug22283022.tsc",
    "setup": "srdbmsini",
    "flags": {
      "log_file": "tsagbug22283022.log"
    },
    "description": "tsagbug22283022.tsc - Testcase for bug#22283022\n\nBug#22283022 test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug22283355.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug22283355.tsc - ilom next reset time attaribute test\n\nilomNextResetTime attribute should be within reasonable range",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug22302800.tsc",
    "setup": null,
    "flags": {
      "dbnode": "^compute_node^",
      "dbnodeconnstr": "root@^compute_node_f^"
    },
    "description": "tsagbug22302800.tsc - Test for bug 22302800\n\nThis test checks that minimum cores can be set as 2 for x5_2.\n     This test also checks that proper alerts are created when cores\n     are changed using resorse control.",
    "platform": null
  },
  {
    "test_name": "tsagbug22315651.tsc",
    "setup": null,
    "flags": {
      "x5_af_setup": "true",
      "num_af_dev": "12",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "nvme_size": "512"
    },
    "description": "tsagbug22315651.tsc - Bug 22315651 - TEST REQUEST FOR BUG 21330097",
    "platform": null
  },
  {
    "test_name": "tsagbug22338154.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^.log"
    },
    "description": "tsag22338154.tsc - Add for bug22338154\n\nWhen we set events for offloadgroup the CC should not work",
    "platform": null
  },
  {
    "test_name": "tsagbug22506481.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug22506481.tsc - Test for BUG 22506481\n\n22506481 - CELL LED LOCATOR NEVER TURNED OFF AFTER POWER BUTTON HIT\n    WITH REDUCED REDUNDANCY",
    "platform": null
  },
  {
    "test_name": "tsagbug22541648.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug22541648.tsc - Test for Bug 22541648\n\nMULTIPLE ALERTS SHOULD NOT BE GENERATED AFTER IB CABLE PULL",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug22579316.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "scas15celadm12",
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug22579316.tsc - Test for Bug 22579316",
    "platform": null
  },
  {
    "test_name": "tsagbug22605762.tsc",
    "setup": "tsagnini",
    "flags": {
      "TZ": "PST8",
      "datafilelist": "^rtest^",
      "tsnum": "0",
      "bugtesti": "bugtest^tsnum^",
      "log_file": "tsagbug22605762.log"
    },
    "description": "tsagbug22605762.tsc - NEW TEST REQUESTS FOR BUG 22605762\n\nTest procedure:\n      1. Test scrubbing kickoff for normal schedule:\n         - alter cell harddiskscrubinterval=biweekly\n         set IO error simulation on one CD\n         - alter cell events=\"cellsrv_simevent[BLOCKIO_READ_ERR] err_type=internal,frequency=1,count=1,\n                              evarg1=c9datafile0\"\n\n         get current time and set start time as two weeks ago:\n          - alter cell harddiskscrubstarttime=\"2016-01-19T21:32:43-08:00\"\n         wait for 20sec, we should see scrubbing kicked off on all CDs\n      \t> Begin scrubbing CellDisk:c9logfile0.\n      \t> Begin scrubbing CellDisk:c9datafile0.\n      \t> Begin scrubbing CellDisk:c9datafile1.\n      \t> Begin scrubbing CellDisk:c9datafile2.\n      \t> Begin scrubbing CellDisk:c9standby0.\n      \t> Begin scrubbing CellDisk:c9datafile3.\n      \t> Begin scrubbing CellDisk:c9datafile4.\n      \t> Begin scrubbing CellDisk:c9controlfile0.\n      \t> Begin scrubbing CellDisk:c9logfile1.\n      \t> CELLSRV error simulation: location BLOCKIO_READ_ERR count 1 CD c9datafile0 offset 0 size xx\n\n      2. Test scrubbing kickoff for follow-up schedule:\n         turn off IO error simulation:\n         - alter cell events=\"cellsrv_simevent[BLOCKIO_READ_ERR] off\"\n\n         get current time again and set start time as one week ago:\n         - alter cell harddiskscrubstarttime=\"2016-01-19T21:34:43-08:00\"\n         wait for 20sec, we should see scrubbing job kicked off on c9datafile0 only:\n      \t> Begin scrubbing CellDisk:c9datafile0.\n      \t...\n      \t> Finished scrubbing CellDisk:c9datafile0, scrubbed blocks (1MB):1488, found bad blocks:0\n         set start time as one week ago again:\n         - alter cell harddiskscrubstarttime=\"2016-01-19T21:35:43-08:00\"\n         wait for 20sec, we should see no scrubbing job kicked off this time.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug22643740.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug22643740.tsc - deviceName is missing from PhysicalDisk attribute after upgrade\n\nOn upgrade, deviceName attribute in PhysicalDisk is not defined.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug22684839.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sga_target": "2048M"
    },
    "description": "tsagbug22684839.tsc - Test for Bug 22684839\n\nChecks for disabling SI stats\n\nChecks for disabled SI stats in v$cell_state",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug22906230.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug22906230.tsc - \"restricted access of attributes in exacli\"\n\nOnly showing attributes which have permission",
    "platform": null
  },
  {
    "test_name": "tsagbug22961347.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug22961347.tsc - MS thread spin test\n\nWhen /u01 directory size is larger than 80%, MS on DB returns nothing for current metric\n     This is test the fix.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug23093622.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug23093622.tsc - SYSTEM DISK REPLACEMENT MIGHT FAIL\n\nDisk replacement when all of following conditions are satisfied\n     1) It is a system disk\n     2) disk controller has pinned cache\n     3) replacement requires firmware upgrade",
    "platform": null
  },
  {
    "test_name": "tsagbug23170314.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug23170314.tsc - TEST for BUG 23170314",
    "platform": null
  },
  {
    "test_name": "tsagbug23191801.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagbug23252112.tsc",
    "setup": null,
    "flags": {
      "cellconnstr": "root@^cell1_f^",
      "dbconnstr": "root@^compute_node_f^",
      "cntr": "1"
    },
    "description": "tsagbug23252112.tsc - FAST NODE DEATH DETECTION NOT WORKING\n\nThis is a unit test for FNDD using tkmiskgzib",
    "platform": null
  },
  {
    "test_name": "tsagbug23292722.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug23292722.tsc - altesr dbserver syslog attribute test",
    "platform": null
  },
  {
    "test_name": "tsagbug23309395.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug23309395.tsc - disk full makes USB state failure\n\nmkdir fialure on checkdev leads to USB rebuild",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug23521475.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagbug23521475.tsc - INCONSISTENT TIMESTAMP DAY REPRESENTATION IN CELL ALERT.LOG\n\nCustomized optional parameter in alert.log\n     Timestamp check in lrg",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug23521558.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef",
      "disk": "datafile3",
      "raw": "^T_WORK^/raw"
    },
    "description": "tsagbug23521558.tsc - test for bug 23521558\n\nBug Test : 23521558\n     1. On cells don't allow a disk to be re-enabled if CellSrv is not running.\n     2. Only clear ATTR_LUN_REENABLE_REQUIRED when re-enabling a disk fails and will not be retried\n\nTest 1: Try to re-enable a failed disk after cellsrv is shut down.\n     Test 2: Try to re-enable a failed disk after both cellsrv and rs are shut down.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug23632786.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug23632786.tsc - coreCount attribute on upgrade\n\ncoreCount attribute should be removed from unspported platform",
    "platform": null
  },
  {
    "test_name": "tsagbug23735292.tsc",
    "setup": "srdbmsini",
    "flags": {
      "compatible": "11.2.0.0",
      "creatdev_file": "tsagaudef",
      "file": "'+datafile/dbs/tbs_foo2.f'"
    },
    "description": "tsagbug23735292.tsc - Bug 24353249 - TESTCASE FOR BUGS 23751100 AND 23735292\n\n1) create objects (as many as possible) with flashcache keep clause\n        clause:\n        (STORAGE(BUFFER_POOL DEFAULT FLASH_CACHE KEEP CELL_FLASH_CACHE KEEP))\n        ex:\n        SQL> select count(*) from dba_segments where cell_flash_cache = 'KEEP';\n     2) Start workload on these objects\n     3) After some time, restart the cellserver\n     4) No errors should be reported on DB side.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug24291112.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug24291112.tsc - test for bug 24291112\n\nTest if an alerthistory is created when cellofload startup is\n         disabled.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug24297929.tsc",
    "setup": "tsaginit",
    "flags": {
      "num_flash_per_cell": "4"
    },
    "description": "tsagbug24297929.tsc - Test for Bug 24297929\n\nTests  Flashcache drop and recreate in a small loop to check memory usage",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug24461879.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "scas15celadm12",
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagbug24461879.tsc - Test for Bug 24461879\n\n1) get old flash firmeware\n   2) downgrade one flash card:\n   3) cell validate configuration, we should see new alert is generated\n   4) call CheckHWnFWProfile to upgarde the flash firmware\n   5) Alert should be cleared",
    "platform": null
  },
  {
    "test_name": "tsagbug24499641.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug24499641.tsc - Bug 24499641 test\n\nBug 24499641 test",
    "platform": null
  },
  {
    "test_name": "tsagbug24745355.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug24745355.tsc - ASM DISK RESIZE APPEARS TO HANG WHEN OLTP\n                           WORKLOAD IS RUNNING\n\nTest flow:\n     1. create testing tables for OLTP workload: created three tables with 1000rows\n     2. fork 3 oltp workloads and running for 30 mins\n     3. after 10mins, sharply reduce the ASM disk size from 1480M to 256M\n     4. after 10secs, sharply reduce the grid disk size from 1488M to 512M\n     5. then slightly increase the ASM disk size to 500M\n\nThis is a non-standalone test, uses the existing env in the tsagbug25361305.tsc",
    "platform": null
  },
  {
    "test_name": "tsagbug24811725.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug24811725.tsc - EXADATA SCRUB WRITES SHOULD NOT BE REDIRECTED INTO FLASH CACHE\n\nTest checks that disk scrubbing writes do not get into FC\n     1) setup LW caching env with 2 cells\n     2) execute sim event for disk scrubbing\n     3) check cellsrvstat before and after disk scrubbing and ensure stats are same",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug24899249.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug24899249.tsc - Bug 24899249: ensure cell disk metadata is not corrupted\n                         - when a GD is created by using a prior to 12.1.2.1.x cell\n                         - version. In this specific test, we are replacing a CD in\n                         - MAIN w/ a CD created w/ OSS_12.1.2.1.1_LINUX.X64_RELEASE\n                         - cellsrv. It is important to create 31 numRecords i.e.,\n                         - 31 GDs in 12.1.2.1.1 views to hit numRecords=31 then copy\n                         - over CD from 12.1.2.1.1 view prior to creating additional\n                         - GD which should NOT fail. With fix, after GD creation\n                         - cellsrv restart should not result in CD metadata\n                         - corruption.",
    "platform": null
  },
  {
    "test_name": "tsagbug24899249_12.1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug24899249_12.1.tsc - Script that is going to run in OSS_12.1 view\n                                during tsagbug24899249.tsc execution.",
    "platform": null
  },
  {
    "test_name": "tsagbug24899249_main.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug24899249_main.tsc - Script that is going to run in OSS_MAIN view\n                                during tsagbug24899249.tsc execution.",
    "platform": null
  },
  {
    "test_name": "tsagbug24908334.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "scas15celadm12",
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagbug24908334.tsc - Test for Bug 24908334\n\nEMAIL THREADING IS MISSING IN BATTERY ALERTS FROM PDIT ENV",
    "platform": null
  },
  {
    "test_name": "tsagbug24914626.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug24914626.tsc - Celldisk Metadata inconsistent\n\nMoves and replaces the celldisk to temporary directory and checks if metadata is inconsitent\n\nBUG - 24914626",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug25047106.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug25047106.tsc - test for byte swap optimization\n\nThere are two cases, and this is the part 2.\n    The part 1 is in case tsagsiscm.tsc\n    Steps for the test:\n    1. create table with large rows;\n    2. do a scan first and make sure nm_short_column_oprimization increased\n    3. turn on the diag mode : _kcfis_storageidx_diag_mode= 16\n    4. do multiple select sessions and update sessions\n    5. should not see an incident\n\ntest for concurrent read and write for byte swap optimization",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug25094647.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug25094647.tsc - Bug25094647 test\n\nFlashCache failure -> sparse GD recreation test\n\nAvoid duplite disk count from total size calculation",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug25128043.tsc",
    "setup": "tsagnini",
    "flags": {
      "uniq_dsknames": "all"
    },
    "description": "tsagbug25128043.tsc - Bug# 25128043 - FLASH FAILURE IMPACTS OLTP POPULATION / SERVICE LEVEL\n\nThis test contains 2 testcases: For WBFC and For WTFC",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug25199092.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug25199092.tsc - Test Case for Bug 25199092\n\nTest case for 25199092.It is to check in ossIoctlCDHSChange(),if there is an ongoing/pending cd health state change on the celldisk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug25214197.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug25214197.tsc - jstack on MS startup hang\n\ncollect jstack on MS startup timedout case",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug25225942.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug25225942.tsc - Test for Bug-25225942\n\nTest 1\n       1) Creates user with password prompt\n       2) Creates user without using password prompt\n     Test 2\n       1) Alters snmpuser using password prompt\n       2) Alters snmpuser without using password prompt\n     Password supplied contains '$'\n     Error message showing instructions for proper password is expected in all the above steps.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug25361305.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_asm_sec": "true",
      "connstr": "'sys/knl_test7@inst11 as sysasm'"
    },
    "description": "tsagbug25361305.tsc - Test for Bug 25361305 - PREVENT GRIDDISK RESIZE FROM BEING SILENTLY CREATED SMALLER THAN ASM DISK SIZE\n\nTest to check the feature to ensure grid disk size be larger than asm disk size when running griddisk resize cmd in MS.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug25368613.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "cellconnstr": "root@^cell_node^"
    },
    "description": "tsagbug25368613.tsc - TEST FOR BUG 25368613\n\nBUG 25368613 - TRACKING BUG FOR MS CHECKCONFIG ALERT IMPROVEMENT\n\nBUG 25368613 - TRACKING BUG FOR MS CHECKCONFIG ALERT IMPROVEMENT",
    "platform": null
  },
  {
    "test_name": "tsagbug25393979.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug25393979.tsc - ENSURE MINED ALERTS ARE RAISED IN THE ORDER THEY OCCURRED\n\nBUG 25393979 - ENSURE MINED ALERTS ARE RAISED IN THE ORDER THEY OCCURRED\n\nBUG 25393979 - ENSURE MINED ALERTS ARE RAISED IN THE ORDER THEY OCCURRED",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug25737783.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug25737783.tsc - DI:MS'S JAVA PROCESS TRACE SHOULD BE EXCLUDED FROM PURGE LOG HOME\n\nDo not purge current ms_server.trc from $LOG_HOME",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug26320141.tsc",
    "setup": "tsaginit",
    "flags": {
      "log_file": "tsagbug26320141.log"
    },
    "description": "tsagbug26320141.tsc - Test for bug 26320141\n\nTestcase details:\n      - 1. after MS deploy(after patching), $LOG_HOME/*jetty.log should have process call path.\n\n       Example\n        [Sun Jun 26 18:02:50 MDT 2016] startWebLogic.sh is run by user:root\n         call path: 15686(setup_dynamicDe)<-16151(setup_dynamicDe)<-16114(bash)<-14448(sshd)<-1(sshd)",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug26368544.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagimdef",
      "log_file": "^tst_tscname^"
    },
    "description": "tsagbug26368544.tsc - Bug 26368544-CC2:ORA-07445: [KDR9IR2RST0()+16]\n\n1.Have a uncompressed table\n    2.Delete every other row\n    3.Run a scan to populate CC2",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug26504172.tsc",
    "setup": null,
    "flags": {
      "oss_no_asmdb": "true"
    },
    "description": "tsagbug26504172.tsc - Test for BUG 26504172\n\nTests httpsAccess attribute of cellcli and dbmcli",
    "platform": null
  },
  {
    "test_name": "tsagbug26504172_hw.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "slcm01cel13",
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagbug26504172_hw.tsc - Test for BUG 26504172\n\nTests httpsAccess attribute of cellcli",
    "platform": null
  },
  {
    "test_name": "tsagbug26555606.tsc",
    "setup": "srdbmsini",
    "flags": {
      "debugcli": "true",
      "oss_auto_manage_disks": "true",
      "sage_mirror_mode": "normal",
      "sysdba": "'sys/knl_test7 as sysdba'"
    },
    "description": "tsagbug26555606.tsc - TEST FOR ACTIVE GRID DISK WAS NOT ADDED TO THE DISK GROUP\n\nBug-26555606 : Exadata auto mgmt did not add back the active griddisk\n     Test Steps:\n      - use writeback flashcachemode for the cell, normal redundancy\n- pick one griddisk GD_a, no need to be flash griddisk, normal data disk is fine.\n- check its cachedby attribute, it should have a flash celldisk name, let's say FD_b\n- shutdown cellsrv\n- simulate failed state for FD_b disk\n      - startup cellsrv\n      - wait for a little bit and check if datadisks are online on asm side\n      - Re-enable the failed flash disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug26585956.tsc",
    "setup": "srdbmsini",
    "flags": {
      "CELL_SECCOMP_MODE": "'\"permissive_lrg\"'",
      "log_file": "tsagbug26585956.log"
    },
    "description": "tsagbug26585956.tsc - Testcase for bug 26585956 which is related refCount leak\n\nTestcase for bug 26585956 which is related refCount leak",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug26593412.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagbug26593412.tsc - Test for Bug 26593412\n\nLEAVE GRIDDISK IN ACTIVE STATE EVEN IF REACTIVATION FAILS DURING PATCHING",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug26657606.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_no_asmdb": "true",
      "noecho_testname": "true",
      "num_flash_per_cell": "2",
      "duration": "300 # run IOV for 5 minutes",
      "threads": "3   # number of threads per each IOV process",
      "area_sz": "40   # size of each area on gd for an IOV process, in MB",
      "sleep_btw_test": "120",
      "max_areas": "2  # max number of areas on each device",
      "creatdev_file": "tsagrddef",
      "cfgdir": "tsagfciov.sav"
    },
    "description": "tsagbug26657606.tsc -Test for bug 26657606: FLUSH NORMAL FDOM FAILED\n\nIf flash A caches for flash B (ie. for FcLogging), and flash B\n   subsequently fails, and cellsrv then restarts, then flushing\n   flash A may fail because flash B is inactive.\n\nusing BLOCKIO_ALL_HANG for fclogging (write cancellation)",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug26724112.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug26724112.tsc - Test Case for BUG 26724112",
    "platform": null
  },
  {
    "test_name": "tsagbug26733242.tsc",
    "setup": "srdbmsini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true",
      "DEBUGCLI": "1"
    },
    "description": "tsagbug26733242.tsc - 26733242 - UNNECESSARY FC BOOTSTRAP DUE TO DEV REENUMERATION UPON REBOOT\n\n26733242 - UNNECESSARY FC BOOTSTRAP DUE TO DEV REENUMERATION UPON REBOOT\n\n26733242 - UNNECESSARY FC BOOTSTRAP DUE TO DEV REENUMERATION UPON REBOOT",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug26805359.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagbug26805359.tsc - Test for Bug 26805359\n\nBUG 26805359 - ADD SUPPORT TO TOGGLE THE CHECK OF SUPPORTED DISK MODEL",
    "platform": null
  },
  {
    "test_name": "tsagbug26822910.tsc",
    "setup": null,
    "flags": {
      "oss_auto_manage_disks": "true",
      "oss_devdir1": "^T_WORK^/raw^oss_port^"
    },
    "description": "tsagbug26822910.tsc - Test for Cell disk replacement.\n\nTest with multiple test cases to check for cell disk replacement in different situations.",
    "platform": null
  },
  {
    "test_name": "tsagbug27006486.tsc",
    "setup": null,
    "flags": {
      "test_node_name": "^TST_EXE_RESULT^"
    },
    "description": "tsagbug27006486.tsc - Exadata test for Bug 27006486\n\nBug 27006486 - EXAPAAS:12.2.1.1.2:SLCS19ADM0506:DOM0 UPGRADE FAILURE - REBOOT FAILURE",
    "platform": null
  },
  {
    "test_name": "tsagbug27395427.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "scas15celadm08",
      "cellconnstr": "root@^cell^.us.oracle.com"
    },
    "description": "tsagbug27395427.tsc - Test for bug 27395427\n\nTest for BUG 27395427",
    "platform": null
  },
  {
    "test_name": "tsagbug27438679.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug27438679.tsc - Bug test for 27438679\n\n1. Create an environment with griddisks and also add them to the ASM.\n     2. cellcli -e list griddisk attributes name, asmmodestatus, asmdeactivationoutcome\n     3. Drop the diskgroup with force option at ASM.\n     4. Shutdown the ASM instance\n     5. Manually drop the griddisks at cellsrv using \"cellcli -e drop griddisk <gdname> force\" for all griddisks\n     6. Restart the ASM instance\n     7. Recreate the griddisks using cellcli with the same name, size etc as in Step-1\n     8. Create the diskgroups (with same name)\n     9. Manually add the griddisk at ASM.\n     10. Check if the griddisk are not auto dropped at ASM (ASM should not autodrop the griddisks)\n          - Wait for few seconds and then check sql> select name, state from v$asm_disk;\n          - cellcli -e list griddisk attributes name, asmmodestatus, asmdeactivationoutcome\n            the attributes \"asmmodestatus, asmdeactivationoutcome\" of griddisks should be correct with ASM instance",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug27465984.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug27465984.tsc - Multiple-prefix & Parallel GridDisk Add/Drop\n\nMultiple-prefix & Parallel GridDisk Add/Drop",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug27834214.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug27834214.tsc - Test is fix for Bug 27834214.\n\nThis is the test for transaction to clean SNMP subscribers both in MS and on ILOM as a fix for Bug 27834214.",
    "platform": null
  },
  {
    "test_name": "tsagbug27840850.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug27840850.tsc - Bug test for 27840850 and 28030597\n\nBug 28030597 test steps:\n     1. Setup ASM security with defaulty the same key in cell side and cellkey.ora\n     2. Set availableTo to null for all GDs\n     3. Change the cell key for ASM cluster\n     4. Set availableTo to new key for all GDs\n     5. Check the workload with new key, it should not success\n     6. Reset the cluster key on cellsrv\n\n     Bug 27840850 test steps:\n     1. Setup ASM security with defaulty the same key in cell side and cellkey.ora\n     2. alter all GDs to inactive status\n     3. Change the cell key for ASM cluster\n     4. alter all GDs to active status\n     5. Check the workload with new key, it should succeed\n     6. Reset the cluster key on cellsrv",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug28194173.tsc",
    "setup": "tsaginit",
    "flags": {
      "cell_offload_plan_display": "auto",
      "file_dest": "'+DATAFILE'",
      "creatdev_file": "tsagrddef",
      "nflint": "1"
    },
    "description": "tsagbug28194173.tsc - \"SQL ON 12.2 QUARANTINED AND CELL THROWING ORA-00600 [KUTYXTTE_TRANSFORM:FAILED]\"\n\nThis test runs against RDBMS_MAIN, RDBMS_12.2.0.1.0DBRU Sparc64, and OSS_MAIN Linux.x64 DB.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug28329897.tsc",
    "setup": "tsaginit",
    "flags": {
      "log_file": "tsagbug28329897.log"
    },
    "description": "tsagbug28329897.tsc - deadline quota from multiple db test\n\nThis test simulates IO access from multiple databases and verifies if\n     deadline scheduling is working",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug28407929.tsc",
    "setup": null,
    "flags": {
      "dbnode": "^compute_node^",
      "dbnodeconnstr": "root@^compute_node_f^"
    },
    "description": "tsagbug28407929.tsc - CPU failure simulation\n\niaasmode should be ON after cpu faiure incident",
    "platform": null
  },
  {
    "test_name": "tsagbug28716488.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug28716488.tsc - PREDICATECACHEPUT WAS IDLED FOR OVER 5 MINS LEADING TO CLIENT VRPC TIMEOUT\n\nThis test checks for the presence of a race condition due to timezone\n      caching. Two jobs are issued simultaneously. The first job caches the\n      timezone file, but is forced by the sim event to sleep for one minute\n      before completing. The second job should wait for the first job to\n      return before completing, but should not reach an error state where it\n      times out. If either job returns in < 1 minute, then the timezone\n      caching conditions were set up improperly.\n     The test steps are as follows:\n     1.setup env srdbmsini\n     2.set underscore parameter in cellinit.ora\n     3.disable offloading for DB instance system\n     4.In sql, create a large sample data table with date/time values\n     5.Restart cellsrv\n     6.Enable simulation event in cellcli: PREDCACHEPUT_TIMEOUT\n     7.Open two SQL terminal sessions and invoke it together\n     8.If either query takes less than 1 minute or longer than 5 minutes to execute, then the test should fail.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug28815788.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug28815788.tsc - Test for bug 28815788 - MEMORY LEAK IN MS LOGIN",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug28877886.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagccdef",
      "file_dest": "'+DATAFILE'"
    },
    "description": "tsagbug28877886.tsc - Functional test for bug 28877886.\n\nFunctional test for bug 28877886.\n     Test step:\n       1. set _cell_imcpop_max_req_with_io_buf_percent=0 and _cell_region_heuristics_read_count_threshold=0,\n          restart cell services\n       2. Take a snapshot of JobIOContext allocations and look for JobIOContext allocation messages\n       3. Keep heavy workload to keep FC busy, check CC2 population stats,\n          expected to see the pop request does not have IO buffer and during reissuing\n       4. Running the workload for 10 mins and stop all workloads and shutdown DB/ASM\n       5. Take another snapshot of JobIOContext,\n          expected to see the numFreeObjects == numObjects(or difference less than 100)",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug29202850.tsc",
    "setup": "tsagnini",
    "flags": {
      "oss_testing": "2",
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagbug29202850.tsc - test for bug 29202850 - crash seen due to memory leak\n\nTests for bug 29202850 - On getting unsuccessful ASM translations, a\n     crash was seen due to not freeing up memory regions.\n     The biggest memory leak seen was \"RC:KCFIS RamcacheMetadata\".",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug29305661.tsc",
    "setup": null,
    "flags": {
      "cell": "slcm01cel12",
      "oss_hw_testing": "1"
    },
    "description": "tsagbug29305661.tsc - Test for bug 29305661\n\nTest for bug-29305661:  CELL CREATING ALERTS FOR ERROR \"MESSAGE IS TOO LARGE FOR AN SNMP PACKET\" .",
    "platform": null
  },
  {
    "test_name": "tsagbug29492241.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "scas15celadm08"
    },
    "description": "tsagbug29492241.tsc - Test for BUG-29492241 for cell node.\n\nChecks that if any ms process has been hung for 5 mins ,\n     that process is killed.",
    "platform": null
  },
  {
    "test_name": "tsagbug29492241_db.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "dbnode": "root@scas15adm07"
    },
    "description": "tsagbug29492241_db.tsc - Test for BUG-29492241 for DB node\n\nChecks that if any ms process has been hung for 5 mins ,\n     that process is killed.",
    "platform": null
  },
  {
    "test_name": "tsagbug29589243.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug29589243.tsc - test for cc2 with max_string_size extented\n\nQuery the large column and check CC should succeed\n\nthere is a Bug 31109601 - CC2 WRONG CHARACTER WHEN MAX_STRING_SIZE=EXTENDED still open for this test\n     keep disable until the bug fixed, after fix can add to lrgdbconsacfcbug2",
    "platform": null
  },
  {
    "test_name": "tsagbug29712885.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug29712885.tsc - OEDA test for bug 29712885.",
    "platform": null
  },
  {
    "test_name": "tsagbug29713930.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "sage_mirror_mode": "high"
    },
    "description": "tsagbug29713930.tsc - 29713930 - CELLSRV TO FACILITATE AUTO-UPGRADE ALL EXASCALE COMPONENTS\n\ntest the software update code path\n\ntest to be added in lrgsaexacldcellupgrade",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug29713930_2.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug29713930_2.tsc - 29713930 - CELLSRV TO FACILITATE AUTO-UPGRADE ALL EXASCALE COMPONENTS\n\nThe test in an enhancement to the above bug.\n\nTest will be merged with rimircha_fix_pooldisk_status which updated the\n   disk status population logic to filter out disks based on an EGS isalive\n   check that allows the upgrade to proceed in case EGS is found to be dead.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug29774756.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug29774756.tsc - Utilize PMEMCache fully\n\nTests the new PMEM caching algorithm as implemented in\n      vijanaga_bugfix-29774756",
    "platform": null
  },
  {
    "test_name": "tsagbug29796055.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug29796055.tsc - Unit test for bug 29796055",
    "platform": null
  },
  {
    "test_name": "tsagbug29821967qos.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true"
    },
    "description": "tsagbug29821967qos.tsc - run orion with -islgwr after qos failover\n\ntest case for bug 29821967:we should not see crash while running orion\n   after qosfailover\n\ntest to be added in lrgrhx7saipcdatqos",
    "platform": null
  },
  {
    "test_name": "tsagbug3.tsc",
    "setup": "tsagnini",
    "flags": {
      "oss_testing": "1"
    },
    "description": "tsagbug3.tsc - Test case for bug 9780796 and bug 9151718\n\nTest case for bug 9780796\n\nCellsrv used to crash with an assert in skgxpStats_RegisterEPID:Failed\n     when hostname of a compute node was changed. With fix for this\n     bug this should no longer happen.\n\n     Test case is as follows:\n     1). Cell, ASM (and DB if needed) are running.\n         At this point diskmon would have connected using the\n         hostname of the machine where the test is running.\n\n     2). Create $T_WORK/myhostname to have `hostname` appended\n         with the domain name\n            \"reallylongdomainname.us.oracle.com\"\n         even if the hostname already has a domain name.\n         The reason for setting a really long hostname is\n         to make sure that cellsrv doesn't crash due to\n         bug 9151718.\n\n         Code has been modified to look for this file and if\n         present will take precendece over procuring host name\n         from the OS.\n\n     3). Kill diskmon. Agent should automatically restart it\n         and it should connect to cell with the new hostname\n         set in $T_WORK/myhostname\n\n     4). Verify that cellsrv doesn't crash.\n         TODO: We could also compare metrics snapshot before\n               and after.\n\n     5). Remove $T_WORK/myhostname\n\n     6). Kill diskmon. Agent should automatically restart it\n         and it should connect to cell with the actual hostname.\n\n     7). Verify that cellsrv doesn't crash.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug30073295.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug30073295.tsc -ADD SUPPORT FOR CELLSRV TO CHECK INTERNAL SYSTEM",
    "platform": null
  },
  {
    "test_name": "tsagbug30215542.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug30215542.tsc - EMAIL TEST FOR MS ALERT",
    "platform": null
  },
  {
    "test_name": "tsagbug30271145.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "sunservice": "sunservice@^cell_name^-c.us.oracle.com",
      "password": "changeme",
      "tstlog": "^tst_tscname^.log"
    },
    "description": "tsagbug30271145.tsc - Test for Bug 30271145\n\nTests DIMM replacement and also checks PMEM replacement alerts",
    "platform": null
  },
  {
    "test_name": "tsagbug30464655.tsc",
    "setup": "tsagnini",
    "flags": {
      "max_instance": "1",
      "nflint": "0"
    },
    "description": "tsagbug30464655.tsc - interop test for commit cache\n\ninterop test for commit cache BUG 30464655",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug30481821.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug30481821.tsc - Metric in HD only cell\n\nDo not collect FC related metrics when FD device does not exist. This\n     expect X8 XT cell config.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug30495040.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_testing": "1",
      "oss_no_asmdb": "true",
      "nflint": "1"
    },
    "description": "tsagbug30495040.tsc - bug test\n\nThis bug test validates a new ASM mirror hint parameter to Orion.\n     This will allow testing the existing FC functionality which prevents\n     caching of any IO not marked as \"primary\" or \"activesecondary\".\n\n     It is a functional test fo writeThrough FC, where we try to populate\n     FC using an Orion workload with \"primary\" (which should show FC getting\n     populated. The current and default behavior), and compare it to trying\n     to populating FC using an Orion workload with a \"null\" mirror hint, which\n     should show FC not getting populated at all.\n\nThis bug test validates a new ASM mirror hint parameter to Orion.\n     This will allow testing the existing FC functionality which prevents\n     caching of any IO not marked as \"primary\" or \"activesecondary\".\n\n     It is a functional test fo writeThrough FC, where we try to populate\n     FC using an Orion workload with \"primary\" (which should show FC getting\n     populated. The current and default behavior), and compare it to trying\n     to populating FC using an Orion workload with a \"null\" mirror hint, which\n     should show FC not getting populated at all.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug30512925.tsc",
    "setup": null,
    "flags": {
      "loop_num": "3"
    },
    "description": "tsagbug30512925.tsc - Run base test with/without asm/db instance\n\nRun base test with/without asm/db instance\n\n1. Setup cell and DB and base table for CC workload.\n    2. create writeback flashcache;\n    3. Check \"list griddisk attributes name, cachingPolicy, cachedBy\" after\n       sleeping 2 minutes. All griddisks should have cachedBy assigned.\n    4. run scan query of the CC table; and check flashcacheContent. Table\n       should have ColumnarCachedSize.\n    5. alter flashcache all flush;\n    6. Check \"list griddisk attributes name, cachingPolicy, cachedBy\" after\n       sleeping 2 minutes. All griddisks should have cachedBy empty.\n    7. drop flashcache.\n    8. recreate writeback FC.\n    9. repeat 3~4.\n\n    Usage: mode - if it is CC2, then will run the bug test with CC workloads\n                  and asm/db are up\n    Usage:        if it is null, it will not setup asm/db instance\n           runtest tsagbug30512925\n           runtest tsagbug30512925 mode=CC2",
    "platform": null
  },
  {
    "test_name": "tsagbug30543312.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug30543312.tsc - auto creation of griddisk on flashcache test\n\nMake sure disk creation go well if MS restart in middle of disk sync",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug30585062.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug30585062.tsc - Test Case for bug 30585062\n\nTest Case for bug 30585062\n\nTest Case for bug 30585062, the flows is as below:\n\n     1. setup X6 HC simulation by tsagx6config.tsc\n     2. Force predictive failure of m2 device\n       $DEBUGCLI -alter -m2 0 -p status -v \"PREDFAIL\"\n       $DEBUGCLI -ls -m2  -p status\n       cellcli -e list alerthistory\n     3. Shutdown celld and replace m2\n       $DEBUGCLI -rm -m2 0 -path $T_WORK/raw/m2_0 -slot 0\n       $DEBUGCLI -add -m2 0  -path $T_WORK/raw/m2_0 -slot 0\n     4. Restart celld and see the alert is cleard, no remaining alert.",
    "platform": null
  },
  {
    "test_name": "tsagbug30596196.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagcfcdef",
      "oss_testing": "1",
      "log_file": "tsagbug30596196_fake.log"
    },
    "description": "tsagbug30596196.tsc - TestCases for bug 30596196\n\nTestCases for bug 30596196\n\nThis is a new feature in Exadata 20.1 release, which is about postponing\n     creating sparse hash table until the first time we create sparse grid\n     disk. Sparse hash table takes up about 12 GB on a real cell node. Thus\n     itâs possible the creation fails when thereâs no enough memory. The code\n     transaction - yichewan_bug_30596196 tries to drop the offload servers\n     first to release memory, and then retries the creation. Offload groups\n     would be brought back after that. You can have more information here:\n           https://orareview.us.oracle.com/84529514\n     To prove the correctness of this feature, we add 3 new cases to cover it:\n       1. Thereâs enough memory, sparse grid disk creation succeeds at the\n          first try, and does not do the recreation\n       2. Thereâs little memory, sparse grid disk creation fails again even\n          after dropping offload groups\n       3. The creation fails at the firs time, but succeeds at the second try",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug30740145.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug30740145.tsc - test for bug 30740145\n\nwriting back to VES from local linux file for edsfstool\n     and\n     It also adds tests for the folllowing scenarios -\n     1.min_used_size for a Read-only Snapfile and clonefile is 0\n     2.If RO flag for clonefile is unset, the min_used_size is 10% of provisioned size\n     3. Validate the error thrown when we try to resize a snapfile.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug30778938.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug30778938.tsc - Test for bug 30778938\n\nTest valid and invalid EXACLI request with following combination\n     Old EXACLI -> new MS\n     New EXACLI -> new MS",
    "platform": null
  },
  {
    "test_name": "tsagbug30892332.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "eswallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagbug30892332.tsc - test for bug 30892332",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug30899733.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagbug30899733.tsc - Test for bug 30899733\n\nTest 1: Hung Flash on X3/X4\n       Create flash disk with DebugCLI with hw_ver set to 2\n Simulate a hang on one of the flash devices\n Only that particular flash device failed\n And other flash devices are fine\n \"LIST PHYSICALDISK\" will hang a bit due to the underneath hang but it will come back.\n Un-simulate a hang\n\n     Test 2: Hung NVMe on later platforms\n       hw_ver should be 3",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug30924692.tsc",
    "setup": "srdbmsini",
    "flags": {
      "SYS_GROUP_NAME": "^sysoflgrp^"
    },
    "description": "tsagbug30924692.tsc - bug test for 30924692\n\nCC2 chainrow bug test\n     There is an issue in the handling of 9ir2 block chained row pieces with a permutation vector",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug30924915.tsc",
    "setup": null,
    "flags": {
      "log_file": "tsagbug30924915.log"
    },
    "description": "tsagbug30924915.tsc - stress test for CC2 RH\n\ncheck with cellsrvstat and dump trace file",
    "platform": null
  },
  {
    "test_name": "tsagbug30998236.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug30998236.tsc - Test for bug 30998236\n\nTest 1: check whether mmap memory is reducing when grid disk is shrinked\n     Test 2: Run IOV in background and shrink griddisk,\n             expects logs - \"I/O is outside disk space range\"",
    "platform": null
  },
  {
    "test_name": "tsagbug31024636.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug31024636.tsc - TEST REQUEST FOR BUG 30960085\n\nTEST REQUEST FOR BUG 30960085 - VMCORE FILE NOT PRODUCED AFTER KERNEL CRASH",
    "platform": null
  },
  {
    "test_name": "tsagbug31045790.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug31045790.tsc - Test case for Bug 31045790\n\nTest case to check for Bug 31045790\n\nWhen a DG is dismounted while patching happens there will be a hang in the patching process.\n     With fixm there will be an error when completing the upgrade process if a DG is dismounted",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug31066155.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug31066155.tsc - RS startup fails  if cell accounts are not present in /etc/group",
    "platform": null
  },
  {
    "test_name": "tsagbug31110714.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug31110714.tsc - Bug test for 31110714\n\nBug test for 31110714",
    "platform": null
  },
  {
    "test_name": "tsagbug31170489.tsc",
    "setup": null,
    "flags": {
      "program": "^T_WORK^/dcli.py",
      "cell_list": "^T_WORK^/cell_list.lst"
    },
    "description": "tsagbug31170489.tsc - Testcase for bug 31170489 (simplify dcli key setup)\n\nTestcase for bug 31170489 (simplify dcli key setup)\n\nTestcase for bug 31170489 (simplify dcli key setup)",
    "platform": null
  },
  {
    "test_name": "tsagbug31201082.tsc",
    "setup": null,
    "flags": {
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'"
    },
    "description": "tsagbug31201082.tsc - testcase for bug 31201082\n\ntestcase for bug 31201082, code txn - liaowang_bug-31201082\n     The basic test flow is:\n       1. create 2 EHCC tables and load some data into them\n       2. delete the max/min value for these tables\n       3. RESTART CELLSRV\n       4. RUN QUERIES TO VERIFY SI SAVINGS for max/min functions, both of them should see\n          the SI saving\n\n     And also covers below conbinations:\n        1. uncompress table->SI\n        2. OLTP table->SI\n        3. EHCC table->SI\n        4. CC1 table->SI\n        5. CC2 table->SI\n        6. OLTP->CC1->SI, OLTP->CC2->SI\n        7. EHCC->CC1->SI, EHCC->CC2->SI\n\ntestcase for bug 31201082",
    "platform": null
  },
  {
    "test_name": "tsagbug31214467.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug31214467.tsc - test for bug 31214467: PROBLEMS IN DEPLOYING A DIAG OFFLOAD SERVER\n\n1- Install a cell rpm:\n 2- Check which offload groups we have along with the cell rpm by\n    default (should be 3)\n 3- Check which offload package we have along with the cell rpm by default\n 4- Install an additional user cell oflsrv rpm (with a different date):\n 5- Check offload groups again to make sure no new offload group\n    is created now\n 6- Check offload package again, we should have one new offload\n     package, and it should be private:\n 7- Now try to create a new user offload group with the additional package:\n 8- Wait for a while, and check offload group again. We should be\n    able to see the new offload group is created, and it is running:\n\ntest to be added in lrgrhx9sams1",
    "platform": null
  },
  {
    "test_name": "tsagbug31215177.tsc",
    "setup": "srdbmsini",
    "flags": {
      "uniq_dsknames": "all",
      "oss_auto_manage_disks": "true",
      "sage_mirror_mode": "high",
      "cdb": "true",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug31215177.tsc - Bug 31215177 : ADD TEST FOR BUG 31099116\n\nBug 31099116 - SAGEASM-E NON-PARTNER DISK OFFLINE CAUSE DOUBLE\n#                   FAILURE AND DISKGROUP DISMOUNT FORCE\n\ntest to be added in lrgdbconsamsbug16",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug31247087.tsc",
    "setup": "tsaginit",
    "flags": {
      "cell_with_pmem_cache": "0",
      "cell_with_xrmem_cache": "0",
      "asm_disk_def": "tsagrdmadef",
      "log_file": "tsagbug31247087.log"
    },
    "description": "tsagbug31247087.tsc - Large RedoLog write not bypass Flashcache Test\n\nLarge RedoLog write not bypass Flashcache Test\n\nLarge RedoLog write not bypass Flashcache Test\n     below is test steps:\n\n       1. create one data griddisk and writeback flashcache. have FC larger\n          than griddisk size.\n       2. create orion.lun and run 4KB, 8KB and 1M write.\n          (1) example for logwriter 8KB write.\n          (2) example for logwriter 1MB write.\n          (3) example for mixed 8KB/1MB write.\n          (4) example for 4KB write.\n          (5) example for ADVM write.\n       3. After orion test, check cellsrvstat, all the lgwr writes shall be\n          cached and there is no bypass.\n        \"Number of redo log write to cache\" --> shall be non-zero\n        \"Total size of redo log write to cache (KB)\" --> shall be non-zero\n         \"Number of redo log write bypass cache\" --> shall be 0\n         \"Total size of redo log write bypass cache (KB)\" --> shall be 0",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug31249535.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "scas15celadm08"
    },
    "description": "tsagbug31249535.tsc - Test for ilom syslog forwarding\n\nsupport to configure ILOM syslog forwarding\n     New cell attribute -  ilomSyslogClients",
    "platform": null
  },
  {
    "test_name": "tsagbug31319266.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug31319266.tsc - Test automatic diagpack purging",
    "platform": null
  },
  {
    "test_name": "tsagbug31319266_db.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug31319266_db.tsc - Automatic diagpack purge test on real hardware dbnode",
    "platform": null
  },
  {
    "test_name": "tsagbug31369749.tsc",
    "setup": "tsagnini",
    "flags": {
      "uniq_dsknames": "all",
      "sage_mirror_mode": "normal"
    },
    "description": "tsagbug31369749.tsc - one disk permanent hang with another temporary\n#                         hang disk caused cell reboot - Bug 31369749\n\n[1] Test case: IO error disk is excluded from reboot check\n    [2] Test case: IO error trigger ignore confinedOffline CD for\n                   hang time.\n    test case description at the start of each test.\n\ntest to be added in lrgdbconsadebugcli15",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug31417285.tsc",
    "setup": "tsagnini",
    "flags": {
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagbug31417285.tsc - Test for bug 31417285\n\nPurpose of the bug fix:\n       1. Quickly detect the powered-off physical hard disk and actupon it so that the whole powercycle will be faster which will reduce the possibility of errors because of pinned cache not flushed.\n       2. If a Hard disk powering off takes longer than the certain time (by default it is 90% of the population cycle time = 90 seconds), then MS will fail that particular disk.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug31421532.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug31421532.tsc - Test for bug 31421532\n\nTest ensures that alert mail is sent even if the diagpack size\n     is above limit. This diagpack will be attached along with alert mail\n     when size is below limit. If size is above limit, mail will be sent\n     in a 2nd attempt, without attachment.",
    "platform": null
  },
  {
    "test_name": "tsagbug31435082.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug31435082.tsc - test for bug 31435082 SI eviction",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug31439012_1.tsc",
    "setup": null,
    "flags": {
      "cdb": "true",
      "x5_hc_setup": "true",
      "nflint": "true",
      "sage_mirror_mode": "normal",
      "cell_with_pmem_cache": "0",
      "oss_failgroup": "failalldbdg",
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagbug31439012_1.tsc - Test for enhancement 31439012\n\nTests for alter PD drop for replacement MAINTAIN REDUNDANCY",
    "platform": null
  },
  {
    "test_name": "tsagbug31439012_2.tsc",
    "setup": null,
    "flags": {
      "cdb": "true",
      "sage_mirror_mode": "high",
      "x7_hc_setup": "true",
      "oss_failgroup": "failalldbdg",
      "allow_pmem_gd": "true",
      "differentszfg": "true",
      "dangerousfg": "true",
      "asm_ausize": "1048576",
      "cellname": "thirdcell",
      "asmdisks_created": "2",
      "nflint": "1",
      "numlogfiles": "2",
      "do_not_set_numlogfile": "true"
    },
    "description": "tsagbug31439012_2.tsc - Test for enhancement 31439012\n\nTests for alter PD drop for replacement MAINTAIN REDUNDANCY\n      - Flash & PMEM",
    "platform": null
  },
  {
    "test_name": "tsagbug31442122.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug31442122.tsc -  test for bug 31442122, OCl RH and cc support ves virtual addr del\n\nOCL RH AND CC SUPPORT VES VIRTUAL ADDRESS DELETION",
    "platform": null
  },
  {
    "test_name": "tsagbug31468845.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug31468845.tsc - test for bug 31468845\n\ntest aggregation push down for large CC2 table",
    "platform": null
  },
  {
    "test_name": "tsagbug31470720.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug31470720.tsc - Test for bug 31470720\n\nAdditional: Bug 31470720 - SCALING SNAPSHOT AND BACKUPS LIST: NEED PAGING SUPPORT FOR LIST BACKUP AND SNAPSHOT",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug31513116.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug31513116.tsc - Test for bug 31513116",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug31518516.tsc",
    "setup": null,
    "flags": {
      "cellconnstr": "root@^cell3f^"
    },
    "description": "tsagbug31518516.tsc - Test case for bug-31518516",
    "platform": null
  },
  {
    "test_name": "tsagbug31560651.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal"
    },
    "description": "tsagbug31560651.tsc - test for BUG 31560651 - ADD CHECK FOR SERVICE REDUNDANCY IN EGS\n\nThe test description is given below: This test adds check for\n    cellsrv, usereds, syseds, ers and egs\n\nTest to be added in lrgsaexacldcellupgrade3",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug31560651_2.tsc",
    "setup": "xblockini",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal"
    },
    "description": "tsagbug31560651_2.tsc - test for BUG 31560651 - ADD CHECK FOR SERVICE REDUNDANCY IN EGS\n\nThe test description is given below: This test adds check for BSM, BSW and cell offline\n\nTest to be added in lrgsaexacldcellupgrade2",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug31604359.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug31604359.tsc - Test case for checking volume rate limitting\n\nAdditional: Volume rate limiting. Bug 31604359 - EXASCALE: RATE LIMITING SHOULD BE ON A VOLUME, NOT ATTACHMENT",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug31614878.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "cell1_f": "^cell1^.us.oracle.com",
      "cellconnstr": "root@^cell1_f^"
    },
    "description": "tsagbug31614878.tsc - Test for bug 31614878",
    "platform": null
  },
  {
    "test_name": "tsagbug31636193.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "dbnode": "root@scas15adm04"
    },
    "description": "tsagbug31636193.tsc - Checks that diagpack has dbserver directory\n\n1. Checks if dbserver directory is present in custom diagpack\n     2. Checks if dbserver directory is present in alert diagpack",
    "platform": null
  },
  {
    "test_name": "tsagbug31642424.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug31642424.tsc - testcase for bug 31642424\n\ntestcase for bug 31642424\n\ntestcase for bug 31642424",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug31644590.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tstbug31644590.tsc - this is for bug 31644590, celltest with timezone file",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug31661455.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug31661455.tsc - Test case for bug 31661455",
    "platform": null
  },
  {
    "test_name": "tsagbug31663198.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "dbnode": "^compute_node_f^",
      "dbnodeconnstr": "^dbconnstr^"
    },
    "description": "tsagbug31663198.tsc - test for bug 31663198: DB NODES IGNORES SUSPEND_CHECK FLAGS ON DISK DRIVE\n\n- downgrade the disks' FW\n   - simulate disks' replacement\n   - restart MS\n   - Make sure it detects disks' replacement and triggers FW upgrade\n\ntest to be added in lrgrhx3firmwdown",
    "platform": null
  },
  {
    "test_name": "tsagbug31700288.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug31700288.tsc - test for bug 31700288\n\nCreate many snapshots one by one",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug31705501.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagbug31705501.tsc - WLS FAILS TO START IN BDSQL 4.1 INSTALLATION\n\nMS startup is failing because gid lookup for the MS<->CLI socket fails\n     because of a group with large number of members.",
    "platform": null
  },
  {
    "test_name": "tsagbug31707729.tsc",
    "setup": "tsagnini",
    "flags": {
      "format_long_identifier": "true"
    },
    "description": "tsagbug31707729.tsc - Add test case for bug 31707729\n\ntest for celldisk metadata scrubbing\n\ncorrupt celldisk metadata and it should work fine",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug31724205.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug31724205.tsc - Error message of RS restart\n\nDBMCLI should not show ip port such as 9903 in error message",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug31816888.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1"
    },
    "description": "tsagbug31816888.tsc - OOM on MS startup test\n\nEnforce OutOfMemory error by using large persisted metric file\n     MS raises OutOfMemory error on startup if it reads the current very large metric file.\n     To simulate this, prepare sample metric archive, rename it with current date to mimic the latest file.\n     On startup, if MS has the ability to remove this file at OOM state, the file is removed. This may fail\n     because JVM could be unstable state at OOM exception. So, I added recheck logic in this test.\n     120 sec sleep X 5 , hope this will remove the file after several MS restart.\n     Successfull behavior should be MS restart with no OOM by purging the large metric by MS.\n\nRemove the large file on failure",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug31845073.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug31845073.tsc - Test for bug 31845073\n\nFail a flash disk and verify that diagpack has diagnostics #      regarding that failed disk.",
    "platform": null
  },
  {
    "test_name": "tsagbug32022401.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug32022401.tsc - Functional Test Simulation for Bug 32022401\n\nFunctional Test Simulation for Bug 32022401 on Fake Hardware",
    "platform": null
  },
  {
    "test_name": "tsagbug32022401_2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug32022401_2.tsc - Functional Test Simulation for Bug 32022401\n\nFunctional Test Simulation for Bug 32022401 on Real Hardware",
    "platform": null
  },
  {
    "test_name": "tsagbug32022509.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "nflint": "1",
      "differentszfg": "1",
      "dbconn": "'sys/knl_test7 as sysdba'",
      "asmconn": "'sys/knl_test7@inst11 as sysasm'"
    },
    "description": "tsagbug32022509.tsc - Test case for asmDiskGroupType attribute\n\nCheck the value of attribute in following tests:\n       1. Drop & Add the disks in the disk group\n       2. Drop & Recreate any one of disk groups",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug32022509_hw.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug32022509_hw.tsc - Upgrade/downgrade test  for bug 32022509\n\n1) check the redundancy of asm diskgroups\n     2) List griddisk attribute asmDiskGroupType and\n        it shows same redundancy as step 1\n     3) Downgrade cell rpm\n     4) Trigger Quarantines and list them - Test for bug 33509290\n     5) Upgrade to latest rpm\n     6) List quarantines and expect them to be empty - Test for bug 33509290\n     7) Repeat step 2",
    "platform": null
  },
  {
    "test_name": "tsagbug32057728.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug32057728.tsc - Unit Test for Bug tsagbug32057728\n\nWhen user deletes a file or a snapshot, EDS first pushes Snapshot Tree\n      info to cellsrvs, and then sends requests to cellsrvs to drop file\n     extents. If cellsrv restarts in the middle, it loses the SSTree info.\n     EDS automatically reconnects and re-sends drop file extents requests,\n     but it does not resends SSTree. This causes the  drop to fail",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug32188677.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug32188677.tsc - Test for bug tsagbug32188677.tsc\n\n1. Checks that sosreport is present in diagpack\n        Also check if \"cell\" dir is present and is not empty\n     2. Checks that sosreport is present in sundiag\n     3. Check that cell directory is present in alert diagpack\n     4. Check that celloflsrv traces are present in diagpack",
    "platform": null
  },
  {
    "test_name": "tsagbug32193510.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug32193510.tsc - Check if cellsrvstat can show very large number.\n\nrun LARGE_MEMT simulation event\n\nUse the same command which is used in cellmem.sh",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug32207003.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug32207003.tsc - Test checks whether the following hang is fixed : if we attach snapshot first then attach its volume with the same initiator id, and then restart BSW, the vip will never move to BSW because of a hang.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug32228446.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug32228446.tsc - The user having cl_monitor privilege can monitor devices created by other users.\n\nAdditional: Bug 32228446 - /EXASCALEAPI/V1/VOLUMEMETRICS ENDPOINT DOES NOT RETURN DATA WITH USER HAVING \"VLT_READ|CL_MONITOR\" PRIVILEGE. The user having cl_monitor privilege can monitor devices created by other users.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug32291469.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug32291469.tsc - Test for bug 32291469 - WRONG BROADCAST ADDRESS ASSIGNMENT FOR THE VIP INTERFACE BY THE BSW",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug32436767.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1"
    },
    "description": "tsagbug32436767.tsc - snmp, mail alert check",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug32452008.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_failgroup": "failalldbdg",
      "SAGE_MIRROR_MODE": "normal",
      "oss_auto_manage_disks": "true",
      "log_file": "tsagbug32452008.log"
    },
    "description": "tsagbug32452008.tsc - Test for bug BUG 32452008 - EXACS: CELL PATCHING\n                           FAILED WITH [ERROR] POST_MIGRATION FAILED\n\nPls see below\n\ntest added in lrgdbconsamsbug26",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug32472865.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug32472865.tsc - Test for bug 32472865\n\nCreate volume of size = 30T , create its attachment , volume snapshot\n     and backup and remove everything.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug32497666.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true"
    },
    "description": "tsagbug32497666.tsc - Test for bug 32497666\n\nTest steps:\n       1) Install a dbnode rpm from previous label\n       2) Ensure that both ms and rs are running\n       3) Remove all files in /opt/oracle/dbserver/dbms/ on real hw\n       4) Try to upgrade to current label rpm. Will failas ms/rs is up\n       5) Kill ms and upgrade. It will fail as rs is up\n       6) kill rs and upgrade. It will succeed.",
    "platform": null
  },
  {
    "test_name": "tsagbug32547291.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug32547291.tsc - Test case for bug 32547291\n\nTest case for bug 32547291\n     cellcli command returns null pointer exception when cellinit.ora is incorrect\n\nTest steps for below bug:\n     1. Bring up clean environment\n     2. Startup cellcli\n     3. Run \"list cell detail\" and confirm output\n     4. shutdown the cell server\n     5. modify cellinit.ora file, replace \"_cell_server_event=\" with \"events=\"\\\n     6. Restart cellcli\n     7. Run \"list cell detail\" and observe the exception",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug32557314.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "intc1": "re0",
      "intc2": "re1"
    },
    "description": "tsagbug32557314.tsc - Test for BUG - 32557314\n\nTest checks the fix for BUG-32557314 CELLSRV SHOULD BE AUTO\n                 RESTARTED WHEN RESOURCE BECOMES AVAILABLE AGAIN",
    "platform": null
  },
  {
    "test_name": "tsagbug32578019.tsc",
    "setup": null,
    "flags": {
      "imcfclv": "97"
    },
    "description": "tsagbug32578019.tsc - simple test for bug 32578019\n\nsteps for the test:\n     1: Use OLTP/uncompressed table CC2 test framework, disable CC2 system wide\n     2: restart the cellsrv\n     3: set the parameter events throught cellcli:\n        \"_cell_imc_pop_job_job_limit\" to 2\n        \"_cell_imcpop_max_req_with_io_buffer_percent\" to 0\n     4:  turn on CC2 in the current session\n     5:  scan the table\n     6:  wait for CC2 pop completion and check cell state",
    "platform": null
  },
  {
    "test_name": "tsagbug32625419.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug32625419.tsc - Ambiguous alert - ERROR ENCOUNTERED WHILE POPULATING DISK\n\nRemoval of alert ERROR ENCOUNTERED WHILE POPULATING DISK as it is not useful",
    "platform": null
  },
  {
    "test_name": "tsagbug32633966.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1"
    },
    "description": "tsagbug32633966.tsc - CELL.BIN FAILS DURING UPGRADE\n\nMS deployment might fail due to keystore generation errors\n    This test runs MS deployment in loop a number of times.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug32709432.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "max_iters": "20"
    },
    "description": "tsagbug32709432.tsc - Test for bug - 32709432\n\n32709432 - ORA600 EGSESNPROOT::BCASTDEADESNPFENCE::NOESNP OCCURRED\n               JUST AFTER OEDA STEP 9 [EXASCALE ON COMPUTE]\n\n   Test to check that there are no EGS crashes when ESNP is started\n    before the cluster is operational\n   Test Steps:\n    1. Setup exascale stack\n    2. The following steps are performed in a loop for 20 times-\n       a. Shutdown ESNP\n       b. Restart EGS Leader to trigger new leader election and SP reconfig\n       c. Wait until new leader is elected\n       d. Startup ESNP\n       e. Check for ESNP registration message in the egs traces",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug32711157.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug32711157.tsc - Test to verify the fix for bug32711157.\n\nIn this test, we are going to restart EGS in background before running various blockstore operations.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug32715879.tsc",
    "setup": null,
    "flags": {
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'"
    },
    "description": "tsagbug32715879.tsc - bug test\n\nbug test\n\nbug test",
    "platform": null
  },
  {
    "test_name": "tsagbug32721387.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug32721387.tsc - Testcase for bug 32721387\n\nVolume API failure test",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug32776647.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug32776647.tsc - test case for bug 32776647\n\ntest case for bug 32776647\n\ntest case for bug 32776647",
    "platform": null
  },
  {
    "test_name": "tsagbug32808152.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "Shutdown cellsrv if disk controller fails",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug32812549.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug32812549.tsc - Test for bug 32812549\n\nWith this bug fix, exacli session will not be\n     invalidated by ms restart.\n     Test cases :\n      Test 1 : MS restart doesnot invalidates a exacli session\n      Test 2 : MS will restore session file if its destroyed\n      Test 3 : MS will not be able to restore session file during its absence\n      Test 4 : Session will get invalidated if user logs out\n      Test 5 : Session will get invalidated if password expires\n      Test 6 : MS redeployment doesnot invalidates a exacli session",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug32812549_hw.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cellconnstr": "^dbnodeconnstr^",
      "hardware": "cell"
    },
    "description": "tsagbug32812549_hw.tsc - Test for bug 32812549\n\nTest case to verify that exacli session persists\n     across rpm upgrade.",
    "platform": null
  },
  {
    "test_name": "tsagbug32843838.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef"
    },
    "description": "tsagbug32843838.tsc - Test case for bug 32843838\n\nTest case for bug 32843838\n\nTest steps for below bug:\n     Bug# 32843838 - PREDICATIVELY FAILED FLASH CAUSES LGWR HANG / APPLICATION BLACKOUTS / INSTANCE CRASH\n\n     (1) create 2 grid disks and create WBFC with 2 flash stores;\n         (disable pmemcache/ramcache)\n     (2) list cachedBy of grid disks;\n     (3) Run concurrent orion small OLTP workload in background, make sure the orion\n         testing area is smaller than FC size, so that data would get 100% cached after some time;\n     (4) Simulate failureType=predictivefailure on one flash cache store;\n     (5) Check celldisk and flashcache status to ensure the flash store is in predicative failure.\n     (6) Check cachedBy, there should be no change.\n     (7) Check cellsrvstat IO stats delta, all read/write should continue going to FC.\n          Number of read hits -- non-zero\n          Number of cache writes -- non-zero\n          Number of nocache writes -- expected to be 0.\n     (8) Un-simulate the failure and re-enable the store.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug32854573.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug32854573.tsc - Test case for Bug 32854573",
    "platform": null
  },
  {
    "test_name": "tsagbug32885572.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug32885572.tsc - testcase for bug 32885572\n\ntestcase for bug 32885572\n\ntestcase for bug 32885572",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug32912597.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagbug32912597.tsc - Exascsale skip bootstrap on offline disk",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug32912597_1.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagbug32912597_1.tsc - Testcase 2 for bug 32912597\n\nInactivate a VESGD and restart cellsrv, the VESGD should skip bootstrap\n     and EGS should keep it as offline all the time.\n     When the lrg completes and does env clean up, the VESGD should be dropped\n     successfully without outstanding reference.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug32912597_2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagbug32912597_2.tsc - Test_case 3 for bug 32912597\n\nConfine a CellDisk and simulate read metadata IO error. When CellDisk\n     health state transitions back to GOOD, it triggers VESGD bootstrap which\n     should hit the metadata read IO error and fails the bootstrap. EGS\n     should offline the VESGD.\n     Confine the CellDisk again without simulating read metadata IO error,\n     when cellDisk health state transitions back to GOOD, VESGD bootstrap should\n     complete successfully.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug32913123.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagddef"
    },
    "description": "tsagbug32913123.tsc - testcase for bug 32913123\n\ntestcase for bug 32913123, code txn - gvelayut_bug-32913123\n\n32913123 - EXA 20.1.3 : RS-7445 [SERV CELLSRV HANG DETECTED]\n                [IT WILL BE RESTARTED] DUE TO FC REMAPPING OF HDD GDS",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug32937338.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug32937338.tsc - Test for bug 32937338\n\n1. Compare v$sql (for the specific sql_id that was executed): physical_read_requests, optimized_phy_read_requests should be close to/similar to\n           v$sesstat 'physical read IO requests'  and 'physical read requests optimized'\n     2. 'cell RDMA reads' + 'cell ram cache read hits' + 'cell pmem cache read hits' + 'cell flash cache read hits'\n           should be close to 'physical read requests optimized",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug32995282.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1"
    },
    "description": "tsagbug32995282.tsc\n\nBug32995282 - GRID DISK ARE IN CACHECONTENTLOST STATUS DURING\n                   CELL PATCHING.",
    "platform": null
  },
  {
    "test_name": "tsagbug33005004.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1"
    },
    "description": "tsagbug33005004.tsc - Test Case for BUG-33005004\n\nTest to check if RDS-TCP works when clients have higher IP addresses",
    "platform": null
  },
  {
    "test_name": "tsagbug33005054.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33005054.tsc - Test for bug 33005054\n\n1. On cell check if diagpack holds exc traces and nginx logs\n     2. On cell check if sundiag holds exc traces and nginx logs\n     3. DB node doesnot have exc trace, therefore diagpack will not have\n     4. DB node doesnot have exc trace, therefore sundiag will not have\n     5. On dbnode check if diagpack holds exc traces\n     6. On dbnode check if sundiag holds exc traces",
    "platform": null
  },
  {
    "test_name": "tsagbug33038849.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0"
    },
    "description": "tsagbug33038849.tsc - Test case for bug 33038849\n\nTest case for bug 33038849, code fix - weizhang_ctrlfile_unaligned_write\n\nTest steps for below bug:\n     Bug# 33038849 - ENABLING FLASH CACHE CACHING FOR CONTROL FILE HEADERS\n\n     (1) Run orion 100% small ControlFile read workload on 100MB disk region.\n         The small read is 1KB, i.e. not 4KB aligned. The small read would\n         trigger FC prefetch and populatiion. At the end of workload.\n         The data would get 100% cached.\n     $ADE_VIEW_ROOT/rdbms/bin/orion -run advanced -type rand -write 0 \\\n       -filetype Controlfile -testname orion -size_small 1 -size_large 1 \\\n       -matrix point -num_small 1 -num_large 0 -seed 90338  -duration 30 \\\n       -disk_end_offset 100M\n\n     Please list the following fast metrics (via cellsrvstat) for reference.\n     Number of ctrlfile read cache hit\n     Total size of ctrlfile read cache hit (KB)\n     Number of ctrlfile read cache miss\n     Total size of ctrlfile read cache miss (KB)\n     Number of ctrlfile read population           ==> should be non-zero\n     Total size of ctrlfile read population (KB)  ==> should be non-zero\n\n\n     (2) Run orion 100% small ControlFile write workload on 100MB disk region.\n         The small write is 1KB, not 4KB aligned. Without fix, the write would\n         all go to disk and invalidate cache. With fix, the write would go to cache.\n     $ADE_VIEW_ROOT/rdbms/bin/orion -run advanced -type rand -write 100 \\\n        -filetype Controlfile -testname orion -size_small 1 -size_large 1 \\\n        -matrix point -num_small 1 -num_large 0 -seed 90338  -duration 30 \\\n        -disk_end_offset 100M\n\n     Please check the following fast metrics (via cellsrvstat):\n     Number of ctrlfile write to cache                ==> should be non-zero\n     Total size of ctrlfile write to cache (KB)       ==> should be non-zero\n     Number of ctrlfile write bypass cache            ==> should be zero\n     Total size of ctrlfile write bypass cache (KB)   ==> should be zero\n     Number of ctrlfile nocache write unaligned       ==> should be zero\n     Total size of ctrlfile nocache write unaligned (KB) ==> should be zero\n\n     (3) Repeat control file small read in (1). Without fix, there would be\n         ctrlfile read cache misses. With fix, all read would get cache hit.\n     $ADE_VIEW_ROOT/rdbms/bin/orion -run advanced -type rand -write 0 \\\n       -filetype Controlfile -testname orion -size_small 1 -size_large 1 \\\n       -matrix point -num_small 1 -num_large 0 -seed 90338  -duration 30 \\\n       -disk_end_offset 100M\n\n     Please check the following fast metrics.\n     Number of ctrlfile read cache hit           ==> should have non-zero delta\n     Total size of ctrlfile read cache hit (KB)  ==> should have non-zero delta\n     Number of ctrlfile read cache miss          ==> should have zero delta\n     Total size of ctrlfile read cache miss (KB) ==> should have zero delta\n     Number of ctrlfile read population          ==> should have zero delta\n     Total size of ctrlfile read population (KB) ==> should have zero delta",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33075868.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1"
    },
    "description": "tsagbug33075868.tsc - Test for BUG # 33075868\n\nBug 33073868 - DISK GOES INTO CONFINED STATUS WITHOUT APPARENT REASON\n     REASON FOR CONFINEMENT: SERVICE TIME IS HIGH RELATIVE TO OTHER DISKS\n     WITH SIMILAR WORKLOADS",
    "platform": null
  },
  {
    "test_name": "tsagbug33135374.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug33135374.tsc - test for bug 33135374",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33160009.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1"
    },
    "description": "tsagbug33160009.tsc - Compatibility test for xml parser change\n\nTransaction changed alert parser from DOM parser to SAX parser",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33162798.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug33162798.tsc - /u01 diks usage warning test\n\nRaise alert/clear on /u01 at DB node",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33176405.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33176405.tsc - functional test for the IMCPop error message",
    "platform": null
  },
  {
    "test_name": "tsagbug33293911.tsc",
    "setup": "tsaginit",
    "flags": {
      "max_areas": "5                 # max number of areas on each device",
      "gd": "'datafile1'              # datafile griddisks",
      "duration": "300                # run IOV for 5 mins",
      "threads": "5",
      "area_sz": "4",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug33293911.tsc - Test for BUG 33293911\n\nBug 33293911 - ADD OPENDISK TIMEOUT ERROR IF FC BOOTSTRAP IS TAKING\n                    TOO LONG\n     The test checks for OpenDisk timeout support in case FC bootstrap takes\n     very long time and client IO is not allowed",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33294627.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33294627.tsc - testcase for bug 33294627\n\ntestcase for bug 33294627 - MKVOLUMEATTACHMENT THROWS BAD ERROR CODE FOR\n     AN EDV ATTACHED VOLUME",
    "platform": null
  },
  {
    "test_name": "tsagbug33295138.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1"
    },
    "description": "tsagbug33295138.tsc - MS encoding test for special character\n\nEncode ampersand character in xml format",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33304775.tsc",
    "setup": "tsaginit",
    "flags": {
      "max_areas": "5                # max number of areas on each device",
      "gd": "'datafile1'             # datafile griddisks",
      "duration": "120               # run IOV for 2 mins",
      "threads": "3",
      "pct_read": "100",
      "area_sz": "4",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug33304775.tsc\n     Bug 33304775 - DO NOT CREATE IPCDAT EP FOR PROCESSES NOT ELIGIBLE\n                     FOR RDMA\n\nThe test verifies the fix by checking that the number of QP/EP created\n         is 0 when RDMA request are not allowed in IOV.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33318017.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug33318017.tsc - Test case for bug 33318017\n\nAdds data to vault and checks if it properly reflected in lsstoragepool",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug33324999.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33324999.tsc - Test for Bug #33324999\n\nRS-600 [OSSRSUTL_MONITOR_MONPR_THD:CAP_SET_PROC_FAILED]\n     WHEN KILLING DBRSMAIN\n\nTest Steps:\n\n1. Restart all services and then set up DBserver\n2. List alert history and then drop all alerts\n3. Grep and kill dbrsMain process\n4. Sleep for a duration of 120 seconds\n5. Check if dbrsMain was restarted by dbrsBackup\n6. Sleep for a duration of 300 seconds\n7. Check if RS-600 was created in alert history",
    "platform": null
  },
  {
    "test_name": "tsagbug33355297.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_testing": "1",
      "flash_size": "576",
      "num_flash_per_cell": "1",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "cell_with_ram_cache": "0",
      "cache_flint_only": "1",
      "creatdev_file": "tsagcfcdef",
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'"
    },
    "description": "tsagbug33355297.tsc - Test case for bug 33355297\n\nbug 33355297 - UNINTENTIONALLY USED PMEMCACHE AFTER ESS UPGRADE\n\nCustomer reported that pmemcache got unintentionally used even when\n    database has IORM plan pmemcache=off configured.\n\n    If cell got reboot after setting plan pmemcache=off, pmemcache got used\n    unintentionally, although PMemCache group dump shows that the group indeed\n    has pmemcache=off (i.e. hardMax=0).\n\n    It turns out that the env doesn't have any group with non-zero limit.\n    From space management's viewpoint, there is only one default group really used.\n    Hence system mistakenly took a fast path (i.e. fccGroupEnabled=false),\n    which bypassed IORM limit check during runtime space allocation and\n    caused new pmemcache space allocation to sneak in.\n\n    The problem would get resolved automatically if there is IORM plan set/change\n    or if there is any workload on any database with non-zero pmemCacheMin\n    or pmemCacheLimit or pmemCacheSize within the same cellsrv lifecycle.\n    In those code paths, system would auto-fix the in-memory state of whether\n    group is enabled.\n\n\n  The steps to reproduce the issue are:\n    1. Create one DB with similar to the other replacement test, create\n       for tablespace that contains test table. And disable caching for all other\n       system grid disks. Populate the test table with some initial data.\n    2. Drop and recreate writethrough pmemcache.\n    3. Set DB IORM plan to have \"pmemcache=off\". List iormplan; list database detail;\n       list pluggabledatabase detail ( if it is CDB test).\n    4. Perform OLTP query of the test table for 10 mins.\n    5. Check DB_PC_BY_ALLOCATED for the database, it is supposed to be 0.\n    6. Restart cellsrv ignore redundancy.\n    7. Wait for 2 mins.\n    8. List iormplan; list database detail; list pluggabledatabase detail\n       The pmemCacheSize for the database should be 0.\n    9. Repeat 4 - Perform OLTP query of the test table for 10 mins.\n    10. Check DB_PC_BY_ALLOCATED for the database.\n        Without fix, it is non-zero. With fix, it would be 0.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33364956.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug33364956.tsc - Test case as mentioned in bug 33364956",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33372595.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33372595.tsc - Test for bug 33372595\n\nEXASCALE: CELL ROLLING UPGRADE - PERMISSIONS FOR\n     /OPT/ORACLE/CELL/CELLSRV/DEPLOY/LOG IS INCORRECT,\n     EXPECTED:0770 FOUND:2775\n\nTest steps:\n      1. Cleanup the cell - restart services, drop and create cell,\n   drop all alerts\n      2. Change asm/cell permissions, restart ms\n      3. Grep for \"Unexpected file ownership or permissions\" in alerthistory\n      4. Change deploy/log permissions, restart ms\n      5. Grep for \"Unexpected file ownership or permissions\" in alerthistory\n      6. Reset to normal permissions, restart ms\n      7. Grep for \"Unexpected file ownership or permissions\" in alerthistory\n\n   With bug 35187540, we are using rpm commands in permissions_check.py to\n   help verify files in cell package. The verification is done in two steps:\n   (1). Perform a generic check on several import locations. (i.e., OSSCONF)\n        and capture any violations.\n   (2). Use RPM commands to verify all package files.\n\n   Note that some files or directories might be checked in both steps. In\n   these cases, we only count errors reported by step 2 as RPM commands are\n   more accuate.\n\n   Additional Testcases:\n   (1). Altering permission/ownership of a file which is checked by generic\n        rules (step 1), but not owned by RPM (step 2). This is to ensure\n        step 1 is working as intended.\n\n        Method:\n          create a new file under OSSCONF which violates OSSCONF's generic\n          file rules: \"root, root, 0600\".\n          $ touch $OSSCONF/permissions_check_tempfile\n          $ chown celladmin:cellusers $OSSCONF/permissions_check_tempfile\n\n   (2). Altering permission/ownership of a file owned by RPM but is not\n        checked by generic rules. This is to ensure step 2 is working as\n        intended.\n\n        Method:\n          Modifying $OSS_SCRIPTS_HOME/unix/permissions_check.py (itself)",
    "platform": null
  },
  {
    "test_name": "tsagbug33402139.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug33402139.tsc - ilom management network link state check\n\nCreate HW alert if management link is down state",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33409795.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagbug33409795.tsc - Test for BUG 33409795\n\nScenario - Start the SP with 6 cells, let disks of cell_5\n                  be force dropped, resize SP to add 6 more cells",
    "platform": null
  },
  {
    "test_name": "tsagbug33427539.tsc",
    "setup": "tsaginit",
    "flags": {
      "noecho_testname": "true",
      "num_gd": "2    # number of griddisks to create and use",
      "duration": "180 # run IOV for 300 minutes",
      "threads": "3   # number of threads per each IOV process",
      "area_sz": "40   # size of each area on gd for an IOV process, in MB",
      "gdsz": "368     # gd size in MB",
      "max_areas": "5  # max number of areas on each device",
      "creatdev_file": "tkfgiovdef",
      "cfgdir": "tsagfciov.sav"
    },
    "description": "Script to validate the fix created for bug 33427539\n\nWithout the fix for bug 33427539 the testscript produces one of these\n     errors:\n     ORA-07445:\n     [_ZN12LinuxBlockIO13dumpIODetailsEP8IOParamsP14LinuxIORequestP14Basici\n     IORequestP9Cacheablemmi+149 IOContext.h:261] [11]\n\n     ORA-00600: [IO overlaps w/ cancelled buffer]\n\n     The test runs IOV and also perform these changes:\n\n     1. setting disk confinment time to 200000\n     2. disable RAM cache (_cell_ramcache_mode=off)\n     3. settting _cell_flashcache_diag_reads_frequency=1 to produce debug FC\n        IOs (FC_PIN_DEBUG)\n     4. enable FC tracing (CELLSRV_Flash_Cache_Layer.*)\n     5. enable the simulation BLOCKIO_SKIP_CLEANUP_CXLIO\n\n     The script produces cancelled IOs (error 2201):\n      FC: Debug Dump: Handle Disk Compl rqid=94632 err=2201 DataBuf=..\n          type=Diag\n\n     To execute the script:\n            oratst -d tsagbug33427539",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33436070.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug33436070.tsc - Test for bug 33436070\n\nThe signal handler is updated to include the file and\n     line number information when reporting an ORA-7445.\n\nTest steps:\n1. Include tsagnini\n2. Restart all cell services\n3. Note down trace file size\n4. Trigger cellsrv event\n5. Wait for pattern \"ORA-07445\" in trace file\n6. Check if line number is included in ORA-07445 error reported",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33456703.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33456703.tsc - Test for Bug 33456703\n\nALTER TABLE TO CELL_FLASH_CACHE KEEP SHOULD NOT INVALIDATE CURSORS",
    "platform": null
  },
  {
    "test_name": "tsagbug33480831.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true"
    },
    "description": "tsagbug33480831.tsc - Test for bug 33480831\n\nTest steps:\n        1) Install old rpm which doesn't have bug fix\n        2) Multiple cellsrv restarts will be seen in alert.log\n        3) Upgrade to current label\n        4) Only single cellsrv restarts will be seen in alert.log",
    "platform": null
  },
  {
    "test_name": "tsagbug33481285.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagr2def",
      "flash_size": "4000",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback"
    },
    "description": "tsagbug33481285.tsc - test case for bug 33481285:\n                           ENABLE OBJECTS TO BE KEPT IN FLASH CACHE WITHOUT A SCAN\n\nThis test case has below steps:\n     Case 1: for partitioned table\n       Step 1: create partitioned table\n       Step 2: check that cell_flash_cache is default for all partitions\n       Step 3: check size and get object id of each partition\n       Step 4: flush flashcache\n       Step 5: alter table to keep\n       Step 6: check that cachedSize and cachedKeepSize are\n               90% - 110% of partition size\n     Case 2: for non-partitioned table\n       Step 1: create non-partitioned table\n       Step 2: check that cell_flash_cache is default\n       Step 3: check size and get object id of table\n       Step 4: flush flashcache\n       Step 5: alter table to keep\n       Step 6: check that cachedSize and cachedKeepSize are\n               90% - 110% of table size\n     Case 3: for IOT overflow mapping table\n       Step 1: create IOT overflow mapping table\n       Step 2: check that cell_flash_cache is default\n       Step 3: check size and object id for each partition\n       Step 4: flush flashcache\n       Step 5: alter table to keep\n       Step 6: check that cachedSize and cachedKeepSize are\n               90% - 110% of each partition size\n     For Exascale, this test has one test case which is similar to case 2\n       Step 1: create non-partitioned table\n       Step 2: check that cell_flash_cache is default\n       Step 3: check size and get object id of table\n       Step 4: flush flashcache and check FlashCache pop keep in cellsrvstat\n       Step 5: alter table to keep and check FlashCache pop keep again\n       Step 6: check that the change in FlashCache pop keep is\n               90% - 110% of table size",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33481452.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "OSS_ENABLE_NC_PERSISTENCE": "off",
      "creatdev_file": "tsagr2def",
      "uniq_dsknames": "all"
    },
    "description": "tsagbug33481452.tsc - Test for Bug 33481452\n\n1. Have a workload to seed dirty data, orion (random, 100% small write) used.\n     2. The test then picks a candidate cacheline through event ? immediate cellsrv.cellsrv_flashcache(dumpdirtylru..)? , using the floc location through dumptranslatedgdaddress.\n     3. Simulate the read IO error on the disk selected using cellsrv_simevent[BLOCKIO_READ_ERR] ..\n     4. Run another workload to get datasync write jobs to kick in and drain dirty lru list. As the cacheline set for simevent was from dirty lry list, the dkwr will hit IO error when it tries to sync this line.\n     5. Verify that this line is hit only once with ? immediate cellsrv.cellsrv_flashcache(dumpcorestats..), where the output looks like\n     # numDkwrScrubbedCLs 1\n     # numDkwrWriteActivatedCLs 0\n     6. The test then turns off the IO error event, and runs the first workload again to write to the cacheline and mark it as active again.\n     7. Verify again with dumpcorestats for the following\n     # numDkwrScrubbedCLs 1\n     # numDkwrWriteActivatedCLs 1",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33481475.tsc",
    "setup": "tsagnini",
    "flags": {
      "uniq_dsknames": "all",
      "sage_mirror_mode": "normal"
    },
    "description": "tsagbug33481475.tsc - test case for bug\n        33481475 - FLASHCACHE METADATA FLUSH RACE WITH CONFINEMENT DISK POWERCYCLE\n\nPlease see below\n\nTest to be added in lrgdbconsadebugcli15",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33488114.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug33488114.tsc - Functional test for bug-33488114\n\nThis test verifies that even if enough clients try to connect to\n     cellsrv after it has been restarted (with all EGS servers down),\n     it will not result in a deadlock in RemoteListener in cellsrv.\n\nThe test steps are as follows:\n     1. Get an ExaScale cluster with no ASM grid disk.\n     2. Shutdown ExaScale cluster by running \"sh restartexc.sh shutdown\".\n     3. Restart CELLSRV.\n     4. Run orion 5 times to create old connections to the CELLSRV.\n     5. Restart MS.\n     6. Execute command \"cellcli -e list griddisk\". It should succeed.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug33505931.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug33505931.tsc - Avoid MS hang on asrm http port hang\n\nCheck thread hang if asrm port is hanging\n     Force simulation device error and see if MS does not hang at\n     asrm access. Hanged asrm is simulated by python http server\n     And confirm if the asrm is not available, new alert is made",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33514905new.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33514905new.tsc - Check that caching policy is correctly updated on system upgrade\n\nNone\n\n   Notes\n     None",
    "platform": null
  },
  {
    "test_name": "tsagbug33571271.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug33571271.tsc - Test for bug 33571271\n\nTest for bug 33571271 - ensure that cell status is correctly\n     reset in case of MS crash during upgrade",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug33580624.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1"
    },
    "description": "tsagbug33580624.tsc - Test for Bug 33580624\n\n33580624-KVM HOST REBOOTED IN THE MIDDLE OF THE EXECUTION OF DBNODEUPDATE",
    "platform": null
  },
  {
    "test_name": "tsagbug33601358.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33601358.tsc - Test for bug 33601358\n\nTxn yfliu_bug-33601358 changes the start order of exadata-capacity-on-demand.service\n       during machine startup to avoid offlined cpus to be onlined again by other services.\n       Reboot and verify that the value of cell attribute cpuCount is still good after reboot.",
    "platform": null
  },
  {
    "test_name": "tsagbug33603123.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagbug33603123.tsc - TEST FOR BUG\n     BUG 33603123 - EXASCALE: REBALANCE HANG IN DOUBLE CANCEL\n#     TEST (DISK FORCE DROP/CANCEL/FORCE DROP)\n\nPLEASE SEE BELOW\n\ntest to be added in lrgsaexcvesdeletemd",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug33605554.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33605554.tsc - test script for bug 33605554\n\nstep 1: create table for testing\n     step 2: verify that everything is good with selecct count\n     step 3: updata chains set\n     step 4: verify that it's working normally after\n             setting _dbg_scan to 0 and to 2",
    "platform": null
  },
  {
    "test_name": "tsagbug33636066.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true",
      "SAGE_MIRROR_MODE": "normal",
      "celltrcfile": "^celltrcdir^alert_^oss_port^.log"
    },
    "description": "tsagbug33636066.tsc - Test for bug 33636066.\n\nTest for bug 33636066 - DISK SCRUB RESULTING IN PERFORMANCE DEGRADATION",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33667027.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33667027.tsc - bug 33667027: EXASCALE CELLSRV CRASH DURING STORAGEIDX\n                           HASHTABLE BUCKET WRITE RESULTING IN RIDX CORRUPTION\n\nThis test has below steps:\n     Step 1: create table for the test\n     Step 2: run SI query to populate SI\n     Step 3: set the simulation event \"SI_CRASH_RGN_WRLOCK\"\n     Step 4: run the SI query again, cellsrv will crash and restart\n     Step 5: dump the bootstrap report and check RIDX",
    "platform": null
  },
  {
    "test_name": "tsagbug33672042.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug33672042.tsc - Test for RAFT metadata corruption\n\nFor detailed scenarios :\n     https://oradocs.oracle.com/documents/office/edit/DF0A025696BA8FC6A3E674FF6D09B5E5DCAE5E8F2AC3",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug33692220.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33692220.tsc - Test for BUG 33692220\n\nCheck if running sudiag.sh on a cell hangs.\n     uses ssh root@host \"bash -x /opt/oracle.SupportTools/sundiag.sh\"",
    "platform": null
  },
  {
    "test_name": "tsagbug33694596.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagbug33694596.tsc - Test for BUG 33694596\n\nScenario - Start the SP with 6 cells, restart the EGS Leader\n                 and drop 3 cells to resize the SP to 3 cells",
    "platform": null
  },
  {
    "test_name": "tsagbug33695568.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug33695568.tsc - Test for bug 33695568\n\n33695568 - LNX64-21.4-EXASCALE,MS UNABLE TO START AFTER REBOOT 1 CELLNODE TWICE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33731471.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33731471.tsc - Test for bug 33731471",
    "platform": null
  },
  {
    "test_name": "tsagbug33734670.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cc_mode": "cc2",
      "enable_columnar_cache": "65"
    },
    "description": "tsagbug33734670.tsc - functional test for bug 33734670\n\nWhen CC2 is populated and next scan hit CC2 I/O error in CC layer, the existing\n   logic will return the I/O error back to DB (kcfis), and kcfis will retry a different\n   mirror for the read and also notify ASM to check the block on primary mirror. But\n   ASMâs check I/O is block I/O, it will read from disk (so it can succeed) and\n   also it will not trigger the population for CC2 on the cell. As a result,\n   the CC2 on first cell is lost and not rebuilt automatically, the next scan will\n   hit a performance degradation since some CC was removed.\n\n   With the fix, upon a CC read I/O error, the cell will retry the I/O again\n   through disk and then it will automatically populate the CC2 back to FC so that\n   next scan will still benefit from CC2.\n\n   We also support this bug test with cc1 table if cc_mode=cc1\n\nThe test steps are:\n       1). Setup normal redundancy with writeBack flashcache\n       2). Populate CC2 and wait for no IMC pop req.\n       3). Scan the table once:  select count(*) from temp;\n       4). Query the CC hit count = CC_hits_1\n       5). Simulate read I/O error on one of the flash celldisk on first cell by CellCLI:\n           alter cell events=\"cellsrv_simevent[BLOCKIO_READ_ERR] err_type=internal,frequency=1,\n              count=10, evarg1=FD_01_raw60422\"\n       6). Scan the table once and stop the simulation event:\n       7). alter cell events=\"cellsrv_simevent[BLOCKIO_READ_ERR] off\"\n       8). Wait for no IMC pop req, and then query the CC hit count again, CC_hits_2,\n           this hit count should be less than CC_hits_1 as some CC read hits I/O error,\n           no need to check the hit value in this step as it is depending on how many\n           I/O error on FC, it is hard to control.\n       9). Scan the table once again and query the CC hit count again, CC_hits_3,\n           this time CC_hits_3 should be very close to CC_hits_1 (as CC2 should be repopulated\n           along with previous scan, without the fix, this CC2 hit count would be similar to CC_hits_2),\n           we can tolerate delta +-2 for the CC hit count check.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33753695.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug33753695.tsc - Test for Bug - 33753695\n\nAdditional: RAFT LOG SIZE INCREASED TO 1.6G LEAD TO FULL FS AND RAFT CORRUPTION- SNAPSHOT PROCESS WAS NOT ABLE TO DELETE OLD FILES",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug33756086.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug33756086.tsc - add tests for bug 33756086\n\nadd tests for bug 33756086\n\nThese tests are for bug 33756086; they simulate various conditions (disk\n     failures/ restores, flash I/O hangs, etc.) that should cause Flash Log\n     and PMEM Log to be disabled/enabled.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33774230.tsc",
    "setup": "srdbmsini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true",
      "sage_mirror_mode": "normal",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "oss_testing": "2",
      "oss_failgroup": "failalldbdg     # mirror controlfile and flint diskgroups too"
    },
    "description": "tsagbug33774230.tsc - Test for bug 33774230",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33789860.tsc",
    "setup": "tsagnini",
    "flags": {
      "max_areas": "4                # max number of areas on each device",
      "gd": "'datafile0'             # datafile griddisks",
      "duration": "120               # run IOV for 2 mins",
      "threads": "3",
      "pct_read": "75",
      "creatdev_file": "tsagdiskdef",
      "OSS_AUTO_MANAGE_DISKS": "true",
      "SAGE_MIRROR_MODE": "normal"
    },
    "description": "tsagbug33789860.tsc - Test for BUG -33789860\n\nTest for BUG 33789860 - MISMATCHED RESILVERING IO ADDRESS DURING\n                             DISK SCRUBBING",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33802782.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug33802782.tsc - Test for bug 33802782 : ORA-00600 [PREDICATEDISK::QUEUETIMEZONETIMEDACTION_1] ON EXADATA CELL\n\nSteps:\n       1) Set the event: event is simulating the case that time zone information is\n          requested by cellsrv but there is metadata changes at DB side. Now DB is\n          pushing metadata then time zone information. This reordering of information is\n          causing assert in Cell side\n       2) Run db workload\n       3) Expects that this goes fine and there is no ORA-00600 [PREDICATEDISK::QUEUETIMEZONETIMEDACTION_1]",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33824499.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "normal",
      "uniq_dsknames": "all"
    },
    "description": "tsagbug33824499.tsc - test for bug 33824499 : [CORRUPTED CELLDISK\n#     METADATA] FOUND FOR LRGSAFCIOVR ON OL8 OSS RUN OSS_PT.EXC_LINUX.X64_220122.TROL8\n\npls see below\n\nto be added in lrgdbconsamsbug8",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33842040.tsc",
    "setup": null,
    "flags": {
      "sort_retain": "100000",
      "log_name": "^tst_tscname^.log"
    },
    "description": "tsagbug33842040.tsc - test case for bug 33842040\n\nthis test script has 6 test cases:\n    Test case 1: negative test for setting main_workload_type for CDB\n    Test case 2: negative test for setting main_workload_type for PDB\n    Test case 3: Step 1: do not set the main_workload_type for PDB/CDB\n                 Step 2: make FCGroup dump and check workloadType\n                 Step 3: workloadType for all should be 0 (OLTP) by default\n\n    Test case 4: Step 1: set the main_workload_type to ANALYTICS for CDB1_PDB1 only\n                 Step 2: make FCGroup dump and check workloadType\n                 Step 3: workloadType for CDB1_PDB1 should be 1(ANALYTICS)\n                         others should be 0 (OLTP)\n\n    Test case 5: Step 1: set the main_workload_type to ANALYTICS for CDB1_PDB1\n                         set the main_workload_type to OLTP for CDB\n                 Step 2: make FCGroup dump and check workloadType\n                 Step 3: workloadType for CDB1_PDB1 should be 1 (ANALYTICS)\n                         others should be 0 (OLTP)\n    Test case 6: Step 1: set the main_workload_type to ANALYTICS for CDB\n                 Step 2: run DB workload that generates temp spills to land in FC\n                 Step 3: make FCGroup dump and check workloadType\n                 Step 4: workloadType for all db should be 1 (ANALYTICS)\n                 Step 5: check cellsrvstat for non-zero stat\n                         \"num lw force absorbed based on workload priority\"",
    "platform": null
  },
  {
    "test_name": "tsagbug33860125.tsc",
    "setup": null,
    "flags": {
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'"
    },
    "description": "tsagbug33860125.tsc - Test case for Bug 33860125\n\nTest case for Bug 33860125\n     Bring up RDBMS with sga_max_size=10G\n     Once rdbms is up and has enough sga space, set:  alter system set \"db_32k_cache_size\"=1M;\n     create two tablespaces, one with 8K block size (default) and 32K block size.\n\n       create table temp(a int, b int, c int, d int, e int); and then insert dataâ¦\n       create tablespace tbs32k  datafile '+DATAFILE/tbs32k.dbf' size 20M autoextend on blocksize 32K;\n       create table test32k organization hybrid columnar compress for archive high tablespace tbs32k as select * from temp;\n       create tablespace tbs8k  datafile '+DATAFILE/tbs8k.dbf' size 20M autoextend on blocksize 8K;\n       create table test8k organization hybrid columnar compress for archive high tablespace tbs8k as select * from temp;\n\n     Populate columnar cache for the 8K table.\n     Once columnar cache for the 8K table is populated, run the following:\n\n       alter system set \"_kcfis_fast_response_enabled\"=FALSE;\n       select t8.a, count(distinct t32.a), count(*) from test32k t32 join (select a, b from test8k where a < 12345 and rownum <= 10) t8 on t32.b = t8.b group by  t8.a;\n\n     Without the fix, you should see an oflsrv assert:  ORA-00600: internal error code, arguments: [PredicateOflDiskComn::addToBeFilteredIOSize::colc],\n     With the fix, oflsrv should not fail.\n\nTest case for Bug 33860125",
    "platform": null
  },
  {
    "test_name": "tsagbug33862579.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug33862579.tsc - Test for Bug 33862579\n\nBug - TWO EGS CLAIMING LEADERSHIP AT THE SAME TIME",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug33867643.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1"
    },
    "description": "tsagbug33867643.tsc - ilom temprature sensor fail case",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33875341.tsc",
    "setup": null,
    "flags": {
      "guest_nosetup": "false",
      "dbnode": "^compnode1^",
      "MACH_NAME": "^dbnode^",
      "MACH_PASSWD": "welcome1",
      "dbnodeconnstr": "root@^MACH_NAME^"
    },
    "description": "tsagbug33875341.tsc - Test for vCPU scaling/offline/online on kvm guest",
    "platform": null
  },
  {
    "test_name": "tsagbug33878584.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagbug33878584.tsc - Tests if BSM crashes when it is restarted after\n     creating 512 snapshots of a volume.\n     Test for bug34850747: Simulate BsmRestoreCfg hang by setting event\n     BSM_RETRY_RECFG_IOCTL and restarting BSM then BSW. Once this event is\n     unset, BSM should be able to continue processing the ioctl without a hang.\n\nAdditional: Tests if BSM crashes when it is restarted after creating 512 snapshots of a volume. Simulate BsmRestoreCfg hang by setting event    BSM_RETRY_RECFG_IOCTL and restarting BSM then BSW. Once this event is unset, BSM should be able to continue processing the ioctl without a hang.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug33895960_kvmhost.tsc",
    "setup": null,
    "flags": {
      "compnode1": "^compute_node^",
      "dbnode": "^compnode1^",
      "MACH_NAME": "^dbnode^",
      "MACH_PASSWD": "welcome1",
      "dbnodeconnstr": "root@^MACH_NAME^"
    },
    "description": "tsagbug33895960_kvmhost.tsc - Test for 33811325 : COLLECT XEND, QEMU AND YUM.LOG FILES IN SUNDIAG\n\nTest Cases:\n       1) On KVM HOST , verify contents of sundiagpack\n       2) On KVM HOST , verify contents of diagpack\n       3) On KVM GUEST , verify contents of diagpack",
    "platform": null
  },
  {
    "test_name": "tsagbug33942865.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true"
    },
    "description": "tsagbug33942865.tsc - Test for bug 33942865\n\n1) Shutdown all cell services\n     2) Corrupt cell_disk_config.xml\n     3) On restarting cell services check for expected error message from alert log",
    "platform": null
  },
  {
    "test_name": "tsagbug33988339.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sage_mirror_mode": "normal",
      "cdb": "true"
    },
    "description": "tsagbug33988339.tsc -Bug# 33988339 - PERFORMANCE ISSUE AFTER SINGLE FLASH DISK WENT INTO CONFINEMENT\n\n#    - Set an ADE environment with 2 cells\n   - Start a smart scan workload.\n   - Once we confirm that the smart scan workload is running, simulate a\n         flash failure on one of the cells.\n   - This would change the health factor of the ASM disks to BAD\n   - With this we should ideally see the ASM redirecting the IOs to the\n         partner cells. But due to this bug, smart scan IOs still land on the\n         disks on the cells where the flash disk failed.\n\nto be added in lrgdbconsascbug26",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug33992818.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33992818.tsc - roce self test common for hardwares(in desc)\n\nroce cell/comp, ib cell/comp, kvm(sf)guest/host, x9_amd1 cell(sf)\n\ntest infra to check RoCE card health.\n     https://confluence.oraclecorp.com/confluence/display/EXD/RoCE+Self-test+Project",
    "platform": null
  },
  {
    "test_name": "tsagbug33992818_com.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug33992818_com.tsc - roce self test tsagbug33992818.tsc helper file\n\nit is helper file for roce self test => tsagbug33992818.tsc\n\ncommon code called from tsagbug33992818.tsc",
    "platform": null
  },
  {
    "test_name": "tsagbug33998880.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug33998880.tsc - Test for bug 33998880\n\nTest for bug 33998880 - in this bug, when EGS followers are restarted\n     for many times, the threads in the EGS leader get stuck in RemoteCellMgr\n     driver loop, and we see a Raft eviction.\n\nTest Steps:\n     1) Set up an exascale stack\n     2) Get EGS servers configuration (Leader and followers)\n     3) In a loop of 100, restart both EGS followers\n     4) Get EGS servers configuration (Leader and followers)\n     5) Compare initial and final configuration - leader must remain the same",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34049108.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug34049108.tsc - 34049108 - REDUNDANCY NOT RESTORED AFTER M.2 DRIVE FAILURE\n\ndisk failure simulation on the M2_SYS_[0|1] disks.\n    Please see tsagbug34049108.sh for more details\n\nto be added in lrgrhx7saipcdat",
    "platform": null
  },
  {
    "test_name": "tsagbug34127760.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug34127760.tsc - Test for BUG-34127760\n\nAdditional: Test for Bug 34127760 - Tries to induce EGS crash using simulation event in all 3 EGS - RAFT_SLOW_NM_CALLBACK",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34166085.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_auto_manage_disks": "true",
      "sage_mirror_mode": "normal",
      "oss_failgroup": "failalldbdg"
    },
    "description": "tsagbug34166085.tsc - Test for Bug 34166085\n\nTest for  BUG 34166085 - MS DID NOT AUTO RECOVER DISKS WHEN\n       FLASH CACHE AND DISK CONTROLLER CACHE BOTH FAILED\n     Test simulates dual failure of flash and disk controller cache\n       and verifies if auto recovery is kicked off",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug34173236.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagbug34173236.tsc - test for bug 34173236\n\n1. create a volume\n     2. simulate EDS error simulatio for different error codes\n     3. remove volume : should return error\n     4. turn off EDS eror simulation\n     5. remove volume : should be successful",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34195805.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug34195805.tsc - 34195805 - NOTIFICATIONSTATE\n\n34195805 - NOTIFICATIONSTATE: DELIVERY FAILED IN ALERTHISTORY",
    "platform": null
  },
  {
    "test_name": "tsagbug34205694.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^.log"
    },
    "description": "tsagbug34205694.tsc - test scipt for bug 34025694\n\ntest script for bug 34025694.\n     failure degrades smartscan to pass through\n\nstep 1: run smart scan query and run checkSmartScan\n             smart scan should be working and not in passthough mode\n     step 2: turn on simevent\n     step 3: run smart query again and run checkSmartScan again\n             result should show passthru mode\n     step 4: turn off simevent",
    "platform": null
  },
  {
    "test_name": "tsagbug34228985.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cdb": "true"
    },
    "description": "tsagbug34228985.tsc - 34228985 - [AH]ADD MPROTECT TO REID STORED IN OSSDISK DEVICE HANDLE\n\nSee below:\n\nto be added in scbug24",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug34250915.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1"
    },
    "description": "tsagbug34250915.tsc - exacli command logging test\n\nsave ms-odl.log by not tracking list command",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug34267177.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagbug34267177.tsc - Test for BUG - 34267177\n\nAdditional: Test to add new cells with different sized disks in existing EGS cluster in an EGS Sandbox env.",
    "platform": null
  },
  {
    "test_name": "tsagbug34324780.tsc",
    "setup": null,
    "flags": {
      "devdir1": "^T_WORK^/raw",
      "logcaps": "PMEM",
      "logdevicetype": "PMEM"
    },
    "description": "tsagbug34324780.tsc - Test for bug 34324780\n\nThis test is ran in below lrg's -\n     i.   lrgdbcsapmemlog2\n     ii.  lrgdbcsapmemlog2_cmainr19000\n     iii. lrgdbconsaflog5\n     iv.  lrgdbconsaflog5_excld\n\nThis script has tests for the fix for bug 34324780.  It simulates various\nfailure scenarios for PMEM/Flash Log:\n- missing stale GUID file\n- missing devices from cell_disk_config.xml\n- missing devices\n- unhealthy devices at cellsrv startup",
    "platform": null
  },
  {
    "test_name": "tsagbug34339421.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug34339421.tsc - Test for BUG - 34339421\n\nAltering esnode restbackend with empty string should not automatically start ERS",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34376468.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug34376468.tsc - Test for Bug 34376468\n\nBug 34376468 - INCREASED LIBCELL TRACING IN RFS TRACE FILES AFTER 19.14 RDBMS UPGRADE\n    Steps:\n     (1) Setup DG\n     (2) enable write IO latency event\n        CellCLI> alter cell events=\"cellsrv_simevent[BLOCKIO_WRITE_LAT] \\\n              err_type=internal, frequency=1\"\n     (3) check RFS process whether it is generating outlier summary for redo IO\n          It should not generate outlier summary.\n     (4) check regular log write is generating outlier summary.\n          It should generate the outlier summary.\n     (5) Now enable log_write outlier in  RFS process. Once it is enabled,\n          we should see outlier summary in RFS trace file.\n     (6)  disable write IO latency.\n         alter cell events=\"cellsrv_simevent[BLOCKIO_WRITE_LAT] off\";",
    "platform": null
  },
  {
    "test_name": "tsagbug34405417.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug34405417.tsc - Real HW test related to simulate realistic rpm imaging, and file/group permissions.\n\nTest steps:\n       i.) - After a fresh reimage, Creating backup of cellinit.ora file\n      ii.) - checking permission of cellinit.ora before and after setting up dbserver\n     iii.) - deleting cellinit.ora, setting up dbserver, then checking permission of cellinit.ora\n      iv.) - Bringing back permission of cellinit.ora to normal\n\ntest is added in oss_rhnocell.tsc and lrgrh5saswupd_db lrg",
    "platform": null
  },
  {
    "test_name": "tsagbug34414262.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1"
    },
    "description": "tsagbug34414262.tsc - cipher test for MS https server\n\ncheck available cipher suites\n\nVerify ciphers\n     Enable supported ciphers, disable deprecated ciphers\n     These are deprecated\n       TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384       ECDHE-RSA-AES256-SHA384\n       TLS_DHE_RSA_WITH_AES_256_CBC_SHA256         DHE-RSA-AES256-SHA256\n       TLS_DHE_RSA_WITH_AES_128_CBC_SHA256         DHE-RSA-AES128-SHA256\n       TLS_DHE_RSA_WITH_AES_256_GCM_SHA384         DHE-RSA-AES256-GCM-SHA384\n     These are supported in TLSv1.2\n       TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384       ECDHE-RSA-AES256-GCM-SHA384\n       TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256       ECDHE-RSA-AES128-GCM-SHA256\n     These are supported in TLSv1.3\n       TLS_AES_256_GCM_SHA384                      TLS_AES_256_GCM_SHA384\n       TLS_AES_128_GCM_SHA256                      TLS_AES_128_GCM_SHA256",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug34448915.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug34448915.tsc - Test for Bug 34448915\n\nBUG 34448915 - REVERSED REQUEST VOTE RPC IN RAFT",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34457981.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug34457981.tsc - 34457981 - RS-7445 INCIDENT WITH MISSING CELL SW VERSION\n\n34457981 - RS-7445 INCIDENT WITH MISSING CELL SW VERSION\n\n34457981 - RS-7445 INCIDENT WITH MISSING CELL SW VERSION",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug34480720.tsc",
    "setup": null,
    "flags": {
      "compnode1f": "^compnode1^.us.oracle.com"
    },
    "description": "tsagbug34480720.tsc - Add test for bug 34480720 - dbmadmin user cannot execute dbmcli\n\n- Test dbmadmin user can execute dbmcli with the bug 34480720 fix\n     - Test dbmmonitor user can only list service attributes permission\n     - Test runs on online upgrade HW lrgrhexcupgrade_online",
    "platform": null
  },
  {
    "test_name": "tsagbug34486756.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef"
    },
    "description": "tsagbug34486756.tsc - test script for bug 34486756\n\nthe test script has below steps:\n     1. drop existing flashcache and change flashcache mode to writeback\n     2. add additional flshadisks\n     3. restart cellsrv to make new disks online and create celldisk\n     4. set parameters in cellinit.ora\n     5. set IORM plan with flashcache soft limit and run orion\n     6. monitor large write rejections statistics with cellsrvstat\n     7. set IORM plan with flashcache hard limit\n     8. rerun orion and monnitor large write rejections",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug34489283.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug34489283.tsc - test script for bug34489283\n\nPlease see below",
    "platform": null
  },
  {
    "test_name": "tsagbug34493145.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug34493145.tsc - Test case for bug 34493145\n\nThis test verifies that user who created a password file\n     using orapwd utility tool must be listed in acl of file.\n\nThe set of steps in the test are:\n     1. create a user with no privileges.\n     2. create a vault.\n     3. add created user to acl of created vault with inspect privileges.\n     4. use orapwd utility tool to create password file.\n     5. verify the password file is created and user created above is added to the file's acl.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34534060.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback"
    },
    "description": "tsagbug34534060.tsc - Test for bug 34534060\n\n1. Simulate IO Errors on Flash to transition CD to FAIL state\n     2. Enable simulation of MD update failure\n     3. Reenable failed CellDisk to force a drop\n     4. Drop will fail with MD update failing\n     5. Ensure that FlashCache is not in an inconsistent state",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug34563445.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagbug34563445.tsc - Test for BUG 33694596\n\ncreate storagepool with 3 cells,mark all griddisks in a cell are permanently dropped.",
    "platform": null
  },
  {
    "test_name": "tsagbug34590620.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagbug34590620.tsc - Test for bug 34590620\n\nUsing simevents , injecting errors when create backup is in progress\n     and check if the error handling is working fine and the operation succeeds.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34592319.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^.log"
    },
    "description": "tsagbug34592319.tsc - bug test for bug 34592319\n\ntest case for bug 34592319\n\nStep 1: change flashcache to WriteThrough mode\n     Step 2: run workload\n     Step 3: check PDB_FC_BY_ALLOCATED\n     Step 4: dump FC group\n     Step 5: drop PDB and dump FC group again\n     Step 6: check in dump file that dropped PDB is marked as dropped(type=3)\n     Step 7: Check that PDB_FC_BY_ALLOCATED don't have value for dropped PDB",
    "platform": null
  },
  {
    "test_name": "tsagbug34600270.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug34600270.tsc - Test for BUG 34600270\n\nroce kvm(sf)guest/host - Test for BUG 34600270/followup roce-selftests\n\ntest infra to check RoCE card health.\n     https://confluence.oraclecorp.com/confluence/display/EXD/RoCE+Self-test+Project\n     https://confluence.oraclecorp.com/confluence/display/~yicheng.wang@oracle.com/Test+Spec+for+RoCE+Self-test+Follow-up+Transaction",
    "platform": null
  },
  {
    "test_name": "tsagbug34606492.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug34606492.tsc - testscript for bug 34606492\n\nPlease see below\n\nNo",
    "platform": null
  },
  {
    "test_name": "tsagbug34614747.tsc",
    "setup": null,
    "flags": {
      "cell_with_flash_cache": "all",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug34614747.tsc - test for bug Bug# 34614747 - [AH] ABSORB DBWR\n#                            NO-CACHE SMALL WRITES INTO FLASHCACHE\n\npls see below\n\ntest to be added in lrgdbconsamsbug8",
    "platform": null
  },
  {
    "test_name": "tsagbug34635040.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug34635040.tsc - Test for BUG -34635040\n\nTest Steps -\n     1. Perform stackup - We have 3 EGS servers.\n     2. Let us say EGS A is the leader.\n     3. We shutdown all EGS servers.\n     4. We startup EGS C and B. On startup, we should make sure that none of\n        the alert logs of EGS B and EGS C print out that EGS A is the leader\n        since it is dead.\n     5. There will be a new election, either B or C will win. Again, we\n        should confirm this from the alert logs that either of EGS B or\n        EGS C became the leader.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34645518.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagbug34645518.tsc - test for bug 34645518\nBUG 34645518 - FC_BY_ALLOCATED SHOULD NOT INCLUDE FREE CACHELINES FROM FCGROUP 0\n\npls see below.\n\nto be added in lrgdbconsamsbug15",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug34645518_db.tsc",
    "setup": null,
    "flags": {
      "cdb": "true",
      "num_dbs": "2 #number of databases to be created",
      "creatdev_file": "tsagfcgdef",
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'"
    },
    "description": "tsagbug34645518_db.tsc - test for bug 34645518\nBUG 34645518 - FC_BY_ALLOCATED SHOULD NOT INCLUDE FREE CACHELINES FROM FCGROUP 0\n\npls  see below\n\nTO BE ADDED IN lrgdbconsamsbug15",
    "platform": null
  },
  {
    "test_name": "tsagbug34646001.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug34646001.tsc - Test for bug 34646001\n\nEXACS: EXADATA DID NOT DETECT CPU ISSUE SPX86A-8004-L1 ALTHOUGH\n     ILOM DETECTED SPX86A-8004-L1",
    "platform": null
  },
  {
    "test_name": "tsagbug34668759.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagbug34668759.tsc - Test for BUG - 34668759\n\nTest restarts EGS Leader during StoragePool reconfig\n     1. Create SP with 6 cells\n     2. Reconfigure SP to add 12 more cells\n     3. During REBALANCE, restart EGS Leader\n     4. Let the rebalance complete",
    "platform": null
  },
  {
    "test_name": "tsagbug34672136.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "sage_mirror_mode": "high",
      "delta_sync": "true",
      "flash": "true",
      "cmd_mirror_mode": "normal",
      "phase_dur": "120",
      "setup_blockstore": "true"
    },
    "description": "tsagbug34672136.tsc - test to try out flash disk failure during blockstore operations\n\n1. run blockstore workload in background\n     2. Loops flash disk failure + wait for resilver to finish for 30 times.\n     4. check for success of blockstore operations",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34681643.tsc",
    "setup": "xrdbmsini",
    "flags": null,
    "description": "tsagbug34681643.tsc - testcases for bug 34681643\n\nbug 34681643 - AIM4EXA:ORA-7445 [EVAOPN3RSET()] - EVAREORSET\n\nRun some queries that use deferred-constant-folded expressions.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34684790.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "multiple_bst": "true # Inorder to setup 2 BSM + 3 BSW instances",
      "setup_blockstore": "true"
    },
    "description": "tsagbug34684790.tsc - Test for bug 34684790\n\nTest Steps:\n       1. Setup 2 bsw environment\n       2. Kill bsw1 and both vips will move to bsw2\n       3. Create 2 attached volumes and 2 unattached volumes\n       4. Now we should have one attached volume and one unattached volume on each VIP\n       5. At this point, if we restart BSW1, then one of the VIP from BSW2 will try to\n          move to BSW1. This will trigger a clearcfg ioctl from BSM to BSW2.\n       6. The bug fix solves the ora 600 during step 5",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34716433.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug34716433.tsc - Test for Bug 34716433\n\n34716433 - EGS HANG DETECTED DUE TO DEADLOCK IN RAFT LAYER",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34734974.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug34734974.tsc - Test for bug 34734974\n\n34734974 - DCLI, GREP FOR KEYS IN AUTHORIZED FILE DOES NOT EXCLUDE COMMENTED KEY",
    "platform": null
  },
  {
    "test_name": "tsagbug34753867.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug34753867.tsc - Test for Bug fix for 34753867\n\nAIM4EXA:ORA-7445 [__INTEL_AVX_REP_MEMCPY()] - SAGEDATAWRITECU0\n   In this bug, query is interested in rowid information, column 0  and\n   other colums from the table. when query is processed in synthCU code path,\n   due to timeout, we aborted CC processing  and trying to send partially\n   processed rows. but in this case, we are not setting valid nrows. this\n   leads to not populating rowid CU information in recreaterowidCU. This\n    further leads to assert in recreate CU aligned.",
    "platform": null
  },
  {
    "test_name": "tsagbug34771169.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagaudef"
    },
    "description": "tsagbug34771169.tsc - Test case for bug 34771169\n\nTest case for bug 34771169, code fix - minjikim_bug-34771169_filter_priority\n\nTest steps for below bug:",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug34776156.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^.log"
    },
    "description": "tsagbug34776156.tsc - simple test case for encryption/decryption performance of IPP LIB\n\ntest for bug 34776156",
    "platform": null
  },
  {
    "test_name": "tsagbug34799112.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "num_cells": "4",
      "cmd_mirror_mode": "normal"
    },
    "description": "tsagbug34799112.tsc - test script for bug 34799112\n\nPlease see below.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34799112_qlc.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "oss_multims_testing": "true",
      "num_cells": "4",
      "cmd_mirror_mode": "normal"
    },
    "description": "tsagbug34799112_qlc.tsc - test script for bug 34799112\n\nPlease see below.\n\ntest is same as tsagbug34799112.tsc but with QLC DISKS setup",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34816401.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_failgroup": "faillalldbdg",
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagbug34816401.tsc - test for bug\n#    BUG 34816401 - RESILVERING TABLE FILE WAS NOT DELETED WHEN DISK GOT DROPPED IN ASM\n\npls see below",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug34817252.tsc",
    "setup": null,
    "flags": {
      "egs_deplmode": "exacc",
      "vault_db": "DATA",
      "cdb": "true",
      "db_noarchivelog_mode": "true"
    },
    "description": "tsagbug34817252.tsc - test for bug 34817252\n\nBUG 34817252 - TESTS TO EMULATE CSSD MELTDOWN IN GI + NON-RAC DATABASE\n#       Bug 34784474 NON-RAC DB INSTANCE MISSING REGISTRATION WITH CSSD TO ASSIST\n#       FAST FAILOVER\n\nto be added in lrgdbcsaexacldsearch",
    "platform": null
  },
  {
    "test_name": "tsagbug34818139.tsc",
    "setup": "xrdbmsini",
    "flags": null,
    "description": "tsagbug34818139.tsc - COLLECT OFFLOAD SERVER STATEDUMP ON PREDICATE\n                           IO OUTLIERS\n\nWhen Smart I/o hangs, After 5 mins, latency\n     detection in cellsrv is dumping all predicate information\n     and missing corresponding predicate offload server information.\n     This test validates that offload server state dump is taken as well.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug34845670.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "lstfile": "^tst_tscname^.lst"
    },
    "description": "tsagbug34845670.tsc -Bug# 34845670 - X10: CELLSRV HANG AFTER FLASHDISK BLOCKIO_SUBMIT_STALL\n\npls see below\n\nto be added in lrgrhx7safnddfailure",
    "platform": null
  },
  {
    "test_name": "tsagbug34852723.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug34852723.tsc - Functional Test for Bug 34852723\n\nFunctional Test for Bug 34852723\n     Developer Txn : iginzbur_bug-34852723\n\n1. List physicaldisk\n     2. Check output of /opt/oracle.cellos/CheckHWnFWProfile -action\n        list -component Flash | grep \"SerialNumber\\|FIRMWARE_ID\"\n     3. Verify that the serial numbers of flash disks is same in both\n        the cases for each flash disk",
    "platform": null
  },
  {
    "test_name": "tsagbug34863472.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug34863472.tsc - Test for BUG 34863472\n\nIn this bug, commit cache ioctl is waiting for more than 1 hour and ORA-700 is generated.\n   \tFC  garbage collection system thread is blocking other system jobs\n\n1. set _cell_oflissue_job_timeout to 1000\n      2. set SYS_THREAD_HANG event\n      3. Test a workload\n      4. There should be no ORA-700",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug34897220.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug34897220.tsc - Test for bug 34897220\n\nThis test run parallel list metricHistory commands to check whether\n     we got all results or the cellcli hangs in three different case\n     when listMetricHistoryConcurrence is 1 or 5 or 10.",
    "platform": null
  },
  {
    "test_name": "tsagbug34948358.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "compute_node_f": "^compute_node^.us.oracle.com",
      "dbnodeconnstr": "root@^compute_node_f^"
    },
    "description": "tsagbug34948358.tsc - Add test for bug 34948358 - dbmadmin user\n                           cannot execute dbmcli after running setup_permission_compute.sh\n\n1- Get one compute node in real hardware.\n     2- Run the following command as root:\n        sh /opt/oracle/dbserver/dbms/deploy/scripts/unix/setup_permission_compute.sh\n     3- Login as dbmadmin:\n        su - dbmadmin",
    "platform": null
  },
  {
    "test_name": "tsagbug34952866.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1"
    },
    "description": "tsagbug34952866.tsc - test to do I/Os on QLC disks\n\nThis test attempts to catch block corruptions when I/O is done to\n     QLC disks. The test creates a fake DB, sets low iorm limit to queue I/Os,\n     and then run an Orion workload with large writes. In case of I/O going\n     to wrong celldisk, there would be an assert in cellsrv that was\n     added by  aksshah_bug-34952866\n\nWith X10M EF, 2 QLC celldisks share the same DiskIOSched object as they\n    belong to a single physical device. We had a bug in the DiskIOSched code\n     where we returned an IO back to celldisk layer but the IO belonged to\n     the peer celldisk. This IO is then issued to the wrong block IO\n     context resulting in the wrong data and data corruption issues.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug34955449.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug34955449.tsc - Test for bug-34955449\n\nThis test is to make sure that the name of dbdserver is real system\n     name instead of localhost.",
    "platform": null
  },
  {
    "test_name": "tsagbug34978498.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug34978498.tsc - Test to cover bug- 34978498\n\nThis test is to make sure the metrics collection thread is able to\n     function well when there is a space problem on ROOT folder inside a\n     compute node. Steps are:\n    1) create some dummy metrics files under $OSSCONF/metrics folder\n    2) create /scratch folder then create a huge file using \"fallocate**\"\n       on /scratch folder\n    3) make sure / folder space exceeds 85%, but not too full, like around 90%\n    4) create some dummy metrics files under $OSSCONF/metrics folder\n    5) make sure all the metrics files are deleted\n    6) make sure \"list metriccurrent\" can work\n    7) manually remove the problematic file from 2)\n    8) Check space full alert and space full resolved alert in alerthistory",
    "platform": null
  },
  {
    "test_name": "tsagbug34979233.tsc",
    "setup": null,
    "flags": {
      "sysdba": "'sys/knl_test7 as sysdba'",
      "initsize": "8M",
      "segsize": "128k",
      "extent_mgmt": "local",
      "asm_ausize": "1048576"
    },
    "description": "tsagbug34979233.tsc - Functional test for BUG 34979233\n\n   DESCRIPTION                                                                                                                                          #\n 1. Create a test tablespace (uniform extent size=128KB) with a specific back-end grid disk.\n 2. Disable caching for all the other grid disks.\n 3. Create a large enough FC.\n 4. Create 10K small tables in that tablespace\n 5. Check unique entries based on key <dbID, objectNumber, tableSpaceNumber>. There should be no duplicated entries.",
    "platform": null
  },
  {
    "test_name": "tsagbug34992676.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "cdb": "true",
      "auto_local_undo": "true",
      "vault_db": "DATA"
    },
    "description": "tsagbug34992676.tsc - Main script to test for bug 34992676\n\nTest script to test for bug 34992676 : ADDITIONAL PDB\n                               SNAPSHOT TESTS FOR EXASCALE\n\nThis script validates various snapshot pdb creation\n     scenarios which are :\n     1 : PDB snapshot is dropped (OMF only and some non-OMF\n                                              file scenarios)\n     2 : Multiple PDBs using snapshot copy of parent PDB are\n         dropped   (OMF only and some non-OMF file scenarios)\n     3 : Create read-only snapshot and use it to create a PDB\n                   (OMF only and some non-OMF file scenarios)\n     4 : PDB creation using snapshot with all OMF path files",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug35002515.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35002515.tsc - test case for bug 35002515\n     This crash happned at decreasing myAutoQuarantinedPDBs counter\n     in cdbObj link at dropping \"DB IMC POP\" quarantine. This counter\n     was not incremented for \"DB IMC POP\" quarantine event.\n     But cdb link was created by other quarantine incident.\n\n     This test case recreates the steps that causes the crash. Namely,\n     we create 2 \"SQL PLAN\" quarantines and 2 \"DISK REGION IMC POP\"\n     quarantines and lead to\" DB IMC POP\" quarantine. We want to test\n     that we do not get the ORA-600, QMDBObject: dropMyAutoQuarantinedPDBs\n     when we drop the \"DB IMC POP\" quarantine.\n\nThis test has the following steps:\n     Step 1: update cellinit.ora with _cell_qm_db_imc_threshold=2\n     Step 2: create test table\n     Step 3: run CC2 query with CCPOP_CELLOFLSRV_DEATH simulation\n     Step 4: drop \"DISK REGION\" quarantines\n     Step 5: create Escalated quarantine by runing workload\n     Step 6: drop the escalated quarantine",
    "platform": null
  },
  {
    "test_name": "tsagbug35008165.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagbug35008165.tsc - Check cell offline time in 4 cell env\n\nThis script tests for cell offline time in 4 cell env\n     for bug-35008165\n\nTest steps:\n       1 : Setup 4 cell env with db workload and randomly select\n           3 cells\n       2 : Drop the first cell\n       3 : Wait for rebalance to start then make second cell\n           unreachable\n       4 : Sleep for 5 minutes\n       5 : Make second cell reachable again\n       6 : Wait for disks of second cell to come online\n       7 : Add the dropped cell back to storagepool\n       8 : Wait for rebalance to start and then make third cell\n           unreachable\n       9 : sleep for 5 minutes\n      10 : Make third cell reachable again\n      11 : Calculate time taken to make cell offline\n      12 : Wait for disks of third cell to come online",
    "platform": null
  },
  {
    "test_name": "tsagbug35043649.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_testing": "1",
      "flash_size": "2000",
      "num_flash_per_cell": "1",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "pdbs_per_cdb": "2",
      "num_dbs": "2",
      "cell_with_ram_cache": "0",
      "cell_with_xrmem_cache": "0",
      "cache_flint_only": "1",
      "creatdev_file": "tsagrh2def",
      "columnar_cache": "65",
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'"
    },
    "description": "tsagbug35043649.tsc - Test case for bug 35043649\n\nbug 35043649 - WITH FC MAXSIZE, CC UNABLE TO GROW\n\n1) setup 2G FC and a cdb with two pdbs, then setting IORM plan\n        with flashcachesize=900MB for CDB, each PDB limit is 437MB.\n     2) create lineitem table in each pdb and another OLTP table in pdb2.\n        LINEITEM is 600+MB.\n        OLTP table is 400M (larger than 50% of PDB FC limit).\n     3) Perform CC scan workload in PDB1, so that PDB1 is populated with DW\n        data. DW usage should be close to the PDB1 FC limit since there is\n        no OLTP usage.\n     4) Perform OLTP read workload in PDB2, so that PDB2 is populated\n        with OLTP data. OLTP usage should be close to the PDB2 FC limit.\n     5) Perform CC scan workload in PDB2. DW should be able to replace OLTP.\n        OLTP usage below 50% OLTP reservation should not be replaced.\n        DW should be able to use the remaining 50% quota.\n        We should see CC hits.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35064358.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35064358.tsc - bug 35064358: memory policy of SI stores'\n                           fragment changed to interleave\n\nSteps of this test case:\n       Step 1: run srdbmsini and run SI workload\n     (Steps 2-4 in one shell script)\n       Step 2: call numactl -H to get the available nodes\n       Step 3: save from /proc/<cellsrv_pid>/numa_maps to T_WORK\n       Step 4: grep from the saved file for si store ids\n               and obtain the size",
    "platform": null
  },
  {
    "test_name": "tsagbug35121767.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1"
    },
    "description": "tsagbug35121767.tsc - test-case for bug 35121767\n\nSimulate RemoteListener hang to validate RS can suspend CELLSRV\n     and once the simulation is removed, MS can auto restart CELLSRV",
    "platform": null
  },
  {
    "test_name": "tsagbug35140650_x11.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35140650_x11.tsc - Functional Test for Bug 35140650 - x11 version\n\nFunctional Test for Bug 35140650 - x11 version\n     Developer Txn : jiayawan_bug-35140650\n\nTest needs to verify that we are not able to simulate disk failure\n    on all system drives at the same time. Doing so (simulating failure\n    on all system disks) will cause the system to go down and hard to\n    recover from it.\n\n    For X8 and older platforms, instead of flash, we use harddisks. We\n    pick any two disks to fail, and the second failure should not be allowed.\n\n    In case of only two physicaldisk in compute node, when we fail one\n    pd then we need to wait for sapre rebuilding after cancelling failed pd.\n\n       Bug 37914527 - NOT ABLE TO FAIL PHYSICALDISK B AFTER SIMULATING\n              AND UN-SIMULATING FAILURE ON PHYSICALDISK A",
    "platform": null
  },
  {
    "test_name": "tsagbug35145671.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35145671.tsc - Functional Test for Bug 35145671\n\nFunctional Test for Bug 35145671\n     Developer Txn : iginzbur_bug-35145671\n\nAdd this command to run on a E4-2c, and it will trigger FW upgrade\n    on Flash even if not needed. The goal is just to see that it is able\n    to do so:\n\n     /opt/oracle.cellos/CheckHWnFWProfile -action updatefw -component Flash\n     -mode force\n\n    Before running the above command, shut down MS and startup at the end",
    "platform": null
  },
  {
    "test_name": "tsagbug35151896.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug35151896.tsc - BUG 35151896 - [EXASCALE - PP1]ERS CONNECTIVITY\n#     WENT DOWN BRIEFLY DURING END OF STORAGE UPGRADE FROM 1106.4 TO 1201.4\n\npls see below\n\nto be added in lrgsaexacldegs3",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug35152390.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35152390.tsc - TESTCASE FOR 35152390 - UPDATE JAKARTA MAIL TO\n                           VERSION 2.1.2 OR MOVE TO ANGUS MAIL\n\nTESTCASE FOR 35152390\n       a. Set smtp related attributes in cell with 1 email id\n       b. cellcli -e alter cell validate mail",
    "platform": null
  },
  {
    "test_name": "tsagbug35170237.tsc",
    "setup": null,
    "flags": {
      "x7_ef_setup": "true",
      "sage_mirror_mode": "normal",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "cellname": "secondcell",
      "asmdisks_created": "2",
      "nflint": "1",
      "numlogfiles": "2",
      "do_not_set_numlogfile": "true"
    },
    "description": "tsagbug35170237.tsc - Test for bug 35170237\n\nTEST STEPS:\n 1. Bring up a debugcli EF env with at least 2 cells.\n 2. Bring up database/ASM and create a diskgroup with the flash disks.\n 3. Run workload against the disks.\n 4. Simulate flash failure for one disk ï¿½ should move the health factor to bad.\n 5. Re-enable flash disk ï¿½ should move the health factor back to good.\n 6. Restart cellsrv in a minute while rebalance is running ï¿½ health factor should not get set to bad again during cellsrv bootstrap.",
    "platform": null
  },
  {
    "test_name": "tsagbug35177097.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35177097.tsc - CC2 region heuristics population failure support for EHCC table\n\nStep 1: set the parameter cell_region_heuristics_read_count_threshold=10\n             and make sure CC2 persistence is disabled\n     Step 2: create EHCC table and flush flashcache\n     Step 3: simulate IMC population failure and run query then\n             unset the simulation\n     Step 4: Dump OCL RH stats, check the stat\n     Step 5: Run CC2 query, expect fail for check_columnar\n     Step 6: Run an update query\n     Step 7: Dump OCL RH stats again, check the stat\n     Step 8: Run CC2 query, expect success for check_columnar",
    "platform": null
  },
  {
    "test_name": "tsagbug35192099.tsc",
    "setup": null,
    "flags": {
      "genlogfile": "tsagbug35192099_ext"
    },
    "description": "tsagbug35192099.tsc - write new redo which overlaps with saved redo\n\nThis test verifies the behaviour that FLASH/PMEM log can\n     allow write of new redo which overlaps with previous saved redo\n\nThis test is ran with a option savedredo which can take any of below 3 arguments:\n     1. pmem - saved redo present on pmem disks\n     2. extfile - saved redo present in external file\n     3. flash - saved redo present on flash disks",
    "platform": null
  },
  {
    "test_name": "tsagbug35207230.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug35207230.tsc - Test for bug 35207230\n\nWhen a process besides RS sends a state dump signal (SIGUSR2)\n     to a service monitored by RS, and then later on that service crashes\n     RS will not wait for its state dump and terminate it prematurely.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug35210404.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true",
      "skip_oci_cred_bkp": "true"
    },
    "description": "tsagbug35210404.tsc - Tests scenario where object store credentials are incorrect\n\nThe test verifies that error message is thrown for volume backup operations when oci credentials are not properly set.\n     Following scenarios are tested:\n      1) When oci credentials are not set in the environment\n      2) When wrong credentials are set + the environment does not have any backup objects\n      3) When wrong credentials are set + the environment had backup objects\n      4) Correct credentials are set",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug35219016.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug35219016.tsc - check incorrect shadow metadata resilvering\n\nWhile cellsrv run resilvering of metadata part of sparse griddisk, we\n     check if it does not check shadow metadata by wrong if clause",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35227101.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35227101.tsc - Real HW Test for Bug 35227101\n\nPlease see below.\n\nadded in lrgrhx5safclarge1",
    "platform": null
  },
  {
    "test_name": "tsagbug35242323.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug35242323.tsc - Test for bug 35242323\n\nThis txn contains fix for a bug in a function ksz_get_oss_feature_support().\n     When all=FALSE is provided an argument (indicating that the caller wants to\n     know if *any* cells support the given feature), the function was incorrectly\n     returning TRUE when in fact no cells supported the feature.\n\nThe test has a 2 cell setup initially having PMEMLOG present\n     in both the cells. Now the test checks for PMEMLOG presence where\n     code internally uses ksz_get_oss_feature_support() function to check\n\n     1. PMEMLOG present on both cells\n     2. PMEMLOG dropped from cell1\n     3. PMEMLOG dropped from both cells",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35245686.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug35245686.tsc - Test for bug-35245686\n\nThis test covers case that there will be NO VES extent leakage in a snapshot collapse scenario\n\nThe set of steps in test are:\n     1. Enable the file collapse simulation event\n     2, Create a file\n     3. Create a snapshot\n     4. Resize the file\n     5. Restore the file to original size\n     6. Create snapshot again\n     7. Drop second snapshot first, then file and then first snapshot\n     8. Disable the file collapse simulation event\n     9. Fetch vault ID and look for specific extID from VES dump trace file,\n        should not exist",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug35254392_ib.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35254392_ib.tsc - Test for bug 35254392 for IB port\n\nCreate cell when switch port is disabled on ib hardware\n\nshould run on cell node with active/active InfiniBand network config",
    "platform": null
  },
  {
    "test_name": "tsagbug35254392_roce.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35254392_roce.tsc - Createcell-> switch-port disabled\n\nCreate cell when switch port is disabled on roce hardware\n\n-",
    "platform": null
  },
  {
    "test_name": "tsagbug35280507.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35280507.tsc - test case for bug 35280507\n\nBug 35280507 - RDBMS SIDE SUPPORT FOR ADDING PDB-LEVEL\n                    INSTANCE STATS FOR IMC POPULATION JOBS\n     This test case had these steps:\n     Step 1 : create table for testing and create snapshot of v$sysstat\n     Step 2 : clear cc before test begin\n     Step 3 : run fullscan to trigger background CC population\n     Step 4 : compare v$sysstat and sysstat_v1,\n              expect \"cell IMC population jobs submitted\" increase\n     Step 5 : do a full scan and expect CC savings\n              expect \"cell IMC population jobs succeeded\" increase\n     Step 6 : clear CC and set simevent\n     Step 7 : do full scan and expect CC savings\n              expect \"cell IMC population jobs failed\" increase",
    "platform": null
  },
  {
    "test_name": "tsagbug35305123.tsc",
    "setup": null,
    "flags": {
      "disable_testcases": "true",
      "cell": "^cell_node^",
      "cellconnstr": "root@^cell^.us.oracle.com",
      "oss_hw_testing": "true"
    },
    "description": "tsagbug35305123.tsc - Test for bug 35305123\n\nNew attribute: \"listeningInterface\" is being tested here.\n     The test case is being added in transaction - yifanch_bug-35305123\n\nMS used to listen on all interfaces by default with jetty.ssl.host\n     configured as \"0.0.0.0\". However, from a secruity perspective, this setup is not\n     desirable. To address the security concern, in this transaction,\n     introducing support for users to specify interface(s) that MS should listen on.\n     This ensures that MS server can be configured to only listen on interfaces\n     trusted by the user. This feature is tested here.",
    "platform": null
  },
  {
    "test_name": "tsagbug35305232.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35305232.tsc - Test for bug 35305232\n\nSet the caching policy for disks with the same\n     prefix to different values, wait for several minutes for the\n     polling task to complete, and check that an alert is generated.\n     Test Steps\n    1)- lrgdbcsaexcmdbresync1_hybrid sets up hybrid(exascale+exadata) env\n    2)- Change HWPOLL_INVL in cellinit.ora to 1\n    3)- Change cachinPolicy of a griddisk to none and see alert within\n        12*36*2 sec\n    4)- Change back cachingPolicy of griddisk in previous step, should clear\n        alert in 12*36*2 sec\n    5)- Change cachinPolicy of a 2 griddisks of same group to none and see\n        a single alert within 12*36*2 sec\n    6)- Change cachinPolicy of a 2 griddisks of different group to none and\n        see two alert messages in 12*36*2 sec\n\n\n   Notes\n     None",
    "platform": null
  },
  {
    "test_name": "tsagbug35315393.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagbug35315393.tsc - Test for fix for BUG 35315393\n\nThe following test covers two scenarios where Simulation event is set on EGS leader and\n     validating that the Rebalance completes successfully while the Cellsrv behaves as expected\n\n     In Scenario 1 - New Disk Case\n     1. Move a disk to dead state.\n     2. Once EGS drop forces the disk, we simulate the assert event:\n        'before_format_persist' on EGS leader.\n     3. Bring the dead disk back and wait for EGS leader to go crash due to the event.\n     4. As the new EGS leader is elected and REBALANCE starts, validate that cellsrv doesnot\n        hang/crash with ORA-00600[EgsConcurCtrl:: UpdateOngoingMap:request not found for DRAIN]\n\n     In Scenario 2 - returning Disk Case\n     1. Move the disk to dead state and wait for it to be FORCE DROPPED.\n     2. Simulate a delay event 'before_format_ioctl' on EGS leader.\n     3. Add the dead disk back and move it to Missing state.\n     4. Bring back the disk to Online state and then simulate disk failure.\n     5. Bring back the disk to Online state.\n     6. This will kick in REBALANCE and we validate that cellsrv doesnot crash due to\n        ORA-00600[EgsConcurCtrl:: UpdateOngoingMap: request not found for DRAIN]\n\nTest to be added in lrgdbcsaexacldegs12",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug35318878.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35318878.tsc - Functional test for BUG 35318878\n\n1. Manually trigger a kernel panic by running: echo c > /proc/sysrq-trigger .\n      2. The node will reboot. And once it's back, there should be a new directory under the crashfile directory. Wait until MS compresses it, which might take about 5 mins after MS starts up.\n      3. Decompress the final tar file and leave the , and delete the core file in the extracted directory (the one with hundred of MBs and ending in vmcore). Leave the original tar file at the same time.\n      4. Restart MS, wait until MS compresses the crashfile directory again.\n      5. The original tar file should remain untouched, while the extracted directory gets compressed to a new tar, whose name is identical to the original tar file except that it has a suffix starting from 1 to differentiate.\n      6. List the content of the 2 tars. The original one should contain the core file while the new one doesn't.",
    "platform": null
  },
  {
    "test_name": "tsagbug35333180.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "cell1_f": "^cell1^.us.oracle.com",
      "cellconnstr": "root@^cell1_f^"
    },
    "description": "tsagbug35333180.tsc - Test for bug 35333180\n\nTest to check whether the value of OS_NET_TX_BY_SEC and\n    OS_NET_RX_BY_SEC is different.\n\n    Steps:\n    1. Calculate average TX and RX speed of 50 samples retrieved\n       from list metricstream.\n    2. Check whether TX and RX speed is different or not.\n    3. Calculate TX and RX speed manually using \"ip -s link\".\n    4. Calculate average TX and RX speed of 50 samples retrieved\n       from list metricstream again.\n    5. Check the difference between manually calculted speed and\n       speed retrieved from list metricstream in step 1 and 3 for\n       for both TX and RX.",
    "platform": null
  },
  {
    "test_name": "tsagbug35333697.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "test_name": "^tst_tscname^_a"
    },
    "description": "tsagbug35333697.tsc - RAFT Log Corruption Test\n\nTests for RAFT Log Corruption Automatic Rescue\n     More details - https://confluence.oraclecorp.com/confluence/display/EXC/Automatic+Rescue+of+Corrupted+RAFT+Logs",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug35393726.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35393726.tsc - test case for bug 35393726\n\nbug 35397326: when running SSST with minOffsetMB: 0, it\n     will generate a ORA-600: PREDICATEOFLDISKCOMN::FILLUPQMENTITIESFORDISKREGION_2\n     This test case has below steps:\n     Step 1: start up the stack by srdbmsini\n     Step 2: generate a config file with ssst -g\n     Step 3: Modify the generated config file to add \"minOffsetMB: 0\"\n     Step 4: Run SSST by ssst -f config_file_name\n     Step 5: make sure there's not ORA-600",
    "platform": null
  },
  {
    "test_name": "tsagbug35404861.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug35404861.tsc - BUG 35404861 - EXACS | FLASHLOG SIZE REDUCED TO 384M\n\nTest for bug BUG 35404861 - EXACS | FLASHLOG SIZE REDUCED TO 384M\n   pls see below for test steps\n\nto be added in msbug19",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35411325.tsc",
    "setup": "srdbmsini",
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback"
    },
    "description": "tsagbug35411325.tsc - test case for bug 35411325 UNKEEP SECONDARY\n                           MIRROR POPULATION FOR POP KEEP IOCTL\n\nThis test has below steps:\n     Step 1: set up with 2 cells and create test table\n     Step 2: offline cell 2\n     Step 3: flush flashcache and alter test table\n     Step 4: check that cachedKeepSize is about half of cachedSize",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35421232.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "file_dest": "@^vault_db^"
    },
    "description": "tsagbug35421232.tsc - test case for bug 35421232: memory leak in reIssueIO error\n\nthis test has below steps:\n     Step 1: bring up ExaScale with srdbmsini\n     Step 2: update cellinit.ora to trigger reIssueIO and restart cellsrv\n     Step 3: set simulate event: IMCPOP_REISSUEIO_FAILURE\n     Step 4: populate CC2 and dump stat\n     Step 5: check that numObjects and numFreeObjects should match",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug35443703.tsc",
    "setup": "srdbmsini",
    "flags": {
      "disable_multims": "true",
      "SAGE_MIRROR_MODE": "normal"
    },
    "description": "tsagbug35443703.tsc - Test for bug 35443703\n\nTest for improvement in RQ dump collection\n\nThis test verifies the time for RQ dump\n     collection to be less than 50 seconds.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35449231.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug35449231.tsc - Test for BUG 35449231\n\nTest for BUG where v$tempstat does not reflect smart scan operations for global temporary tables\n\n1. Create global temp table, populate it\n      2. Query with cell_offload_processing FALSE, check v$tempstat;\n      3. Query with cell_offload_processing TRUE, check v$tempstat;\n      4. There should be significant increase in Physical Read Block stats",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35451570.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug35451570.tsc - Test case verifies the filePath format\n\nTest case for bug fix transaction - aembarca_fix.getfilename\n   1. Create a volume (1:*)\n\t   2. Create a snapshot from volume 1:\n   3. Create a bkp from snap 1.1*\n \t   4. Create a clone from snap 1.1*\n\t   5. Create a snapshot from the clone 2:*\n\t   6. Create a clone from snap 2.1:*\n\t   7. Create a snapshot from clone 3:*\n \t   8. Create a clone from previous snap 3.1:*\n\t   9. Create a snapshot from clone 4:*\n  \t   10. Create a bkp from snap 4.1:\n \t   11. Do \"lsvolume --attributes filePath\", expect:\n\t      @<vault>/vol.[a-f0-9]{32}\n\t      @<vault>/.clones/[0-9]{14}/[a-f0-9]{32}/vol.[a-f0-9]{32}\n\t      @<vault>/.clones/[0-9]{14}/[a-f0-9]{32}/vol.[a-f0-9]{32}\n\t      @<vault>/.clones/[0-9]{14}/[a-f0-9]{32}/vol.[a-f0-9]{32}\n   12. Do \"lsvolumesnapshot --attributes filePath\", expect:\n\t      @<vault>/.snaps/[0-9]{14}/vol.[a-f0-9]{32}\n\t      @<vault>/.snaps/[0-9]{14}/2:[a-f0-9]{32}\n\t      @<vault>/.snaps/[0-9]{14}/3:[a-f0-9]{32}\n\t      @<vault>/.snaps/[0-9]{14}/4:[a-f0-9]{32}\n   13. List bswmeta vaults using escli as admin:\n      \"escli --user admin:welcome1 ls @\\$BSTbswmeta0/ @\\$BSTbswmeta1/\n       @\\$BSTbswmeta2/ @\\$BSTbswmeta3/ @\\$BSTbswmeta4/ @\\$BSTbswmeta5/ @\\$BSTbswmeta6/\n       @\\$BSTbswmeta7/\"\n \t\tFrom output select only lines with \"snap_meta\" or \"bkp_meta\", we expect:\n\t      bkp_meta/myvault1.vol.[a-f0-9]{32}\n\t      snap_meta/myvault1.vol.[a-f0-9]{32}\n\t      snap_meta/myvault1.2:[a-f0-9]{32}\n\t      snap_meta/myvault1.3:[a-f0-9]{32}\n\t      bkp_meta/myvault1.4:[a-f0-9]{32}\n\t      snap_meta/myvault1.4:[a-f0-9]{32}",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug35491521.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug35491521.tsc - test case for BUG 35491521:\n                           ORACLE USER SHOULD NOT SEE SYSTEM EXASCALE VAULTS\n\nThis test has below steps:\n    Step 1: Create 1 admin user and 3 regular users\n    Step 2: Create vaults and files\n    Step 3: Change ACLs for vaults and files\n    Step 4: use edstool to check the contents users can see",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug35524505.tsc",
    "setup": "srdbmsini",
    "flags": {
      "SAGE_MIRROR_MODE": "high",
      "oss_testing": "3"
    },
    "description": "tsagbug35524505.tsc - bug 35524505: DO NOT INVALIDATE STORAGE INDEX ON WRITES TO SECONDARY MIRROR\n\nThis test has below steps:\n     Step 1: set up 3 cell environment and change cellinit.ora\n     Step 2: create table and run full table scan\n     Step 3: Purge SI for cell 2 and cell 3\n     Step 4: offline the griddisks for cell 1\n     Step 5: switch to cell 2 and run SI query\n     Step 6: update the table\n     Step 7: check that stat num_bytes_write_updated_for_secondaries increased\n             from step 5 for either cell 2 or cell 3",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35528147.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35528147.tsc - Test for bug 35528147:\n                           ORA-600 [DISKIOSCHED::GENERATESUBPLAN:PL MISMATCH]\n\nPlease see below",
    "platform": null
  },
  {
    "test_name": "tsagbug35572071.tsc",
    "setup": "srdbmsini",
    "flags": {
      "nflint": "1",
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'"
    },
    "description": "tsagbug35572071.tsc - Bug test for 35572071\n\nBug 35572071 - EXADATA: LIBCELL DOES NOT ALLOW RDMA READ VIA IPCDAT AFTER A CELL IS BACK ONLINE\n     1). Set up a test with 2 cellsrvs\n     2). Set up a DB workload that uses RDMA. The table should be large enough to span\n         multiple AUs (>>4MB ), but should be small enough to fit in the XRMemCache of\n         both cellsrvs (default is 200MB of XRMemCache each, so table should be less than 400M)\n     3). Verify that all IOs (or large proportion 99%) are going through RDMA. This can be done\n          with the existing RDBMS stats.\n     4). offline and online the cell\n      inactive griddisks on the cell\n      restart cell\n      activate griddisks on the cell\n     5). Wait for XRMemCache to get populated again. This may take a few minutes\n     6). Verify that after the warmup step above, all IOs are again going through RDMA\n\n  Comparing the ratio between \"cell single block physical read: xrmem cache\" and \"cell single block physical read: RDMA\"\n  is enough. (In Qian's case, that ratio was ~99% RDMA/1% non-RDMA before the cell offline/online,\n  and afterward she was seeing ~83% RDMA/17% non-RDMA in her 6 cell setup).\n  EVENT                                                            TOTAL_WAITS\n  ---------------------------------------------------------------- -----------\n  cell single block physical read                                            1\n  cell single block physical read: RDMA                                     11\n  cell single block physical read: xrmem cache                               6\n  cell single block physical read: flash cache                              35",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35597062.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagbug35597062.tsc - Test for transaction aembarca_missing.parent.snap.gc\n\nTest steps:\n\t1. Clean all volumes, snaps, bkps, etc.\n\t2. Create a volume\n\t3. Make 2 nested clones from that volume (clone of clones), there should be 3\n   volumes\n\t4. Delete all snapshots\n\t5. Set the simevent 'bsmEvents=\"ebs_simevent[BSM_VOL_DELETE_GC_CRASH] evarg1=2\"'\n         to crash BSM after BSW reply, when volume delete is on progress on GC mode\n         on volume 2:*\n\t6. Delete all volumes\n\t7. Wait until GC runs and does the simevent (1 min on fake HW).\n\t8. Let services restart and complete volume deletion ( could take up to 5 min)\n\t9. Confirm services were restored fine:\n         Create a volume (1:*) and a snapshot (1.1:*), then delete them.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug35625481.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug35625481.tsc - leak check at ROOT.war update\n\nCheck rs hb , cli handler socket\n\nFix stops application reload at ROOT.war update\n     Reloading fails at MS on DB",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35656837.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug35656837.tsc - test script for bug35656837\n\nPlease see below",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35678599.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "cell1_f": "^cell1^.us.oracle.com",
      "cellconnstr": "root@^cell1_f^"
    },
    "description": "tsagbug35678599.tsc - Test to cover bug 35678599 in cell\n\nTest Steps:\n       1. Run bunch of list cell attributes <attribute names> commands\n       2. Check for No such file or directory in ms-odl trace",
    "platform": null
  },
  {
    "test_name": "tsagbug35678599_db.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "compute_node_f": "^compute_node^.us.oracle.com",
      "dbnodeconnstr": "root@^compute_node_f^"
    },
    "description": "tsagbug35678599_db.tsc - Test to cover bug 35678599 in db node\n\nTest Steps:\n       1. Run bunch of list dbserver attributes <attribute names> commands\n       2. Check for No such file or directory in ms-odl trace",
    "platform": null
  },
  {
    "test_name": "tsagbug35695228.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^.log"
    },
    "description": "tsagbug35695228.tsc - testcase for bug 35695228: WRONG RESULT USING SUBSTRÂ\n\nThis testcase has below steps:\n     Step 1: create test table\n     Step 2: run the query with \"substr + in\" and\n             check for wrong result",
    "platform": null
  },
  {
    "test_name": "tsagbug35718589.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "cell": "^cell_node^",
      "cellconnstr": "root@^cell^.us.oracle.com"
    },
    "description": "tsagbug35718589.tsc - Bug 35718589 - AFTER PMEM REPLACEMENT, AN ADDITIONAL\n#                    PMEM IS LISTED IN PHYSICALDISK-DETAIL.OUT\n\npls see below\n\nTo be added in lrgrhx9sams4",
    "platform": null
  },
  {
    "test_name": "tsagbug35734268.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4",
      "standalone": "true"
    },
    "description": "tsagbug35734268.tsc - Test for Bug 35734268 - LNX64-22.4-EXASCALE,\n                 DISKS FORCE DROP AND ADD,DB INSTS CRASH WITH ORA-29770\n\nPlease see below\n\nTest is added in lrgdbcsaexacldegs17",
    "platform": null
  },
  {
    "test_name": "tsagbug35752336.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35752336.tsc - Test for bug 35752336\n\ncellsrv is aborting CC population if buffer is NON EHCC type and io target is set to CC flag.\n\nThe test cannot run alone. Should run it after tsaginitcc2.tsc",
    "platform": null
  },
  {
    "test_name": "tsagbug35766415.tsc",
    "setup": "srdbmsini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagbug35766415.tsc - Test for bug 35766415\n\nPlease see below",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35769438.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "OSS_ENABLE_NC_PERSISTENCE": "off",
      "creatdev_file": "tsagr2def",
      "uniq_dsknames": "all"
    },
    "description": "tsagbug35769438.tsc - Test for Bug 35769438\n\n1. Perform disk writes to fill up most of FC ~80%\n     2. Wait for data sync to kick in until amount dirty is below threshold, and stabilize\n     3. Perform disk writes to disjoint set ~45% size of FC\n     4. 2nd workload should be able to quickly reuse the data synced cache lines from workload 1\n     5. Wait for data sync to kick in and stabilize\n     6. Check how much synced/unsynced dirty cachelines we have\n\n     With the bug throttling disk writer from being able to repurpose data synced cache lines\n     the overall effect is that we are constantly in a state where OLTP AUX is under pressure\n     forcing almost all cache lines to disk writer for aging writes\n     Only after worklod is fully complete, we are able to perform most of the aging writes\n     for replenishing the AUX, but now we end up over replenishing the AUX\n     which leaves tFC with very little dirty cachelines left in FC\n\n     With the patch, the repurposing is able to provide cache lines to the 2nd workload\n     Ultimately, there's no exessive replenishment of OLTP AUX, and thus after the second\n     workload we should still see handful of dirty cache lines. (Test specifically checks for > 30%)",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35770440.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagdiskdef"
    },
    "description": "tsagbug35770440.tsc - Test for Bug 35770440: need a method to partition\n                           the flash cache into oltp, dw, and lw sections\n\nPlease see below",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35770444.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0",
      "CELL_ENABLE_FC_OPPO": "false"
    },
    "description": "tsagbug35770444.tsc - Bug 35770444 - FLASH CACHE LARGE WRITE DISK SYNC\n#                            DOES NOT COALESCE TO 1MB WRITE I/O SIZES\n\nSteps:\n     Run orion 100% large write workload on 10MB disk region\n     and make sure\n   \t'num 1MB LW Aging Writes' is non-zero\n'num LW Aging Writes below 1MB' and 'num LW Aging Write Chunks below 1MB' is zero.\n\nTo be added in lrgsafciovgroup4",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35770566.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug35770566.tsc - 35770566 - DISK IS POWER CYCLED WITHOUT CELLSRV BEING NOTIFIED FIRST\n\npls see below\n\ntest added to msbug19",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35771049.tsc",
    "setup": null,
    "flags": {
      "cell_with_flash_cache": "all",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug35771049.tsc - test for bug\n    35771049 - SLOW_RLTV CONFINEMENT SHOULD LOOK FOR A LONGER PERIOD OF TIME\n\npls see below\n\ntets to be added in msbug8",
    "platform": null
  },
  {
    "test_name": "tsagbug35778216.tsc",
    "setup": null,
    "flags": {
      "x7_ef_setup": "true"
    },
    "description": "tsagbug35778216.tsc - Test case for bug 35778216\n\nThe test is not using \"_cell_send_emails_and_asr_for_simulated_errors=true\" in cellinit.ora.\n     We need to check the the \"ASR is suppressed\"  log in ms-odl.trc for both the flash disk under\n     simulation as well as its peers and need to check no new ASRs are generated under simulation.\n     3 failure simulations are tested:\n     1) Flash Disk(TLC) w/ predictive failure -> back to normal\n     2) Flash Disk(TLC) failed -> back to normal\n     3) Poor disk perf(TLC) -> back to  normal",
    "platform": null
  },
  {
    "test_name": "tsagbug35780678.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35780678.tsc - Functional Test Simulation for Bug 35780678\n\nTest for Bug-35780678 - THE SYSTEM DOES NOT CONTAIN ALL OF THE LATEST KSPLICE UPDATES",
    "platform": null
  },
  {
    "test_name": "tsagbug35796999.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35796999.tsc - Functioanl test for BUG 35796999\n\nThe test steps includes :\n     1. list cell detail\n     2. list cell attributes offloadGroupEvents\n     3. Check the ms-odl.trc and ms-odl.log files and ensure no new instances of the string 'OSS IOCTL code string not available' have been added.",
    "platform": null
  },
  {
    "test_name": "tsagbug35807528.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "cell_with_pmem_cache": "true       ## create pmem cache",
      "log_name": "^tst_tscname^.log"
    },
    "description": "tsagbug35807528.tsc - test case for 35807528: IORM VAULT PLAN IN EXASCALE i\n                           ISN'T RE-SENT AFTER FLASHCACHE RESIZING\n\nIn this test, we have below steps:\n     Step 1. set limit for PDB and no limit for CDB\n             so that PDB get a portion of all available physical flashcache,\n     Step 2. check the flashcachelimit for the PDB\n     Step 3. resize flashcache to half of original value\n     Step 4. check flashcachelimit for PDB again, expect half of previous limit\n     Step 5. resize the pmemcache to half of original value\n     Step 6. check pmemcachelimit , expect half of previous limit\n     Exascale test is in oss_dbcsaiormvault.tsc (lrgdbcsaiormvault)",
    "platform": null
  },
  {
    "test_name": "tsagbug35811404.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "cell": "^cell_node^",
      "cellconnstr": "root@^cell^.us.oracle.com"
    },
    "description": "tsagbug35811404.tsc - Bug 35811404 - CELLDISK STUCK IN CONFINEDOFFLINE\n#                  STATUS AFTER MS IN-MEMORY DROP TIMED OUT\n\nPLEASE SEE BELOW",
    "platform": null
  },
  {
    "test_name": "tsagbug35823525.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35823525.tsc - Test case for Bug 35823525\n\nCheck interconnectCount from dbserver detail and network_ipconf_print.sh\n     is equal.",
    "platform": null
  },
  {
    "test_name": "tsagbug35831348.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35831348.tsc - Test for Bug 35831348- MULTIPLE AND REPEATED DOM0\n\nTest to induce the CPU failure and  check did we try to do the retry here.\n    1. Modify the shell script get_locate_led_status.sh\n    2. Then check for the error message in ms-odl.trc.\n    3. Cleanup and replace with original file.",
    "platform": null
  },
  {
    "test_name": "tsagbug35843759.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "devdir1": "^T_WORK^/raw^oss_port^"
    },
    "description": "tsagbug35843759.tsc - test for\n     Bug 35843759 - STORAGE TIER SOFTWARE MUST SUPPORT MEMBER LEVEL EXADATA\n                     FENCING IN A PERSISTENT MANNER\n\nPlease see below\n\ntest added in lrgsaexcmemfence1",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug35855296.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug35855296.tsc - Test case for bug-35855296\n\nTest case for enabling RQ DUMP for cell to cell copy ioctl\n\nSteps followed in the test are:\n\n     1. Set C2C_OFFLOAD_HANG event\n     2. Start C2C offload operation in DB side\n     3. Look for RQ dump collection messages in alert logs\n     4. Disable C2C_OFFLOAD_HANG event",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35865781.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagbug35865781.tsc - Test for BUG 35865781\n\nThe test steps includes\n      1. Create 3 cell setup\n      2. Create 100 vaults\n      3. Sleep 2 min to allow periodic 'space usage' metric collection to run\n      4. Stop usreds instance_1. EGS will reassign affected DataStores to remaining usreds instance_2\n      5. Sleep 2 min to allow periodic 'space usage' metric collection to run.\n      6. Start usreds instance_1. EGS will assign some DataStores back from instance_2 to instance_1\n      7. Add an 8 MB empty file (redundency = none) to each vault\n      8. Sleep 2 min to allow periodic 'space usage' metric collection to run\n      9. Check number of vaults with 'space usage'=8 MB. Supposed to be 100 vaults\n      10. Cleanup vaults created above",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug35871582.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35871582.tsc - Test for BUG 35871582\n\nBug 35871582 fixes a race condition that may happen when dropping a griddisk\n     and altering a griddisk on the same celldisk. Without the fix, the alter\n     operation may return a \"corrupt metadata\" error even though the metadata is fine.\n     1. Create 1 non-pooldisk and 1 pooldisk on the same celldisk.\n        Create 2 other pooldisks on different celldisks.\n     2. Set drop griddisk delay simulation (\"DROPGD_DELETE_ENTRY_DELAY\") to reliably\n        trigger race condition.\n     3. Drop non-pooldisk.\n     4. Create storagepool with pooldisks, which will alter the griddisk metadatas.\n        - Without fix: Get corrupt metadata error in traces, storagepool force drops\n                       the pooldisk on same celldisk as non-pooldisk.\n        - With fix: No error messages, all pooldisks in storagepool are online",
    "platform": null
  },
  {
    "test_name": "tsagbug35875060.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug35875060.tsc - Test for Bug 35875060 : ADD ALERT IF FFI IS OUT OF SPACE AND WON'T KICK IN\n\n1. srdbmsini to set up db and 1 cell.\n       2. set _cell_fc_max_FFI_region_allowed=50 in cellinit.ora. Each FFI region manages 4MB, so it\n       is 200MB FFI space.\n       3. restart cellsrv, ms.\n       4. wait 2 mins.// cell export FFI usage to MS per minute\n       5. CellCLI> list metriccurrent where objecttype='cell'\n          CL_FFI_CAP          raw    200 MB\n          CL_FFI_USED         raw    0.000 MB\n       6. CellCLI> list alerthistory //confirm MS gets FFI usage successfully and would alert based on usage\n        Limited flash storage space is available for Fast File Initialization (FFI). FFI is currently unavailable for files larger than 1 TB and\n        all file initialization may take longer if the available space is exhausted.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35920202.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug35920202.tsc - Test for Bug 35920202: Add an option to set\n                           default disk controller cache mode to wt\n\nPlease see below",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35920202_hw.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35920202_hw.tsc - Test for Bug 35920202\n\nPlease see below",
    "platform": null
  },
  {
    "test_name": "tsagbug35950218.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "cell": "^cell_node^",
      "cellconnstr": "root@^cell^.us.oracle.com",
      "intc1": "re0",
      "intc2": "re1"
    },
    "description": "tsagbug35950218.tsc - Bug 35950218 - NEED THE CAPABILITY TO AUTO TRIGGER\n          NETDIAG COLLECTION UPON NETWORK ISSUE\n\nPlease see below\n\nto be added in rhx9sams4",
    "platform": null
  },
  {
    "test_name": "tsagbug35960426.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35960426.tsc - Bug test for BUG 35960426\n\nWhen celltest is run on an incident file that contains XrMEM CC2 (with a buffer >1MB),\n     Celltest fails with the following error: POFC_processIMCPopMsg_invalidCCPopMode.\n     This is because celltest did not have the capability to handle CC buffers > 1MB.\n     This has now been fixed. Thus, we want to create a functional test that crashes the\n     XrCC population and generates an incident file with a buffer that is >1MB. We then\n     want to run celltest on this incident file and ensure that it completes without throwing\n     an error, thus confirming the fact that celltest is now able to handle Population buffers > 1MB.",
    "platform": null
  },
  {
    "test_name": "tsagbug35964819.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug35964819.tsc - Test case for bug 35964819\n\nStep:\n     1. Set up db and cell by srdbmsini\n     2. Enable kcfis trace\n     3. Set cell event immediate cellsrv.cellsrv_setparam\n     4. Create table of around 300MB\n     TestCase 1.\n       a. Clean db buffer cache\n       b. select count(*) from <table>\n       c. Check db trace and no trace should be present\n     TestCase 2.\n       a. alter table <table> storage(cell_flash_cache default);\n       b. Clean db buffer cache\n       c. alter table <table> storage(cell_flash_cache keep)\n       d. Check db trace all appliances are busy with popkeep,\n          there should be trace\n     TestCase 3.\n       a. Run TestCase 1.a. and TestCase 2.c parallely\n       b. Check db trace, there should be new trace",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug35968174.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug35968174.tsc - Test case for bug 35968174\n\nTest case for Bug 35968174 : DECREASE CELL CUSTOM DIAGPACK SIZE BY IMPROVING METRIC COLLECTION\n\nThis test covers below mentioned 2 cases:\n     Case 1 : Verify the size of user triggered diagpack is within a\n              threshold, and change in size for two diagpacks within\n              few minutes is within a threshold.\n     Case 2 : Verify the size of alert logs in trace and diagpacks to\n              be of same size when cellsrv and RS are shutdown. Also,\n              two diagpacks in such an interval should have same size",
    "platform": null
  },
  {
    "test_name": "tsagbug35994483.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35994483.tsc - Test for Bug 35994483: cellsrv startup delayed\n                           beyond 4mins due to rds-ping test failure in celld\n\nPlease see below",
    "platform": null
  },
  {
    "test_name": "tsagbug35998752.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug35998752.tsc - RAISE AN ALERT FOR VKTM FORWARD TIME DRIFT\n\nTest to verify that MS raises a forward time drift alert for occurrence\n     of forward time drift messages in the alert log on dbserver. Also checks\n     that at most one such alert is raised every minute.\n\nBug 35998752 - RAISE AN ALERT FOR VKTM FORWARD TIME DRIFT",
    "platform": null
  },
  {
    "test_name": "tsagbug36006006.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug36006006.tsc - Test for bug-36006006\n\nTest for bug 36006006 DO NOT MATCH COMMAND LINE PROCESS IN RS\n     WHEN ARGUMENT IS TOO LONG\n      This happends when RS grabs wrong MS pid from search after\n      MS restart\n\nTest Steps :\n     1 : Shutdown MS and verify RS status\n     2 : Run C script from /bin folder to trick RS\n     3 : Startup MS\n     4 : Kll the fake process from step 2",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36006239.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagbug36006239.tsc - Bug test for Bug 36006239 and Bug 35548952\n\nFunctional Test for Bug 36006239 and Bug 35548952\n     Detailed steps:\n       1. Check cellsrvstat for all 3 cells. We want to record the values for âFlashCache pop requestsâ and âXRmemCache incr touch count reqsâ.\n       2. Create several large tables (several hundred MB at least).\n       3. Simulate a failure for all the regular datafiles on cell 1.\n       4. Run a workload to generate many ramcache/FC pop ioctls. In my experience, populating 3 tables with ~50MB should be enough.\n       5. Check the cellsrvstat for all 3 cells.\n            a. Cell 1 should not have any change in âFlashCache pop requestsâ or âXRmemCache incr touch count reqsâ\n            b. Cell 2 and cell 3 should see these stats increase.\n            c. The sum of âFlashCache pop requestsâ on cell 2 + cell 3 should be 20 â 40% of the sum of âXRmemCache incr touch count reqsâ on cell 2 + cell 3 in case of exadata and 15 - 55% in case of Exascale.\n       6. End the failure simulation on cell 1.\n       7. Run another workload to generate ramcache/FC pop ioctls.\n       8. Check the cellsrvstat for cell 1.\n            a. Cell 1 should see an increase in âFlashCache pop ioctlsâ but not âXRmemCache incr touch count ioctlsâ.\n\n      Orareview: https://orareview.us.oracle.com/135308152\n      Owner email: chad.hayen@oracle.com\n\nTest script accepts 2 boolean parameters:\n         1. skip_big_table (TRUE|FALSE): If True skip populating big tables for test (Default: FALSE).",
    "platform": null
  },
  {
    "test_name": "tsagbug36015718.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "OSS_ENABLE_NC_PERSISTENCE": "off",
      "creatdev_file": "tsagr2def",
      "uniq_dsknames": "all"
    },
    "description": "tsagbug36015718.tsc - Test for Bug 36015718\n\n1. Perform OLTP small writes to fill up FC ~50%\n     2. Wait for Data Sync to kick in and have ~50% synced cache lines\n     3. Disable Data Sync\n     4. Perform additional disjoint OLTP small writes\n        to fill up additional ~20%. These are unsynced. We do 20% so that\n        we don't prematuraly deplete the AUX and trigger replenishment\n     5. Perform OLTP reads on disjoint set for >100% of FC space\n        to cause AUX depletion\n     6. Check that majority of replenished cache lines came from\n        synced cache lines rather than unsynced\n\n     We want to ensure that synced cache lines get priority for disk writer processing\n\n     With the bug, we will see roughly similar amounts of both synced and unsynced\n     cache lines that got replenished\n\n     With the patch, we should see replenishment targets mostly synced cache lines",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36062216.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagbug36062216.tsc - Test for Bug 36062216\n\nPlease see below\n\nTest is added in lrgsaexacldvesscrub",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36101321.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_failgroup": "failalldbdg",
      "oss_auto_manage_disks": "true",
      "debugcli": "1"
    },
    "description": "tsagbug36101321.tsc - Test case for bug 36101321\n\nBug 36101321 - EF CLUSTER - NO AUTOMATIC DROP/REBALANCE ON ASM AFTER FLASH DISK FAILED\n\nThis test covers 2 cases:\n\n   1. Removing/Adding Flash disk\n     i.   Connect to cell2\n     ii.  Record the grid disks that are cached by c9FLASH0\n     iii. Shut down services\n     iv.  Remove a flash disk\n     v.   Start up services in the order of \"rs-ms-cellsrv\"\n     vi.  Clean-up: Recreate the flash disk\n\n   2. Moving disk file (failing physical disk)\n     i.   Connect to cell2\n     ii.  Pick a test hard disk, and record its serial number and\n          device name  (which is also its fake disk file path).\n     iii. Shut down services\n     iv.  Move the disk file datafile0 outside $OSS_DEVDIR\n     v.   Set parameters \"_disk_last_powercycle_time_limit=1\" and\n          \"_cell_disk_fail_list=<physicalDiskID>\" in cellinit.ora\n     vi.  Start up services in the order of \"rs-ms-cellsrv\"\n     vii. Clean-up: move back disk file datafile0 back\n          to $OSS_DEVDIR and cancel the failure simulation",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36108866.tsc",
    "setup": null,
    "flags": {
      "oss_columnar_cache": "true"
    },
    "description": "tsagbug36108866.tsc - Bug 36108866 - WRONG RESULT ISSUE EVEN AFTER APPLYING FIX FOR EX83\n\n1. Run srdbmsini oss_columnar_cache=true\n\t2. In one session set the ASM env:\n\t3. Copy datafile to ASM:\n\t4. Copy the dmp file to DATA_PUMP_DIR:\n\t5. Create user:\n\t6. Add the parameter to support 16k db block size:\n\t7. run the impdp to plugin the ts:\n\t8. resize the tempfile, after adding the new disks to ASM:\n\t\ta. Add physical disks, celldisks and griddisks\n\t\tb. In the session that can access ASM, you may need to modify the IP:port information:\n\t\tc. Resize the temp file in the database: -> This step ended with error , but its fine to proceed with next step\n\t9. Create table ehcc1\n\t10.Run problematic query for 3 times, we will see wrong result. With the fix, we will get correct results.",
    "platform": null
  },
  {
    "test_name": "tsagbug36113791.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug36113791.tsc - Bug test for BUG 36113791",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36128594.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug36128594.tsc - Test for bug 36128594\n\nThe test runs IOV and monitors cellsrvstat during the time.\n     And expects to see 10 large write reasons listed in the stat\n     and the values would be sorted in descending order",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36135016.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug36135016.tsc - Functional test for BUG-36135016\n\nActual Bug details : Bug 36135016 - REDUCE PS.EXAWATCHER COLLECTION",
    "platform": null
  },
  {
    "test_name": "tsagbug36149249.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug36149249.tsc - Test for Bug 36149249\n\nBug 36149249 - ESCS-PP1: DATA HARD DISKS ENTERED CONFINEMENT STATUS\n\n     Test Steps:\n      1. Add _cell_allow_reenable_predfail=true in $OSSCONF/cellinit.ora\n      2. Save the copy of the below original files in directory\n         $OSS_SCRIPTS_HOME/unix/hwadapter/diskadp/harddisk/\n         a. power_control_disk_x7.sh\n         b. sn1/check_and_wait_for_disks_power_off.sh\n\n      3. Add \"exit 0\" at the beginning of power_off_disk.sh and\n         power_control_disk_x7.sh\n      4. Add \"exit 1\" in sn1/check_and_wait_for_disks_power_off.sh\n      5. Simulate failure on any 1 hard disk ( non-system disk)\n      6. Check disk status as it will get stuck in\n         \"warning - confinedOffline - powering off\"\n      7. Then Agian check disk again after 1 minute and the disk status\n         should be in \"warning - confinedOffline - powering off\"\n      8. Restore all script files to the original (i.e replace the file\n         with original which is saved in step-2)\n      9. Again check the disk status, it should move to failed state.\n         If not check periodically every 1 minute for 10 times.\n     10. After getting disk status as \"failed\". unsimulate the failed disk",
    "platform": null
  },
  {
    "test_name": "tsagbug36192770.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug36192770.tsc - Test case for bug 36187811\n\nTest case for bug 36187811\n\nTest case for changes fixing a regression of object keep ioct.\n     Steps in the test are:\n     1. Restart DB\n     2. Set event on DB as well as CELL side\n     3. Create a table with KEEP and insert some data into it\n     4. Get objectID of created table\n     5. Verify hoursExpirationTime should be 1 hours\n     6. Wait for 1 hour\n     7. Verify the cachedKeepSize should be same\n     8. Verify KCBFLC traces in DB traces",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36223770.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug36223770.tsc - Bug test for Bug 36223770\n\nDB trace files get filled up with IPCDAT_WC_REM_ACCESS_ERR, if a cell key is\n     created on the cell (cellcli -e assign key for cell), but without ASM scoped\n     security enabled at all (no cellkey.ora on DB nodes)\n\n     Steps for test script:\n      1. Setup a cell without ASM SCOPED SECURITY\n      2. Create a key for cell\n      3. Assign key for cell\n      4. Look for \"IPCDAT_WC_REM_ACCESS_ERR\" in DB traces.",
    "platform": null
  },
  {
    "test_name": "tsagbug36228557.tsc",
    "setup": null,
    "flags": {
      "cell_server_simevent": "'\"cellsrv_simevent[CREATEGD_NON_STRICTASMGROUP] frequency=1\"'"
    },
    "description": "tsagbug36228557.tsc - Bug test for bug 36228557\n\nRDMA ERRORS WHEN ASM SCOPED SECURITY IS ENABLED ON DB BUT WITHOUT\n     GRIDDISK \"AVAILABLETO\" SET\n\nTest uses following parameters:\n        threshold (INT): tolerance for IPCDAT_WC_REM_ACCESS_ERR\n     For ade view run, execute as: \"oratst -d tsagbug36228557.tsc threshold=50\"",
    "platform": null
  },
  {
    "test_name": "tsagbug36237852.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "num_cells": "3"
    },
    "description": "tsagbug36237852.tsc - Test for Bug 36237852\n\nPlease see below\n\nadded in lrgsaexcfcgroups",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36249710.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug36249710.tsc - Test case for BUG 36249710\n                            CELLSRV CRASH WITH ORA-07445\n\nThis test has below steps:\n     Step 1: Create test table that exceeds 100M in size\n     Step 2: Set _cell_imc_pop_job_limit=2 in cellinit.ora\n     Step 3: Query table and wait for CC2 populate in FC\n     Step 4: Set sim event for XRCC pop job\n     Step 5: Query table again\n     Step 6: Drop FC, Will see cellsrv crash without the fix",
    "platform": null
  },
  {
    "test_name": "tsagbug36269236.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug36269236.tsc - ATP purging test on real hardware cell\n\nbug36269236 - Tests purging of ATP files",
    "platform": null
  },
  {
    "test_name": "tsagbug36269236_db.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug36269236_db.tsc - ATP purging test on real hardware cell\n\nbug36269236 - Tests purging of ATP files",
    "platform": null
  },
  {
    "test_name": "tsagbug36269517.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true"
    },
    "description": "tsagbug36269517.tsc - Test for bug 36269517\n\nTest for purging logic in MS to delete stale MS-OSS libcell\n     trace files that are older than 14 days on a daily basis.",
    "platform": null
  },
  {
    "test_name": "tsagbug36299135.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug36299135.tsc -Bug test for BUG 36299135",
    "platform": null
  },
  {
    "test_name": "tsagbug36303832.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef"
    },
    "description": "tsagbug36303832.tsc - Test for Bug 36303832\n\nplease see below",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36308573.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug36308573.tsc - Test for bug 36308573\n\n1. Create 20k vaults\n2. Stress ERS with repeated calls for vault metrics\n3. Check for ERS Memory leak, there should be no leak\n4. Check for memory usage by ERSSRV, should be < 4.5g till an hour\nWithout the fix ERS mem leaks and ers mem usage grows > 5g",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36309013.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "cellconnstr": "root@^cell^.us.oracle.com"
    },
    "description": "tsagbug36309013.tsc - Test for Bug 36309013\n\n36309013 (alter physicaldisk drop for replacement is\n         causing a heavy impact on workload)\n\ndropping a physical disk from the WBFC in MS triggers  flash cache\n    flushing, impacting client workload.\n   Now, MS translates a WBFC physical disk replacement into a disk failure\n    simulation inside of MS. This will avoid FC flushing.",
    "platform": null
  },
  {
    "test_name": "tsagbug36323491.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^.log"
    },
    "description": "tsagbug36323491.tsc - test case for bug 36323494\n\nStep 1: restart cellsrv and run tsagbug36323491.sql\n     Step 2: simulate the offloadGroupEvents FPLIB_REQUEUE\n     Step 3: Run cellsrvstat to check for SQL memory consumption\n             prior to start of query\n     Step 4: Run the query\n     Step 5: check that the query does not exceeds 30M in Total",
    "platform": null
  },
  {
    "test_name": "tsagbug36329475.tsc",
    "setup": "tsaginit",
    "flags": {
      "flash_size": "512",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "asm_disk_def": "tsagrdmadef"
    },
    "description": "tsagbug36329475.tsc - test case for Bug 36329475\n\nThis test case has below steps:\n     Step 1: run 100% write onrion workload, IO size 1KB\n     Step 2: Save cellsrvstat after workload\n     Step 3: run 100% small read orion workload to populate remaining into FC\n     Step 4: Save cellsrvstat after workload\n     Step 5: shutdown cellsrv and startup cellsrv and drop xrmemcache\n     Step 6: set sim event FLASHCACHE_READ_CORRUPT_DATA\n     Step 7: run 100% LARGE read orion workload\n     Step 8: Save cellsrvstat after workload\n     Step 9: check for FLASHCACHE_READ_CORRUPT_DATA in alertlog\n     Step 10: check that there are no read errors in the alertlog",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36335753.tsc",
    "setup": null,
    "flags": {
      "cell_with_flash_cache": "all",
      "cell_with_pmem_cache": "true",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug36335753.tsc - Bug 36335753 - MESSAGE \"POST RDMA OP: IBV QP\n          SEND WORK REQUEST QUEUE IS OUT OF SPACE. MAX: 256\" IN SMON TRACE FILES\n\npls see below\n\nto be added in lrgsanvcache1",
    "platform": null
  },
  {
    "test_name": "tsagbug36360109.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug36360109.tsc - Test for Bugfix -36360109\n\nThe test validates an error message for alter flashcache all",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36395123.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug36395123.tsc - Bug 36395123 - DO NOT CREATE INVALID STORAGE INDEX SUMMARY\n\nPlease see below",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36400858.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug36400858.tsc - Test for bug 36400858\n\nThe test verifies that async snap delete retry does not run into errors",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36409525.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "OSS_ENABLE_NC_PERSISTENCE": "off",
      "creatdev_file": "tsagr2def",
      "uniq_dsknames": "all"
    },
    "description": "bug36409525.tsc - Test for Bug 36409525\n\n1. Drop FC\n     2. Populate test region with data version 1\n     3. Enable write hang simulation\n     4. Issue write with data version 2\n     5. Create FC while write is hung\n     6. After write is done, ensure that the next read is version 2\n\n     Test ensures that FC correctly handles writes that are issued\n     prior to bootstrap.\n\n     In the bug we saw that a population read can land right after\n     FC creation before the outstanding write lands. The write then only\n     lands on disk and not cache as the write is not fully tracked.\n     This led to stale data in FC.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36447092.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagbug36447092.tsc - Test for Bug 36447092\n\nBUG 36447092 - Move large write with filetype = Flashback Log from MRW to NRW\n\n     Test Steps:\n     1. Setup cell\n     2. Get IP Address of the cell and store in get_ipaddr.dat file.\n     3. Mask values and whitespaces from ipaddress. o/p file - tsagorip.log\n     4. Create a fc.lun for write I/Os and copy to rdbms/bin folder\n     5. Check cellsrvstat before Orion workload and store it in $T_WORK/afterorion.lst\n     6. Run the Orion workload to move LW with filetype 'Flashbacklog' from MRW to NRW\n     7. Again check the cellsrvstat after Orion workload and store it in $T_WORK/afterorion.lst\n     8. Compare the value of new (num NRW currently cached in flash) before and after Orion workload\n      a. Using compare.sh\n      b. Compare 'num NRW currently cached in flash' before($n1) and after($n2) orion Workload\n      c. If n2 is greater than n1 and not equals to n1\n      d. Flashbacklog File LW absorbed in NRW",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36457966.tsc",
    "setup": "srdbmsini",
    "flags": {
      "file_dest1": "'+'datafile"
    },
    "description": "tsagbug36457966.tsc - Test for bug 36457966\n\nDuring smart scan, when we are using large metadata feature, we have a âgold-copyâ\nshared metadata and all threads performing smart scan take a sparse clone metadata.\nThis shared+cloned metadata usually help save memory usage during smart scan.\nBut there are some queries whose metadata does not benefit from this shared+cloned\nmetadata approach.  When we detect that a given shared+clone metadata is inefficient\nfor a query, we disable the shared+cloned metadata feature for that specific query.\n\nWhile cleaning up the shared metadata for an inefficient query, we have a race that is\nleading to the assert.\n\nORA-00600: internal error code, arguments: [PMShared::deallocateMemoryForMetadata_clones],\n[0x14A4482C0], [7], [], [], [], [], [], [], [], [], []",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36467429.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0"
    },
    "description": "tsagbug36467429.tsc - orion test case for bug 36467429\n\nTest case for lower level flash cache touch count increment when\n     blocks are cached in XRMemCache and accessed via SKGXP.\n\nTest steps:\n     1. Disable IPCDAT on client.\n     2. Warm up OLTP data set by running orion 100% small read workload\n        on 10MB disk region. 10MB region should be cached by both\n        FlashCache and XRMemCache.\n     3. Reset stats.\n     4. Repeat the small read workload on 10MN disk region.\n        All the reads get served from XRMemCache.\n     5. Verify XRMemCache issues touch count increment requests to\n        FlashCache. And all the FlashCache touch count increment requests\n        succeed, which indicate all the disk regions are cached in\n        FlashCache as well.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36519890.tsc",
    "setup": "srdbmsini",
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "cdb": "true",
      "OSS_AUTO_MANAGE_DISKS": "true",
      "sage_mirror_mode": "high",
      "SAGE_MIRROR_MODE": "normal"
    },
    "description": "tsagbug36519890.tsc - Test case for bug 36519890\n\nTest checks if the following stats bumps up across OLTP read.\n     1) numAllocPrimaryMirror\n     2) numAllocOtherMirror\n     3) FlashCache pop ioctls",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36559132.tsc",
    "setup": null,
    "flags": {
      "cell_with_flash_cache": "all",
      "cell_with_pmem_cache": "0",
      "cell_with_xrmem_cache": "1",
      "num_flash_per_cell": "2",
      "flash_size": "256",
      "num_gd": "4",
      "gdsz": "368",
      "creatdev_file": "tkfgmydef"
    },
    "description": "tsagbug36559132.tsc - UNMAP DIRECT ACCESS XRMEMCACHE CC REGIONS\n                           IN OFLSRV, TO SAVE PTE MEMORY\n\nThis test has below steps:\n     Step 1: Run ssst to populate XRCC\n     Step 2: check cellsrv stat is non-zero\n     Step 3: Run orion to invalidate xrcc hits\n     Step 4: Check cellsrvstat again, XRCC should be invalidated",
    "platform": null
  },
  {
    "test_name": "tsagbug36559380.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cellconnstr": "root@nshqap01celadm02.us.oracle.com"
    },
    "description": "tsagbug36559380.tsc - Test case for bug 36559380\n\nSince the cloud Exascale systems are not internal systems, this\n     softwareUpdate store value should not be set by default.\n\n     Uses underscore parameter \"_force_production_system=true\" to simulate\n     non internal system and verify that softwareUpdate store value is not set",
    "platform": null
  },
  {
    "test_name": "tsagbug36565814.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug36565814.tsc - test for Bug 36565814, check if there is\n     any existing OAL memory leaks, also simulate a leak to ensure\n     this script itself works correctly.\n\nThis test runs the below steps twice, without and with simulated\n     OAL memory leaks:\n     Step 1: create test table\n     Step 2: Drop and recreate XRMEMCache\n     Step 3: get the state dump\n     Step 4: Check the state dump\n     Step 5: Run query\n     Step 6: Check the state dump after query\n     Step 7: compare the 2 dump results\n     Expect no new item without the leak, new items with the leak.",
    "platform": null
  },
  {
    "test_name": "tsagbug36587918.tsc",
    "setup": null,
    "flags": {
      "egsrs_stop_timeout": "2",
      "sage_mirror_mode": "high",
      "oss_multims_testing": "true",
      "oss_exascale_asm_testing": "true"
    },
    "description": "tsagbug36587918.tsc - checks if there is a race condition between EGS leader and cellsrv.\n\nTest for checking if there is a race condition between EGS leader and cellsrv in a multims , multicell exascale environment.",
    "platform": null
  },
  {
    "test_name": "tsagbug36608794.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug36608794.tsc - Test for bug 36608794\n\nWARNING FOR DBRSMAIN HAVING BOTH SETUID-ROOT AND EFFECTIVE",
    "platform": null
  },
  {
    "test_name": "tsagbug36616063.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagbug36616063.tsc - Test cases for bug 36622685 and 36616063\n\nFor testing fix against bug 36622685 what we want to verify\n     is that when there is a long running backup job and there\n     is a reconfiguration event and sending signals to suspend\n     ongoing operation, we do not mistakenly take an active long running job\n     as a dead one and assert. So, the steps to verify this would be:\n       1. create a volume vol1 of size 10G on v1\n       2. create snapshot snap1 of vol1\n       3. set event BSW_BKP_CREATE_DELAY_SIM and create backup bkp1 of snap1.\n          This will delay the backup creation for 10 minutes\n       4. after 9 minutes, turn off the event simulation and restart the BSM\n          to trigger a reconfiguration event.\n       5. Expect the workload to continue and no assert should happen.\n\n    For bug 36616063, event simulation to simulate a scenario where in bsw\n    a vip install job is not added to vip run queue and is directly replying to bsm,\n    to test this we will:\n       1. set the new event simulation to simulate a vip incarnation mismatch on bsw (once only)\n       2. restart bsm to trigger a reconfiguration event\n       3. Expect no assert happening.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36621269.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug36621269.tsc - Test for bug 36621269\n\nIgnoring the Cell ID attribute from config xml and always read that from hw cace",
    "platform": null
  },
  {
    "test_name": "tsagbug36626859.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug36626859.tsc - Test for BUG-36626859\n\nBug - make sure the egs deployment fails, if there are\n           bad ips in the oeda xml file",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36627308.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug36627308.tsc - Test for bug36627308\n\nThis test runs in two part of failure , one with egs killed and another with ipaddress deleted from cellinit.ora and egs killed\n\n     Bug 36627308 DBMCLI -E LIST ESNODE RESULTS IN ERROR: DBM-02559: THERE IS A COMMUNICATION ERROR BETWEEN MS AND CELLSRV\n     This project aims to provide better error handling of some cellcli/dbmcli commands\n     when EGS is (1) configured but unavailable or (2) ip misconfigured.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36632571.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagbug36632571.tsc - chcluster shutdown with AEP\n\n36632571 - AEP STARTUP SERVICES WHILE CHCLUSTER --SHUTDOWN IS IN PROGRESS",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36651647.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug36651647.tsc - Test for Bug 36651647 and \t36598913\n\nPlease see below",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36655283.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagbug36655283.tsc - Bug 36655283 - EXACS : POST FLASHDISK REPLACEMENT LUN\n#                           DETAILS MISSING | FLASHDISKS ARE NOT NORMAL\n\npls see below\n\nto be added in lrgrhx9sacel",
    "platform": null
  },
  {
    "test_name": "tsagbug36657778.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug36657778.tsc - Bug 36657778 IORM FAKE DBS IOS ARE NOT MAPPED CORRECTLY\n\nThis test has below steps:\n       Step 1: create Fake dbs uder vaults\n       Step 2: list vault/database detail\n       Step 3: issue IOs to fake DBs with Orion\n       Step 4: Dump FC groups and check type is not 0\n       Step 5: Check the IORM metrics",
    "platform": null
  },
  {
    "test_name": "tsagbug36678104.tsc",
    "setup": "srdbmsini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true",
      "sage_mirror_mode": "normal",
      "oss_failgroup": "failalldbdg     # mirror controlfile and flint diskgroups too"
    },
    "description": "tsagbug36678104.tsc - test for bug 36678104\n\nTestcase for Bug# 36678104 -\n      HDD REPLACED DIDN'T RECEIVED CLEAR MESSAGE IN ALERT HISTORY.\n      Added in lrgdbconsaprdd13\n      The test does the following things\n      â¢\tSimulate the disk failure and ensure that the\n      . rebalance is complete.\n      â¢ Waits for 5 mins and check the alert history to confirm\n      . the rebalance completion\n      â¢\tSets some parameters in the cellinit.ora parameter and restart MS\n      â¢\tCancels the disk failure simulation.\n      â¢\tWaits for 6 â 8 mins and validates the alert history and ensure\n      . that the disk drop alert is cleared.\n      â¢\tChecks the MS trace and ensure that the expected code\n      . path is executed.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36691396.tsc",
    "setup": null,
    "flags": {
      "differentszfg": "true",
      "dangerousfg": "true",
      "oss_auto_manage_disks": "true",
      "oss_exascale_asm_testing": "true",
      "sage_mirror_mode": "high",
      "oss_multims_testing": "true",
      "use_prefix_hybrid": "true"
    },
    "description": "tsagbug36691396.tsc - Test for Bug 36691396\n\nShutdown Exascale stack and ASM should be able rebalance when needed\n     Cellsrv should not crash",
    "platform": null
  },
  {
    "test_name": "tsagbug36735886.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug36735886.tsc - Functional test for Bug 36675629\n\nPlease see below",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36742280.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "media_type": "ALL"
    },
    "description": "tsagbug36742280.tsc - Test for BUG 36742280\n\nTest to verify libcell I/O outlier tracing",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36760261.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagbug36760261.tsc - Test for Bug-36760261\n\nThe test includes the following testcases\nTest Case-1:\n1. Create storagepool with 3 cells\n2. Set the state of the disk pd00_simcell00 to INACCESSIBLE_UNKNOWN\n3. 20 seconds later, set the state of the disk pd00_simcell1 to INACCESSIBLE_UNKNOWN\n4. 20 seconds later, set the state of those two disks to DEAD\n   This should trigger a rebalance.\n5. Recreate those two disks with ACCESSIBLE_GOOD\n   This should trigger a rebalance. Wait for rebalance to complete.\nTest Case-2:\n1. Create storagepool with 3 cells\n2. Set the state of the disk pd00_simcell00 to INACCESSIBLE_UNKNOWN\n3. 20 seconds later, set the state of disk pd00_simcell00 to DEAD\n   This should trigger a rebalance\n4. At about the same time, set the state of the disk pd00_simcell1 to INACCESSIBLE_UNKNOWN\n5. 20 seconds later, set the state of pd00_simcell1 to DEAD\n   Rebalance should continue to run.\n6. Recreate those two disks with ACCESSIBLE_GOOD\n   This should trigger a rebalance. Wait for rebalance to complete",
    "platform": null
  },
  {
    "test_name": "tsagbug36766407.tsc",
    "setup": null,
    "flags": {
      "DBOSSCONF": "^T_WORK^/db/^ORACLE_SID^"
    },
    "description": "tsagbug36766407.tsc - Unit test for bug 36766407\n\nBug 36766407 - [ESCS-SCL1/SJC1] STUCK EDV THREAD ON KVM\n     COMPUTE RESULTING IN OSSTLS_HANDSHAKE FAILURE\n\n     The goal of this unit test is to simulate a TLS handshake hang in EDV's\n     EGSLIB shared thread, and make sure the thread can sync with the primary\n     thread for the latest TLS certificate, and move forward to do the next\n     round of handshake after the hang.\n\n     Test steps:\n       1. oratst -d xblockini ledvsetup=true\n       2. Use escli to set certDurationInMins to 20 minutes\n       3. Shutdown EDV\n       4. Shutdown DBRS, so RS won't kill EDV due to our hang simulation\n       5. Append \"libcell.tls_handshake_hang_sim hang_in_min=25\"\n          to _cell_client_event of the EDV's cellinit.ora\n       6. Start up EDV directly(Don't use RS, since RS will kill the EDV\n          if it hangs for more than 5 minutes)\n       7. Check and make sure the hang simulation is enabled in one of the\n          EDV's EGSLIB shared thread.\n       8. Wait for 25 minutes, so the hang simulation completes. Check and\n          make sure the share thread got the latest TLS cert from EGSLIB\n          primary.",
    "platform": null
  },
  {
    "test_name": "tsagbug36777711.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug36777711.tsc - Test case for bug 36777711\n\nThe test uses simevent for crash simulation on BsmVolCreateJob(58) at state\n     BSMVOLCREATEJOBSTATE_FILE_CREATE_REQ(8)\n\n     Test Steps:\n     - set simevent\n     - create a volume\n     - create a snap\n     - restart bsw\n     - check for volume and snap existence\n\n     - set simevent\n     - restore the backup to a new volume\n     - create a snapshot from new restored volume\n     - restart bsw\n     - check all objects existence with lsvolume, lsvolumesnapshot, lsvolumebackup\n\n     - set simevent\n     - create a clone from latest snap created\n     - delete all objects except the volume clone\n     - create a snap from the volume clone\n     - restart bsw\n     - check volume clone and volume clone's snap with ls commands\n     - restart bsm\n     - check volume clone and its snap with ls commands once more.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36785013.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug36785013.tsc - Test for bug 36785013\n\nBug 36785013 - AN EXASCALE FILE, WHICH IS OPENED FOR IO, CAN BE\n     DROPPED WITHOUT FORCE OPTION DURING SNAPSHOT CREATION\n\n    Steps:\n    1. Drop the vault if there it is existed\n    2. Create the vault and file\n    3. Open the file in background\n    4. Take a snapshot on the file\n    5. Delete the file, the file should not be dropped because\n       the file is in use\n    6. List the file in vault, the file foo still exists",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36798423.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug36798423.tsc -  Bug 36798423 - REPLACE DETECTING DISKCONTROLLER\n     HANG + POWER CYCLE WITH LESS INVASIVE CELLSRV ASSERT WHEN REAP IO STARVATION DETECTED\n\npls see below\n\nto be added in debugcli15",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36830064.tsc",
    "setup": null,
    "flags": {
      "cell_with_xrmem_cache": "1",
      "pdbs_per_cdb": "2",
      "num_dbs": "2",
      "cell_xrmem_cache_size": "2048",
      "creatdev_file": "tsagrh2def",
      "CELL_MAX_MEMORY": "14000"
    },
    "description": "tsagbug36830064.tsc - 36830064 - ADDING CELLCLI EVENT TO PURGE ALL CC FROM FLASHCACHE\n\nThis test has below steps:\n     Step 1: set up table and populate CC2\n     Step 2: check that columnar_cache_size is non-zero\n     Step 3: Purge FlashCache CC\n     Step 4: check that columnar_cache_size is 0\n\nPurgeCC will purge columnar_cache_size for both Flashcache and XRMEMcache\n     purgeXrCC will only purge columnar_cache_size in XRMEMcache\n     The command is of the below sytax:\n  alter cell events = \"immediate cellsrv.cellsrv_columnarcache \\\n  ('purgeCC/purgeXrCC', 'single', 'clusterName.vaultId.cdbid.dbid', \\\n   'table_space_number', 'objid')\"\n   Test case 1.1: Purge all for FCCC and XRCC\n   Test case 1.2: Purge all for XRCC only\n   Test case 2.1: Purge single object for FCCC and XRCC\n   Test case 2.2: Purge single object for XRCC only\n   Test case 3.1: Wildcard test, purge all objects in a PDB for FCCC and XRCC\n   Test case 3.2: Wildcaed test, purge all objects in a CDB for FCCC and XRCC\n     For wild card function, the wildcard must follow a top-down hierarchy:\n     if the higher hierarchy has a wildcard the lower hierarchy parameter\n     must have it too. The hierarchy is: cdb->dbid->vaultID->tsn->objid.",
    "platform": null
  },
  {
    "test_name": "tsagbug36831386.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug36831386.tsc - Functional test bug 36831386\n\nBug 36831386 - MS SHOULD ALERT FOR CONTROLLER FAILURE\n\n    Dev's txn - xiaohshe_bug-36831386",
    "platform": null
  },
  {
    "test_name": "tsagbug36852525.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cellconnstr": "root@^cell_node^.us.oracle.com"
    },
    "description": "tsagbug36852525.tsc - Test for bug 36852525\n\nScenario 1:\n\n1.\tSet _cell_check_peer_phys_disk_missing_enabled=false in cellinit.ora\n2.\tSet _cell_allow_reenable_predfail=true in cellinit.ora\n3.\trestart MS\n4.\tshutdown MS\n5.\tpower off one FDOM(lest say FDOM_2) (echo 0 > /sys/bus/pci/slots/<slot number>/power)\n6.\tRename serial numbers to FDOM(lets say FDOM_1, which in this example is FLASH_8_1).\n7.\tStart MS\n8.\tNow we can see the disks recovering to normal state\n9.\tNow reenable the physical disk by running,\n10.\tBoth FDOMs are powered on now.\n\n\n     Scenario 2:\n\n1.      set _cell_allow_reenable_predfail=true in cellinit.ora\n2.      Restart MS\n3.      drop one disk for replacement\n4.      physical disk status will show like dropped for replacement\n5.      shutdown MS\n6.      Rename both serial numbers\n7.      startup MS and now status should show renamed serial numbers and dropped for replacement status.\n8.      reenable  force the disk\n9.      Both FDOMs are powered on and serial numbers are restored.",
    "platform": null
  },
  {
    "test_name": "tsagbug36855672.tsc",
    "setup": null,
    "flags": {
      "oss_columnar_cache": "true"
    },
    "description": "tsagbug36855672.tsc - Functional test for BUG-36855672\n\n1. srdbmsini.tsc oss_columnar_cache=true\n      2. create table with 17 columns\n          - create table t1710k(col1 number, col2 number, col3 number, col4 number, col5 number,col6 number,col7 number,col8 number,col9 number,\n            col10 number,col11 number,col12 number, col13 number,col14 number,col15 number,col16 number,col17 number);\n      3. Insert 2000 rows of data\n      4. set the below event to simulate DB is in upgrade mode.\n          - alter session set events='27605 trace name context forever, level  0x00000002';\n      5. Trigger cellsrv event using\n          - cellcli -e alter cell events=\"cellsrv_simevent[PRED_BLOCK_CORRUPT] frequency=1\";\n      6. Check row count\n          - select count(*) from t1710k;\n             - Without gvelayut_bug-36855672 fix we get ORA-00600: internal error code, arguments: [POFC::filterInPassThruModeCopyDataAndMap_cp02]\n             - With gvelayut_bug-36855672 fix we get below error\n                - ORA-01578: ORACLE data block corrupted (file # 8, block # 33704)\n                - ORA-27616: Exadata Allocation Unit: 650",
    "platform": null
  },
  {
    "test_name": "tsagbug36855672_1.tsc",
    "setup": null,
    "flags": {
      "disable_multims": "true",
      "SAGE_MIRROR_MODE": "normal",
      "oss_columnar_cache": "true"
    },
    "description": "tsagbug36855672_1.tsc - Non deterministic test for BUG-36855672\n\n1. srdbmsini.tsc oss_columnar_cache=true\n      2. create table with 17 columns\n          - create table t1710k(col1 number, col2 number, col3 number, col4 number, col5 number,col6 number,col7 number,col8 number,col9 number,\n            col10 number,col11 number,col12 number, col13 number,col14 number,col15 number,col16 number,col17 number);\n      3. Insert 2000 rows of data\n         Follow further steps from Notes.\n\nFrom MinJi Kim :\n\n    4. Set the following in RDBMS in sqlplus (put all smart scan into passthru mode from timezone upgrade,\n    and disable certain passthru optimization to exercise the code change as much as possible)\n\n    alter session set events='27605 trace name context forever, level  0x00000002';\n    alter session set \"_kcfis_cell_passthru_dataonly\" = FALSE;\n    alter session set \"_kcfis_dump_corrupt_block\" = FALSE;\n\n    5. Set the following for cellsrv (With 10% chance, we will declare a block corrupt.\n    And with 50% chance, we will fail to allocate output buffer.)\n\n    alter cell events=\"cellsrv_simevent[PRED_BLOCK_CORRUPT] frequency=10\";\n    alter cell events=\"cellsrv_simevent[PRED_DESTBUF_ALLOC_FAIL] frequency=2\";\n\n    6. Then, run several smart scan queries in a loop.  Let's make sure that some queries are less selective than others\n    (i.e. permissive predicate with a sum(column)).\n\n    There is a very small chance that the query will fail with ORA-1578, but I am hoping that it will be extremely rare.",
    "platform": null
  },
  {
    "test_name": "tsagbug36869563.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug36869563.tsc - bug 36869563. For this issue, we see ORA-7445\n                           when all Databases are idle for 1 hour\n\nThis test has below steps:\n     Step 1: shutdown db\n     Step 2: wait 1 hour after shutdown db\n     Step 3: list dababase again, should not see any entry\n     Step 4: set cluster plan to trigger the incident",
    "platform": null
  },
  {
    "test_name": "tsagbug36908108.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug36908108.tsc - Test for Bug 36908108\n\nBug 36908108 - EDSMGR TAKING TOO MUCH MEMORY\n     A new parameter is added in EDS server _cell_edsmgr_keep_seconds\n       that sets how long each item pushed to EdsMgr should be kept for after\n       it's last access.\n      Test Steps -\n      1. In cellinit.ora file for cellsrv,\n          add _cell_edsmgr_keep_seconds=10000\n      2. Restart cellsrv and usreds\n      3. Run a loop that just creates 200 random files and writes to them\n      4. Sleep 70 seconds\n      5. Run cellcli -e \"alter cell events=\\\"immediate cellsrv.cellsrv_dump('edsmgrdump', 0, 0)\\\"\"\n      6. Take the trace file listed and grep \"edsFileTreeInfoHT\"\n      7. Check that size= value is three digits long\n      8. Check that EdsMgr fixed size allocator is not used\n      9. Repeat step 1 - 7 except in 1. make parameter now 10\n          seconds _cell_edsmgr_keep_seconds=10 and in 7,\n          check that size= is 0 instead of triple digit",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36913693.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagbug36913693.tsc - Functional test for BUG 36913693\n\nFunctional test for BUG 36913693 : ELU EXASCALE UPDATE RS-7445_EGSSERVICESHUTDOWN.DIF\n                              RS-7445_SERVEGSISABSENT.DIF 600_RAFT_METADATA_IS_CORRUPT.DIF\n\n      When the space in the partition used to save Raft logs is full,\n      the system throws an ORA-600_Raft_Metadata_is_Corrupt error.\n      However, Raft logs are not actually corrupt. This may occur if\n      some large file(s) were copied into the partition that Raft is\n      using and takes up all the space. We modified some error codes\n      so that the customer can see more useful information in this scenario",
    "platform": null
  },
  {
    "test_name": "tsagbug36919029.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "iscsi": "true",
      "media_type": "EF,HC",
      "dsk_size": "large",
      "setup_blockstore": "true",
      "vault_db": "DATA"
    },
    "description": "tsagbug36919029.tsc\n             IPCDAT READS FAILED ON RDBMS 23.5 WITH EXASCALE STORAGE ON EXADB-XS\n\nTest teps:\n     1. Setup xrdbmsini, which will create an exacdbusr and store the DB\n        files in a vault (eg. @DBX11).\n     2. Create a different wallet and different user (call it testuser)\n     3. Use moss tool here to point to this testuser wallet.\n     4. Drop and recreate XRMemCache (otherwise, usually by this point the DB\n        has already done a few IOs and \"claimed\" the FCGroup for exacdbusr)\n     5. Access any files in the vault with the test tool;\n        Use small reads eligible for RDMA.\n        This should register the FCGroup for the new \"testuser\"\n     6. Try to run an OLTP workload on the DB side.\n        This will hit REM_ACCESS errors.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug36922952.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug36922952.tsc - Test for bug 36922952\n\nThe target is to have a blanket stats represent the total blocks/bytes\n      rejected by fplib, and separate rejection reason stats to break it down\n      for each case that fplib will reject the block.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36925947.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0",
      "cell_with_xrmem_cache": "0"
    },
    "description": "tsagbug36925947.tsc - orion test case for bug 36925947\n\nTest case for LW imbalance check.\n\nTest steps:\n     1. Run the test with LW imbalance check enabled\n     (1) Run LW workload on 4 grid disks with 20MB region per disk.\n     (2) All LW requests should get absorbed without any imbalance failure.\n     (3) Run LW workload on extra 40MB disk region of single grid disk.\n     (4) Some LW space allocation failed due to LW imbalance check.\n     2. Run the test with LW imbalance check disabled\n     (1) Run LW workload on 4 grid disks with 20MB region per disk.\n     (2) All LW requests should get absorbed.\n     (3) Run lW workload on extra 40MB disk region of single grid disk.\n     (4) All LW requests should get absorbed.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36932466.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug36932466.tsc - Test for Bug 36932466\n\nPlease see below\n\nTest is added in lrgdbconsaclstat",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36936762.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug36936762.tsc - Test for bug 36936762\n\nCELLCLI LIST METRICHISTORY FROM ADB-S CELL IS 20 TIMES SLOWER THAN\n     NON ADB-S CELL\n     test is to stress MS processes multiple \"list metrichistory\" requests\n     from cellcli and rest api all together on large metric files to make\n     sure MS doesn't run into out-of-memory error.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36944934.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug36944934.tsc - Testcase for 36944934 \"ONE DATABASE IN CELL \"LIST DATABASE\", BUT NOT EXIST IN RAC CLUSTER\"",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36946603.tsc",
    "setup": "tsaginit",
    "flags": {
      "cell_with_pmem_cache": "0",
      "asm_disk_def": "tsagrdmadef",
      "creatdev_file": "tsagrdmadef"
    },
    "description": "tsagbug36946603.tsc - test for Bug 36946603 - CC2 DISPLACED IN FC\n     WHEN TEMP REACHING 20% OF FC LIMIT WITH ANALYTICS-ONLY WORKLOAD\n\npls see below\n\ntest added to lrgdbconsafclarge1",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36952541.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cdb": "true",
      "cdb_dd": "pdbs_across_cdbs",
      "num_dbs": "2",
      "oss_asm_sec": "true",
      "clusterid": "ASMClusterName",
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'"
    },
    "description": "tsagbug36952541.tsc - Testcase for bug 36952541\n\nTestcase for bug 36952541\n\n   1. Set up 2 CDB/ASM+OSS env with ASM-scoped security enabled.\n   2. shutdow all dbs/asm and restart cellsrv to get a fresh state in cellsrv for testing\n   3. Startup asm and cdb1, run some oltp workload in cluster1.CDB1.\n      By this time, cell would have created DBClient with clientName='cluster1' and PD1.\n      cell would have created ClientDescriptor with clientName=CLUSTER1:CDB1, and got PD1\n      from DBClient-cluster1.\n      XRMemCache would have created cache group for cluster1 and registering MR using PD1.\n   4. Stop DB and ASM for cluster1.\n   5. Update griddisk availableTo to empty.\n   6. Reset key of cluster1 to empty.\n      Without fix, cell would remove the DBClient with clientName='cluster1' from DBClientDirectory.\n      With fix, cell keeps the DBClient with clientName='cluster1' in DBClientDirectory wiht key update.\n   7. Assign key1 to asmcluster cluster1.\n      Without fix, cell would create  DBClient with clientName='cluster1'.\n      With fix, cell would update existing DBClient with clientName='cluster1 with new key.\n   8. Update griddisk availableTo cluster1\n   9. Start up ASM. Create cluster1.CDB2, and run some oltp workload.\n     Without fix, DBClient with clientName='cluster1' would use PD2.\n     cell would have created ClientDescriptor with clientName=CLUSTER1:CDB2,\n     and got PD2 from DBClient-cluster1. Since cache group for cluster1 uses PD1,\n     and client descriptor CLUSTER1:CDB2 uses PD2, WC_REM_ACCESS_ERR is reported.\n     With fix, DBCleint with  clientName='cluster1' would continue using PD1.\n     CDB2 can have RDMA access without problem.\n\nTestcase for bug 36952541",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36983217.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cdb": "true",
      "cdb_dd": "pdbs_across_cdbs",
      "num_dbs": "2",
      "oss_asm_sec": "true",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback"
    },
    "description": "tsagbug36983217.tsc - Bug 36983217 - REMOVE MEMORY REGISTRATION IN IO\n                 PATH TO TIGHTEN CELLSRV 8K RANDOM WRITE LATENCY HISTOGRAM\n\nplease see below\n\nto be added in lrgdbconsascbug6",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug36991540.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "multi_storagepool": "true"
    },
    "description": "tsagbug36991540.tsc - Test for Bug 36991540\n\nERS SHOULD DISALLOW MULTIPLE SPS OF THE SAME MEDIA TYPE",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37007022.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagbug37007022.tsc - test for bug 37007022\n\nTest MS creates stack trace when rest api or cli command times out",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37043549.tsc",
    "setup": "xblockini",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagbug37043549.tsc\n\nBug test for BUG 37043549 - ALTER SOFTWAREUPDATE HANGS: MS DETECTED ERROR: CELL-02034",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37050635.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "mixed_workload": "true",
      "maxpdb": "1"
    },
    "description": "tsagbug37050635.tsc - This transaction adds tests case for the bug fix transaction vzsridha_bug-37050635\n\nHere are the test steps\n       1. We need to try this in a 3 cell setup (xrdbmsini)\n       2. Start DB workload\n       3. Shutdown cell 1\n       4. Wait for 5-10 mins\n       5. Startup cell 1 back\n       6. As soon as disks are in 'BEING ONLINED' state (in EGS alert log), fail a disk on this cell\n       7. Then kill EGS Leader\n       8. A new leader will be elected ? kill that as well",
    "platform": null
  },
  {
    "test_name": "tsagbug37070615.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagbug37070615.tsc - Test case for bug 37070615.\n\nThis test case is for bug 37070615 which addresses an incident, ORA-600\n     [BsmVolume::releaseIOPSParentsunexpected usingFamilyIOPS value], which occurs\n     when a volume clone releases the list of parent volume from which it inherits the IOPS",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37076755.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37076755.tsc - Functional test for BUG 37076755\n\nBug 37076755 - EXADATA: LIST ALERTHISTORY COULD NOT HANDLE SOME INCIDENT FORMAT\n                    AND A BETTER PARSING/OUTPUT FORMAT IS PREFERRED",
    "platform": null
  },
  {
    "test_name": "tsagbug37092179.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37092179.tsc - High capacity cell make model suffix missing test\n\nThis test verifies if returned model string on high capacity cell is having suffix\n     High Capacity or not",
    "platform": null
  },
  {
    "test_name": "tsagbug37092376.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagbug36754332.tsc - Test for BUG 36754332\n\nBug 36754332 - LNX64-22.4-EXASCALE,EGS CRASH WITH ORA-00600[EGSCPCELL::RECLAIM:RPM USAGE]\n     Test Steps -\n       1. Create storagepool with 5 cells\n       2. Add 6th cell and run alter storagepool\n       3. While REBALANCE for alter storagepool is running, FORCE DROP one disk\n       4. As soon as FORCE DROPPED is seen for the 1st disk, FORCE DROP 2nd disk",
    "platform": null
  },
  {
    "test_name": "tsagbug37102013.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37102013.tsc - Test to cover bug 37102013\n\nBug 37102013 - DROP CELLDISK ALL ERASE=7PASS FAILED\n                    TO ERASE HDDS USING CRYPTO ERASURE\n\n     run cellcli -e drop celldisk all erase=7pass\n     then check all the hdd are dropped or not",
    "platform": null
  },
  {
    "test_name": "tsagbug37106513.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37106513.tsc - Test coverage for Bug 37106513\n\nFunctional test for cover Bug 37106513 - SUPPORT ENDPOINT WITH\n    SELF-SIGNED CERTIFICATE FOR HTTPS METRIC STREAMING ENDPOINT\n\n   Test steps:\n     1. set up a HTTPS Apache server with self-signed certificate on it.\n        Currently apache server is running on:\n       phoenix107581.dev3sub3phx.databasede3phx.oraclevcn.com\n\n      To setup it locally, set local_testing to true in the test.\n\n     2. Configure the metricStreamEndPoint with host set to this system with\n       HTTPS procotol and  'certFileName' attribute pointing to the server's\n       self-signed certificate (copied from the system to the cell).\n\n     3. Check certificate file is preserved inside $OSSCONF/security folder\n     4. In  ms-odl trace file, check the \"success\" message\n\n     The negative tests include:\n     1. supply a big file for the 'certFileName', CLI will error\n     2. supply a wrong certificate file or no certificate file, 'failure'\n         message should be logged in MS trace file",
    "platform": null
  },
  {
    "test_name": "tsagbug37107582.tsc",
    "setup": null,
    "flags": {
      "dtype": "non_jbod"
    },
    "description": "tsagbug37107582.tsc - Test for Bug 37107582 - SET OCT ON RAID AND\n                           JBOD DRIVES PERMANENTLY TO 3S\n\nWe want to add a real hw functional test for bug 37107582. Test steps :\n    1. Imaging/upgrade/reboot -\n      1.1. Intentionally set OCT to some undesired value\n      1.2. Get device and Verify new oct value\n      1.3. Image/upgrade/reboot the node\n      1.4. Get and Verify the OCT is set to 3s after imaging upgrade\n\n    2. Disk replacement -\n      2.1. Intentionally set OCT to some undesired value for this disk\n      2.2. Get device and verify new oct vlaue\n      2.3. Simulate a disk failure\n      2.4. Simulate a replacement\n      2.5. Get device and Verify the OCT is set to 3s\n\n1. RAID refers as non-JBOD\n    2. Reboot case covered, reimage is covered in tsagrh_reimage.tsc\n    3. To verify new oct value, need to get disck detail again",
    "platform": null
  },
  {
    "test_name": "tsagbug37116767.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "oss_testing": "3",
      "setup_blockstore": "true"
    },
    "description": "tsagbug37116767.tsc - This transaction is the testcase for validation of bug 37012678 fix\n\nThis transaction is the testcase for validation of bug 37012678 fix.\n      Here are the test steps:\n\n      1. Run exascale volume block store workload, maybe something similar to what is\n         done in bscellfail* lrgs using tsagbsworkload.sh\n\n      2. At the end of workload run, take a state dump of bswsrv using command sudo\n         kill -12 `pidof bswrv` , this will generate a state dump file in bsw_`pidof\n         bwsrv`_0.trc   file.\n\n      3. There will be a section that dumps buffer usage, in this case we would like\n         to verify that no buffers are leaked after all the workload is stopped.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37120487.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true",
      "dsk_size": "large"
    },
    "description": "tsagbug37120487.tsc - Test case for validation of bug fix 37120487\n\nTest case for validation of bug fix 37120487",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37136289.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_auto_manage_disks": "true",
      "sage_mirror_mode": "normal",
      "oss_failgroup": "failalldbdg"
    },
    "description": "tsagbug37136289.tsc - Test for bug 37136289\n\nTest case to verify fix for bug 37136289\n     BUG - WRITE OF PMEM LOG DATA TO GRIDDISK FAILED DUE TO ERROR 205 (HARD CHECK FAILED)\n\nSteps followed in test are:\n     1. Setup srdbmsini with OSS_AUTO_MANAGE_DISKS=true sage_mirror_mode=NORMAL oss_failgroup=failalldbdg\n     2. Set simulation event to introduce DB crash on running any DML query\n        EVENT NAME : libcell.cellclnt_xrmemlog_harderr_sim enabled=1\n     3. Introduce crash by running DMl operation.\n     4. Verify crash logs in DB and CELL alert logs\n     5. Start DB again using pfile t_init1.ora",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37137966.tsc",
    "setup": null,
    "flags": {
      "cell_with_flash_cache": "all",
      "cell_with_pmem_cache": "0",
      "cell_with_xrmem_cache": "1",
      "num_flash_per_cell": "2",
      "flash_size": "256",
      "num_gd": "4",
      "gdsz": "368",
      "creatdev_file": "tkfgmydef"
    },
    "description": "tsagbug37137966.tsc - SKIP XRCC POPULATIONS AND MAPPINGS WHEN\n                           VMPTE USAGE LIMIT IS EXCEEDED\n\nThis is divided in to 3 parts:\n     Part 1: Run ssst and verify XrCC is populated and oflsrv mppings created in oflsrv.\n     Part 2: Restart oflsrv to clear mapping and set low VmPTE limit.\n             Rerun ssst and verify mappings are not created.\n     Part 3: Restart cellsrv to clear XrCC populations and set low VmPTE limit.\n             Rerun ssst and verify XrCC is not populated.",
    "platform": null
  },
  {
    "test_name": "tsagbug37140353.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug37140353.tsc - Testcase for the test bug fix 37140353 change.\n\nTestcase for the test bug fix 37140353 change.\n    Here are the test steps:\n    - create a volume, a snapshot and a backup\n    - set BSM crash sim event on final state of BsmCreateVolJob\n    - cellcli -e \"alter cell bsmEvents='ebs_simevent[BST_SRV_CRASH_SIM] evarg1=58,evarg2=20'\"\n    - send the following commands in parallel\n    - restore bkp to new volume\n    - restore bkp to new volume\n    - sleep for 0.5 sec and delete bkp so that backup is not deleted before\n    restoring volume\n    - the restored volume will be created and backup can not be deleted till the\n    restored volume creation is completed.\n    - Keep deleting backup and bkp will be deleted after few trials(within 120\n     seconds)\n    - do lsvolumebackup, bkp is not present (this is an error since other volume is\n                                        waiting to use the bkp for restore)\n    - delete the snap\n    - delete the parent volume\n    - restart BSM (ORA)",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37145430.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagbug37145430.tsc - Test case for making contentType as a settable attribute\n\nThis test covers various scenarios for making contentType as a settable attribute while\n     creating volume, volume snapshot, clone and restoring volume from a backup.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37146355.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37146355.tsc - Test for bug 37146355\n\n37146355 - EXADATA 24AI AND RDBMS_23AI: CELLOFLSRV CORE DUMP WITH ORA-700\n     [UPDATECRYPTOINFO: FILE CRYPTO KEY CHANGE]\n     Problem\n     When we create a tablespace again using same datafile after dropping, writes attempt to use\n     this stale key(of previous tablespace) to decrypt the IO buffer and update the SI summary. Since the\n     buffer becomes junk after decrypted with the wrong key, this could lead to\n     a set of undeterministic behaviours\n\n     Test\n     1- Create table space with encryption\n     2- Drop and create same tablespace again",
    "platform": null
  },
  {
    "test_name": "tsagbug37150405.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^_a"
    },
    "description": "tsagbug37150405.tsc - Functional Test for Bug 37150405\n\nBug 37150405 : LNX64-22.4-EXASCALE,EGS CRASH WITH\n      ORA-00600 [EGSDISK::CANPERMANENTDROP:TOOMANYTRIESTODROP]",
    "platform": null
  },
  {
    "test_name": "tsagbug37150510.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagbug37150510.tsc - Test for Bug 37150510\n\nTest Steps\n     1. SP creation with 6 Cells\n     2. EGS Leader is continuously restarted at 1 minute intervals\n     3. In a loop, Disks {3, 4, 5, 9} from Cell-6 is continuously\n        FORCE DROPPED and FORCE ADDED",
    "platform": null
  },
  {
    "test_name": "tsagbug37150551.tsc",
    "setup": "tsagnini",
    "flags": {
      "use_1m_scrub_io": "true",
      "OSS_AUTO_MANAGE_DISKS": "true",
      "SAGE_MIRROR_MODE": "normal"
    },
    "description": "tsagbug37150551.tsc - functional test for Bug 37150551\n\nSteps:\n        1) Setup 2 cell Exadata test env with _cell_use_1m_scrubbing_io=true\n        2) Run cellcli and simulate IO errors - this would simulate 6 bad blocks\n        3) Start disk scrub by running cmd\n        4) Wait disk scrub to finish\n        5) Check alert log for 6 read error traces\n        6) Check alert log for 6 error clear traces showing these blocks are repaired\n        7) Verify device name and sector numbers in both abive traces match\n        8) Check alert log for 2 extents skipped",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37161468.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "lstfile": "^tst_tscname^.lst"
    },
    "description": "tsagbug37161468.tsc - Test for bug 37161468\n\nSteps :\n           1.\tHave a normal setup with celldisks and griddisks on it\n           2. Set flags in cellinit.ora to simulate failure of RESCAN ioctl\n           3.\tSimulate the poor performance on the flashdisk\n           4. Wait for the disk to move to confinement and then powercycle,\n              then actUponInsertion will be called which will call the rescan\n              and check if the rescan fails.",
    "platform": null
  },
  {
    "test_name": "tsagbug37162724.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37162724.tsc\n\nFunctional test for bug 37162724\n\nThis is a test for the txn sabedigo_bug-37162724 which allows\n     to deploy Exascale in environments with 8 sockets.",
    "platform": null
  },
  {
    "test_name": "tsagbug37163937.tsc",
    "setup": null,
    "flags": null,
    "description": "Functional test for Bug 37163937 - DCLI SHOULD SUPPORT DIFFERENT DECODING STANDARD",
    "platform": null
  },
  {
    "test_name": "tsagbug37190926.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug37190926.tsc - Test for Bug 37190926\n\n37190926 - LONG-RUNNING: VOLUME RESTORATION STUCK AT STATE RESTORING EVEN\n        AFTER BUCKET DELETION WORKAROUND FROM BUG 36791620",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37204031.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef"
    },
    "description": "Unit test for bug 37204031. Check to make sure that DW can\n     replace OLTP in Flash under certain conditions",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37206941.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37206941.tsc - Test case for bug 37206941\n\nTest 1:\n     0: Select a random disk (say disk in Slot-7) which starts with the normal state.\n     1: Backup the below files in 2 different directries (say /root/orig. and /root/mod):\n         $OSS_SCRIPTS_HOME/unix/hwadapter/diskadp/harddisk/sn1/get_cfgdsply_output.sh\n   $OSS_SCRIPTS_HOME/unix/hwadapter/diskadp/harddisk/sn1/get_lsscsi_output.sh\n   $OSS_SCRIPTS_HOME/unix/hwadapter/diskadp/harddisk/sn1/get_pdlist_output.sh\n     2: Go to /mod directory and run the above scripts and save them in separate files in the same directory:\n         ./root/mod/get_cfgdsply_output.sh > cfgdsply_output\n   ./root/mod/get_lsscsi_output.sh  > lsscsi_output\n         ./mod/get_pdlist_output.sh > pdlist_output\n         ./mod/get_pdlist_output.sh > pdlist_output_foreign\n     3: Open the above output files and modify as below:\n        cfgdsply_output: Search for the block which contains Slot Number: 7 and\n                         delete the entire block for this slotNumber\n        lsscsi_output: remove the 2 lines of Key: 7    devicename:/dev/xyz\n        pdlist_output_foreign:\n           Search for the block which contains Slot Number: 7 and update the below 2 lines as below:\n           1. Firmware state: Unconfigured(good), Spun Up   (FROM:   Firmware state: Online, Spun Up)\n           2. Foreign State: Foreign   (FROM: Foreign State: None)\n        pdlist_output: Search for the block which contains Slot Number: 7 and\n                       delete the entire block for this slotNumber\n     4: Open the below script files and add the line at the beginning of it as below:\n         get_cfgdsply_output.sh:\n            echo \"$(cat /root/mod/cfgdsply_output)\"\n            exit 0\n   get_lsscsi_output.sh:\n      echo \"$(cat /root /mod/lsscsi_output)\"\n\t      exit 0\n         get_pdlist_output.sh:\n      if [ -e /root /mod/tempfile ]\n      then\n        echo \"$(cat /root /mod/pdlist_output)\"\n      else\n        touch /root /mod/tempfile\n\techo \"$(cat /root /mod/pdlist_output_foreign)\"\n            fi\n            exit 0\n     5: replace the original script files with the above modified script files :\n   cd /root/mod/\n         yes | cp -f get_* $OSS_SCRIPTS_HOME/unix/hwadapter/diskadp/harddisk/sn1/\n     6: wait for 2 mins and then check the physicaldisk status and it should be either\n         in power cycle state or failed. (The disk should eventually end in failed state)\n     7: Replace back the orginal script files backedup in /root/orig and then reenable the\n         physicaldisk in slot-7 back to normal(Not necessary but just to cleanup)\n         cd /root/orig/\n         yes | cp -f get_* $OSS_SCRIPTS_HOME/unix/hwadapter/diskadp/harddisk/sn1/\n         cellcli -e alter physicaldisk <pdid in slot-7> reenable force",
    "platform": null
  },
  {
    "test_name": "tsagbug37209785.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug37209785.tsc\n\nTest for bug 37209785 - ORA-00600: INTERNAL ERROR CODE, ARGUMENTS: [SCHEDULER::ONE_OR_MORE_CELLSRV_THREADS_STUCK]\n\nTest steps :\n     1. Bring up asm and oss stack using oratst -d tsagnini  OSS_AUTO_MANAGE_DISKS=true\n     2. After the bring up, set _skgxp_gen_ant_ping_misscount to 1 in cellinit ,ora and restart cellsrv\n     3. Set event in cellcli alter cell events=\"cellsrv_simevent[NOPATH_PORTID_DELAY] frequency=1\";\n     4. shutdown diskmon using crsctl stop res ora.diskmon -init\n     5. After 225 seconds, we will see ?Heartbeat with diskmon (pid 2521691) stopped ? message.\n     6. Once above message is printed on cell alert logs,  Run parallelly 500 opendisk requests in arbiter which will exhaust all available threads in cellsrv\n     7. wait for 3 minutes, we will see below crash in cellsrv without fix.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37212710.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37212710.tsc\n\nTest for BUG 37212710 - SUPPORT OFFLOAD SERVER ONEOFF PATCH",
    "platform": null
  },
  {
    "test_name": "tsagbug37214533.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0",
      "cell_with_xrmem_cache": "0"
    },
    "description": "tsagbug37234462.tsc - orion test case for bug 37234462\n\nTest case for scan caching\n\nTest steps:\n     1. Create FC of 500MB\n     2. Run large temp write (ROW) to use up LW quota\n     3. Run large flasback log write to 300MB disk region, which is much\n        higher than the LW quota. Flashback log write should all get cached\n        and added to the HighPriority NRW list.\n     4. Run large temp read, ROW LW cachelines should be moved to NRW list.\n     5. Enable LW background flushing. FlashBack Log write should get\n        destaged very quickly at a consistent rate.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37214536.tsc",
    "setup": null,
    "flags": {
      "num_dbs": "1",
      "pdbs_per_cdb": "2"
    },
    "description": "tsagbug37214536.tsc - Test case for bug 37214536: CDB$ROOT FLASH CACHE USAGE SHOULD\n                           NOT BE LIMITED TO 5%\n\nThis test has below steps: we need 1 CDB and 2 PDBs for this test\n     Step 1: set inter DB IORM plan\n     Step 2: Create a CDB plan that sets memory for CDB1_PDB1 to 50%\n     Step 3: Check info and dump FCgroup/stats\n     Step 4: Run oltp insert workload to CDB1_PDB1 to generate REDO\n     Step 5: Check DB_FC_BY_ALLOCATED and PDB_FC_BY_ALLOCATED.",
    "platform": null
  },
  {
    "test_name": "tsagbug37218405.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef"
    },
    "description": "tsagbug37218405.tsc -\n\nOrion test for bug 37218405.\n     The bug is that a new workload can't repurpose cachelines in XRMemCache.\n\n     The test does the following:\n      1.) read workload to populate XRMemCache\n      2.) Nocache write workload to put XRMemCache in state where bug 37218405 manifests\n      3.) read workload to a different region. This should populate XRMemCache if everything works properly\n\nadded in lrgsanvcache1",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37229283.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug37229283.tsc - Test for Bug 37229283\n                           POTENTIAL STORAGE SPACE LEAK IN EDS\n\nTest plans is documented on the Confluence pages below:\n     https://confluence.oraclecorp.com/confluence/display/~sweksha.sinha@oracle.com/Bug+37229283+Test+plan\n\ntest added in lrgsaexacldeds",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37233657.tsc",
    "setup": null,
    "flags": {
      "cell_with_flash_cache": "all",
      "num_gd": "4",
      "gdsz": "368",
      "creatdev_file": "tkfgmydef"
    },
    "description": "tsagbug37233657.tsc - Bug 37233657: CELLSRV SHOULD SKIP STATEDUMP FOR\n                           OSSMISC_TIME_JUMPED_FORWARD ASSERTION\n\nThis bug has below steps:\n     Step 1: mark the length of oflsrv alert log\n     Step 2: Set simulation event and expect no statedump for celloflsrv\n     Step 3: wait for cellsrv restart and new incident occurs\n     Step 4: check that there is no state dump message in the alert log\n     Step 5: set simulation event again and expect statedump for celloflsrv\n     Step 6: wait for cellsrv restart and new incident occurs\n     Step 7: check that there is state dump message in the alert log",
    "platform": null
  },
  {
    "test_name": "tsagbug37234462.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0",
      "cell_with_xrmem_cache": "0"
    },
    "description": "tsagbug37234462.tsc - orion test case for bug 37234462\n\nTest case for scan caching\n\nTest steps:\n     1. Create FC of 500MB\n     2. Run OLTP write workload to populate FC with 300MB of OLTP data\n     3. Run the following workloads in parallel to simulate mixed\n        SCAN/OLTP workload\n     (1) Run scan on the same 300MB region that was previously cached via\n         OLTP with orion delay 20ms. This is to simulate cold scan to\n         existing cached data.\n     (2) Run another hot scan on a different 300MB region in parallel\n         with orion delay 10ms. This is to simulate hot scan that brings\n         in new cached data.\n     (3) Run oltp read on different 200MB region in parallel to introduce\n         new hot oltp data.\n     4. Check cold and hot scan cached size. The expectation is that\n        hot scan should be able to replace colder oltp data and cache\n        more data than cold scan.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37234542.tsc",
    "setup": null,
    "flags": {
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'"
    },
    "description": "tsagbug37234542.tsc - testcase for bug 37234542\n\nThe bug seems to be a latent problem but somehow was discovered only\n     recently.  The root cause of the problem is, when smart scan is\n     submitting IO, the disk is still online but once the IO submitted the\n     disk is dropped.  In some cases of IO submission failure, Cache (IO)\n     layer doesn't clean up some state (i.e. JobIOContext) and leaves it,\n     which leads to an assert.\n\n     The simulation event, PRED_INVALID_GDNUM, sets the disk address to an\n     invalid value, causing the above problem where Cache layer cannot find\n     a valid disk during smart IO operation.\n\ntestcase for bug 37234542",
    "platform": null
  },
  {
    "test_name": "tsagbug37236818.tsc",
    "setup": null,
    "flags": {
      "sga_target": "3072M",
      "flash_size": "1000",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "creatdev_file": "tsagrh2def",
      "ENABLE_XRMEM_CC_CHECK": "1"
    },
    "description": "tsagbug37236818.tsc - SAGEASM-E HIT CELL-02550 AND ORA 600\n        [PREDICATEDISK::FREEIOBUFFER_3] WHEN DROP FORCE GRIDDISK\n\nTest case 1:\n     Step 1: create test table\n     Step 2: run query to populate xrcc\n     Step 3: set simulation event PRED_READ_IOERR, evarg1=200 and rerun query,\n      with this bug, it will generate ORA-600 [PREDICATEDISK::FREEIOBUFFER_3]\n     Step 4: set simulation event evarg1 to 201/1001/1100 and rerun query.\n   Test case 2:\n     Step 1: recreate xrmemcache\n     Step 2: set simulation event PRED_READ_IOERR, evarg1=200 and rerun query,\n      with this bug, it will generate ORA-600 [~ColumnarCacheIOCookie_2]\n     Step 3: set simulation event evarg1 to 201/1001/1100 and rerun query.",
    "platform": null
  },
  {
    "test_name": "tsagbug37241910.tsc",
    "setup": "srdbmsini",
    "flags": {
      "debugcli": "1"
    },
    "description": "tsagbug37241910.tsc - Test for Bug 37241910\n\nPlease see below\n\nAdded in lrgdbconsamsbug4",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37260718.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37260718.tsc - bug 37260718: WRONG RESULT WITH 24.1.4.0.0.241007\n\nWith this bug, it will have wrong result from a query that has\n     a WHERE clause with coalesce",
    "platform": null
  },
  {
    "test_name": "tsagbug37270120.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "max_rpm": "5000",
      "max_cmd_rpm": "3000"
    },
    "description": "tsagbug37270120.tsc - Test case for bug 37270120\n                              DROP STORAGEPOOL - CRASH RECOVERY - EGSRPMROOT::RECOVER:INVLRPMBATCH\n\nTest follows the below sequence\n     1.Create 3 cell setup env\n     2. Simulate the event:\n          alter cell egsEvents=\"ebs_simevent[EGS_SRV_RSCROOTJOB_SM_SIM] frequency=2, count=1, evarg1='rr', evarg2='drop', evarg3='rpm_batch_dropped', evarg4='assert'\"\n     3.Drop OPC_HC_STORAGE\n     4.Wait for ExaScale StoragePool OPC_HC_STORAGE is Being DROPPED force\n     5.EGS should crash due to the cell egsEvent\n     6.Check new leader resumes dropping of the StoragePool\n     7.Wait for rm storagepool OPC_HC_STORAGE done, and verify StoragePool is successfully dropped",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37273995.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug37273995.tsc - Test for bug 37273995\n\nThis test verifies whether there is reduction in the delay of vault\n     resize/creation being reflected in the storage pool details.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37285557.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "num_cells": "4",
      "media_type": "XT"
    },
    "description": "tsagbug37285557.tsc - Test for BUG 37285557\n\nThis bug addresses the case where a cell runs into some unrecoverable\n     problem and can never come back. In such a case, we want to permanently\n     drop the cell along with its disks\n     To address this, we now have a mechanism inside EGS to permanently drop\n     a cell, if the cell has been unreachable to EGS for more than some\n     amount of time - 3 days, by default. Admin still has to run \"alter\n     storagepool reconfig\" to effect the permanent drop.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37297325.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37297325.tsc - Functional test for bug 37297325\n\nThis test verifies that Pool Disk creation is not allowed in InfiniBand environments because Exascale is not supported there.",
    "platform": null
  },
  {
    "test_name": "tsagbug37302378.tsc",
    "setup": "xblockini",
    "flags": {
      "sage_mirror_mode": "high",
      "oss_multims_testing": "true",
      "noniscsi": "true"
    },
    "description": "tsagbug37302378.tsc - Stateless cluster alert by AEP on marking impossible placement decision\n\nFunctional Test for Bug 37302378 (Stateless cluster alert by AEP on marking impossible Placement Decision)",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37316616.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagbug37316616.tsc - Test for BUG 37316616\n\nBug 37316616 - LNX64-22.4-EXASCALE,EGS CRASH WITH\n     ORA-00600[EGSCRCELL::PROCESSCONFIG:MISMATCHEDDROPPERM]",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37340909.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagbug37340909.tsc - Test for BUG 37340909\n\nBUG 37340909 - EXASCALE: DROP CELL TEST HIT ORA-600\n                    [INVALID_RWLOCK_GROUP_TYPE]",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37350347.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug37350347.tsc - This testcase tests the fixes for the issue of unknown idemp entry when BSM restarts while snapshot deletion is underway.\n\nThis testcase tests the fixes for the issue of unknown idemp entry when BSM restarts while snapshot deletion is underway.\n     Test steps:\n       1. Create volume snapshot\n       2. Set snap file deletion delay\n       3. Delete snapshot which will hit delay simulation\n       4. Restart bsm while the snap deletion is underway\n       5. Verify snapshot got deleted by listing the snapshot\n       6. Verify snapshot got deleted by deleting volume and vault",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37352143.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0",
      "cell_with_xrmem_cache": "0"
    },
    "description": "Test case for bug 37352143: unset nocache bit for cachelines replenished\n     by iorm violation\n\nTest steps:\n     1. setup cell, create flash cache in writeback mode with size = 288M\n     2. create fake databases\n     3. run workload1: oltp read/write workload to dbid = 0(default flash\n        cache group) with disk region1 = 300M. It should occupy whole flash\n        cache with clean/dirty cachelines\n     4. create softmax flash cache group with min set to 270M\n     4. run workload 1 and\n        workload2: oltp read/write workload to dbid = 6(new softmax flash\n        cache group) with disk region2 = 300M. It should trigger the new\n        softmax group replenishment since it cannot get enough avaliable\n        cachelines to satisfy is flashcachemin.\n     5. after both workload finished, check:\n        5.1 numNoCacheEvict = 0\n            with the new fix, during cacheline replenishment, the evicted\n            cache headers should not have nocache bit set when replenishing\n            from iorm violated group(default flash cache group)\n        5.2 size of new fcg(dbid = 6) > flashcachemin(270M)\n            without nocache bit set, the new force flush bit should still\n            help those dkwr cachelines be picked as target of aging write\n        5.3 numCLForceFlushIssued  > 0\n            numCLForceFlushSkipped > 0\n            numCLForceFlushDone > 0\n            with the new fix, cachelines be evicted due to iorm policy\n            violation should have force flush bit set(numCLForceFlushIssued  > 0)\n            Since workload1 was concurrently running when fc replenished\n            cachelines, we expect to see some cachelines in dkwr with force\n            flush bit set have its bit cleared due to cache hit\n            (numCLForceFlushSkipped > 0). However, eventually, due to background\n            replenishment, we will see cachelines be invalidated(and synced, if needed)\n            so flashcachemin can be satisfied(numCLForceFlushDone > 0).",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37356088.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37356088.tsc - Functional test for BUG-37356088\n\nTest steps includes :\n      1. Set ulimit to 100\n      2. Populate wrong.cgi file with random invalid host names\n      3. Run dcli with key-with-one-password using wrong.cgi\n          ./dcli.py -l root -g wrong.cgi -v --key-with-one-password\n      4. Make sure Parsed ulimit value:100 & batch size to 12\n\nTest flow includes :\n      tsagbug37356088.tsc -> tsagbug37356088.sh ->\n      dcli.py -l root -g wronghost.cgi -v --key-with-one-password",
    "platform": null
  },
  {
    "test_name": "tsagbug37357748.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagbug37357748.tsc - Test for Bug 37357748\n                           LNX64-22.4-EXASCALE,VES DISK METADATA CORRUPTION\n\nSimulate VES metadata write IO cancellation when disk hangs.\n\nAdded in lrgsaexcvesdeletemd",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37383499.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "dbnodeconnstr": "root@^compute_node_f^"
    },
    "description": "tsagbug37383499.tsc - Bug 37383499 - WE NEED A WAY TO REVERT A KNOWN BAD FIRMWARE ON SYSTEMS AUTOMATICALLY\n\npls see below for test steps\n\ntest added in lrgrh11sasrvmondb",
    "platform": null
  },
  {
    "test_name": "tsagbug37383829.tsc",
    "setup": "tsaginit",
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback"
    },
    "description": "tsagbug37383829.tsc - Test for Bug 37383829\n\n1. Perform Repair Small Writes to non cached disk regions\n     2. Verify Repair Writes were not absorbed\n     3. Population writes to make disk region dirty\n     4. Perform Repair Small Writes to non cached disk regions\n     5. Verify Repair Writes were not absorbed and cache was invalidated\n     6. Repeat 1-5 but with large repair writes",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37387531.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37387531.tsc - Test for BUG 37387531\n\nThis test restarts MS when the software update is happening to verify that\n     MS is recieving the correct status value",
    "platform": null
  },
  {
    "test_name": "tsagbug37389711.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcvolsnap.tsc - test for Bug: 37389711\n\nTest steps:\n     1. Create a volume\n     2. Create 50 snapshots for volume\n     3. List myvault\n\nThis test is for Bug: 37389711 as creating 20 snapshots\n    of a volume lead to ERS crash which is fixed from\n    txn: janeche_bug-37389711",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37393066.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "num_cells": "3"
    },
    "description": "tsagbug37393066.tsc - Tets for bug 37393066\n\nBug 37393066 - EXASCALE: UNKNOWN ENTRIES REPORTED BY LSSTORAGEPOOLOPERATION\n                    WITH RESILVERING RUNNING IN THE SYSTEM\n\n     Test spec : https://confluence.oraclecorp.com/confluence/display/~rishideep.rallabandi@oracle.com/Test+spec+for+rirallab_bug-37393066",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37393566.tsc",
    "setup": "xrdbmsini",
    "flags": null,
    "description": "tsagbug37393566.tsc - Test for BUG 37393566 - CDB$ROOT OPENS MESS UP DATASETS\n\nThis test makes sure that when CDB$ROOT opens a datafile that is used by\n     a PDB, the dataset association will not change. It verifies that the\n     output of edstool list <vault> -t dataset command remains same if the\n     file is opened by CDB$ROOT",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37459697.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37459697.tsc - Functional test for bug 37459697\n\nTest Steps:\n      1. Make sure MS is running on the cell:\n      2. Rename storcli binary to simulate a missing binary scenario.\n      3. Wait and check MS raise a missing storcli alert after some time.\n      4. Restore the storcli binary\n      5. Wait and check MS raise a clear alert",
    "platform": null
  },
  {
    "test_name": "tsagbug37464711.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37464711.tsc - Bug 37464711: ASM SCOPED SECURITY ALLOWS\n     ACCESS TO SOME INCORRECT CONFIGURATIONS BUT XRMEMCACHE SECURITY\n     MODEL DOES NOT\n\nThis test has 2 test cases:\n      Test Case 1: Make sure clients associated with an ASM cluster can not\n             use GridDisks not associated with any ASM cluster\n      Test Case 2: Make sure pre-existing disks can be accessed and that\n             access is no longer allowed once disk is dropped by ASM.\n      Test Case 3: Make sure clients associated with an ASM cluster still\n             have access to pre-existing grid disks (which are NOT\n             associated with any ASM clusters) after the replacement.",
    "platform": null
  },
  {
    "test_name": "tsagbug37464719.tsc",
    "setup": null,
    "flags": {
      "num_flash_per_cell": "2",
      "flash_size": "256",
      "num_gd": "2",
      "gdsz": "256",
      "creatdev_file": "tkfgmydef"
    },
    "description": "tsagbug37464719.tsc - test for Bug 37464719 - XRMEMCACHE RETAINS\n                         CACHED DATA, WHOSE PERMISSIONS HAVE CHANGED\n                         DUE TO ENABLING ASM SCOPED SECURITY ON GRIDDISK\n\nBug test for Bug 37464719\n\nThis test has below steps:\n     Step 1: Enable IORM fake plan\n     Step 2: Run orion workload\n     Step 3: change ASM scoped security for griddisk\n     Step 4: Check cellsrvstats\n     Step 5: Create a cellkey.ora in T_WORK\n     Step 6: Run the Orion workload again\n     Step 7: Check cellsrvstats again",
    "platform": null
  },
  {
    "test_name": "tsagbug37474733.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "OSSFENCE_DEBUG": "1"
    },
    "description": "tsagbug37474733.tsc - Test for Bug 37474733\n          37474733 - ENHANCE THE SCAN LOGIC FOR REMOTEOPENINFO AND REMOTESENDPORT DURING\n                     FENCING OPERATIONS\n\nTest Steps:\n          1. Setup tsagexastackup with OSSFENCE_DEBUG as 1\n          2. Skip the fence authorization check in both ERS and cellsrv using cellcli\n          3. Use Arbiter to simulate 20 parallel IOCTL workload to ERS for 10 minutes\n          4. While the above workload is running, use arbiter to issue a fence of the above REID\n          5. Make sure the 20 arbiters in step 3) are all terminated in 40 seconds\n          6. Check ERS trace, make sure the fence is completed\n          7. Repeat steps 3 to 6 for CELLSRV",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37486615.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37486615.tsc - Test file for BUG 37486615\n              AUTO ASM POWER LIMIT FEATURE SHOULD PRIORITIZE REBUILD PHASE IN MULTI-CLUSTER ENVIRONMENT\n\nFunction Test Spec:\n  https://confluence.oraclecorp.com/confluence/display/EXD/Functional+Test+For+Bug+37486615\n  https://confluence.oraclecorp.com/confluence/display/EXD/Prioritize+Rebuild+Phase+In+ASM+Automatic+Rebalance+Power+Adjustment\n\n    Test Steps:\n   1.  Create folder in workspace : /root/rebal\n   2.  Get \"autoRebalTest.jar\" from oss/src/tools/autoRebalPolicyEngineTest\n   3.  Get \"arbiter\" from oss/bin\n   4.  Make sure the cell has 2 different types of Griddisks, since we need 2 Diskgroups for this test.\n   5.  Modify file to contain correct IP address for the cell and the names for the Griddisk etc\n       This configuration file specifies the properties of rebalance.\n   6.  Modify cell1_io_2dg.conf file to contain correct IP address for the cell and the names for the Griddisks.\n       This configuration file specifies the properties of the I/O workloads.\n   8.  Run the test : java -jar autoRebalTest.jar -ios cell1_io_2dg.conf -asm asm_2dg.conf -dir /root/rebal -verbose\n   9.  Validate the top Diskgroup transition from votehis.log file\n   10. Validate the transition of top Diskgroup from the policy engine trace file\n   11. Validate vote for priority and non-priority Diskgroup",
    "platform": null
  },
  {
    "test_name": "tsagbug37522105.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagbug37522105.tsc - Functional test for BUG 37522105\n\nThis test fails redundancy checks in cell 1 and checks that\n     request cannot be fulfilled due to redundancy failures. It then\n     enables syseds in cell 2 which should allow cell 1 to upgrade\n\nRuns in lrgsaexacldcellupgrade7",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37525412.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug37525412.tsc - Functional test for bug 37525412\n                           IMPROVE THE CELLDISK CREATION LOGIC WHILE ALLOCATING VIRTUAL SLOT NUMBER\n\nTest Steps:\n        1. Capture any celldisk name from the existing celldisk on any cell. Let's name it \"celldiskTest\"\n        2. Add an underscore parameter in cellinit.ora.\n              _celldisk_simulation_failure=celldiskTest\n        3. Restart the MS services\n        4. Drop all the celldisk\n              cellcli -e drop celldisk all force\n        5. Create all celldisk\n              cellcli -e create celldisk all\n        6. Check the virtual slot number now\n              cellcli -e list celldisk attributes virtualslot where diskType=harddisk\n            This should return all unique numbers. There should not be any repeat.\n        7. Check for the log from ms-odl.trc \"Injecting error while creating celldiskTest\"\n            (This is to make sure the simulation has taken place.)",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37531809.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug37531809.tsc - It tests the fix for  bug 37531809\n\nThis testcase tests the fix for  bug 37531809\n     Test steps:\n       1. Create a vault with space size 2G\n       2. Set the crash simulation after submitting file creation\n       3. Create a volume with larger size, we should receive an error without a crash",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37540726.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug37540726.tsc - Test for BUG 37540726\n\nThis test aims for testing the correctness of alter storagepool operation\n     upon providing the spaceOverProvisionPercentage attribute change request.\n     It tests that if the new provisionable space populated from the provided\n     spaceOverProvisionPercentage is less than the current provisioned space,\n     the alter storagepool operation should fail without modifying the\n     spaceOverProvisionPercentage attribute.\n\nThe change storagepool operation can only be executed from system admin.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37560525.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "setup5egs": "true"
    },
    "description": "tsagbug37560525.tsc - Functional Test for Bug 37560525\n\n37560525 - [ESCS - INTLSN] AFTER SCALE-OUT WITH 3 CELLS, TRUSTSTORE IS\n                  SHOWING ONE ADDITIONAL ENTRY\n     Test Steps:\n          1. Setup using tsagexastackup with setup5egs=true\n          2. Find out root certificate of current egs leader:\n              cellcli -e list esnode attributes localRootCert\n          3. Find out the current egs leader process:\n              cd log ; grep -nr ./ -e \"This server is LEADER\".\n          4. Restart the egs leader server:\n              pkill egssrv3; $T_WORK/startegs3.sh\n          5. Find out the listen port of new egs server :\n              cellcli -e list escluster detail\n          6. Change the listening port of ms and then restart ms\n          7. Find current rootUrl and trust store by using cellcli -e list esnode detail.\n             Remove url in step 3) from the current rootUrl and set the new rootUrl by using\n              cellcli -> alter esnode rootUrl=\"<new root urls>\";\n             remove the root cert in step 2) from the current truststore and set the new t\n             rust store by using cellcli -> alter esnode trustStore=\"<new root certs>\"\n          8. Restart syseds cellcli -e alter cell restart services syseds\n          9. ls vault by escli --wallet $T_WORK/esadmin_wallet -> ls.\n             The expected behavior is \"ls\" command should return the vault names.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37571075.tsc",
    "setup": null,
    "flags": {
      "cellconnstr": "^dbnodeconnstr^",
      "tst_tscname": "^tst_tscname^_^type^"
    },
    "description": "tsagbug37571075.tsc - Test for bug 37571075\n\nCREATE DIAGPACK PACKSTARTTIME IS NOT COLLECTING ADR INCIDENT FILES\n     When collecting a diagpack with clause packStartTime durationInHrs\n     the collection is not including ADR incident files generated\n     during that period of time\n\n     Test Steps\n     1)- Create diagpack with packStartTime, durationInHrs\n         (this diagpack is for reference)\n     2)- Create incident in\n         /opt/oracle/nodetype/log/diag/asm/type/$host/incident\n     3)  Verify that incident in step2 are reflected in diagpack created\n         with packStartTime, durationInHrs in\n         $LOG_HOME/$pack1/incidentFiles\n\n     Test is added to\n     Exadata cell+compute -> lrgrhx9sa8rack2\n     Exascale cell        -> lrgrhx9saexc1cell",
    "platform": null
  },
  {
    "test_name": "tsagbug37577976.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cell_with_flash_cache": "all",
      "cell_with_pmem_cache": "true"
    },
    "description": "tsagbug37577976.tsc - Test for bug 37577976\n\nTestcase for 37577976 - [AH] ADD STALE BLOCK DIAGNOSTICS INTO LIBCELL",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37578771.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug37578771.tsc - This transaction fixes 37578771.\n\nVolume clone creation from parent volume was timed out, this is because parent volume is still being restored from a backup.\n    To fix it, return error directly if someone tries to create a clone from a volume that is being restored and restore is not complete yet.\n\n    We can verify the fix by following steps:\n      1. Create a backup\n      2. Set restore delay simulation\n          CellCLI> alter cell bswEvents=\"ebs_simevent[BSW_BKP_RESTORE_DELAY_SIM]\"\n          Cell EDSCELL1 successfully altered\n      3. Create a volume from a backup, which would hit the simulation\n      4. Create a clone from the volume which is created at step 3, which is also\n         being restored. It should get error instead of timing out.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37587385.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug37587385.tsc - This transaction fixes bug 37587385\n\nlsvolume on deleted resource failed with error code 10049 - Permission denied which is a bug. This transaction fixes this bug 37587385.\n     It should recieve error code 10005 - Volume absent on doing lsvolume on deleted resource.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37593306.tsc",
    "setup": null,
    "flags": {
      "ora_sid": "exadb",
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'",
      "tmpdbname": "dbs^i^_uniq_name"
    },
    "description": "tsagbug37593306.tsc - test case for bug 37593306 - MISSING DATABASE INFORMATION IN CDB AWR REPORT EXADATA\n\ntest case for bug 37593306 - MISSING DATABASE INFORMATION IN CDB AWR REPORT EXADATA\n\ntest case for bug 37593306 - MISSING DATABASE INFORMATION IN CDB AWR REPORT EXADATA",
    "platform": null
  },
  {
    "test_name": "tsagbug37612888.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug37612888.tsc - Test for bug 37612888\n\nProblem : EDS uses the wrong buffer size for allocation during file deletion.\n     The bug reproduces when\n     1. We have a large snapshot tree with ~89 nodes.\n     2. There are on-going snapshot creation workload during the file deletion on the\n        same tree branch, so that the tree keeps growing when we delete the file.\n     3. There are OSS generic errors occurred during file delete.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37614733.tsc",
    "setup": "xblockini",
    "flags": {
      "bug_37672311": "true",
      "bug_37659681": "true"
    },
    "description": "tsagbug37614733.tsc - Test for bug 37614733\n\nCreate vaults with unlimited IOPS\n     Create a volume on that vault with fixed IOPS > storage pool IOPS - this should fail.\n     Create a volume on that vault with fixed IOPS < storage pool IOPS - this should pass.\n     Now , do chvolume to increase its IOPS to > storage pool IOPS. This should also fail.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37619348.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37619348.tsc - Bug 37619348: FLASHCACHE DOESN'T ABSORB HDD\n                           LARGE WRITES IN EXASCALE EDV VOLUMES DUE TO IORM\n                 DEEMED THE DISKS NOT BUSY WHILE IOSTAT SHOWS HIGH HDD UTIL%\n\nThis test has below steps:\n     Step 1: drop flashcache and xrmemcache\n     Step 2: check cellsrv stat for IO_util before running workload\n     Step 3: Run orion workload on harddisk\n     Step 4: check stat again after workload, epxpect over 1 percent",
    "platform": null
  },
  {
    "test_name": "tsagbug37620699.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug37620699.tsc - Functional test for BUG 37620699\n\nThis bug is related to a race condition between a usreds\n     thread processing a DB's request to abort an uncommitted file\n     and another usreds thread also processing a DB fencing request\n     at the same time.\n\nRuns in lrgsaexacldeds2",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37639257.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37639257.tsc - Test for Bug 37639257\n\nIssue: The existing label allocates victimcache fcgroup object by kgh. High kgh\n     allocation contention caused cachethrashingdetector command queue built-up.\n     Fix: This txn switches allocation from kgh to FixSizeAllocator, which should\n     eliminate command queue build-up.\n     Test: Simulate kgh allocation contention during victimcache fcgroup object allocation.\n     We should not more see the command queue build-up after the fix applied.",
    "platform": null
  },
  {
    "test_name": "tsagbug37652001.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug37652001.tsc - Test for Bug 37652001\n\nTest case for IORM tuneIormDisk hang with BlockIO hang tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37670988.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37670988.tsc - Test for exaportmon bug 37670988\n\nTEst added in lrgrhx9saexaportmon_nsf",
    "platform": null
  },
  {
    "test_name": "tsagbug37693615.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug37693615.tsc - Test for Bug 37693615\n\nPlease see below\n\nAdded in lrgsaexacldeds",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37694117.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37694117.tsc - test for Bug: 37694117\n\nTest for Bug: 37694117\n     1. Attempt to connect to cell using -k option\n     2. If wrong password is passed, dcli should prompt for\n        typing another password, total of 3 attempts\n     3. After, entering 3 wrong passwords, node should print\n        permission denied error.",
    "platform": null
  },
  {
    "test_name": "tsagbug37695353.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagbug37695353.tsc - Test for BUG 37695353\n\nBUG 37695353 - EXASCALE: EGS CRASH WHILE GRIDDISK DROP ORA-600[EGSDISK::CANPERMANENTDROP:TOOMANYTRIESTODROP]",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37705108.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37705108.tsc - Test for Bug 37705108\n\nPlease see below",
    "platform": null
  },
  {
    "test_name": "tsagbug37725949.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug37725949.tsc - It tests the fix for bug 37725949 where delete volume backup\n                           when deleted again fails with wrong HTTP code and error code.\n\nIt tests the fix for bug 37725949 where delete volume backup\n     when deleted again fails with wrong HTTP code and error code.\n     It should fail with 400 status code instead of 404.\n     Steps:\n        1. Create a volume backup using rest api test framework so that you can print status code.\n        2. Delete volume backup using rest api test framework.\n        3. Delete volume backup again which will fail and print the response status code as 400.\n        4. Delete volume backup again using escli command to see correct error code coming as response.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37740554.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37740554.tsc - Test for bug 37740554\n\nProblem\n          Wrong results, crash, or assert failure for a query\n       1) involving a group-by with multiple grouping operands, one of which is all constants, with no columns\n       2) group-by is pushed to Exadata\n\nThere should be no ORA-600_qesnh_qeshrPackRowFastHeadercvl2",
    "platform": null
  },
  {
    "test_name": "tsagbug37757735.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug37757735.tsc - Test for Bug 3775773\n                              THROTTLE MS EMAIL NOTIFICATIONS\n\nTest Steps:\n          1. Verify that statelessAlertIntervalInMins and statelessAlertSimilarityThrInPct are not set.\n          2. Generate a cellsrv absent alert by killing cellsrv and verify that the\n             alert is successfully generated.\n          3. Kill cellsrv again and verify that there are no new cellsrv absent alerts.\n          4. Set cell traceLevel to finest and kill cellsrv. Check for patterns in ms-odl.trc.\n          5. Kill MS and verify that an alert is generated.\n          6. Change the value of statelessAlertIntervalInMins and wait for 60 seconds.\n          7. Kill cellsrv again and verify that a new cellsrv absent alert is generated.\n          8. Verify statelessAlertSimilarityThrInPct.\n          9. Verify removal of statelessAlertIntervalInMins and statelessAlertSimilarityThrInPct.\n          10. Both statelessAlertIntervalInMins and statelessAlertSimilarityThrInPct should be\n              removed from $OSSCONF/cell_disk_config.xml now.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37768255.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug37768255.tsc - Functional test for bug 37768255\n\nThere are 4 scenarios to cover this bug -\n  1. a. Shutdown EGS\n     b. wait for a short time (less than threshold 30 minutes)\n     c. startup EGS: nothing should happen\n     d. make sure no alert should produce in the alert log.\n\n  2. a. Shutdown EGS\n     b. wait for a long time (longer than threshold 30 minutes)\n     c. startup EGS\n     d. make sure WARNING ALERT and CLEAR ALERT are produced in alert log.\n\n  3. a. Shutdown EGS\n     b. immediately restart ESNP\n     c. wait for a long time (longer than threshold 30 minutes)\n     d. startup EGS\n     e. make sure WARNING ALERT and CLEAR ALERT are produced in alert log.\n\n  4. a. Shutdown EGS\n     b. wait for a long time (longer than threshold 30 minutes)\n     c. restart ESNP\n     d. startup EGS\n     e. make sure WARNING ALERT and CLEAR ALERT are produced in alert log.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37768504.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "cell": "^cell_node^",
      "cellconnstr": "root@^cell^.us.oracle.com"
    },
    "description": "tsagbug37768504.tsc - Test for BUG -37768504\n\nBug# 37768504 - NETWORKINFO ATTRIBUTE REPEATS ENTIRE DETAILS FOR A DEVICE WITHIN SAME PARENS INTERMITTENTLY, BREAKING CONSUMERS",
    "platform": null
  },
  {
    "test_name": "tsagbug37787501.tsc",
    "setup": null,
    "flags": {
      "oss_exascale_asm_testing": "true",
      "vault_db": "DATA",
      "oss_asm_sec": "true"
    },
    "description": "tsagbug37787501.tsc - IORM Batch Processing Test on Hybrid Environment\n\nThis test is to check Bug 37787501 for IORM exascale IOCTL batch\n     processing in the hybird environment.\n     The test proceeds with the following steps:\n     1) Set PLAN_DELAY_MSEC and issue multiple vault create IOCTLs.\n        These vault IOCTLs will be batched and wait for 3 minutes in\n        SetPlan().\n     2) Set BATCH_TEST to 3 mins. After 3-wait from Step 1) is expired, the\n        batch processing will proceed and wait on the last IOCTL. At this\n        point, the batch processing is not completed.\n     3) Issue the intra-db plan from RDBMS node and check if the incident\n        due to incorrect mapping of Bug 37787501 happens.",
    "platform": null
  },
  {
    "test_name": "tsagbug37847244.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37847244.tsc - Functional Test for Bug 37847244\n\nFunctional Test for Bug 37847244\n     Developer Txn : ssaghosh_bug-37847244\n\nTest needs to verify that we are when we have image failure for\n    a disk the status of the disk should show that properly.\n\n       Bug 37847244 - SERVICE LED ON FOR NVME DISK AFTER DISK\n                      REPLACEMENT ON DBNODE",
    "platform": null
  },
  {
    "test_name": "tsagbug37847354.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug37847354.tsc - Test for Bug 37847354\n\nPlease see below",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37847814.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "db_restore": "true",
      "_swrf_mmon_metrics": "false",
      "_swrf_mmon_flush": "false"
    },
    "description": "tsagbug37847814.tsc\n\nAWR EXADATA: STORAGE POOL SECTION MISSING ON EXASCALE",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37870595.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37870595.tsc - Functional test for 37870595\n\nBug 37870595 - EXADATA CELL || LOGROTATE ERROR DUE TO NGINX\n     Check the nginx logrotate config has correct parameter values",
    "platform": null
  },
  {
    "test_name": "tsagbug37890287.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell1_f": "^cell1^.us.oracle.com"
    },
    "description": "tsagbug37890287.tsc - Bug 37890287  ASM-SCOPED: 9 Cells under +DATA\n     Throw Endless Incorrect Offset Read and IO Errors (Access Beyond Disk Size) on GDISK\n\npls see below\n\nto be added in lrgrhx10samiscbugs",
    "platform": null
  },
  {
    "test_name": "tsagbug37896528.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "celltrcdir": "^t_diag^^s^diag^s^asm^s^'cell'^s^^HOSTNAME^^s^trace^s^"
    },
    "description": "tsagbug37896528.tsc - testcase for bug 37896528\n\nExascale pooldisk high metadata usage test\n     The test writes lots of small files which uses\n     thin extent to get the pooldisk with high metadata\n     usage.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37921740.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "Steps to Reproduce:\n      - Create user1 with the category \"compute_server\" using the command: mkuser user1 --attributes id=user1,category=compute_server\n      - Create user2 with the category \"compute_server\" using the command: mkuser user2 --attributes id=user2,category=compute_server\n      - Create a wallet for user1.\n      - Add user1 to the wallet (same steps as for any exascale user).\n      - Create a file in the vault: mkvault user1vault\n      - Modify the access control list (chacl) to allow both user1 and user2 access to the vault: chacl @USER1VAULT user1:M;user2:M\n      - Use moss to write to the file using user1's wallet: moss -W $T_WORK/user1.wallet -d '@USER1VAULT/testfile1' -s 8192 -w 1000 -f 8M\n      - Use moss to read from the file using user1's wallet: moss -W $T_WORK/user1.wallet -d '@USER1VAULT/testfile1' -s 8192 -r 1000 -f 8M\n      - Use moss to read from the file using user2's wallet: moss -W $T_WORK/user2.wallet -d '@USER1VAULT/testfile1' -s 8192 -r 1000 -f 8M\n      - Check cellsrvstat to verify there are no RDMA conflicts: XRmemCache number of RDMA conflicts 0 0\n\nAdditional: test for Bug 37921740 - EXASCALE: ASM ON EDV: \"IPCDAT WORK REQ COMPLETION ERROR: 10 (IPCDAT_WC_REM_ACCESS_ERR)\" IN EDV TRACES",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37927239.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagbug37927239.tsc - Test for Bug 37927239\n          AIM4EXA:ORA-600 [EGSFEATUREMGR::REMOVEEGSNODE EGS NODE NOT FOUND] -\n               EGSFEATUREMGR::REMOVEEGSNODE\n\nSetup\n    1. Setup 3 cell Exascale environment\n    2. Add _egs_server_event=\"ebs_simevent[EGS_ALLOW_TRUSTSTORE_UPDATE_SIM]\"\n    3. Find 1 EGS follower and shut it down\n    4. Remove the follower from all EGS rootURL\n    5. Remove the follower root certificate from other EGSes trustStore\n    6. Wait for 10min for the trustStore job to run\n    7. Restart all EGSes\n  Test:\n    1. Add the follower back to all EGS rootURL, but DO NOT add it to the trustStore\n    2. In escli, list the service, we should see this EGS, but the info should not be populated\n    3. Remove this EGS follower from the rootURL\n    4. We should now see : ?EgsFeatureMgr::removeEgsNode node %s not found in list\"",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37927974.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug37927974.tsc - functional test for bug 37927974 [KEEP BUFFERS FOR BACKGROUND CC2 POPULATION WHEN THE QUEUE IS NOT FULL]\n\nThere are two queues for pending CC2 population requests.\n   Queue A: Each population request is associated with a data buffer. The capacity is computed by number of total 1MB\n   buffer (_cell_num_1mb_buffers) multiplied by a percentage (_cell_imcpop_max_req_with_io_buf_percent)\n   Queue B: Each population request is not associated with a data buffer. The capacity of this queue is much larger than the capacity of queue A.\n   Whenever we create a new background CC2 population request, we always queue the request to A first if it hasnât reached its capacity.\n   If A is full, then we free the data buffer from the request, and queue the request to B, and bump up the stat ('imc_io_requests_requeued').\n   This mechanism prevents background CC2 populations from hogging too many data buffers.\n\nThe test steps are:\n      1. use srdbmsini to setup exadata asm and enable columnar cache by setting oss_columnar_cache\n2. get tsagcrtpch.sql and run it to create a large EHCC table\n3. Setting the cell init parameter \"_cell_imc_pop_job_limit\" to 1 and restart cellsrv.\n      4. Do the following in a loop with 30 times:\n          4.1 Populate CC2 by using table full scan query after setting session parameter \"_kdzf_columnar_cache\"=\"enabled, pop_cc2_for_capacity\".\n\t    4.2 use \"exec sys.columnar_cache.check_columnar\" to wait until CC2 background population is completed (may be slower due to the cell parameter)\n    \t    4.3 Purge all CC from the cell: alter cell events = \"immediate cellsrv.cellsrv_columnarcache('purgeCC', 'all', '0', '0', '0')\"\n    4.4 use \"exec sys.tsagmdbchk.checkCellStat\" to check if \"imc_io_requests_requeued\" has increased\n\nImportant note:\n\ti) _cell_num_1mb_buffers=1000 & _cell_imcpop_max_req_with_io_buf_percent=100\n\tThese two parameters would increase the capacity of queue A from 120 to 1000. Without these, a single scan of\n\tlineitem would overflow queue A and force new requests to go to queue B, which was the reason why 'imc_io_requests_requeued' was bumped up.\n      ii) _cell_enable_tier1_columnar_cache=false\n\tThis parameter disables XrCC as its unrelated to this test. XrCC requests always queue to B, although they do not\n bump up 'imc_io_requests_requeued', they can create other noise so itâs better to just disable it for this test.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37928224.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37928224.tsc - Test case for bug 37928224\n\nBug 37928224 - FREQUENT ALERT ABOUT /TMP FILE SYSTEM FULL\n      This test simulates filling up the /tmp directory to more than 80%\n      and then adds a /tmp/sos.xxx file to simulate a temporary file made\n      during sosreport. Then it checks that there's no alert generated within a minute. The alert\n      should only be generated if the space does not go down within 2 minutes\n\nAdded in lrgrhx9saswupd",
    "platform": null
  },
  {
    "test_name": "tsagbug37940818.tsc",
    "setup": "tsaginit",
    "flags": {
      "flash_size": "2624"
    },
    "description": "tsagbug37940818.tsc - Test for Bug 37940818 - FLASHCACHE DATASYNC\n                           DOESN'T WORK AFTER THE FLUSH\n\nPlease see below\n\nAdded in lrgsafcpartition1",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug37946606.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug37946606.tsc - Test for bug fix 37946606\n\nThis test scripts tests the fix for bug 37946606.\n     Here are the test steps:\n       1. Create vault and volume\n       2. Create 2 snapshots from the created volume\n       3. Restart bsm service\n       4. Enable highest log level for bsm\n       5. List volume snapshots\n       6. Grep to check nothing is found for below entities",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug37955718.tsc",
    "setup": null,
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "OSS_ENABLE_NC_PERSISTENCE": "off",
      "uniq_dsknames": "all",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug37955718.tsc\n\nTest for Bug 37955718.\n\nThe following is what the test does:\n        1. populate block cache for grid disk datafile0, datafile1 and datafile2\n        2. populate columnar cache for grid disk datafile2 and datafile2\n        3. drop datafile0, datafile1 and datefile2\n        4. observer FC_BY_STALE_DIRTY and FC_COL_BY_USED drops to zero shortly",
    "platform": null
  },
  {
    "test_name": "tsagbug37961918.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37961918.tsc - cell flash type tests for yichewan_bug-37961918\n\nThis test validats the new rule that storage pool disks are only allowed\n     on disks with WBFC. pool disk creation will NOT be permitted when:\n     1) There is no flash disk present at all (i.e., on legacy XT platforms\n        without flash cache)\n     2) The flash cache is configured as write-through.\n\nThe following test cases are tested here :\n     1: Create pool disks without FC - should fail\n     2: Create pool disks with WTFC - should fail\n     3: Create pool disks with no flash - should fail\n     4: Alter FC mode to WT with pool disks created - should fail",
    "platform": null
  },
  {
    "test_name": "tsagbug37967978.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37967978.tsc\n\nTest for Bug 37967978 - OFFLOAD SERVER ONEOFF PATCH CANNOT ACTIVATE WHEN ESWALLET IS CONFIGURED\n\nRuns in lrgrhx9saexc1cell",
    "platform": null
  },
  {
    "test_name": "tsagbug37974265.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug37974265.tsc - test case for bug 37974265\n\ntest case for bug 37974265\n\ntest case for bug 37974265",
    "platform": null
  },
  {
    "test_name": "tsagbug38014051.tsc",
    "setup": null,
    "flags": {
      "media_type": "XT"
    },
    "description": "tsagbug38014051.tsc - Test for BUG 38014051\n\nThis Bug test validates -\n     1. DataSet has XT attributes\n     2. flashLogProv value is set to true by default for vault with XT media\n     3. flashLogProv in a vault can be altered",
    "platform": null
  },
  {
    "test_name": "tsagbug38014105.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug38014105.tsc - Test for Bug 38014105\n          EXASCALE: WHEN A FEATURE IS ENABLED/DISABLED,\n          THE CHANGE SHOULD TAKE EFFECT ACROSS THE STACK IMMEDIATELY\n\nTest to verify ability for servers to perform feature registration\n      immediately with the newly elected EGS leader after a leader switch.\n      (Previously, this process could take up to 8 hours.)\n\n  The test steps are:\n    1. Setup xblockini\n    2. Search for patten \"EgsFeatureMgr::serverRegister\" in the\n       current EGS leader trace files, and make sure we see AT LEAST 1\n       record for each types of the servers(Multiple records are fine),\n       for example:\n      a. egs_1748040_71.trc:2025-07-14 23:58:11.072814 :8109E0B2: EgsFeatureMgr::serverRegister type:cellsrv, name:phoenix96010_8.3_58358_8, nfeat:3\n      b. egs_1748040_66.trc:2025-07-14 23:58:11.867070 :810B393E: EgsFeatureMgr::serverRegister type:ers, name:phoenix96010_8.3_55838_2, nfeat:10\n    3. Restart the EGS leader to trigger a leader election.\n    4. Again, search patten \"EgsFeatureMgr::serverRegister\" in the current\n    EGS leader trace files, and make sure we see AT LEAST 1 record for each types of the servers.\n    5. Repeat 2 - 3 for 5 times.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38018992.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug38018992.tsc - Test for bug 38018992\n\nThe bug symptom is that user temp LW cannot land in flashcache. Even\n    the default LW usage is 20% of flashcache size, in customer case, their LW\n    usage always < 1% of flashcache size. And we observed flash cache mode cannot\n    be retrieved correctly from cell_disk_config.xml during bootstrap.\n   Notes\n    4G flashcache with 2 fc",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug38020691.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "vault_db": "DATA",
      "flash_size": "272"
    },
    "description": "tsagbug38020691.tsc - Test for Bug 38020691\n\nPlease see below\n\nadded in lrgsaexcfcgroups",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38034345.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug38034345.tsc - Test for Bug 38034345 - VOLUME BACKUP TO VAULTS: SET BACKING FILE AS READ ONLY\n\nTest checks that the eds file of datcopy backup is set as readOnly.\n     And when the backup is restored to a volume, the readOnly flag is removed\n     Steps :\n     1) create a volume\n     2) Loop 3 times :\n     3)   Add data to volume\n     4)   Create volume snapshot.\n     5)   Create datacopy backup and verify if the eds file is readOnly.\n     6)   Compare data in volume snapshot and backup. Data should be same.\n     7)   Try writing to backup. Since its readOnly, write is expected to fail.\n     8)   Compare data in volume snapshot and backup. Data should be same.\n     9)   Remove the backup with --keep-data option. And eds file should still be readOnly.\n     10)  Restore the backup using eds file. ReadOnly flag will be removed automatically.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38034409.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sage_mirror_mode": "normal"
    },
    "description": "tsagbug38034409.tsc - Test for Bug 38034409\n\nPlease see confluence page for test details -\n     https://confluence.oraclecorp.com/confluence/display/~lansing.chen@oracle.com/%5BTest+Doc%5D+DISK+REENABLMENT+INCOMPLETE+DUE+TO+MANUAL+MS+RESTART\n\nAdded in lrgdbconsamsbug40",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug38044324.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug38044324.tsc - Test for bug 38044324\n\nBug 38044324 - ORA-07445 [LNXDIV()+4445] AFTER UPGRADE CELL TO 24.1.11\n     We had regression issue from OSS_24.1.8 - OSS_24.1.11\n     After running customer specified query, we were getting\n     ORA-07445 [LNXDIV()+4445] assert, adding test to interop and normal lrg\n     lrgdbconcmainr19000sascbug32(interop), lrgdbcsascbug28\n\nto be added in scbug32",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug38056750.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug38056750.tsc - Test for bug 38056750\n\nTest to verify skip ILOM info population for 'list cell'\n     if there is an ILOM reset triggered by MS\n\nTest steps:\n      1. Shutdown services\n      2. Add _sp_reset_interval=7 in cellinit.ora\n      3. Start services\n      4. Wait for \"Waiting and checking if ilom reset\" in ms-odl.trc\n      5. Run cellcli commands and catch the execution time\n         a. cellcli -e list cell detail\n         b. cellcli -e list cell attributes doNotServiceLEDStatus\n      6. Verify the runtime for cellcli commands are less than 10 secs\n      7. Wait for \"Ilom reset is done\" in ms-odl.trc\n      8. Shutdown services\n      9. Restore cellinit.ora\n     10. Start services\n     11. Make sure that doNotServiceLEDStatus is off\n     12. Turn on the do-not-service led by running set_dns_led_status.sh\n     13. Make sure that doNotServiceLEDStatus is on\n     14. Turn off the do-not-service led by running set_dns_led_status.sh\n     15. Make sure that doNotServiceLEDStatus is off",
    "platform": null
  },
  {
    "test_name": "tsagbug38065453.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug38065453.tsc - Test for BUG38065453\n\nThis is a test for yujduan_bug-38065453 which fixes an issue that when usrEDS restarts\n     and datastore dismounted and mounted again, it is possible that vault deletion failed ini\n     the background. The vault will be remained there until the user retries. With the txn fix,\n     sysEDS will automatically retry the failed vault deletion again in the background,\n     so the remained vault will be cleaned up.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38080515.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug38080515.tsc - Functional test for bug 38080515\n\nCheck for ERS crash",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38081703.tsc",
    "setup": null,
    "flags": null,
    "description": "This test validates the fix for bug 38081703, which previously caused\n     `rocelinkinfo` to fail when switch SSH equivalency was configured\n     incorrectly (e.g., not from a true root shell but from `sudo su`\n     by a non-root node). It runs a helper shell script to reproduce\n     the scenario and verifies the result.",
    "platform": null
  },
  {
    "test_name": "tsagbug38097600.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagbug38097600.tsc\n\nmore details at : liyangy_bug-38097600 ([exascale-iad3]: bsm leader ran out of buffers impacting volume operations)\n\n     During the IAD3 region outage one of the eds process was stuck in BEING_ONLINED\n     status, causing outage for multiple vaults to be inaccessible which also include\n     multiple system vaults that hold volume metadata\n\n     Test steps\n\n     1. Set event simulation to simulate failure in writing volume metadata by using\n      cellcli (in bash)\n      cellcli -e \"alter cell bsmEvents='ebs_simevent[BST_META_FILE_WRITE_ERROR]\n      err_type=internal,frequency=1,count=10,evarg1=1006,evarg2=\\\"@\\\\\\$BSTbsmmeta/lun.meta\\\"'\"\n\n      2. create a volume.\n\n      3. In alert log and traces we can see event simulation will be triggered and i\n      it will inject edslib error 1006 for 10 times to simulate failed metadata\n      writes, with the fix, as the transient error clears, the volume creation will\nsucceed.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38097600_2.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug38097600_2.tsc - Test for 38097600\n\nPlease see below\n\nAdded in lrgsaexacldeds",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38097698.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug38097698.tsc - Test for bug 38097698\n\nTestcase for bug 38097698 - This test checks for the scenario if the two system routine\n     jobs added for both usreds and syseds to regularly check if DataStore Shard directory\n     file is in error state and try to recover it by remounting the DataStore Shard.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38101321.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug38101321.tsc - Functional test for bug 380101321\n\nTest to check the racing problem that if a server dies in the middle\n     of BEING_ONLINED , its status can get stuck there , instead of\n     moving to OFFLINE.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38113661.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug38113661.tsc - BUG 38113661 - MS ALERT HISTORY MISSED\n                           ALERTS FOR CELLOFLSRV QUARANTINES\n\nThis test has below steps:\n       Step 1: create test tables\n       Step 2: Set simulation event\n       Step 3: run query to trigger IMC population and oflsrv crash\n       Step 4: check for incident with quarantine\n       Step 5: check for incident in cell alert log\n       Step 6: make sure we see the quarantine in MS alerthistory",
    "platform": null
  },
  {
    "test_name": "tsagbug38113879.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "maxiops": "unlimited",
      "media_type": "EF,XT",
      "setup_blockstore": "true",
      "vault_db": "DATA"
    },
    "description": "tsagbug38113879.tsc - Test for BUG38113879\n\nAdditional: Test for Bug 38113879 - FLASH CACHE IS NOT ENABLED IN IORM PLAN WHEN ONLY XT IS PROVISIONED IN VAULT",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38119371.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagbug38119371.tsc - testcase for bug 38119371\n\nverifying the bug fix 38119371, where an invalid iorm tag is passed by bsw for one debug I/O.\n    when a bad metadata block is detected,\n    proper iorm tag for the mirror validation I/Os are added a bug fix\n\n    more details : https://orareview.us.oracle.com/157940882",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38123316.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0",
      "cell_with_xrmem_cache": "0"
    },
    "description": "tsagbug38123316.tsc - LW stat discrepancy check upon small ROW writes\n\nThis test exercises LW stat and check discrepancy upon small ROW writes\n\nTest steps:\n     1. Set up cell with writeback flashcache.\n     2. Record cellsrvstat LW stats and client write stats, should be all 0.\n     3. Run small temp write workload with orion, it should go through LW path.\n     4. Record cellsrvstat LW stats and client write stats, should see:\n        eligible LW requests ~= reject LW requests + absorb LW requests\n        absorb LW requests ~=\n              LW requests first write absorbed + LW requests redirty absorbed",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug38139635.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug38139635.tsc - test case for bug 38139635 - RAISING ERRORS\n                           (ORA-919) IN STORIDX CAUSES OFLSRV TO ASSERT\n\nThis test has below steps\n     Step 1: Create test table\n     Step 2: Run SI query, expect SI savings\n     Step 3: Simulate fplib error on oflsrv\n     Step 4: Run SI query again, should not expect SI savings\n     Step 5: Expect error msg in oflsrv alert log",
    "platform": null
  },
  {
    "test_name": "tsagbug38140007.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug38140007.tsc - Test for Bug 38140007\n\nBug 38140007 - FIX RPM -E NO PACKAGE NAME WARNING\n     Fixed by - shujuche_bug-38140007",
    "platform": null
  },
  {
    "test_name": "tsagbug38161472.tsc",
    "setup": null,
    "flags": {
      "cellconnstr": "root@^cell1^"
    },
    "description": "tsagbug38161472.tsc - flash type upgrade test for yichewan_bug-38161472\n\nThis test validates that the flashcache is automatically upgraded\n     to WriteBack when cell upgrade is done in HC/new EF cells.\n\nThe following test cases are validated here:\n     1: RPM upgrade with a non-degraded, default WTFC\n     2: RPM upgrade with a non-degraded, customized-size WTFC\n     3: RPM upgrade with a WBFC",
    "platform": null
  },
  {
    "test_name": "tsagbug38161825.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug38161825.tsc - Functional test for bug 38161825\n\nTest for timeout issue of 'list cell detail'\n     Test steps:\n       1. shut down MS and RS\n       2. add the following lines in $OSSCONF/cellinit.ora\n           _ms_testing=true\n           _cmd_failure_simulation_pattern=.*get_sensor_list.*\n       3. start MS and RS\n       4. run the following commands and verify the execution time is < 10 seconds\n           cellcli -e list cell detail\n           cellcli -e list cell attributes locatorLEDStatus\n           cellcli -e list cell attributes powerStatus\n       5. monitor the MS trace file till the following pattern shows up:\n       Command Failure Simulation throwing a timeout exception for:\n       /opt/oracle/cell/cellsrv/deploy/scripts/unix/hwadapter/bmcadp/get_sensor_list.sh after sleeping for 300 seconds\n       6. run the commands in 4. again and verify the execution time is < 10 seconds\n       7. shut down MS and RS\n       8. remove the lines added in 2.\n       9. start up MS and RS",
    "platform": null
  },
  {
    "test_name": "tsagbug38168185.tsc",
    "setup": "xblockini",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagbug38168185.tsc - This test script tests the fix for the bug 38168185 where bsw accidentally removed\n                           ip addresses that does not belong to it.\n\nThis test script tests the fix for the bug 38168185 where bsw accidentally removed ip addresses that\n     does not belong to it. To verify this fix, we have this test case in this test script.\n     Here are the test steps:\n       1. Bring up xblockini.tsc with sage_mirror_mode=high and iscsi=true\n       2. For each of the bsw network interface, add some extra ips with named interface aliases, e.g. run as root:\n              ifconfig bsw1:hello 192.168.71.120 netmask 255.255.255.0 up\n              ifconfig bsw2:hello 192.168.71.121 netmask 255.255.255.0 up\n              ifconfig bsw3:hello 192.168.71.122 netmask 255.255.255.0 up\n       3. Shutdown all bsws\n       4. Bring up bsws one by one.\n       5. Verify all those extra ips are not removed. and blockstore ip address is properly configured\n       6. Remove the added ip address.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38177083.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug38177083.tsc\n\nTest for BUG-38177083 : PARTNER READS ARE ENABLED FOR BAD HEALTH DISKS\n\nSteps followed in the test:\n\n     1. Drop a physical disk and verify it's health state changes from GOOD to BAD\n     2. Add the previously dropped physicaldisk and verify:\n      a. Disk's health state changes from GOOD to BAD\n      b. Proxy read requests are started\n     3. Drop the physical disk again and verify:\n      a. Disk's health state changes from BAD to GOOD\n      b. Proxy read requests are stooped now",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug38183475.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug38183475.tsc - Test for Bug 38183475\n\nPlease see below",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38183531.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug38183531.tsc - Functional test for bug 38183531\n\nTest for inconsistent privileges for spaceused files attribute",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38202465.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagbug38202465.tsc - Test for Bug 38202465\n\nPlease see below\n\nAdded in lrgsaexacldegs4",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38220668.tsc",
    "setup": null,
    "flags": {
      "oss_testing": "2",
      "flash_size": "420",
      "creatdev_file": "tsagrh2def",
      "asmconn": "'sys/knl_test7@inst11 as sysasm'"
    },
    "description": "tsagbug38220668.tsc - test case for proxy read periodic health reset\n\nThis test is borrowed from tsagproxyread.tsc and has following steps:\n     Step 1: create the test tables\n     Step 2: populate FC with first table - lineitem_1\n     Step 3: Switch to cell 1 and shutdown cellsrv\n     Step 4: populate FC with second table - lineitem_2\n     Step 5: Start up cell 1 (Reduce proxyread param to 5 mins) & validate\n\nCC is disabled on System level - Not necessary for this test",
    "platform": null
  },
  {
    "test_name": "tsagbug38221553.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagbug38221553.tsc\n\nTest script to verify bug fix for 38221553\n\nHere are the test steps:\n       1. Create a vault\n       2. Create a volume\n       3. Add data to volume, create snapshot and backup\n       4. Set event simulation to simulate network error between bsm and bsw\n       5. Restart bsw\n       6. Verify that backup creation is complete and no complaints from eds",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38231567.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug38231567.tsc - Test for Bug 38231567\n\nPlease see below\n\nAdded in lrgsaexacldegs11",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38257032.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug38257032.tsc - Test for Bug - 38257032\n\nBug 38257032 - ESCLI LSUSER DOES NOT WORK WITH CL_OPERATOR PRIVILEGES",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38260214.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug38260214.tsc\n     This test scripts tests the fix for bug 38260214.\n     Here are the test steps:\n       1. Create a 10G vault\n       2. Create a 6G volume\n       3. Try to create a 6G data copy backup to the same vault\n       4. Verify that proper out of space error is thrown",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38262726.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug38262726.tsc - Functional testcase for bug 38262726 -\n                  IMPROVE RQ DUMP FOR OSS_OPEN WAITS\n\nFunctional testcase for bug 38262726",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug38274184.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high"
    },
    "description": "tsagbug38274184.tsc - Test for Bug 38274184\n\nPlease see below\n\nAdded in lrgsaexcmiscbugsnondb_3cell",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38295850.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "cell": "^cell_node^",
      "cellconnstr": "root@^cell^.us.oracle.com"
    },
    "description": "tsagbug38295850.tsc - Bug 38295850 - ALERT NOT CLEARING AFTER PMEM REPLACEMENT\n\npls see below\n\nTo be added in lrgrhx9sams4",
    "platform": null
  },
  {
    "test_name": "tsagbug38303224.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug38303224.tsc - test against fix for bug 38303224\n     This script also test bug fix for 38277686\n\nHere are the test steps, step 1-6 is for 38303224 and rest for 38277686:\n       1. Create a vault\n       2. Create a volume, write data to volume\n       3. Create volume backup and snapshot\n       4. Set simevent for creating race condition in backup suspension\n       5. Restart bsm to trigger backup suspend/resume\n       6. Verify backup creation is successful, and no incident (by lrg)\n       4. Set simevent for creating race condition in backup restoration\n       5. Restart bsm to trigger backup suspend/resume\n       6. Verify backup restoration is successful, and no incident (by lrg)",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38330972.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug38330972.tsc - Test for bug 38330972\n\nora-00600:\n    internal error code, arguments: [scheduler::one_or_more_cellsrv_threads_stuck]\n    Bug\n    In a two-node cluster with heavy OLTP workload and many databases,\n    restarting a node triggers cellsrv to process mass removal and addition of\n    over 20,000 ports due to diskmon events. The add operation is blocked by a\n    mutex needed for disk operations, causing threads to hang and thread\n    shortages for parallel fencing. This results in all fencing being handled\n    by a single thread, leading to delays (>10 seconds) and a cellsrv thread hang assertion.\n    --------------------------------------------------------------\n    The fix is to process all fencing operations in a dedicated thread.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug38395293.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagbug38395293.tsc - Functional Test for Bug 38395293\n\nThe bug is about EXC services sometime does not choose thread 0 as state dump thread,\n      causing potential hang when a state dump is triggered.\n      Test Steps:\n          1. Setup xblockini(as we also need bsm and bsw)\n          2. Send USR2 signal to each exascale services\n          3. Verify the state dumps trace file (<service_name>_<pid>_0.trc) using methods\n              specified below in test.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38496500.tsc",
    "setup": null,
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "OSS_ENABLE_NC_PERSISTENCE": "off",
      "uniq_dsknames": "all",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug38496500.tsc - functional test for bug 38496500\n\nThe test mainly focuses on verifying the write IOs with the\n     ioreason of KG_SNR_LOG_WRITE (106) will be serviced by the\n     flash cache.\n\nNone.",
    "platform": null
  },
  {
    "test_name": "tsagbug7198335.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line"
    },
    "description": "tsagbug7198335.tsc\n\nTest case for simulation of block repair initiated during Smart I/O\n     (read) operations.\n\nMore details on the test is give in the tsagblkrprsim.tsc for which\n     this tsc is a wrapper.",
    "platform": null
  },
  {
    "test_name": "tsagbug7460216.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line"
    },
    "description": "tsagbug7460216.tsc - test for bug 7460216\n\nrun test tsagmount\n\nRUNS_STANDALONE     YES\n\n   DRIVER_ONLY         NO",
    "platform": null
  },
  {
    "test_name": "tsagbug8908802.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug8908802.tsc - Test fo bug: 8908802\n\nKFK retries write i/o once if there is a HARD check failure. This\n     file has a test case for the same.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug9352217.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "creatdev_file": "tsagrddef",
      "nflint": "1",
      "redund": "external",
      "oss_testing": "1"
    },
    "description": "tsagbug9352217.tsc - Test case for bug 9352217\n\nImports a test schema and then runs the same query 200 times.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug9434755_1.tsc",
    "setup": null,
    "flags": {
      "MAX_INSTANCE": "4"
    },
    "description": "tsagbug9434755_1.tsc - Bug 9434755 test\n\nSetup 4-node cluster (emulated one) with 4 ASM and RDBMS instances.",
    "platform": null
  },
  {
    "test_name": "tsagbug9434755_2.tsc",
    "setup": null,
    "flags": {
      "MAX_INSTANCE": "4"
    },
    "description": "tsagbug9434755_2.tsc - Bug 9434755 test (part II)",
    "platform": null
  },
  {
    "test_name": "tsagbug9434755_3.tsc",
    "setup": null,
    "flags": {
      "MAX_INSTANCE": "4"
    },
    "description": "tsagbug9434755_3.tsc - Bug 9434755 test (part III)",
    "platform": null
  },
  {
    "test_name": "tsagbug9900592.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagbug9900592.tsc - Test case for bug 9900592 for testing DBMS_FILE_TRANSFER.PUT_FILE and\n\t\t    DBMS_FILE_TRANSFER.GET_FILE function.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug_13070696.tsc",
    "setup": "tsaginit",
    "flags": {
      "file_dest": "'+DATAFILE'",
      "nflint": "1",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug_13070696.tsc - Test mentioned in bug 13070696\n\nTest cases for smart scan testing",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug_ms_space_test.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbug_ms_space_test.tsc\n     Management Server Space Management Test\n\nTest  MS Space Management solution",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug_sigsegv.tsc",
    "setup": "tsagnini",
    "flags": {
      "log_file": "tsagbug_sigsegv.log"
    },
    "description": "tsagbug_sigsegv.tsc - Test CELLSRV sigsegv and RS restart\n\nTest that during a cellsrv sigsegv, RS will exit, without detecting cellsrv as\n     hunged during stack dump.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbugfixes.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbugfixes.tsc - Exadata Miscellaneous bug fixes\n\ntsagbugfixes.tsc\n\nWhen running standalone\n     tsagbugfixes.tsc mach_name=[cell] mach_passwd=[password]....",
    "platform": null
  },
  {
    "test_name": "tsagbugfplib.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagrddef",
      "compatible": "^max_compatibility^",
      "file_dest": "'+DATAFILE'"
    },
    "description": "tsagbugfplib.tsc - Test for bug number - 9348294\n\nTests Exception handling of FPLIB",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbugmtst.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagbugmtst.tsc - test for bug 12847686\n\nTests whether the metrics N_MB_RECEIVED, N_MB_SENT, N_MB_RECEIVED_SEC,N_MB_SENT_SEC  are non zero.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbugsuit.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "sga_target": "2048M",
      "OSS_TESTING": "1"
    },
    "description": "tsagbugsuit.tsc - Driver for bug fix tests\n\nDriver for bug fix tests.",
    "platform": null
  },
  {
    "test_name": "tsagbuildcheck.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbuildcheck.tsc - Test for build and make rpm\n\ntests 'make all' followed by 'make rpm', if build succeeds",
    "platform": null
  },
  {
    "test_name": "tsagc2c_2cell.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagc2c_2cell.tsc - tests for Cell to Cell offload between 2 cells\n\ntests for Cell to Cell offload between 2 cells",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagc2c_3cell.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagc2c_3cell.tsc - tests for Cell to Cell offload between 3 cells\n\ntests for Cell to Cell offload between 3 cells",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagc2c_3cell2.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagc2c_3cell2.tsc - tests for Cell to Cell offload between 3 cells\n\ntests for Cell to Cell offload between 3 cells",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagc2chtwqm.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagccdef",
      "disk": "datafile0"
    },
    "description": "tsagc2chtwqm.tsc - check qurantine in high throughout write mode\n\nset parameter _kxdbio_ut_ctl=4 in db and asm sides",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagc2cmanualqm.tsc",
    "setup": null,
    "flags": {
      "tst_nowarn_tmp": "^tst_nowarn^"
    },
    "description": "tsagc2cmanualqm.tsc - Test manual quarantine\n\nNo workloads or cell to cell offloads",
    "platform": null
  },
  {
    "test_name": "tsagc2crebalqm.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagccdef",
      "disk": "datafile0"
    },
    "description": "tsagc2crebalqm.tsc - check rebalance qm\n\nto trigger cell to cell offloads:\n     (1) remove disk\n     (2) recreate it using dd\n\nafter qm is dropped and the cell to cell offloads should complete",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcachewrites.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagcachewrites.tsc - Test for cache write only\n\nat the starting of the test. Please see below.\n\nto be added in lrgdbconsamsbug17",
    "platform": null
  },
  {
    "test_name": "tsagcachpol.tsc",
    "setup": null,
    "flags": {
      "errbasename": "^outbasename^"
    },
    "description": "tsagcachpol.tsc - test cachingPolicy\n\nCalled by tsagdropflash.tsc to test alter cachingPolicy and\n     creating griddisks with different cachingPolicy values.",
    "platform": null
  },
  {
    "test_name": "tsagcachscan.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_ausize": "4194304",
      "ifile_var": "dbinit.ora",
      "cell_offload_plan_display": "always",
      "cell_with_ram_cache": "0",
      "cacheobj": "flashcachecontent",
      "sep": "','",
      "errbasename": "^tst_tscname^tmp.log",
      "dumpSizePerGdisk": "'flint|standby'",
      "selfReplCnt": "'$kdelta{numScanSelfReplCnt} > $kvalue{FC_used_size}/655360'",
      "oltpReplCnt": "'$kdelta{numScanOltpReplCnt}'",
      "dwReplCnt": "'$kdelta{numScanDwReplCnt}'",
      "replCnt": "'$kdelta{numScanSelfReplCnt} || $kdelta{numScanOltpReplCnt} || $kdelta{numScanDwReplCnt}'",
      "oltpGtUsedSize": "'$kvalue{FC_OLTP_used_size} > 1.2*$kvalue{FC_used_size} && $kvalue{FC_OLTP_reserved_size} < $kvalue{FC_used_size}'",
      "usedGtFcSize": "'$kvalue{FC_used_size} > $kvalue{FC_size}'",
      "2muchCacheWr": "'$kdelta{numCachePopulateWriteBytes} > .05*$kvalue{FC_used_size}'",
      "thrashing": "'$kdelta{numCachePopulateWriteBytes} > $kvalue{FC_size}'",
      "2muchOltpDisplaced": "'$kvalue{FC_OLTP_used_size} < .9*$kvalue{FC_OLTP_reserved_size} && $kvalue{FC_OLTP_used_size}+$kdelta{FC_OLTP_used_size} > $kvalue{FC_OLTP_reserved_size}'",
      "mostlyOltp": "'$kvalue{FC_OLTP_used_size} >= .75*$kvalue{FC_used_size}'",
      "mostlyDw": "'$kvalue{FC_OLTP_used_size} < .3*$kvalue{FC_used_size}'",
      "testdesc": "'Loop to scan and flush - test for bug 16902074'",
      "tblsiz": "^tst_exe_result^",
      "tochk": "'cachedSize{tbl3}<.3*cachedSize{total}'",
      "testcase": "f",
      "tsiz": "tblsiz^t^",
      "chkeep": "'cachedKeepSize>.8*'^^tsiz^' || cachedKeepSize>.7*'^FC_size^",
      "chk0keep": "'cachedKeepSize<1'",
      "nr": "numrows^t3^",
      "t": "3",
      "t2": "1",
      "numiter": "50"
    },
    "description": "tsagcachscan.tsc - test that scans are cached",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcachscana.tsc",
    "setup": null,
    "flags": {
      "incrtbl": "1",
      "incrtbl1": "^incrtbl^",
      "incrtbl2": "^incrtbl^",
      "startbl1": "1",
      "startbl2": "3",
      "errbasename": "^outbasename^",
      "testdesc": "'Delete rows then shrink a table'",
      "t1siz": "tblsiz^t1^",
      "cacheRead": "'''$kdelta{numScanCacheReadBytes}'' >= ''.9*( ''$kvalue{FC_used_size}''<'^^tblsiz^'?''$kvalue{FC_used_size}:'^^tblsiz^'')'",
      "cacheWrite": "'''$kdelta{numCachePopulateWriteBytes}'' >= ''.9*( ''$kvalue{FC_used_size}''<'^^tblsiz^'?''$kvalue{FC_used_size}:'^^tblsiz^'')'",
      "t2siz": "tblsiz^t2^",
      "tblsiz": "^tst_exe_result^",
      "dop": "5",
      "dumpSysstat": "sqlstats",
      "nr": "numrows^t^   # caller should have set numrows^t^ to desired no. of rows",
      "useSI": "'$kdelta{cell_physical_IO_bytes_saved_by_storage_index}>0'",
      "belowReserved": "^tst_exe_result^",
      "tochk": "'cachedSize>.9*'^tblsiz1^"
    },
    "description": "tsagcachscana.tsc - auto-cache scan tests\n\nInvoked by tsagcachscan to run individual tests.",
    "platform": null
  },
  {
    "test_name": "tsagcalib.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcalib.tsc - Test Calibrate command\n\nTests the Calibrate command of cellcli.\n\nThis test will run only for real hardware",
    "platform": null
  },
  {
    "test_name": "tsagcalibrate.tsc",
    "setup": null,
    "flags": {
      "product_name": "^TST_EXE_RESULT^",
      "hardware_type": "'x5'"
    },
    "description": "tsagcalibrate.tsc - Test for calibrate command\n\nTest for Calibrate command",
    "platform": null
  },
  {
    "test_name": "tsagcap1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcap1.tsc - This tests that Cell Capabilities are updated\n         after cell is restarted and RDBMS is notified.",
    "platform": null
  },
  {
    "test_name": "tsagcap2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcap2.tsc - This tests that Cell Capabilities are updated\n                    after disk is offline and online\n                    and runs some DSS load.",
    "platform": null
  },
  {
    "test_name": "tsagcaponiscsi.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagcaponiscsi.tsc - Test configures maximum number of attachments possible in the environment.\n\nTest verifies that the BSW cap on the number of iSCSI attachments can be made\n     is Max devices per thread x Number of iSCSI Threads in a BSW\n     The Max devices per thread is configurable from $OSSCONF/bsw/excloudinit.ora\n     using this configuration parameter -> _exc_bsw_max_devices_per_iscsi_thread.\n     The total number of iscsi threads can be controlled with config param _exc_bsw_num_iscsi_threads.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagcc2.tsc",
    "setup": null,
    "flags": {
      "cc2tab": "tsagcc^j^",
      "refobj": "objid^j^_db^curdb^",
      "dbUqName": "dbname_db^curdb^",
      "qlid": "qlobjid^curdb^"
    },
    "description": "tsagcc2.tsc - basic test for IFC2\n\nquery with following condition:\n       basic predicate\n projection\n min/max function",
    "platform": null
  },
  {
    "test_name": "tsagcc2allencini.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "imc_size": "500M",
      "PWFILE_ON_EXC": "false"
    },
    "description": "tsagcc2allencini.tsc - init setup for CC2 encrypted tbs test\n\ncall from lrgsacc2allenc/lrgsacc2oltpallenc/lrgsacc2uncallenc",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagcc2chain1.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcc2chain1_^tabline^_^imcfclv^"
    },
    "description": "tsagcc2chain1.tsc - Basic CC2 test for chainrow uncompress/otlp table",
    "platform": null
  },
  {
    "test_name": "tsagcc2chainini.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcc2chain1ini"
    },
    "description": "tsagcc2chainini.tsc - Setup for chainrow uncompress/otlp table\n\nmodify lineitem add three large column, make it chainrow for CC2 test",
    "platform": null
  },
  {
    "test_name": "tsagcc2chksum.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcc2chksum.tsc - Prune rows and check column level checksum\n\n3 scenario:\n         1) query all columns will have more checksum bytes compare to query one column\n         2) first predicate filterd out all rows will have less checksum bytes\n         3) min,max prune out all row will have less checksum bytes",
    "platform": null
  },
  {
    "test_name": "tsagcc2coldec.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcc2coldec.tsc - Prune rows and check column level decryption\n\n3 scenario:\n         1) query all columns will have more decrypted bytes compare to query one column\n         2) first predicate filterd out all rows will have less decrypted bytes\n         3) min,max prune out all row will have less decrypted bytes",
    "platform": null
  },
  {
    "test_name": "tsagcc2dl.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcc2dl.tsc - CC2 new DDL test, query against differnet cellmemory memcompress table",
    "platform": null
  },
  {
    "test_name": "tsagcc2ini.tsc",
    "setup": null,
    "flags": {
      "curdb": "1"
    },
    "description": "tsagcc2ini.tsc - init setup for tsagcc2.tsc",
    "platform": null
  },
  {
    "test_name": "tsagcc2lite.tsc",
    "setup": null,
    "flags": {
      "tmp_file": "tsagcc2liteo.log",
      "lvl": "1",
      "imaoflgrp": "''^sysoflgrp^''"
    },
    "description": "tsagcc2lite.tsc - run tpch query using CC2 with different scenario\n\nrun tktgifc.sql,tktghqry.sql,tsagcc2ima.sql",
    "platform": null
  },
  {
    "test_name": "tsagcc2liteini.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagcc2liteini.tsc - setup env for lrgsacc2lite",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcc2qenc.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcc2qenc.tsc - create ehcc table in encrypted tbs and run Q19",
    "platform": null
  },
  {
    "test_name": "tsagcc2qm.tsc",
    "setup": null,
    "flags": {
      "message": "'QuarantineMgr: quarantining .*'",
      "qm_crash_freq": "3",
      "pdbs_per_cdb": "1",
      "dbuqname": "^qm_clusterid^:^dbuqname^"
    },
    "description": "tsagcc2qm.tsc -  Test cases of quarantine manager for background CC2 population",
    "platform": null
  },
  {
    "test_name": "tsagcc2si.tsc",
    "setup": null,
    "flags": {
      "log_file": "tsagcc2sibloom.log",
      "ref_log": "tsagcc2sibloom.log"
    },
    "description": "tsagcc2si.tsc - Run SI Dense Bloom Test with columnar cache 2\n\nSI Dense Bloom Test with columnar cache 2",
    "platform": null
  },
  {
    "test_name": "tsagcc2version.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcc2version.tsc - Functional test for CC2 versioning\n\n1. Setup CC2 env and check CC2 successfully.\n     2. Set event to make next CC2 hit abort due to version mismatch.\n     3. Check three stats to see increasing\n     4. Turn off the event and check CC2 hit successfully again.",
    "platform": null
  },
  {
    "test_name": "tsagcccelltest.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcccelltest.tsc - Celltest utility test for CC\n\nRuns celltest utility after crashing test using event\n\nNeeds to run after schema in tktgifc is created",
    "platform": null
  },
  {
    "test_name": "tsagcccelltest2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcccelltest2.tsc - Celltest utility test for CC2\n\nRun celltest utility after crashing test using event and reproduce the\n     crash.",
    "platform": null
  },
  {
    "test_name": "tsagccconvsql.tsc",
    "setup": null,
    "flags": {
      "imcfclvwd": "'cellmemory '^imcfclvwd^"
    },
    "description": "tsagccconvsql.tsc - convert existing sql/log for ehcc table to uncompress/oltp compress table\n\nonly support lower case keyword is on purpose\n     for example, alter table move COMPRESS FOR, add cellmemory is invalid\n     in this case, can use upper case to skip this conversion\n\nimcfcunc if set then convert sql/log to uncompress version\nimcfcoltp if set then convert sql/log to oltp compressed version\nimcfclv use to determine cellmemory memcompress level\nimcfclvwd if set, use cellmemory memcompress level provide outside this script\ncc2ddl if set add cellmemory DDL into sql/log\nskipcc2ddl if set don't add cellmemory DDL into sql/log",
    "platform": null
  },
  {
    "test_name": "tsagccini.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "high",
      "cre_file": "tsagcaudef"
    },
    "description": "tsagccini.tsc - Exadata Columnar compression initilization\n\nInitializes for col compression lrgs",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagccpersistencectl.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagccdef2"
    },
    "description": "tsagccpersistencectl.tsc - set flag for CC persistence lrgs\n\nset flags for CC persistence lrgs",
    "platform": null
  },
  {
    "test_name": "tsagccpmode.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_no_asmdb": "true",
      "errbasename": "^tst_tscname^o.log"
    },
    "description": "tsagccpmode.tsc - Test for the columnar cache persistence mode attribute\n\nFunctional test for the columnar cache persistence mode attribute",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagccpopulation.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sys_group_name": "^sysoflgrp^",
      "log_file": "tsagcc2population.log"
    },
    "description": "tsagccpopulation.tsc - Test flush flashcache while CC background population\n\nTest flush flashcache while CC2 background population\n       1) Create 5 EHCC tables from lineitem, like t1 to t4\n       2) have 5 concurrent sessions, each session do a select count(*) on one of the tables\n       3) when the query returns, do a \"alter flashcache all flush\"\n       4) We should see the flush finished successfully, and dump the v$cell_state:\n            a). outstanding_imcpop_requests is 0.\n            b). total_pred_imcpop_reqs_dropped_flush is not zero and equal to\n               (total_pred_imcpop_reqs_without_buf_dropped_flush +\n                 total_pred_imcpop_reqs_with_buf_dropped_flush )",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcctoggle.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sys_group_name": "^sysoflgrp^",
      "log_file": "tsagcctoggle.log",
      "log_file1": "tsagsetpara.log",
      "oss_memleak_skip_restart": "1"
    },
    "description": "tsagcctoggle.tsc - Toggling and Throttling test case on CC1 AND CC2 switch mode\n\nToggling and Throttling test case on CC1 AND CC2 switch mode",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcctoggle_wkld.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcctoggle_wkld.tsc - Toggling and Throttling test case on CC1 AND CC2 switch mode\n\nToggling and Throttling test case on CC1 AND CC2 switch mode",
    "platform": null
  },
  {
    "test_name": "tsagccverify.tsc",
    "setup": null,
    "flags": {
      "asm_ausize": "1048576"
    },
    "description": "tsagccverify.tsc - test case for bug in CC verification modeline\n                        LRG no. 27651534\n\nCC verification does not abort after block read IO failure\n     so it freed the buffer but later on cell offload server\n     crash because of the freed buffer\n     Test steps :\n     Step 1: Create test table\n     Step 2: Set sim event CC_VERF_READ_ERROR\n     Step 3: Run CC workload with CC verification on",
    "platform": null
  },
  {
    "test_name": "tsagcdbccfcpmem.tsc",
    "setup": null,
    "flags": {
      "pmemmode": "WriteBack",
      "fcmode": "WriteBack",
      "mirrormode": "normal",
      "tsaglogappend": "1"
    },
    "description": "tsagcdbccfcpmem.tsc - check pmem and fc population along cc\n\nThis test runs cc workload and validates that PMEM and FC are\n     populated as expected",
    "platform": null
  },
  {
    "test_name": "tsagcdbfcgroupstests.tsc",
    "setup": "srdbmsini",
    "flags": {
      "exa_cdb": "true",
      "compatible": "12.0.0.0",
      "creatdev_file": "tsagfcgdef"
    },
    "description": "tsagcdbfcgroupstests.tsc - cdb tests for Exadata flash cache groups feature\n\ncontains cdb tests for Exadata flash cache groups feature\n\nThe Flash Cache is partitioned into multiple groups and these partitions are\n     created by logically grouping the cache lines. Every database will have an\n     association with a Flash Cache groups, this mapping would be done by IORM layer",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcdbfclogger.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagr2def",
      "sage_mirror_mode": "normal",
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'",
      "lunsize": "5243000",
      "tblspacesize": "4800",
      "numofrec": "8000"
    },
    "description": "tsagcdbfclogger.tsc - hits fc logger path with cdb set up\n\nThis test simulates BLOCKIO HANG to one of the flash disk\n     and make sure if fc logger code path is exercised.\n\nTest steps:\n     1. Run db workload in the background\n     2. Hung IOs on flash disk that is caching grid disk used by test table\n     3. Clear event set in step 2\n     4. Reset event\n     5. Check if fc logger path is hit",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcdiag.tsc",
    "setup": null,
    "flags": {
      "tst_nowarn_tmp": "^tst_nowarn^"
    },
    "description": "tsagcdiag.tsc -  test the celldiag script\n\ntest the celldiag script",
    "platform": null
  },
  {
    "test_name": "tsagcdmderr.tsc",
    "setup": null,
    "flags": {
      "oss_testing": "1",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback"
    },
    "description": "tsagcdmderr.tsc - bug test for cd md error\n\nTest for transient celldisk meatadata update errors\n   Test case 1:\n     Step 1: List griddisk, expect all active\n     Step 2: Simulation event, CDMETA_UPDATE_ERR\n     Step 3: alter griddisk with comment\n     Step 4: list griddisk, expect all active\n   Test case 2:\n     Step 1: List griddisk, expect all active\n     Step 2: Simulation event, CDMETA_UPDATE_ERR with flash failure\n     Step 3: alter griddisk with comment\n     Step 4: list griddisk, expect all active",
    "platform": null
  },
  {
    "test_name": "tsagceflsuit.tsc",
    "setup": null,
    "flags": {
      "recon_cell_attempt_count": "48",
      "asm_ausize": "1048576"
    },
    "description": "tsagceflsuit.tsc - SAGE Cell Failure Test Suite\n\nDriver test for Cell failure tests",
    "platform": null
  },
  {
    "test_name": "tsagcelfl.tsc",
    "setup": "tsagnini",
    "flags": {
      "cluster_database": "true",
      "asm_allow_sysdba": "true",
      "creatdev_file": "my_tsagcelfl"
    },
    "description": "tsagcelfl.tsc - SAGE cell failure\n\nTests cell failure for normal redundancy diskgroup (multi-oss)\n     - Foll actions are forked:\n        -- tablespace file create\n        -- direct writes\n        -- predicate query\n        -- rman incremental backup\n     - While these actions are performed one cell is brought down\n       (the cell with the mirror)\n     - The oss process is restarted and the affected disks brought online\n     - The above jobs should keep going on and should not error out",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcelflhr.tsc",
    "setup": "tsagnini",
    "flags": {
      "sysdbapdb1": "'sys/knl_test7@cdb1_pdb1 as sysdba'",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagcelflhr.tsc - SAGE cell failure for high redundancy\n\nTests cell failure for high redundancy diskgroup (multi-oss)\n     - Foll actions are forked:\n        -- tablespace file create\n        -- direct writes\n        -- predicate query\n        -- rman incremental backup\n     - While these actions are performed two cells are brought down (the cells with the mirror)\n     - The oss process is restarted and the affected disks brought online\n     - The above jobs should keep going on and should not error out",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcelflof.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagaudef",
      "fsize": "112"
    },
    "description": "tsagcelflof.tsc - Cell Failure offline\n\nTests cell failure for normal redundancy diskgroup (multi-oss)\n        -- Workload spawned against 2 Cells against two diskgroups (Datafile\n           and Fred DG).\n        -- Stop I/O's to one particular Fred diskgroup,  keeping\n           the foreground alive (sleep).\n           Continue workload that does I/O's to  the Datafile DG.\n        -- cellsrv for the fred dg is killed.\n        -- Validate that grid disks to which I/O was done (Datafile)\n           gets offlined.\n           But the grid disk belonging to fredDG should still be seen\n           online by ASM, since there was no I/O generated to it.\n        -- Start cell1\n        -- Wake up the sleeping sql session and make more inserts to Fred DG.\n        -- Online the grid disks of Datafile dg the one that went offline due\n           the cell coming down while io's were in progress.\n        -- Make more inserts to Datafile and make sure everything is ok.\n\nRuns with two cellsrv process (normal redundancy)",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcelflscel.tsc",
    "setup": "tsagnini",
    "flags": {
      "auto_undo_management": "true",
      "asm_ausize": "65536",
      "creatdev_file": "tkfgrddef",
      "redund": "external"
    },
    "description": "tsagcelflscel.tsc - SAGE single cell cell-failure test\n\n- start single oss cell, diskmon, asm, rdbms\n     - do ios-finish workload\n     - kill oss -> RDBMS comes down, ASM survives but diskgroups are not available\n     - startoss -> oss should not crash.\n     - Mount DG and bring up RDBMS\n     - check if everything is fine without bouncing rdbms & asm",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcell.tsc",
    "setup": null,
    "flags": {
      "snmp_port": "^free_port_number^"
    },
    "description": "tsagcell.tsc - Tests for CELL object\n\ntests the various operations on CELL done through CellCli\n\nNone",
    "platform": null
  },
  {
    "test_name": "tsagcell_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcell_nls.tsc - Cell command of CellCLI\n\nCell commands of various objects on CellCLI",
    "platform": null
  },
  {
    "test_name": "tsagcellclitst.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagcellclitst.tsc - Functional tests for new features introduced in cellcli\n\nTest Outline\n0.\tBring up stack with 2 cells running in Normal redundancy mode\n1.\tCheck if two instance of cellsrv are running, one on each cell\n2.\tConnect to cell 1\n\ta) List griddisk attributes; ensure all griddisk are online\n\tb) Deactivate griddisk datafile0\n      3.      Connect to cell 2\n              a) List griddisk attributes; ensure each griddisk is online and asmdeactivationoutcome is set to \"Cannot Deactivate\"\n\n4.1\tAttempt to shutdown cellsrv without force option - Should fail with error CELL-01548\n4.2\tAttempt to shutdown cellsrv with force option - Should succeed",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellclitst1.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagcellclitst1.tsc - Extension of tsagcellclitst.tsc for diskgroups with External Redundancy\n\n0.\tBring up the stack with 1 cell - External Redundancy setup\n     1.\tList griddisk attributes; ensure all griddisks are online and control file is listed as \"Cannot Deactivate\"\n     2.\tAttempt to shutdown cellsrv without force option - Should fail with error CELL-01548\n     3.\tAttempt to shutdown all services without force option - Should fail with error CELL-01548\n     4.       Attempt to shutdown all services with force option - Should succeed\n     5.       Attempt to restart all services with force option - Should succeed",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcelldiag.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagcelldiag.tsc - cellsrv diagnostics commands\n\ncellsrv diagnostics commands as per bug 9352226",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcelldisk.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagcelldisk.tsc - Add test for celldisk creation scenario\n\nTest for celldisk operation when PD/Luns are not normal",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellexadoop.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagcellexadoop.tsc - Driver file to run exadoopsrc with cellsrv\n\nSets up srdbmsini and then brings up exadoopsrv\n\nThis driver file is not called anywhere. It is just for testing purpose\n     to run the setup without exadoop node setup just RDBMS/ASM/CLUSTERWARE/CELLSRV\n     with exadoopsrv.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellhealthchk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcellhealthchk.tsc - Check health of physicaldisks ,\n         \t\t\tluns, celldisks after fresh reimage",
    "platform": null
  },
  {
    "test_name": "tsagcellofcp1.tsc",
    "setup": "tsagnini",
    "flags": {
      "cluster_database": "true",
      "maxinstances": "4",
      "creatdev_file": "tkfgrddef"
    },
    "description": "tsagcellofcp1a.tsc - Tests Cell to cell offload feature\n\nOutline of the test :\n * Bring two RDBMS instances with help of RAC.\n       * set event to HOLD IO on first RDBMS instance, insert some records and then RELEASE IO\n       * Run workload on both the RDBMS instances.\n       * shutdown abort first ASM\n       * workload on first RDBMS instance should fail, but workload on RDBMS instance shuld succeed\n\nCell-to-Cell offload feature comes into play when :\n        o ASM synchronizes data from an ONLINE disk to one or more partner disks, which are being brought ONLINE.\n        o ASM Diskgroup is undergoing a reconfiguration due to disks being dropped or added.\n      Typially ASM is the one which gets data from the source disk and then copies to the destination disk for disk rebalance. But with cell to cell offload feature\n      ASM instructs destination cell to get the data from the source disk and when this is done the destination disk sends an update to ASM, there by\n      involving ASM as little as possible and hence saving significant amount of bandwidth.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellofcp2.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagcellofcp2.tsc - Test disk rebalance operation when the cell is in split brain mode.\n\nThe test does the following :\n  *) Bring up database on exadata\n  *) Set even in all the cells to simulate split brain mode.\n  *) Run ASM disk rebalance operation by dropping and adding a disk\n  *) The rebalance operation in previous step should succeed.\n\nCell-to-Cell offload feature comes into play when :\n        o ASM synchronizes data from an ONLINE disk to one or more partner disks, which are being brought ONLINE.\n        o ASM Diskgroup is undergoing a reconfiguration due to disks being dropped or added.\n      Typially ASM is the one which gets data from the source disk and then copies to the destination disk for disk rebalance. But with cell to cell offload feature\n      ASM instructs destination cell to get the data from the source disk and when this is done the destination disk sends an update to ASM, there by\n      involving ASM as little as possible and hence saving significant amount of bandwidth.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellofcp3.tsc",
    "setup": "srdbmsini",
    "flags": {
      "differentszfg": "1",
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagcellofcp3.tsc - Tests cell to cell offload feature.\n\nThe test is outlined as below :\n          * Bring up database with high redundancy.\n          * Have a loop which does the following :\n   \t      - Connect to cell i\n          Make griddisk inactive\n\t  wait till the disks are dropped from ASM.\n                Make griddisk active\n\t  Check whether the dropped disks added back again.\n      - Connect to cell (i+1)\n\t  Run disk rebalance operation by dropping and added a disk.\n\nCell-to-Cell offload feature comes into play when :\n        o ASM synchronizes data from an ONLINE disk to one or more partner disks, which are being brought ONLINE.\n        o ASM Diskgroup is undergoing a reconfiguration due to disks being dropped or added.\n      Typially ASM is the one which gets data from the source disk and then copies to the destination disk for disk rebalance. But with cell to cell offload feature\n      ASM instructs destination cell to get the data from the source disk and when this is done the destination disk sends an update to ASM, there by\n      involving ASM as little as possible and hence saving significant amount of bandwidth.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellofcp4.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagcellofcp2a.tsc - Tests cell to cell offload feature by bouncing cellsrvs while runing a DB workload\n\nThis test basically does the following :\n       * Run the following in a loop with high redundancy DG with some write intensive workload on the DB.\n          a) Stop cell-a (here 'a' can be 1, 2 or 3)\n          b) Sleep till cell is evicted\n          c) Stop cell-b (here 'a' can be 1, 2 or 3 but not same value as 'a')\n          d) Sleep till cell is evicted\n          e) Start cell-a\n          f) After a few seconds, Start cell-b\n          g) Wait for all disks to be ONLINE.\n\nCell-to-Cell offload feature comes into play when :\n        o ASM synchronizes data from an ONLINE disk to one or more partner disks, which are being brought ONLINE.\n        o ASM Diskgroup is undergoing a reconfiguration due to disks being dropped or added.\n      Typially ASM is the one which gets data from the source disk and then copies to the destination disk for disk rebalance. But with cell to cell offload feature\n      ASM instructs destination cell to get the data from the source disk and when this is done the destination disk sends an update to ASM, there by\n      involving ASM as little as possible and hence saving significant amount of bandwidth.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellofcp5.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagcellofcp3a.tsc -  Test cell to cell offload feature by checking whether disk rebalance/resync\n\t\t     takes place at the ASM instance while bouncing cellsrv in failure++ mode\n\nThe test does the following :\n   *) Bring up database in failure++ mode ( in this mode cellsrv's are restarted periodically in round robin fashion )\n   *) Run ASM disk rebalance operation by dropping and adding a disk\n   *) The rebalance operation in the previous step should succeed.\n\nCell-to-Cell offload feature comes into play when :\n        o ASM synchronizes data from an ONLINE disk to one or more partner disks, which are being brought ONLINE.\n        o ASM Diskgroup is undergoing a reconfiguration due to disks being dropped or added.\n      Typially ASM is the one which gets data from the source disk and then copies to the destination disk for disk rebalance. But with cell to cell offload feature\n      ASM instructs destination cell to get the data from the source disk and when this is done the destination disk sends an update to ASM, there by\n      involving ASM as little as possible and hence saving significant amount of bandwidth.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellofcp6.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagcellofcp6.tsc - Tests cell to cell offload feature by checking that the querries at the db instance gets through\n\t\t    despite fluctuating state of the underlying ASM disks.\n\nThe test essentially does the following :\n  *) Bring up database on two cells\n  *) Run a database workload while force dropping and adding a disk at ASM.\n  *) The DB workload in the previous step should succeed.\n\nCell-to-Cell offload feature comes into play when :\n        o ASM synchronizes data from an ONLINE disk to one or more partner disks, which are being brought ONLINE.\n        o ASM Diskgroup is undergoing a reconfiguration due to disks being dropped or added.\n      Typially ASM is the one which gets data from the source disk and then copies to the destination disk for disk rebalance. But with cell to cell offload feature\n      ASM instructs destination cell to get the data from the source disk and when this is done the destination disk sends an update to ASM, there by\n      involving ASM as little as possible and hence saving significant amount of bandwidth.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellofcp7.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagr2def",
      "differentszfg": "1"
    },
    "description": "tsagcellofcp7.tsc - Tests cell to cell offload feature\n\nThe test essentially does the following :\n        *) Bring up database.\n        *) Run a database workload on thread 1\n  *) On thread 2 do the following in a loop\n       Drop a disk from ASM.\n       Create a new disk with dd command and add this disk in place of dropped disk\n        *) The DB workload for the thread 1 should succeed.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellofflineforce.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "SAGE_MIRROR_MODE": "high",
      "confdir": "^T_WORK^/raw^oss_port1^_conf"
    },
    "description": "tsagcellofflineforce.tsc - test for cell offline maintenance force event\n\nA cell offline maintenance force event occurs when EGS finds that\n     a cell has already moved to offline maintenance without first\n     consulting with EGS. Test steps:\n\n     1. Bring up exascale stack with high redundancy\n     2. Shutdown all EGS servers\n     3. Shutdown MS on cell 1\n     4. Set status to \"offline\" in $OSSCONF/cell_disk_config.xml for cell 1\n     5. Startup MS on cell 1\n     6. Ensure status is set to offline by running \"cellcli -e list cell\"\n     7. Startup all EGS servers\n     8. Grep for \"getExacEvents: set flag ECS_RESULT_FLAGS_IN_MAINTENANCE\"\n        in svtrc*trc to ensure that cellsrv has set the in maintenance flag\n     9. Grep for the following in EGS leader alert log:\n        a. USR-ExaVault .* is in OFFLINE_MAINTENANCE state\n        b. SYS-ExaVault .* is in OFFLINE_MAINTENANCE state\n  c. ExaScale cell server .* is in OFFLINE_MAINTENANCE (FORCE)\n    10. Restart all services\n    11. Online cell 1 by running cellcli -e \"alter cell fromversion='21.4.0.0.0.220103',patchmode='is_rolling'\"",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagcelloflinpara.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagcelloflinpara.tsc - Negative test to add incorrect para in celloffloadinit.ora\n\nUse incorrect parameter in celloffloadinit.ora and offload group should not started up",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcelloflk.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagcelloflk.tsc - Celloffload Srv kill\n\nKills celloffload srv process and makes sure all dependent process are\n     also killed",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellofln.tsc",
    "setup": "tsagnini",
    "flags": {
      "format_long_identifier": "true"
    },
    "description": "tsagcellofln.tsc - Add for maximum offloadgroups\n\nThis test is to create 15 offloadgroups and the 15th should encounter errors",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcelloflp.tsc",
    "setup": "tsagnini",
    "flags": {
      "format_long_identifier": "true"
    },
    "description": "tsagcelloflp.tsc - Start > 1 Celloffload processes\n\nWhen the number of celloffload srv process is > 1, smart scan is\n     disabled and the test should go through passthru mode",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellparallelread.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagr2def",
      "SAGE_MIRROR_MODE": "normal",
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagcellparallelread.tsc - celldisk parallel read test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellpread2.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_auto_manage_disks": "true",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagcellpread2.tsc - Cellsrv Metadata Read change test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellproxy.tsc",
    "setup": null,
    "flags": {
      "cell_conn1": "o^s^^cellip^:^oss_port1^",
      "cellproxy_opt": "'-p \"'^cell_conn1^'\"  -p \"'^cell_conn2^'\"'",
      "cell_conn2": "o^s^^cellip^:^oss_port2^",
      "log_file": "tsagnetworkproxy_cell^cell_mode^.log"
    },
    "description": "tsagcellproxy.tsc - use cellconnstress to test cellsrv proxy\n\nThis feature includes below basic tests:\n       1. Clients can connect to a cell via diskmon, need Clusterware and\n          cell services but not ASM/RDBMS instance\n          do like: runtest tsagcellproxy cell_mode=1\n       2. Clients can connect to a cell via diskmon, multi-cell\n          do like: runtest tsagcellproxy cell_mode=2\n       3. Clients can connect to a cell via diskmon, need ASM/RDBMS instance\n          do like: runtest tsagcellproxy cell_mode=3\n\nWe are going to test below tests 1-10, which need to run cellconnstress\n     with various different cellinit.ora parameter settings. Some of these\n     parameter settings affect cellsrv, and some affect diskmon. For the\n     tests which have new parameter settings for cellsrv, we need to restart\n     cellsrv.\n\n     For the tests which have new parameter settings for diskmon, we need to\n     restart diskmon, which requires killing the current diskmon, and\n     waiting for cssd to restart it.  WARNING: we can only kill diskmon 5\n     times in a span of 10 minutes.  Tests for cell_mode 1 and 2 currently\n     kill diskmon 5 times, so we are at this limit; if we ever add more\n     tests which need to kill diskmon, we will need to add a sleep of 600\n     seconds after each set of 5 kills. The parameter _enable_proxy is used\n     to enable/disable cellsrv proxy feature.\n\n       - Test 1: Default settings: proxy feature disabled\n       - Test 2: _enable_proxy=TRUE [cellsrv]\n       - Test 3: _enable_proxy=FALSE [cellsrv] and _enable_proxy=TRUE\n                [diskmon]\n       - Test 4: _enable_proxy=TRUE for cellsrv and diskmon\n\n  For tests 5-10, keep _enable_proxy=TRUE.\n\n       - Test 5: _cell_proxy_socket_buffer_size=1 [cellsrv]\n       - Test 6: (also, remove setting for _cell_proxy_socket_buffer_size):\n                _dskm_proxy_socket_buffer_size=1 [diskmon]\n       - Test 7:_cell_proxy_socket_buffer_size=1 [cellsrv]\n                _dskm_proxy_socket_buffer_size=1 [diskmon]\n\n  For tests 8-10, restore the default settings for\n    _cell_proxy_socket_buffer_size and _dskm_proxy_socket_buffer_size.\n       - Test 8: _send_partial_proxy_data=TRUE [cellsrv]\n       - Test 9: _send_partial_proxy_data=TRUE [diskmon]\n       - Test 10: _send_partial_proxy_data=TRUE [cellsrv and diskmon]\n\n  * cellconnstress will need to be run with the following command-line:\n    cellconnstress -C oss_conn -p \"o/<connect_string>\" -d 0 -t 10 -c 10 1 60\n\n     The connect_string argument should be the same as the value of the cell\n     parameter from cellip.ora; for example:\n        cell=\"10.240.3.31;10.240.3.31;10.240.3.31;10.240.3.31:34854\"\n\n     So, the cellconnstress command-line would look like this:\n        cellconnstress -C oss_conn -p \"o/xx:34854\" -d 0 -t 10 -c 10 1 60\n\n     The output from cellconnstress looks like this:\n      #con    #coner    ms/con    con/s    #dis    #diserr    ms/dis    dis/s\n      100    0    2.45    3798.24    0    0    0.00    0.00\n      0    0    0.00    0.00    100    0    1.11    7970.67\n      #con    #coner    ms/con    con/s    #dis    #diserr    ms/dis    dis/s\n      100    0    2.57    3626.08    0    0    0.00    0.00\n      0    0    0.00    0.00    100    0    1.14    6269.20\n      ...\n     There are 3 lines of output for every iteration (the last command-line\n     argument specifies the # of iterations. The key thing is that on the 2nd\n     line (of each iteration), the 2nd column (\"#coner\") value should be zero,\n     and on the 3rd line, the 6th column (\"#diserr\") should also be zero. If\n     they are not both zero, that should be a dif.",
    "platform": null
  },
  {
    "test_name": "tsagcellscaleupfail1.tsc",
    "setup": null,
    "flags": {
      "num_cells": "6",
      "oss_multims_testing": "true",
      "mixed_workload": "true",
      "maxpdb": "1",
      "setup_blockstore": "true"
    },
    "description": "tsagcellscaleupfail1.tsc - Scenarios for eXtra Large cell config\n\n-Adding cells and removing cells - 6 to 10, and then drop 1 cell\n     -Failing hard disk and flash disk  to check for rebalance and\n     resilvering in setup for eXtra Large cells\n\n<to run cell only lrgs>",
    "platform": null
  },
  {
    "test_name": "tsagcellscaleupfail2.tsc",
    "setup": null,
    "flags": {
      "num_cells": "8",
      "mixed_workload": "true",
      "maxpdb": "1",
      "oss_multims_testing": "true",
      "setup3egs": "true"
    },
    "description": "tsagcellscaleupfail2.tsc - Scenarios for eXtra Large cell config\n\n1. Start test with 8 cells and add 7 more cells\n    2. Drop 1 cells and let workload run fo 10 mins\n    3. Add back one of cells dropped\n\nLarge configuration of cells with workload running.",
    "platform": null
  },
  {
    "test_name": "tsagcellsnmpv3.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagcellsnmpv3.tsc - Test for snmp v3 features on cell\n\nTest various positive and negative test for SNMP v3 on Cell.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellsqlstat.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagcellsqlstat.tsc - Tests for CellSqlStat tool to display various stats per queries running on the cellside\n\nTests for CellSqlStat tool to display various stats per queries running on the cellside",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellsrvdump.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagcellsrvdump.tsc - takes cellsrvdump on real hardware\n\nThis test takes cellsrv dump while heavy db workload is running.\n     The dump should be completed in less than four seconds\n\nReal hardware test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcelltest.tsc",
    "setup": "tsaginit",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagcelltest.tsc - Exadata cell utility test\n\nExadata cell utility test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcellupdown.tsc",
    "setup": null,
    "flags": {
      "cell_bin_copy_and_install": "true"
    },
    "description": "tsagcellupdown.tsc - Exadata rpm upgrade/downgrade Test",
    "platform": null
  },
  {
    "test_name": "tsagcellupdowntests.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcelldowntests.tsc - Helper script for Exadata rpm upgrade/downgrade test\n\nAdd pre/post upgrade and downgrade tests/checks in this script.",
    "platform": null
  },
  {
    "test_name": "tsagcesrvstatmem.tsc",
    "setup": "xrdbmsini",
    "flags": null,
    "description": "tsagcesrvstatmem.tsc - Exadata cellsrvstat meme leak check\n\nSets event and checks for mem leak from output of cellsrvstatd\n\nNeeds event to simulate memleak",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagcfc1.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc1.log"
    },
    "description": "tsagcfc1.tsc - Basic tests(select,update,delete,insert) for Columnar cache project\n\ncase 1: populate without columnar FC, select check should fail\n     case 2: populate with columnar FC, select check should succeed\n     case 3: invalidate columnar FC with update, disable scan, update entire object, updated data check should fail\n\nBUG 21806121 - IFC 2.0: DMLS NOT WORKING WHEN ROWIDS NEEDED",
    "platform": null
  },
  {
    "test_name": "tsagcfc1_23.tsc",
    "setup": null,
    "flags": {
      "qryslp": "300",
      "caseno": "3",
      "log_name": "tsagcfc1_2.log"
    },
    "description": "tsagcfc1_23.tsc - Basic tests(select,update,delete,insert) for Columnar cache project\n\ncase 4: invalidate columnar FC with delete, disable scan, delete entire object, deleted data check should fail\n     case 5: insert new data, new data check should fail\n\nSplit from lrgsacfc, become lrgsacfc_23\n     BUG 27473562 - CC2 ENHANCEMENT: ENABLE ROWID FOR CC2 FOR COUNTSTAR CASE",
    "platform": null
  },
  {
    "test_name": "tsagcfc1_23b.tsc",
    "setup": null,
    "flags": {
      "qryslp": "300",
      "caseno": "4",
      "log_name": "tsagcfc1_3_fc.log"
    },
    "description": "tsagcfc1_23b.tsc - Basic tests(select,update,delete,insert) for Columnar cache project\n\ncase 5: insert new data, new data check should fail\n\nSplit from lrgsacfc_23, become lrgsacfc_23b\n     BUG 21806121 - IFC 2.0: DMLS NOT WORKING WHEN ROWIDS NEEDED",
    "platform": null
  },
  {
    "test_name": "tsagcfc1fork.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc1fork.log"
    },
    "description": "tsagcfc1fork.tsc - Fork workload and run columnar FC check\n\nuse tsagcfcforkrun.tsc to run fork test for 4 types(al,ah,ql,qh) of compress tables\n    during workload running, disable columnar FC and check status(fail), then re-enable and check again(success):\n\n\t\t  /---  fork run tsagcfcforkcrud.sql ---\\\n    tsagcfcforkbefore---\t\t\t\t\t -----tsagcfcforkafter\n\t\t  \\---  fork run tsagcfcforkop.tsc   ---/\n    tsagcfcforkbefore:\nsetting before fork test if needed\n    tsagcfcforkcrud.sql:\n      fork CRUD workload sql\n    tsagcfcforkop.tsc:\n      fork test operation(disable/enable columnar FC, check, etc)\n    tsagcfcforkafter:\n      setting after fork test if needed\n    defaultHook.tsc:\ninitial empty tsagcfcforkbefore and tsagcfcforkafter",
    "platform": null
  },
  {
    "test_name": "tsagcfc2.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc2.log"
    },
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagcfc3.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc3ini.log"
    },
    "description": "tsagcfc3.tsc - Columnar cache Failure Testcases\n\nuse tsagcfcforkrun.tsc to run fork test for 4 types(al,ah,ql,qh) of compress table\n     during CRUD workload running, exec fail operation and check columnar FC status\n     failure operation including:\n     1) restart/kill cellsrv(will empty columnar FC)/celloflsrv(will NOT empty columnar FC)\n\nRS-700 expected when we get ORA-600[PredicateOflFilterComn::sim_railroad_crash] which\n     the test simulate. PRAGMA has been added for watson to ignore the RS-700.",
    "platform": null
  },
  {
    "test_name": "tsagcfc3drop.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc3dropini.log"
    },
    "description": "tsagcfc3drop.tsc - Columnar cache Failure Testcases\n\nuse tsagcfcforkrun.tsc to run fork test for 4 types(al,ah,ql,qh) of compress table\n     during CRUD workload running, exec drop flashcache media and check columnar FC status\n\nCalled by tsagcfc3dropcd(2)/fc(2)/media(2).tsc\n     Due to bug 19806218, lrgsacfc3dropXXX are disabled for now",
    "platform": null
  },
  {
    "test_name": "tsagcfc3dropcd.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc3dropcd.log"
    },
    "description": "tsagcfc3dropcd.tsc - Columnar cache Failure Testcases\n\nuse tsagcfcforkrun.tsc to run fork test for 4 types(al,ah,ql,qh) of compress table\n     during CRUD workload running, exec drop celldisk and check columnar FC status\n\nSELECT, INSERT workload",
    "platform": null
  },
  {
    "test_name": "tsagcfc3dropcd2.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc3dropcd2.log"
    },
    "description": "tsagcfc3dropcd2.tsc - Columnar cache Failure Testcases\n\nuse tsagcfcforkrun.tsc to run fork test for 4 types(al,ah,ql,qh) of compress table\n     during CRUD workload running, exec drop celldisk and check columnar FC status\n\nUPDATE, DELETE workload",
    "platform": null
  },
  {
    "test_name": "tsagcfc3dropfc.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc3dropfc.log"
    },
    "description": "tsagcfc3dropfc.tsc - Columnar cache Failure Testcases\n\nuse tsagcfcforkrun.tsc to run fork test for 4 types(al,ah,ql,qh) of compress table\n     during CRUD workload running, exec drop flashcache and check columnar FC status\n\nSELECT, INSERT workload",
    "platform": null
  },
  {
    "test_name": "tsagcfc3dropfc2.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc3dropfc2.log"
    },
    "description": "tsagcfc3dropfc2.tsc - Columnar cache Failure Testcases\n\nuse tsagcfcforkrun.tsc to run fork test for 4 types(al,ah,ql,qh) of compress table\n     during CRUD workload running, exec drop flashcache and check columnar FC status\n\nUPDATE, DELETE workload",
    "platform": null
  },
  {
    "test_name": "tsagcfc3dropmedia.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc3dropmedia.log"
    },
    "description": "tsagcfc3dropmedia.tsc - Columnar cache Failure Testcases\n\nuse tsagcfcforkrun.tsc to run fork test for 4 types(al,ah,ql,qh) of compress table\n     during CRUD workload running, exec drop flashcache media and check columnar FC status\n\nSELECT, INSERT workload",
    "platform": null
  },
  {
    "test_name": "tsagcfc3dropmedia2.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc3dropmedia2.log"
    },
    "description": "tsagcfc3dropmedia2.tsc - Columnar cache Failure Testcases\n\nuse tsagcfcforkrun.tsc to run fork test for 4 types(al,ah,ql,qh) of compress table\n     during CRUD workload running, exec drop flashcache media and check columnar FC status\n\nUPDATE, DELETE workload",
    "platform": null
  },
  {
    "test_name": "tsagcfc3dropspdg.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcfc3dropspdg.tsc - Test for sparse diskgroup\n\nSetup sparse DG with nomal redundancy\n     Fork to run:\n       CC2 workload\n       add/drop disks",
    "platform": null
  },
  {
    "test_name": "tsagcfc4.tsc",
    "setup": null,
    "flags": {
      "eventname_i": "eventname^event_num^",
      "tsagcfceventlist": "^rtest^",
      "workloadtime": "90",
      "hidestr": "'##'",
      "tmp_event": "eventname^i^",
      "log_name": "tsagcfc4_^i^.log"
    },
    "description": "tsagcfc4.tsc - Simulation event tests on Columnar cache mode\n\n1) set simulation event\n2) run CRUD workload\n3) check columnar FC status\n4) set event off\n\n      tsagcfceventlist including:\n  FLASHCACHE_READ_CORRUPT_DATA\ttrigger cell disk failure\n  FLASHCACHE_FALSE_CORRUPT_DATA false positive data corruption\n  FLASHCACHE_OUTOFMEM\t\tflashcache out of memory\n  FLASHCACHE_CORRUPT_MD\t\tflashcache metadata corrupt\n  BLOCKIO_READ_ERR\t\tblock read error, will crash DB instance\n  BLOCKIO_WRITE_ERR\t\tblock write error, will crash DB instance",
    "platform": null
  },
  {
    "test_name": "tsagcfc5.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc5.log"
    },
    "description": "tsagcfc5.tsc - Multiple DML testcases on columnar cache mode\n\nRun multiple DML(CRUD) in a loop sequence, check columnar FC status and flashcachecontent\n     tsagcfcloopdml.tsc:\n       tsagcfcloopinsert.sql: insert data and check columnar FC\n       tsagcfcloopselect.sql: select data and check columnar FC\n       tsagcfcloopupdate.sql: update data and check columnar FC\n       tsagcfcloopdelete.sql: delete data and check columnar FC\n   loop tsagcfcloopdml.tsc 3 times on 4 types(al, ah, ql, qh) of compress table and check columnar FC status",
    "platform": null
  },
  {
    "test_name": "tsagcfc6.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcfc6.tsc - test for manipulate columns of columnar FC table\n\ncall tsagcfc6complv.tsc using different compress level\n     test steps copy from tsagcfc6complv.tsc:\n       1) start with non-compress table, query and check FC status\n    \t 2) alter table to compress and cached by columnar FC, check FC status\n 3) drop column, check FC status\n     \t 4) add column, check FC status\n       5) modify column, check FC status\n     \t 6) rename column, check FC status",
    "platform": null
  },
  {
    "test_name": "tsagcfc6complv.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc6_^colcomplv^.log"
    },
    "description": "tsagcfc6complv.tsc - provide test for tsagcfc6.tsc\n\n-1) start with non-compress table, query and check FC status-\n     -2) alter table to compress and cached by columnar FC, check FC status-\n     combine step 1) and 2), as non-compress table will use CC2 now, step 1 become invalid\n     3) drop column, check FC status\n     4) add column, check FC status\n     5) modify column, check FC status\n     6) rename column, check FC status",
    "platform": null
  },
  {
    "test_name": "tsagcfc7.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc7ini.log"
    },
    "description": "tsagcfc7.tsc - test for manipulate partitions of columnar FC table\n\ncall tsagcfc7complv.tsc using different compress level\n     test steps copy from tsagcfc7complv.tsc:\n1) create table with separate compression level for different partitions\n2) load some data, issue read IOs and ensure data is caching\n3) query different partitions with read/write IO and check columnar FC status\n4) drop a partition from table and check columnar FC status\n5) add a partition, load data? and check columnar FC status\n6) merge partition, issue IO and check columnar FC status\n7) split partition, issue IO and check columnar FC status\n8) truncate partition, check columnar FC status\n\ndue to bug 19562274, ql/qh partition test are disabled",
    "platform": null
  },
  {
    "test_name": "tsagcfc7complv.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagcfc7_^ptcomplv^.log"
    },
    "description": "tsagcfc7complv.tsc - provide test for tsagcfc7.tsc\n\n1) create table with separate compression level for different partitions\n2) load some data, issue read IOs and ensure data is caching\n3) query different partitions with read/write IO and check columnar FC status\n4) add a partition, and check columnar FC status\n5) drop a partition from table and check columnar FC status\n6) split partition, issue IO and check columnar FC status\n7) merge partition, issue IO and check columnar FC status\n8) truncate partition, check columnar FC status",
    "platform": null
  },
  {
    "test_name": "tsagcfc8.tsc",
    "setup": null,
    "flags": {
      "tmp_var": "tbname^i^",
      "tmp_tb_name": "^table_prefix^_^^tmp_var^_comp",
      "tmp_tb_name2": "^table_prefix^_^^tmp_var^_nocomp",
      "log_name": "tsagcfc8_^cell_fc_policy^_1.log"
    },
    "description": "tsagcfc8.tsc - Replace test and Build SI test\n\nTest Case 1.7 - Replace test\n       Test Case 1.8 - Build SI from columnar cache",
    "platform": null
  },
  {
    "test_name": "tsagcfcchk.tsc",
    "setup": null,
    "flags": {
      "tmp_var": "tbname^i^",
      "tmp_tb_name": "^table_prefix^_^^tmp_var^_comp"
    },
    "description": "tsagcfcchk.tsc - check columnar flashcache for test tables\n\n< DESCRIPTION >",
    "platform": null
  },
  {
    "test_name": "tsagcfcini.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "PWFILE_ON_EXC": "false",
      "file_dest": "'+fred'",
      "tbname_i": "tbname^table_num^",
      "table_suite": "^rtest^",
      "oss_columnar_cache": "^imcfclv^",
      "log_file": "tsagcfcini.log"
    },
    "description": "tsagcfcini.tsc - Setup test env. for Columnar cache\n\n< DESCRIPTION >",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagcfcio.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^.log"
    },
    "description": "tsagcfcio.tsc - add for bug20321571\n\nWhen IO request fails, CC should not work",
    "platform": null
  },
  {
    "test_name": "tsagcfciorm.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagcfcdef",
      "log_name": "tsagcfciorm.log"
    },
    "description": "tsagcfciorm.tsc -  Part of bug test for 20919450\n\ncolumnar flashcache iorm bug test for 20919450\n     using 2 DB and alter IORM plan for flashcache usage\n     1) populate block fc for db 1, and populate block fc for db 2,\n     block fc of db2 push out block fc of db1\n     2) restart cell to clear flashcache\n     3) populate columnar fc for db 1, and populate columnar fc for db 2,\n     columnar fc of db2 push out columnar fc of db1\n\nFOLLOWING BUG 21251303 HAS BEEN FIXED\n     file bug 21251303 for new dif due to following check fail:\n     after populate columnar flashcache in 2nd part of test,\n     csize in list flashcachecontent and ccsize in trace file should pass the following check\n      ($ccsize >=  cSize) and ($ccsize - cSize <= 10% of ccSize)",
    "platform": null
  },
  {
    "test_name": "tsagcfcsim.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcfcsim.tsc - simulate errors for columnar cache\n\nTest case 1:CELLOFLSRV_DEATH simulation\n     Test case 2:CORRUPTION simulation",
    "platform": null
  },
  {
    "test_name": "tsagcfcsuit.tsc",
    "setup": null,
    "flags": {
      "cell_fc_policy": "keep"
    },
    "description": "tsagcfcsuit.tsc - test suit for Columnar Flash Cache\n       oss_sacfc.tsc:lrgsacfc\n   basic test for columnar FC\n oss_sacfc2.tsc:lrgsacfc2\n   fork workload test\n oss_sacfc3.tsc:lrgsacfc3\n   failure test\n oss_sacfc4.tsc:lrgsacfc4\n   sim event test\n oss_sacfc5.tsc:lrgsacfc5\n   multiple DML test\n\n< DESCRIPTION >",
    "platform": null
  },
  {
    "test_name": "tsagcfcwb.tsc",
    "setup": null,
    "flags": {
      "nflint": "0",
      "log_name": "tsagcfcwb.log"
    },
    "description": "tsagcfcwb.tsc - Part of bug test for 20919450\n\n1. create a database and populate a table with some data\n     2. drop the flashcache and recreate it in writeback mode\n     Do the following (3,4,5) in a loop (at least 10 times)\n     3. update a single row randomly\n     4. do a full table scan\n     5. Make sure there are no SEGV present.",
    "platform": null
  },
  {
    "test_name": "tsagcfcwrong.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagcfcdef",
      "log_name": "tsagccwrong.log"
    },
    "description": "tsagcfcwrong.tsc - Wrong result test by using simulation event",
    "platform": null
  },
  {
    "test_name": "tsagchapvalidation.tsc",
    "setup": "xblockini",
    "flags": {
      "eswallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagchapvalidation.tsc - testcase for chap validation",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagcheck_anthb.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcheck_anthb.tsc - Checks which lrgs use higher skgxp timeout\n\nReads list of lrgs in file and uses higher timeout of 128s",
    "platform": null
  },
  {
    "test_name": "tsagcheck_asmcompat.tsc",
    "setup": null,
    "flags": {
      "LRG_NAME": "not_set"
    },
    "description": "tsagcheck_asmcompat.tsc - Check what asm compatibility setting to use\n\nMaintaining all SAGE_REGRESS lrgs that run in integration in the file:\n          oss/test/tsage/data/compat_skip_lrglist\n    This script will check if the lrg is amongst this list and will continue\n     to use current compatibility settings\n    If the lrg is not found in the list, it will default to 19.0",
    "platform": null
  },
  {
    "test_name": "tsagcheck_egsmode.tsc",
    "setup": null,
    "flags": {
      "LRG_NAME": "local_run"
    },
    "description": "tsagcheck_egsmode.tsc - Checks if egs_mode is exacs\n\nChecks if lrg is in the list of lrgs that will be run\n     in cloudservice mode",
    "platform": null
  },
  {
    "test_name": "tsagcheck_infinicheck.tsc",
    "setup": null,
    "flags": {
      "tst_tscname": "tsagcheck_infinicheck_ib"
    },
    "description": "tsagcheck_infinicheck.tsc - Test infinicheck\n\nVerifies that infinicheck is working properly\n     also test of bug Bug 36786540 - INFINICHECK FAILS WHEN\n     THERE IS A FILE NAMED 0 IN CURRENT DIRECTORY",
    "platform": null
  },
  {
    "test_name": "tsagcheckfc.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_auto_manage_disks": "true",
      "creatdev_file": "tsagr2def",
      "uniq_dsknames": "all"
    },
    "description": "tsagcheckfc.tsc - Test To check flash cache is being populated\n\nFunctional Test to Check if large control file read and write\n     are being populated into Flash Cache Tested with orion\n     on fake hw",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagchkccstatsfetch.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagchkccstatsfetch.tsc - fetch columnar cache stats from fc dump",
    "platform": null
  },
  {
    "test_name": "tsagchkccstatsfromfcdump.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagchkccstatsfromfcdump.tsc - check cc stats from fc dump logs",
    "platform": null
  },
  {
    "test_name": "tsagchkdbasm.tsc",
    "setup": null,
    "flags": {
      "temp_asminst": "asm_instance^i^"
    },
    "description": "tsagchkdbasm.tsc - Checks the DB and ASM instances for test tsagbug19148107\n\nChecks the DB and ASM instances for test tsagbug19148107",
    "platform": null
  },
  {
    "test_name": "tsagchkdskoncell.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagchkdskoncell.tsc - Check all disk status on the cell\n\nChecks disk status on the allocated cell\n     Creates a dif, if any disk is in failed state and aborts the lrg",
    "platform": null
  },
  {
    "test_name": "tsagchkdskserial.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagchkdskserial.tsc - Test for Dated serial number attribute for physical disks\n\nThis test performs a sanity check on Dated serial number (DSN) attribute for physical disks.\n     All DSN should have length of 10 characters and should be unique.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagchkmeta.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "creatdev_file": "tsagaudef",
      "disk": "datafile0",
      "asm_disk": "datafil2_0000"
    },
    "description": "tsagchkmeta.tsc - test case 1/2 for bug17610525\n\nCheck for celldisk metadata mismatch",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagchkmeta2.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "creatdev_file": "tsagaudef",
      "disk": "datafile0",
      "asm_disk": "datafil2_0000"
    },
    "description": "tsagchkmeta2.tsc - test case 3/4 for bug17610525\n\nCheck for celldisk metadata mismatch",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagchkmultims.tsc",
    "setup": null,
    "flags": {
      "LRG_NAME": "not_set",
      "disable_multims": "true",
      "oss_multims_testing": "true",
      "oss_multi_ms": "true",
      "instcell": "^k^"
    },
    "description": "tsagchkmultims.tsc - File to check if multi MS needs to be disabled\n\nChecks if LRG_NAME is present in multims_notallowedlist_exc.dat\n     and keeps multi_ms disabled for such lrgs",
    "platform": null
  },
  {
    "test_name": "tsagciini.tsc",
    "setup": null,
    "flags": {
      "hidestr": "'##'"
    },
    "description": "tsagciini.tsc - init setup for compress indexes\n\nsmartscan test for compress indexes, support parameter\n      coltype integer/'varchar2(50)'/date\n      partition true/false (partition table)\n      unique true/false\n      indexcol 1/2\n      funtion true/false\n      desc true/false\n      reverse true/false\n      partindex local/globalpart/global (partition index)\n      compress prefix/advance\n\ncreate table, compress index and insert data",
    "platform": null
  },
  {
    "test_name": "tsagciofl.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagciofl.tsc - basic compress index smart scan test\n\ncreate compressed index and run smart scan query against it\n\nuse index_ffs hint to force smart scan use compressed index\n     check 'cell blocks processed by index layer' to ensure smart scan happen on compressed index\n     output execution plan only as reference, not check condition\n     bug 21200919 to track missing STORAGE word in execution plan\n     wait for update about this from dev team",
    "platform": null
  },
  {
    "test_name": "tsagciofl2.tsc",
    "setup": null,
    "flags": {
      "wkldsec": "60"
    },
    "description": "tsagciofl2.tsc - kill/restart cellsrv/celloflsrv, and query compress index\n\nsince cellsrv/celloflsrv will be restart, smart scan still will be used\n     cell blocks processed by index layer still will increase",
    "platform": null
  },
  {
    "test_name": "tsagciofl7.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagciofl7.tsc - simulation event test\n\n1) smart scan query against compressed indexes\n     2) issue simulation event, query again will crash db\n     3) restore db, query again should use smart scan against compressed indexes",
    "platform": null
  },
  {
    "test_name": "tsagckleak.tsc",
    "setup": null,
    "flags": {
      "temp_nowarn": "^tst_nowarn^",
      "USER_TXN_NAME": "na",
      "old_instance": "^instance^",
      "tmpi": "asm_instance^oi^",
      "run_res_leak_chk": "1",
      "ms_status": "'running'",
      "ms_up": "true",
      "cellsrv_status": "'running'",
      "cellsrv_up": "true",
      "ckleaklog": "tsagckleak^ckleak^.log",
      "ckleakdif": "tsagckleak^ckleak^.dif",
      "ckleakcmd": "'alter cell events = \"immediate cellsrv.cellsrv_analyze(ResourceLeak)\"'",
      "stoprscmd": "'alter cell shutdown services rs'",
      "startrscmd": "'alter cell startup services rs'"
    },
    "description": "tsagckleak.tsc - Check for resource leak\n\nInvoke arbiter to check for resource leak and tell cellsrv to\n     dump state if there is a leak.",
    "platform": null
  },
  {
    "test_name": "tsagclall.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagclall.tsc - Tests certain CellCLI commands\n\nTests Certain CellCLI commands.",
    "platform": null
  },
  {
    "test_name": "tsagcldsk.tsc",
    "setup": null,
    "flags": {
      "cell_name": "_second  ## cell name is set to 'second'",
      "raw_path2": "'_'",
      "less_size": "^size_abcd^",
      "more_size": "^size_abcd^",
      "tst_nowarn_tmp": "^tst_nowarn^",
      "initCdskSz": "160   # initial celldisk size is 160M",
      "limit": "2560       # maximum (lun) size is 2560M/2.5G",
      "interval": "400     # increment celldisks/griddisks by 400M at a time",
      "currCdskSz": "^initCdskSz^",
      "currGdskSz": "^size_gdisk^"
    },
    "description": "tsagcldsk.tsc - test for cellcli commands on celldisk\n\nIt tests the various operations that can be performed on CellDisk by SAGE command line interface.",
    "platform": null
  },
  {
    "test_name": "tsagcldsk_hw.tsc",
    "setup": null,
    "flags": {
      "less_size": "^size_abcd^",
      "more_size": "^size_abcd^"
    },
    "description": "tsagcldsk.tsc -\n\nIt tests the various operations that can be performed on CellDisk by SAGE command line interface.",
    "platform": null
  },
  {
    "test_name": "tsagcldsk_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcldsk_nls.tsc - celldisk command of CellCLI\n\nCelldisk commands of various objects on CellCLI\n\n<none>",
    "platform": null
  },
  {
    "test_name": "tsagclean.tsc",
    "setup": null,
    "flags": {
      "tmp_nowarn": "^tst_nowarn^",
      "maxinstances": "^max_instance^",
      "dbgprefix": "'tsagclean::debug::'",
      "oss_no_asmdb": "1",
      "oss_exascale_testing": "0",
      "TKFV_BUILD_EDVMOD": "1",
      "log1": "tsagexastack_rmpwfile^RNDM^.lst",
      "tst_var_mode_backup": "^tst_var_mode^",
      "tst_var_mode": "^tst_var_mode_backup^",
      "PCW_NO_BLOWOUT": "1",
      "cli_log": "cli_log"
    },
    "description": "tsagclean.tsc - clean up between subtests in a LRG\n\nShutdown DB instance, ASM instance, Exadata services",
    "platform": null
  },
  {
    "test_name": "tsagcleancommon.tsc",
    "setup": null,
    "flags": {
      "shutdowncrs": "true"
    },
    "description": "tsagcleancommon.tsc - common cleanup script for remote and local\n\nIf its called locally, it just calls tsagclean, if called in the\n     interop context it calls tsagcleanremote. This way user does not\n     have to bother about checking for interop etc\n\nMentioned above",
    "platform": null
  },
  {
    "test_name": "tsagcleanremote.tsc",
    "setup": null,
    "flags": {
      "dbgprfx": "'tsagcleanremote:debugg::'",
      "logfilel": "tsagcleanremote.log",
      "cleanfile": "tsagcleantmp.tsc",
      "ORACLE_HOME": "^ORACLE_HOME_INTEROP^"
    },
    "description": "tsagcleanremote.tsc - cleanup remote database node\n\nAdd any sort of cleanup that may be required on the database node. Don't\n     worry about ORA 600 as its remote view.",
    "platform": null
  },
  {
    "test_name": "tsagcleanupuser.tsc",
    "setup": null,
    "flags": {
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagcleanupuser.tsc - Script to cleanup setup done by tsagexcsetupuser.tsc\n\nScript to setup 4 user with different privileges",
    "platform": null
  },
  {
    "test_name": "tsagclearhang.tsc",
    "setup": null,
    "flags": {
      "greprtn": "^tst_exe_status^"
    },
    "description": "tsagclearhang.tsc - restart cellsrv to clear hangio",
    "platform": null
  },
  {
    "test_name": "tsagcli1.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagcli1.tsc - Basic test of CellCLI/MS features\n\nThis test compares cellcli output from good and bad commands.\n     MS must be running on the system and listeneing at the standard\n     port.\n\nWe run cellcli with -n option to inhibit output of prompts\n     and the command line enditor.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagclients.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagclients.tsc - create clients to set up ACL\n\nAdd clients asm and db1-4.\n     We may not need db2-4 but it doesn't hurt to add them anyway.\n\nThis does not work for tests where db_unique_name is different from\n     db_name. Those tests will have to alter the id attribute of the\n     appropriate db client to be the same as its db_unique_name.",
    "platform": null
  },
  {
    "test_name": "tsagclncell.tsc",
    "setup": null,
    "flags": {
      "tmp_nowarn": "^tst_nowarn^",
      "cell_log_home": "/opt/oracle/cell/cellsrv/deploy/log/",
      "dbserver_log_home": "/opt/oracle/dbserver/log/deploy/",
      "dbserver_trace": "/opt/oracle/dbserver/log/diag/",
      "cellalertdir": "^ADR_BASE^/diag/asm/cell",
      "msg1": "'Detected thsld poor perf on disk'",
      "msg2": "'Detected relative poor perf on disk'",
      "msg3": "'ASM DROP slow'",
      "msg4": "'NO IO COMPLETION ON DISK'"
    },
    "description": "tsagclncell.tsc - Cleans up the Sage Cell\n\nuninstall Sage from the cell and delete all folders created during the test",
    "platform": null
  },
  {
    "test_name": "tsagclniormpmem.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagclniormpmem.tsc - Clean testing IORM resources\n\nClean testing intra_dbplan, inter_dbplan",
    "platform": null
  },
  {
    "test_name": "tsagclniormqm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagclniormqm.tsc - Clean testing IORM resources\n\nClean testing intra_dbplan, inter_dbplan and category plan",
    "platform": null
  },
  {
    "test_name": "tsagclnmcell.tsc",
    "setup": null,
    "flags": {
      "oss_testing": "1"
    },
    "description": "tsagclnmcell.tsc - clean up for multiple cell views\n\nShutdown DB instance, ASM instance, Exadata services in multiple cell views\n\nShutdown DB instance, ASM instance, Exadata services in multiple cell views",
    "platform": null
  },
  {
    "test_name": "tsagclnmtciormqm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagclnmtciormqm.tsc - Clean testing IORM resources\n\nClean testing intra_dbplan, inter_dbplan and category plan",
    "platform": null
  },
  {
    "test_name": "tsagclsaonswalletconf.tsc",
    "setup": null,
    "flags": {
      "lgfile": "^tst_tscname^.log",
      "nowarn": "^TST_NOWARN^",
      "ons_wallet_dir": "^ORACLE_HOME^/crsdata/^HAS_HOSTNAME_0^/onswallet",
      "crsconfig_params": "^T_HAS_WORK_GLOBAL^/crsconfig_params",
      "s_crsconfig_defs": "^T_HAS_WORK_GLOBAL^/s_crsconfig_defs",
      "node_nums": "0"
    },
    "description": "tsagclsaonswalletconf.tsc - Temp file for HAS Wrapper for tclsaonswalletconf.pl\n\nGenerates an ONS wallet configuration using create_ons_wallet()\n     rootscript call.\n\nPARAMETER DESCRITPION:\n\n           ons_wallet_dir  : Location for the ONS wallet. Will be written to\n                             crsconfig_params as value for the ONSWALLET\n                             parameter.\n                         (*) If no parameter is passed, it will default to:\n                             ORACLE_HOME/crsdata/HAS_HOSTNAME_0/onswallet\n           crsconfig_params: Absolute path of the crsconfig_params file\n                         (*) If no parameter is passed, it will default to:",
    "platform": null
  },
  {
    "test_name": "tsagclstat.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagr3def",
      "tstlog": "tsagclstat.log",
      "tstlog2": "tsagclstat2.log",
      "tstlog3": "tsagclstat3.log",
      "tstlog6": "tsagclstat6.log",
      "tstlog4": "tsagclstat4.log"
    },
    "description": "tsagclstat.tsc - cellsrvstat tests\n\ncellsrvstat tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagclusipurge.tsc",
    "setup": null,
    "flags": {
      "CLU_OSSCONF": "^cell_twork^^s^^OSS_DEVDIR1^_conf",
      "clu_name": "^CSS_CLUSTERNAME^"
    },
    "description": "tsagclusipurge.tsc - Storage Index CellCLI Purge by Database Object tests\n\nStorage Index CellCLI Purge by Database Object tests, it includes:\n\n       Test 1 : SI purge for object-level: one database's single table\n       Test 2 : SI purge for PDB-level: one database's all tables\n       Test 3 : SI purge for CDB-level: all databases under the same CDB\n       Test 4 : SI purge for cluster-level: all databases under the same cluster\n       Test 5 : Incorrect input tests\n       Test 5.1 : Incorrect objid\n       Test 5.2 : Incorrect pdb name\n       Test 5.3 : Incorrect cdb name\n       Test 5.4 : Incorrect cluster name\n\nStorage Index CellCLI Purge by Database Object tests\n     Spec : https://confluence.oraclecorp.com/confluence/display/~guanwen.wang@oracle.com/Storage+Index+CellCLI+Purge+by+Database+Object",
    "platform": null
  },
  {
    "test_name": "tsagclusterplansecurity.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagclusterplansecurity.tsc - Security tests for IORM cluster plan\n\nTestA:\n       Grant alter iormplan privilege to a user, this user can change the cluster plan.\n       Run list iormplan detail without list  iormplan privilege\n       Grant list  iormplan privilege\n       Rerun step 2\n\n     TestB:\n       Revoke  alter iormplan privilege, and run alter command, should fail.\n       Run list iormplan detail\n       Revoke  list  iormplan privilege\n       Rerun step 2\n\n     TestC:\n       Grant alter iormplan privilege to a user, run command with a xml tag:",
    "platform": null
  },
  {
    "test_name": "tsagcncrrntssn.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "cell1_f": "^cell1^.us.oracle.com",
      "cellconnstr": "root@^cell1_f^"
    },
    "description": "tsagcncrrntssn.tsc - Test to check concurrent cellcli active sessions\n\nTest to check if we run 100+ concurrent cellcli active sessions then\n    1. cell must not hang\n    2. Show cellcli sessions exceeds error\n\n     It tests:\n    1. 75 concurrent active sessions - No session exceed error\n    2. 125 concurrent active sessions - Session exceed error",
    "platform": null
  },
  {
    "test_name": "tsagcncrrntssn_db.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "compute_node_f": "^compute_node^.us.oracle.com",
      "dbnodeconnstr": "root@^compute_node_f^"
    },
    "description": "tsagcncrrntssn_db.tsc - Test to check concurrent dbmcli active sessions\n\nTest to check if we run 100+ concurrent dbmcli active sessions then\n    1. DBnode must not hang\n    2. Show dbmcli sessions exceeds error\n\n     It tests:\n    1. 75 concurrent active sessions - No session exceed error\n    2. 125 concurrent active sessions - Session exceed error",
    "platform": null
  },
  {
    "test_name": "tsagcofgstsh.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagcofgstsh.tsc - Celloflsrv group startup/shutdown cellcli tests\n\nBasic tests for startup/shutdown offloadgroup",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcolumnarcachexrcc.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagcolumnarcachexrcc.tsc - AWR Support for Columnar Cache on XRMemCache\n\njiazliu_xrcc_awr txn description :\n       OSS change of adding AWR Support for XRCC.\n       AWR picks up data from v$cell_* views (in sage/kxdcm.*).\n       The stats for fix table are defined in ossmetrics.h/ossmetrics.c,\n       and populated in FxtabMetrics::statCollectGlobalDisk\n       For details, please refer to this doc:\n       https://confluence.oraclecorp.com/confluence/display/EiXD/AWR+Support+for+Columnar+Cache+on+XRMemCache#AWRSupportforColumnarCacheonXRMemCache-Reads\n     Test txn description :\n       This txn wraps the check_cc_awr.sql in lrg environment and compares Assertion values",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcommondb_fail.tsc",
    "setup": null,
    "flags": {
      "eds_encryption_mode": "off",
      "egs_cluster_secmode": "permissive",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true",
      "sage_mirror_mode": "high",
      "vault_db": "DATA",
      "cdb": "true"
    },
    "description": "tsagcommondb_fail.tsc - common file to reproduce failyuer cases issues\n\ntest following failure cases with active db workload",
    "platform": null
  },
  {
    "test_name": "tsagcommonscript.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcommonscript.tsc - common script for corruption test (sparsedb)",
    "platform": null
  },
  {
    "test_name": "tsagconfgcell.tsc",
    "setup": null,
    "flags": {
      "tmp_nowarn": "^tst_nowarn^",
      "processes": "250",
      "CELL_ROOT_PASSWD": "welcome1",
      "rpmoncell": "^TST_EXE_RESULT^",
      "rpminlabel": "^TST_EXE_RESULT^"
    },
    "description": "tsagconfgcell.tsc - Configures the SAGE Cell\n\nInstalls the rpm, copies necessary scripts, creates the Cell.",
    "platform": null
  },
  {
    "test_name": "tsagconfigcelltest.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagsetcelltest.tsc - Configure the env for local celltest run\n\nThis script configures local setup to be able to run\n       celltest utility to reproduce bugs. This setup is currently\n       not used in any LRG",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagconfmnode.tsc",
    "setup": null,
    "flags": {
      "nowarn": "^TST_NOWARN^",
      "MY_SKGXP_DYNAMIC_PROTOCOL": "2"
    },
    "description": "tsagconfmnode.tsc - configure cellinit.ora on remote node",
    "platform": null
  },
  {
    "test_name": "tsagconfteststart.tsc",
    "setup": null,
    "flags": {
      "status": "confineInactive",
      "confteststarted": "1"
    },
    "description": "tsagconfteststart.tsc - wait till confine tests start\n\nWait till confine tests start on a celldisk. Max wait time is 10 minutes.\n     It takes the celldisk as an argument (cdisk=^disk^)",
    "platform": null
  },
  {
    "test_name": "tsagcookiejar.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "testing cookiejar feature of exacli",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcorrupt1.tsc",
    "setup": null,
    "flags": {
      "test": "1",
      "test_p_1_cellsrv_num": "1",
      "test_p_2_cellsrv_num": "2"
    },
    "description": "tsagspcorrupt1.tsc - Corruption Test #1\n\nNormal redundancy base and normal redundancy Snapshot\n     child mirror 1 is sparse and corruption in parent mirror 1\n     c1g0, p1bd",
    "platform": null
  },
  {
    "test_name": "tsagcorrupt2.tsc",
    "setup": null,
    "flags": {
      "test": "2",
      "test_c_1_cellsrv_num": "1",
      "test_c_2_cellsrv_num": "2"
    },
    "description": "tsagspcorrupt2.tsc - Corruption Test #2\n\nBase and Snapshot - Normal redundancy\n     Child mirror 1 is corrupted\n     c1bd, c2gd",
    "platform": null
  },
  {
    "test_name": "tsagcorrupt3.tsc",
    "setup": null,
    "flags": {
      "test": "3",
      "test_c_1_cellsrv_num": "1",
      "test_c_2_cellsrv_num": "2"
    },
    "description": "tsagspcorrupt3.tsc - Corruption Test #3\n\nBase and snapshot - Normal redundancy\n      Child mirror 1 is sparse and corrupted, child mirror 2 is sparse\n     c1bd, c2g0",
    "platform": null
  },
  {
    "test_name": "tsagcorrupt4.tsc",
    "setup": null,
    "flags": {
      "test": "4",
      "test_c_1_cellsrv_num": "1",
      "test_c_2_cellsrv_num": "2",
      "test_read_comment": "tsagspcorrupt4_read_02",
      "tmp_nowarn": "^tst_nowarn^",
      "test_p_2_cellsrv_num": "1",
      "test_p_1_cellsrv_num": "2"
    },
    "description": "tsagspcorrupt4.tsc - Corruption Test #4\n\nBase and Snapshot - Normal redundancy\n     c1bd, c2g0, p1gd, p2bd",
    "platform": null
  },
  {
    "test_name": "tsagcov1.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagcov1.tsc - Coverage Test 1\n\nFirst Coverage test, including system dump, invalid startup, etc\n\nThe tests here are mostly to get code coverage, not really test\n     the behaviour",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcov2.tsc",
    "setup": "tsaginit",
    "flags": {
      "compatible": "^def_compatibility^",
      "auto_undo_management": "true",
      "asm_ausize": "65536"
    },
    "description": "tsagcov2.tsc - Test for code coverage for system dump state\n\nTest dumps system state dump for direct IO's and  for predicate\n\n- Direct IO, predicate tests are forked\n     - through cellcli event is set for system state dump\n     - Verification for now is manual - need to comeup with a way to\n       verify through test later",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagcpurm1.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "extent_mgmt": "local"
    },
    "description": "tsagcpurm1.tsc - SAGE Cell CPU Resource Manager tests\n\nCPU Resource Manager tests",
    "platform": null
  },
  {
    "test_name": "tsagcpurm2.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "extent_mgmt": "local"
    },
    "description": "tsagcpurm2.tsc - SAGE Cell CPU Resource Manager tests\n\nCell CPU Resource Manager tests",
    "platform": null
  },
  {
    "test_name": "tsagcpurmwl.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcpurmwl.tsc - Test SaGe CPU Resource Manager Workload Generator\n\nThis is a non-standalone test for generating worklods to verify CPU\n     Resource Manager mode transitions.\n     It is used in tsagcpurm1.tsc.\n\nSupported workload types: PredicateFilter and IMCPopulation",
    "platform": null
  },
  {
    "test_name": "tsagcputargettwatts.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcputargettwatts.tsc - Test for cpu target watts feature\n\nIn this test, we check the reset persistency of the cpu watts by changing\n     the power consumption of socket 0 manually and then restarting the ms services.\n     After the services restart, the power cpu watts should go back to 400 (default)\n\nAdded in lrgrhx11sapwrsave",
    "platform": null
  },
  {
    "test_name": "tsagcrashbkp1.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagcrashbkp1.tsc - Crash & Recovery test for empty backup\n\nAdditional: Blockstore Crash & Recovery test for empty backup, including incremental backups.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagcrashbkp2.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true",
      "fail_type": "bsw"
    },
    "description": "tsagcrashbkp2.tsc - Randomised bsw failure test for volume backup\n\nCreate a volume and its attachment. Add data to it\n     incrementally and root snapshot, intermediate snapshot and\n     leaf snapshot is created.\n\n     Service: BSM / BSW\n\n     Service is crashed and restarted while following operations happen:\n      1) While creating root bkp, intermediate bkp and leaf bkp\n      2) While restoring root bkp, intermediate bkp and leaf bkp\n      3) While deleting root bkp, intermediate bkp and leaf bkp\n\n\n     These tests are run in a loop for 5 times.\n     Test is looped for the crashes to hit as many random points\n     in volume bkp operation path.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagcrashbkpscripts.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcrashbkpscripts.tsc - Common scripts used by crash & recovery backup tests",
    "platform": null
  },
  {
    "test_name": "tsagcrashdetect.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "cell": "^cell_node^"
    },
    "description": "tsagcrashdetect.tsc - Test for Crash Detector\n\nTest for Exadata Crash Detector\n    Simulates Disk controller crash and tests recovery\n    and then simulation of disk controller replacement.",
    "platform": null
  },
  {
    "test_name": "tsagcreatcell.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcreatcell.tsc - Create cell unit test",
    "platform": null
  },
  {
    "test_name": "tsagcreatesnap.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "redund": "external"
    },
    "description": "tsagcreatesnap.tsc - Test for sparsedb\n\nTry to create snapshot from a snapshot",
    "platform": null
  },
  {
    "test_name": "tsagcrflashtbl.tsc",
    "setup": null,
    "flags": {
      "disklist": "'FLASH*'"
    },
    "description": "tsagcrflashtbl.tsc - create table on diskgroup composed of\n                          flash disks only",
    "platform": null
  },
  {
    "test_name": "tsagcrossclusterbkp_part1.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagcrossclusterbkp_part1.tsc - Functional test for cross cluster backup volume restore\n\nA backup created in one view can be restored in another view.\n     Test steps:\n     1) Create volume\n     2) Add data\n     3) Create volume backup and collect its globalId\n     4) Cleanup the view\n     Test will be continued in tsagcrossclusterbkp_part2.tsc",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagcrossclusterbkp_part2.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagcrossclusterbkp_part2.tsc - Functional test for cross cluster backup volume restore\n\nContinued from tsagcrossclusterbkp_part1.tsc\n     1) Setup xblockini\n     2) Restore backup using the globalId from tsagcrossclusterbkp_part1.tsc\n     3) Compare the md5sum of volume and its restored volume.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagcrossclusterbkprestore_pt1.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagcrossclusterbkprestore_pt1.tsc - cross-cluster backup restore test\n\nFor a cross-cluster backup restore test, we need a backup from other cluster.\n     So, we will have a pre created backup in other cluster and then perform test\n     steps. In this test we will create a backup save its global backup id and then\n    clean up the view. In tsagcrossclusterbkprestore_pt2.tsc, will restore that backup\n\nTest steps are as follows:\n     1. Create a volume and take its backup\n     2. Store global backup id in a variable to pass that to test part 2\n     6. Cleanup the view",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagcrossclusterbkprestore_pt2.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagcrossclusterbkprestore_pt2.tsc - cross-cluster backup restore test\n\nFor a cross-cluster backup restore test, we need a backup from other cluster.\n     This test will restore backup created by tsagcrossclusterbkprestore_pt1.tsc\n\nTest steps are as follows:\n     1. Set a crash simevent on BsmVolCreateJob",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagcrspandgtbl.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcrspandgtbl.tsc - create table with partitions on flash\n                           griddisks and non-flash griddisks",
    "platform": null
  },
  {
    "test_name": "tsagcrtexttab.tsc",
    "setup": null,
    "flags": {
      "cur_user": "'SYS'",
      "external_type": "hdfs",
      "db_con": "^cur_user^'/'^cur_user^",
      "exp_data_file": "'exp_'^tab^'_'^external_type^'.csv'",
      "sav_data_file": "^tab^'_'^external_type^'_export_data.sav'",
      "log_file": "tsagcrtexttab^ext_prefix^_^tab^_^external_type^.log"
    },
    "description": "tsagcrtexttab.tsc - Covert a internal table to external table on Exadoop\n\nBelow is how to use:\n       runtest tsagcrtexttab tab=tsagstrindx1 [cur_user=test,] [cur_pwd=test,] \\\n               [external_type=hive|hdfs] [order_col=col_name]\n         1. cur_user: if do not set, the default value is SYS, don't require cur_pwd\n         2. tab     : required, the existing normal table, after converting process, the\n                      normal table name is: $tab_normal, external table is $tab\n         3. cur_pwd : if do not set, the default value is the same as cur_user\n         4. external_type: the default value is hdfs, it means oracle_hdfs\n         5. order_col: the column name that you want to use it as a order name",
    "platform": null
  },
  {
    "test_name": "tsagcrtpooldsk.tsc",
    "setup": null,
    "flags": {
      "oss_failgroup": "failalldbdg",
      "allstandby": "true",
      "flashsize": "512M",
      "creatdev_file": "^devfilei^",
      "CELL_WITH_QLC_DISK": "1",
      "NUM_QLC_PER_CELL": "8",
      "QLC_DISK_SIZE_MB": "2560",
      "flash2add": "^num_cell_servers^",
      "tmp_port": "^free_port_number^",
      "tag": "'cellsrv[2-'^num_cell_servers^'] tsagfgini'",
      "step": "1",
      "devfilei": "^creatdev_file^1.log"
    },
    "description": "tsagcrtpooldsk.tsc - Creates Pool Disks\n\nHelper Script to create underlying pooldisks\n     if no_gd_with_prefix option is set, it will invoke creatdsk\n       to create individual GDs as pooldisks\n     By default, it will create GDs using prefix option.",
    "platform": null
  },
  {
    "test_name": "tsagcryptokgcetest.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcryptokgcetest.tsc - Try installing rpm\n\nIt installs the rpm(crypto_kgce_test-0.1-1.x86_64.rpm) on rh cell\n\nIt installs the rpm on rh cell",
    "platform": null
  },
  {
    "test_name": "tsagcryptoscript.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcryptoscript.tsc - Common Scripts for Crypto Secure Erase\n\nThis file contains common scripts for crypto secure erase feature",
    "platform": null
  },
  {
    "test_name": "tsagcserase_x3.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagcserase_x3.tsc -Negative Test for crypto-secure erase on x3\n\nThis  is a negative test to check for crypto-secure erase feature on x3 using 1pass. Disk erase should happen normally as crypto-secure is only for x5,x6,x7 machines",
    "platform": null
  },
  {
    "test_name": "tsagcserase_x5.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagcserase_x5.tsc - Test for crypto-secure erase on x5\n\nThis test is to check for crypto-secure erase and non-crypto secure erase feature on x5 using 7pass",
    "platform": null
  },
  {
    "test_name": "tsagcserase_x5_1.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagcserase_x5_1.tsc -Negative Test for crypto-secure erase on x5\n\nThis  is a negative test to check for crypto-secure erase feature on x3 using 1pass. Disk erase should happen normally as crypto-secure is only for x5,x6,x7 machines",
    "platform": null
  },
  {
    "test_name": "tsagcsuit.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcsuit.tsc - SAGE Code Coverage tests driver\n\nDriver for SAGE Code Coverage test.",
    "platform": null
  },
  {
    "test_name": "tsagctasmscseckey.tsc",
    "setup": null,
    "flags": {
      "clusterid": "ASMClusterName"
    },
    "description": "tsagctasmscseckey.tsc - Exadata create asm scope security\n\nWill shutdown stack (clusterware, ASM, RDBMS), create key and assign to Griddisks\n     and then start up stack including clusterware\n\nThis setup is cloned from srdbmsini. Any changes here should also be made in srdbmsini",
    "platform": null
  },
  {
    "test_name": "tsagcurlconcurrency.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagcurlconcurrency.tsc - Curl concurrency test\n\nThis test creates 10 users and lets them create 10 volumes in separate vaults in parrallel",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagcurlconcurrency2.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagcurlconcurrency2.tsc - This test runs multiple operations on\n     different volumes in the same vault by multiple users simultaneously.\n\nThis test creates 5 users and those users perform concurrent blockstore\n     operations in a single vault.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagdailyalert.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagdailyalert.tsc - Test to check daily alert email\n\nThis tests the daily alert email sent by cell for all open alerts",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdal.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagldef",
      "tk_parsearg_force_compatible": "^max_compatibility^"
    },
    "description": "tsagdal.tsc - Test Cellcli Drop&Alter&List commands in a loop\n\nIt will run Cellcli Drop&Alter&List commands in a loop with workload",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdalini.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdalini.tsc - Init testing env for drop&alter&list cases\n         It will setup Exadata with 3 cells and create high DG\n\nIt will setup Exadata with 3 cells and more than 40 griddisk in\n        each cell,the create the DG with high redundancy.",
    "platform": null
  },
  {
    "test_name": "tsagdalrun.tsc",
    "setup": null,
    "flags": {
      "myFailAct": "'run t_work:run_drop_alter_list'"
    },
    "description": "tsagdalrun.tsc - Run the MS & Cellsrv Loop Test\n\nStart a workload and stop it till all Loop test finished, then\n        run Loop test in each cell.",
    "platform": null
  },
  {
    "test_name": "tsagdatafilesnap.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdatafilesnap.tsc - Test for sparsedb\n\nTry to create snapshot from DATAFILE DG.",
    "platform": null
  },
  {
    "test_name": "tsagdateformat_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdateformat_nls.tsc -escribe DateFormat commanof CellCLI",
    "platform": null
  },
  {
    "test_name": "tsagdaxorionrdma.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true"
    },
    "description": "tsagdaxorionrdma.tsc - test to be added in lrgrhx7sadaxorion\n\norion test running PMEMCache RDMA reads",
    "platform": null
  },
  {
    "test_name": "tsagdaxresilver.tsc",
    "setup": "srdbmsini",
    "flags": {
      "disable_multims": "true",
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef",
      "oss_auto_manage_disks": "true",
      "cdb": "true"
    },
    "description": "tsagdaxresilver.tsc - resilver test with cellsrv restart\n\nPls see below:\n\nto be added in lrgdbconsarslv2_part2",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdbalert.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagcaudef",
      "nflint": "0",
      "asm_ausize": "1048576",
      "ENABLE_CELLSRV_DUMP": "1",
      "_swrf_mmon_metrics": "false",
      "_swrf_mmon_flush": "false",
      "disk": "datafile0"
    },
    "description": "tsagdbalert.tsc - add case for bug21210708 needs\n\ncovers the following sections of Exadata AWR\n     1. open alerts\n     2. offline disks\n     3. asm disk groups",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdbasrtrap.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagdbasrtrap.tsc - ASR trap test\n\nSNMP set up to catch ASR trap on DB node",
    "platform": null
  },
  {
    "test_name": "tsagdbciorm.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "cell_with_pmem_cache": "true      ## create pmem cache",
      "job_queue_processes": "0",
      "file_dest": "'+DATAFILE'",
      "path_pdb": "'DATAFILE'",
      "log_file": "tsagbug25695941.log"
    },
    "description": "tsagdbciorm.tsc - CDB IORM test\n\nCDB/ PDB IORM tests",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagdbcorecap.tsc",
    "setup": null,
    "flags": {
      "dbnode": "^compute_node^",
      "dbnodeconnstr": "^dbconnstr^"
    },
    "description": "tsagdbcorecap.tsc - Tests to cover exadata dbms CoreCapping\n\nTests have been written for Bug 18622577.",
    "platform": null
  },
  {
    "test_name": "tsagdbcrt.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbcrt.tsc - Test DB creation from scratch w/ Exadata",
    "platform": null
  },
  {
    "test_name": "tsagdbcsaexcmiscfail2_3cell.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "mixed_workload": "true",
      "maxpdb": "2",
      "dsk_size": "large"
    },
    "description": "tsagdbcsaexcmiscfail2_3cell.tsc - Additional tests from lrg: lrgdbcsaexacldresilver_db\n\nThis test performs following operations:\n     1. Recovery test for blockzero primary copy\n     2. Recovery test for blockzero secondary copy\n     Test for dongqma_ves_bootstrap_mdcorrupt_alert\n       3. Test case 1 - VES disk blockzero corruption with disk drop\n       4. Test case 2 - VES disk blockzero corruption with disk recovery\n     5. Test for bug 36837467\n     6. Test for bug 37145763\n     7. Test for bug 37062463\n\nTest for lrg: lrgmiscfail2_3cell",
    "platform": null
  },
  {
    "test_name": "tsagdbfileparallelread.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbfileparallelread.tsc - db file parallel read\n\nBug 34157387 - TRACKING BUG: TEST CASE FOR CELL LIST OF BLOCKS PHYSICAL READ\n\ndb file parallel read",
    "platform": null
  },
  {
    "test_name": "tsagdbhealthchk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbhealthchk.tsc - Macro with health checks at beginning of LRG\n\nMacro with health checks at beginning of LRG\n\nMacro with health checks at beginning of LRG",
    "platform": null
  },
  {
    "test_name": "tsagdbhealthchk_tst.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbhealthchk_tst.tsc - Macro with health checks between the tests\n\nMacro with health checks between the tests\n\nMacro with health checks between the tests",
    "platform": null
  },
  {
    "test_name": "tsagdbhealthfailseq.tsc",
    "setup": null,
    "flags": {
      "sshstring": "'_ssh-issue'",
      "pingstring": "'_ping-issue'",
      "spacestring": "'_space-issue'",
      "oldimgstring": "'_image-old'",
      "qstring": "'NodeUnhealthy_:'",
      "mode": "quarantine"
    },
    "description": "tsagdbhealthfailseq.tsc - Macro to handle dbnode health failure on dbnode\n\nMacro to handle node health failure on dbnode\n\nMail alert, return and quarantine the dbnode",
    "platform": null
  },
  {
    "test_name": "tsagdbidfccontent.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "scompatible.asm": "19.0.0.0",
      "scompatible.rdbms": "19.0.0.0",
      "flash_size": "1024",
      "uniq_dsknames": "FLASH",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagdbidfccontent.tsc - Test for bug 33947516\n\nTest steps:\n     1. Bring up tsagobjcachini\n     2. Create a new griddisk Flint and a new diskgroup fred using these GD\n     3. Disable caching for all the sysfiles and enable only for flint GD\n     4. Create tablespace on fred DG and a table in it.\n     5. Record the cachedSize for (dbID!=0 && objectNumber=<table's objectID>)\n        and (dbID=0 && objectId=0)\n     6. Perform inserts on the table\n     7. Record the cachedSize again\n     8. Expected behaviour: CachedSize delta for table's objectID should\n        be a positive value. with bug 33947516 fix, delta cachedSize for dbid=0\n        should be 0. Prior to that bug fix, DB incorrectly passed dbid=0\n        for FlashCache population ioctls",
    "platform": null
  },
  {
    "test_name": "tsagdbincidentmining.tsc",
    "setup": "srdbmsini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true",
      "SAGE_MIRROR_MODE": "normal"
    },
    "description": "tsagdbincidentmining.tsc - Test for RDBMS/GI Incident Mining.\n\nTest for RDBMS/GI Incident Mining. EHN - 31435511",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdblive.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "asm_power_limit": "1024",
      "_swrf_mmon_metrics": "false",
      "_swrf_mmon_flush": "false",
      "disk": "datafile0"
    },
    "description": "tsagdblive.tsc - test case for config sections on live data\n\nSlow alerts and offline disks are simulated in the test\n     To ensure that config sections of AWR are populated with no errors\n\nshould test html and xml and be no ORA- errors or 'No data exists' in config part",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdblsparse.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "compatible": "^scompatible.rdbms^",
      "redund": "external",
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagdblsparse.tsc - Snapshot using database link\n\nTests the creation of Exadata Snapshot from remote PDB using DB Links",
    "platform": null
  },
  {
    "test_name": "tsagdbms.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbms.tsc - Tests for DBSERVER object\n\ntests the various operations on DBSERVER done through DBMCLI\n\nbasic tests for List/alter/create commands",
    "platform": null
  },
  {
    "test_name": "tsagdbms_nls.tsc",
    "setup": null,
    "flags": {
      "snmp_port": "^free_port_number^",
      "slash": "/"
    },
    "description": "tsagdbms_nls.tsc - Tests for DBSERVER object\n\ntests the various operations on DBSERVER done through DBMCLI\n\nbasic tests for List/alter/create commands",
    "platform": null
  },
  {
    "test_name": "tsagdbms_space_test.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbms_space_test.tsc - Management Server Space Management Test on DBnode\n\nTest  MS Space Management actions",
    "platform": null
  },
  {
    "test_name": "tsagdbmsallpdfail.tsc",
    "setup": null,
    "flags": {
      "oss_auto_manage_disks": "true",
      "oss_devdir1": "^T_WORK^/raw"
    },
    "description": "tsagdbmsallpdfail.tsc - All PD failure test for DBMCLI",
    "platform": null
  },
  {
    "test_name": "tsagdbmsalrt.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbmsalrt.tsc - Test Alert generation on dbnode\n\nCreate and Alter threshold and verify alerts.",
    "platform": null
  },
  {
    "test_name": "tsagdbmsalrt_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbmsalrt_nls.tsc - NLS Test Alert generation on dbnode\n\nCreate and Alter threshold and verify alerts.",
    "platform": null
  },
  {
    "test_name": "tsagdbmsconfig.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "^creatdev_file^",
      "tmp_nowarn": "^tst_nowarn^",
      "searchstr": "'.us.oracle.com'",
      "mode": "normal"
    },
    "description": "tsagdbmsconfig.tsc - Setup file for running tests related to dbms\n\nThis test file setup the environment to run test related to dbms.\n\nSetup fake hardware environment to run dbms tests.\n     Setup real hardware environment to run test on dbnode.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdbmsdesc.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbmsdesc.tsc - describe and help command of DBMCLI\n\nDescribe and Help commands of various objects on DBMCLI\n\nTest output of various help and describe commands",
    "platform": null
  },
  {
    "test_name": "tsagdbmsdesc_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbmsdesc_nls.tsc -NLS describe and help command of DBMCLI\n\nDescribe and Help commands of various objects on DBMCLI\n\nTest output of various help and describe commands",
    "platform": null
  },
  {
    "test_name": "tsagdbmsmegacli.tsc",
    "setup": null,
    "flags": {
      "oss_auto_manage_disks": "true",
      "oss_devdir1": "^T_WORK^/raw",
      "creatdev_file": "tsagrddefdb"
    },
    "description": "tsagdbmsmegacli.tsc - Megacli test for dbserver on Exadata DB Node\n\nMegacli tests for validations on DB node using debugcli",
    "platform": null
  },
  {
    "test_name": "tsagdbmsmtrc.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbmsmtrc.tsc - METRICDEFN tests\n\nThis tests the list metricdefinition of dbmcli",
    "platform": null
  },
  {
    "test_name": "tsagdbmsmtrc_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbmsmtrc_nls.tsc -NLS METRICDEFN tests\n\nThis tests the list metricdefinition of dbmcli",
    "platform": null
  },
  {
    "test_name": "tsagdbmspd.tsc",
    "setup": null,
    "flags": {
      "disable_pdfail": "true"
    },
    "description": "tsagdbmspd.tsc - Test for physicaldisk and Lun failure scenario\n\nTest physicaldisk and lun by inducing failure. Verify Alerts.",
    "platform": null
  },
  {
    "test_name": "tsagdbmssnmpv3.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbmssnmpv3.tsc - Test for snmp v3 features on dbnode\n\nFake hardware Test for snmpUser , snmpSubscriber on dbmcli.",
    "platform": null
  },
  {
    "test_name": "tsagdbmssuit.tsc",
    "setup": null,
    "flags": {
      "section": "ALL"
    },
    "description": "tsagdbmssuit.tsc - Test suit for DBMCLI\n\nIt calls different tests for DBMCLI.",
    "platform": null
  },
  {
    "test_name": "tsagdbmstempalert.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagtempalert.tsc -  DBNode Temperature Alert Test\n\nThis test calls the tsagdbmstempalert.sh script to test the alert generation on dbnode if\n     the dbnode temperature crosses the threshold value of max and min limit\n\nThis test assumes the following:\n         a. Any latest exadata-dbmmgmt RPM is already installed on the machine",
    "platform": null
  },
  {
    "test_name": "tsagdbmsthrsh.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbmsthrsh.tsc - Thresholds on IORM Metrics.\n\nTests CREATE/ALTER/LIST/DROP THRESHOLD commands of dbmcli",
    "platform": null
  },
  {
    "test_name": "tsagdbmsthrsh_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbmsthrsh_nls.tsc -NLS Thresholds on IORM Metrics\n\nTests CREATE/ALTER/LIST/DROP THRESHOLD commands of dbmcli",
    "platform": null
  },
  {
    "test_name": "tsagdbnodereimage.tsc",
    "setup": null,
    "flags": {
      "SKIP_REIMAGE": "true",
      "FORCE_REIMAGE": "false",
      "dbnode_imagv": "\"\"",
      "view_imagv": "\"\"",
      "image_needed": "true"
    },
    "description": "tsagdbnodereimage.tsc - File with code for re-imaging the dbnode\n\nThis script reimages based on some pre-checks.\n     Checks are as follows:\n\n     The script first determines (1) Image version on dbnode\n                                 (2) Image version in view\n     Additionally, skip_reimage & force_reimage flags are imported.\n     If force_reimage is issued, re-imaging is done in all cases.\n     Else, if skip_reimage is issued, re-image is skipped.\n     If both skip_reimage and force_reimage are not issued,\n     re-imaging occurs if (1)!=(2). Else, it is skipped.\n\n     If re-imaging fails mid-way, the node is commented out\n     and added back to the pool.",
    "platform": null
  },
  {
    "test_name": "tsagdbnodereimgfailseq.tsc",
    "setup": null,
    "flags": {
      "mode": "quarantine",
      "qstring": "'Reimage_Failure'"
    },
    "description": "tsagdbnodereimgfailseq.tsc - Sequence for db reimage failure\n\nSequence to be followed in case of db reimage failure\n     Called from tsagdbnodereimage.tsc\n\nSends mail alert for db reimage failure and returns back\n     the cell to pool with comment",
    "platform": null
  },
  {
    "test_name": "tsagdbrs.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdbrs.tsc - basic testing for bdrs bdms bdsqlsrv bdsqloflsrv\n\nkill bdrs bdms bdsqlsrv bdsqloflsrv and check they are restarted\n\npids are not the same",
    "platform": null
  },
  {
    "test_name": "tsagdbsuppress.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagdbsuppress.tsc - Test case for Exadata AWR project\n\ncellcli parameter test\n\nCommands \"alter cell dbPerfDataSuppress\" to make db info not displayed",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdbupdatenode.tsc",
    "setup": null,
    "flags": {
      "phasewise_upgrade": "false",
      "log_suffix": "upgrade"
    },
    "description": "tsagdbupdatenode.tsc - Exadata DB node upgrade script",
    "platform": null
  },
  {
    "test_name": "tsagdbupdatenode_parse.tsc",
    "setup": null,
    "flags": {
      "update_mode": "iso",
      "sep": "','",
      "repo_log": "dbnodeupdate_yum_repo.log"
    },
    "description": "tsagdbupdatenode_parse.tsc - Helper script to run upgrade on multiple Exadata db nodes",
    "platform": null
  },
  {
    "test_name": "tsagdclibatch.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdclibatch.tsc - Test for dcli utility",
    "platform": null
  },
  {
    "test_name": "tsagdebugcli.tsc",
    "setup": null,
    "flags": {
      "oss_failgroup": "failalldbdg",
      "creatdev_file": "tsagdebugclidef2",
      "errbasename": "^outbasename^",
      "reflogprefix": "^tst_tscname^"
    },
    "description": "tsagdebugcli.tsc - tests for DebugCli command",
    "platform": null
  },
  {
    "test_name": "tsagdebugcli2.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_testing": "2",
      "oss_auto_manage_disks": "true",
      "creatdev_file": "tsagrddef",
      "uniq_dsknames": "all",
      "oss_devdir_1": "^T_WORK^/raw^oss_port^",
      "oss_devdir_2": "^T_WORK^/raw^oss_port2^",
      "asm_ausize": "4194304",
      "mslog": "^celltrcdir^ms-odl.log"
    },
    "description": "tsagdebugcli2.tsc - HDD replacement tests on fake hardware\n\nThis test script covers cases for HDD replacement using debugcli",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdebugcliflush.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_testing": "2",
      "oss_auto_manage_disks": "true",
      "creatdev_file": "tsagrddef",
      "uniq_dsknames": "all",
      "oss_devdir1": "^T_WORK^/raw^oss_port^",
      "oss_devdir2": "^T_WORK^/raw^oss_port2^",
      "asm_ausize": "4194304"
    },
    "description": "tsagdebugcliflush.tsc - Simulate the disk fail case where flushing should#      fail on the disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdebugclix7.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagdebugclix7.tsc - Test to check debugcli commands for x7\n\nTest is to check debugcli commands and M2. disk replacement and removal operations\n\nIt has 5 test cases for different scenarios",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdefdel.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "creatdev_file": "tsagrddef",
      "nflint": "1",
      "redund": "external",
      "oss_testing": "1"
    },
    "description": "tsagdefdel.tsc - Test for bug 9220321 and bug 9271341\n\nCELLSRV SIMULATION EVENTS: DEFERRED DELETES/FPLIB INIT FAILURE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdelcdmdata.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdelcdmdata.tsc - deletes celldisk metadata\n\nDeletes celldisk metadata after drop celldisk all force\n    This might be reqd if create celldisk returns the following error:\n    CELL-02785: Operation failed because the cell disk version is\n         from an incompatible release.",
    "platform": null
  },
  {
    "test_name": "tsagdensebloom.tsc",
    "setup": null,
    "flags": {
      "cell_compress": "'query HIGH'",
      "test_name": "tsag_dense_bloom_",
      "log_file": "tsagdensebloom.log"
    },
    "description": "tsagdensebloom.tsc - Dense bloom filter + Storage Index unit test with various datatype columns",
    "platform": null
  },
  {
    "test_name": "tsagderegister.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "egs_trace_file_rolling": "true",
      "admin_wallet": "^T_WORK^/esadmin_wallet"
    },
    "description": "tsagderegister.tsc - Test for register/deregister ESNODE\n\nRegisters a Fake node and then deregisters that esnode",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagdesc.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdesc.tsc - describe and help command of CellCLI\n\nDescribe and Help commands of various objects on CellCLI\n\n<none>",
    "platform": null
  },
  {
    "test_name": "tsagdesc_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagthrsh_nls.tsc - describe and help command of CellCLI\n\nTests Describe Help commands of cellcli",
    "platform": null
  },
  {
    "test_name": "tsagdgcrsparse.tsc",
    "setup": null,
    "flags": {
      "s": "'\\\\'",
      "asmcmd": "'asmcmd.bat'"
    },
    "description": "tsagdgcrsparse.tsc - Test Cases for Sparse Diskgroup Creations\n\nTest Cases for Sparse Diskgroup Creation for scenarios like redundancy\n     compatibility and various attributes\n\nAddition of Sparse Disk to Non-Sparse Diskgroups is invalid\n     Addition of Non-Sparse disk to Sparse Diskgroup is invalid\n     Requires Log changes once the issue is fixed",
    "platform": null
  },
  {
    "test_name": "tsagdgmnt.tsc",
    "setup": null,
    "flags": {
      "temp_nowarn": "^tst_nowarn^"
    },
    "description": "tprocdgmnt.tsc - Force Mount ASM diskgroups\n\nMount ASM diskgroups\n\nUsage\n      runtest tsagdgmnt <diskgroup> - connects as / and mouts the diskgrp\n      runtest tsagdgmnt <dgroup> sys - connects as sys and mounts the dgroup",
    "platform": null
  },
  {
    "test_name": "tsagdiagpack1.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagdiagpack1.tsc - Auto diag packaging test 1 for Exadata\n\nThe script aims at testing the automatic diag packaging feature for Exadata for 3 failures/alerts - Disk failure alerts, Configuration alert and Incident alert.",
    "platform": null
  },
  {
    "test_name": "tsagdiagpack2.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagdiagpack2.tsc - Diagpack create, list and download command test for Exadata\n\nTest for create, list and download using cellcli and exacli for Exadata",
    "platform": null
  },
  {
    "test_name": "tsagdiagpack_pmem.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagdiagpack1.tsc - Auto diag packaging test for PMEM on x7\n\nThe script aims at testing the automatic diag packaging feature for PMEM disk failure.",
    "platform": null
  },
  {
    "test_name": "tsagdisk_scrub_log.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true",
      "SAGE_MIRROR_MODE": "normal"
    },
    "description": "tsagdisk_scrub_log.tsc - functional test of disk scrub log\n\nSteps:\n        1) Setup test env\n        2) Run cellcli and simulate IO errors - this would simulate 48 bad blocks\n        3) Start disk scrub by running cmd\n        4) Wait disk scrub to finish\n        5) Check alert log for 48 read error traces\n        6) Check alert log for 48 error clear traces showing these blocks are repaired\n        7) Verify device name and sector numbers in both abive traces match\n        8) Check alert log for 2 extents skipped",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdiskconfig.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagaudef",
      "disk": "datafile0"
    },
    "description": "tsagdiskconfig.tsc - Test cellcli error messages when Cell Disk Configuration file is Corrupt\n\nThe Test is outlined as below:\n      1) Bring up the stack\n      2) shutdown RS,MS,cellsrv\n      3) Corrupt cell_disk_config.xml file\n      4) Startup RS\n      5) Startup cellsrv\n      6) CELL-01537 error should be thrown.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdiskfencing.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagdiskfencing.tsc - Test disk fencing through Arbiter.\n\nIn Oracle Exadata RAC clusters it is important that I/O's to disk prior\n     to offlining and onlining back are fenced or blocked before doing any\n     read or write I/O's after disk online. Due to the nature of the\n     network it is possible that I/O's to a disk prior to offlining can\n     reach the cell after the disk is onlined again. We need to guarantee\n     that any in-flight I/O's are completed in order or dropped/fenced\n     before processing the new I/O's to the disk after that disk is onlined.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdiskfencing1.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cftimeout": "'_controlfile_enqueue_holding_time=2400 _controlfile_enqueue_timeout=2400'",
      "logsfx": "event5",
      "iseventset": "1"
    },
    "description": "tsagdiskfencing1.tsc - Disk Fencing Test\n\nTests whether Disk Fencing kicks in by OSS/ASM",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdiskstatetest.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "logfilename": "tsagdiskstatetest"
    },
    "description": "tsagdiskstatetest.tsc - Pooldisk state transition functional\n                             tests\n\nScript to test for all valid pooldisk state transition scenarios\n      while orion workload runs in the back ground\n\nDesign document for this functionality can be found at:\n     https://confluence.oraclecorp.com/confluence/display/~rajeev.k.jain@oracle.com/Functional+Tests+for+PoolDisk+State+Transitions",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagdltvltload.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "eswallet": "^t_work^/esadmin_wallet",
      "egstrcdir": "^ADR_BASE^^s^diag^s^EXC^s^exc^s^^HOSTNAME^^s^trace^s^"
    },
    "description": "tsagdltvltload.tsc - test for race condition between delete and load vault\n\n1. Create a vault and a file in the vault\n   2. set a simulation event in syseds to wait for 10 seconds after usreds completed deletion of all files\n      in the vault and before deleting the vault metadata\n   3. delete vault with options 'force' and 'nowait'\n   4. wait for alert.log msg: 'EBS error simulation: location EDS_CNTR_MD_DELETE_DELAY_SIM'\n   5. try creating a new file in the vault (or some other way to get to the vault content) - the request MUST fail\n   6. clear the simulation event",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagdnsled1.tsc",
    "setup": null,
    "flags": {
      "tst_tscname": "tsagdnsled1"
    },
    "description": "tsagdnsled1.tsc - Includes TEST CASES 1 and 2\n\nTEST CASE 1\n       When an ASM disk is offlined, a cell hosting a partner disk\n       of the offlined ASM disk should have its Do-Not-Service LED turned on,\n       and the LED should be turned off when the offlined ASM disk is back online.\n\n     TEST CASE 2\n       This test case is to clear the good-health bit of an\n       ASM disk to simulate an unhealthy ASM disk and then set the\n       good-health bit back. To clear the good-health bit of an ASM disk,\n       we simulate a disk predictive failure. To set the good-health bit back,\n       we cancel the failure simulation.\n\nFor local testing:\n       oratst -d tsagdnsled1.tsc",
    "platform": null
  },
  {
    "test_name": "tsagdnsled2.tsc",
    "setup": null,
    "flags": {
      "tst_tscname": "tsagdnsled2"
    },
    "description": "tsagdnsled2.tsc - Includes Test Case 3\n\nWhen an ASM disk in an ASM diskgroup dg1 is offlined,\n     a cell hosting a partner disk of gd1 should have its Do-Not-Service LED turned on.\n     Then if dg1 is dismounted, the LED should be turned off.\n     And then if dg1 is mounted again, the LED should be turned on again.\n     During this test, dismounting/mounting another ASM diskgroup without any\n     offlined ASM disks shouldnt change the LED status.\n\nFor local testing:\n       oratst -d tsagdnsled2.tsc",
    "platform": null
  },
  {
    "test_name": "tsagdnsled3.tsc",
    "setup": null,
    "flags": {
      "tst_tscname": "tsagdnsled3"
    },
    "description": "tsagdnsled3.tsc - Includes TEST CASE 4\n\nIn this test case we offline two ASM disks. If a cell hosts partner disks\n     for both of the two disks, its LED can only be turned off after both disks\n     are back online.\n\nFor local testing:\n       oratst -d tsagdnsled3.tsc",
    "platform": null
  },
  {
    "test_name": "tsagdnsled4.tsc",
    "setup": null,
    "flags": {
      "tst_tscname": "tsagdnsled4"
    },
    "description": "tsagdnsled4.tsc - Includes TEST CASE 5 & 6\n\nTEST CASE 5\n      The Do-Not-Service LED on a cell cannot be turned off without force option\n      if some grid disk(s) dont have asmDeactivationOutcome as Yes.\n      Then check if led state is on after proactive polling kicks.\n     TEST CASE 6\n      turn on the Do-Not-Service LED\n\nFor local testing:\n       oratst -d tsagdnsled4.tsc",
    "platform": null
  },
  {
    "test_name": "tsagdrop_cell.tsc",
    "setup": null,
    "flags": {
      "cell_to_drop": "3",
      "newcellport": "^oss_port^"
    },
    "description": "tsagdrop_cell.tsc - drop the cell from the exadata setup\n\npls see below",
    "platform": null
  },
  {
    "test_name": "tsagdropcell.tsc",
    "setup": null,
    "flags": {
      "cell_to_drop": "^numcells^",
      "ctrl1": "'--ctrl localhost:'^nginx_https_port^",
      "ctrl2": "'--ctrl localhost:'^nginx_https_port2^",
      "ctrl": "^ctrl2^",
      "gdprefix": "'HC_c'"
    },
    "description": "tsagdropcell.tsc - Drops a cell permanently from Exascale cluster\n\nThis script is used to permanently drop a cell from Exascale cluster\n       run tsagdropcell cell_to_drop=<cell number>\n         <cell number> should be a valid cell of the cluster",
    "platform": null
  },
  {
    "test_name": "tsagdropextent.tsc",
    "setup": null,
    "flags": {
      "file_dest": "@^vault_db^",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''",
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'"
    },
    "description": "tsagdropextent.tsc - Functional Tests on ExtentDrop\n\nFunctional Tests on ExtentDrop\n       1 Some general SI tests were performed to populate some data and RIDXs\n       2 Dump SI usage, check some valid RIDXs on test table\n       3 Dump all RIDXs and take a note of one extent id\n       4 Perform extent drop command\n       5 Dump SI usage again, see less valid RIDXs show up\n       6 Dump all RIDXs and search for RIDX There is no RIDX with this same extent id\n\nFunctional Tests on ExtentDrop",
    "platform": null
  },
  {
    "test_name": "tsagdropfirst_rebal2nd.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagdropfirst_rebal2nd.tsc - permanently drop griddisks of leader cell,\n then add back them and perform fail/reenable operation on non-leader cell\n\nTest steps:\n\n i.) - Exascale setup with 4 cells and DB workload running\n\n In a loop (count 3); do\n\n     ii.) - pick leader cell and drop its griddisks permanently, reconfig and\n            wait for rebalance to complete\n\n     iii.)  - add the disks back, reconfig, DO NOT wait for rebalance or\n              reconfig to complete\n\n      iv.) - switch to non leader cell and start an fail/reenable hard disk\n             operations for 5 mins while rebalance is still happening.\n\n      v.)  - wait for background running fail/reenable operation to finish\n             and rebalance on leader cell to complete\n\n endloop\n\n vi.) make sure DB workload ran fine",
    "platform": null
  },
  {
    "test_name": "tsagdropfirst_scrub2nd.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagdropfirst_scrub2nd.tsc - permanently drop griddisks of leader cell,\n     then add back them and perform scrub on non-leader cell\n\nTest steps:\n\n i.) - Exascale setup with 4 cells and DB workload running\n\n In a loop (count 3); do\n\n     ii.) - pick leader cell and drop its griddisks permanently, reconfig and\n            wait for rebalance to complete\n\n     iii.)  - add the disks back, reconfig, DO NOT wait for rebalance or\n              reconfig to complete\n\n      iv.) - switching to non leader cell and start disk scrub operation\n      (BLOCK IO READ Errors) for 5 mins while rebalance is still happening.\n\n      v.)  - wait for background running scrub operation to finish and\n             rebalance on leader cell to complete\n\n endloop\n\n vi.) make sure DB workload ran fine",
    "platform": null
  },
  {
    "test_name": "tsagdropflash.tsc",
    "setup": null,
    "flags": {
      "tst": "1",
      "errbasename": "^outbasename^"
    },
    "description": "tsagdropflash.tsc - test flush/drop flash disks and flashcache",
    "platform": null
  },
  {
    "test_name": "tsagdropgriddisk.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_testing": "2",
      "oss_auto_manage_disks": "true",
      "sage_mirror_mode": "normal"
    },
    "description": "tsagdropgriddisk.tsc - Script to drop griddisk and confirm gd owner object is deleted.\n\nAfter dropping the griddisks (manually), the size of the griddisk owner file must reduce. This makes sure that the\n     griddisk owner object is deleted. This test drops 2 griddisks, datafile0 and datafile1.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdropphy.tsc",
    "setup": "srdbmsini",
    "flags": {
      "DEBUGCLI": "^OSS_HOME^/src/tools/DebugCli/DebugCli",
      "oss_testing": "2",
      "oss_auto_manage_disks": "true",
      "disable_multims": "true"
    },
    "description": "tsagdropphy.tsc - test case for physicaldisk removal/reenable\n\nTest case for 'ALTER PHYSICALDISK <DISKNAME> DROP FOR REPLACEMENT/REENABLE'",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdscv.tsc",
    "setup": "tsagnini",
    "flags": {
      "asmalert": "^t_diag^^s^diag^s^asm^s^'+asm'^s^^_asmsid^^s^trace^s^alert_^_asmsid^'.'log",
      "greppatt": "'Cell o/1;1;1;1:5042: not responding'"
    },
    "description": null,
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskbrokenpipe.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagaudef",
      "disk": "datafile0"
    },
    "description": "tsagdskbrokenpipe.tsc - Test to check RDBMS response to a broken Diskmon Pipe\n\nTest is outlined as follows:\n1) Bring up the stack\n2) Shutdown DB instance\n3) Delete the diskmon pipe\n4) startup DB instance\n5) Check for ORA-56867 in RDBMS alert logs",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskchk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdskchk.tsc - Macro with physicaldisk check at beginning of LRG\n\nMacro with physicaldisk check at beginning of LRG\n\nMacro with physicaldisk check at beginning of LRG",
    "platform": null
  },
  {
    "test_name": "tsagdskcntr_ef.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "scas15celadm04"
    },
    "description": "tsagdskcntr_ef.tsc - Disk controller Auto recovery on EF\n\nDisk COntroller Auto rcovery on EF cell- Should be noop\n     as there is no disk controller on EF cell\n     Crash detector should work properly here as well.",
    "platform": null
  },
  {
    "test_name": "tsagdskcntr_manualreco.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdskcntr_manualreco.tsc - Manual recover from disk controller failure\n\nThis is a helper script containing steps on fake hw for recovering\n     manually from disk controller crash",
    "platform": null
  },
  {
    "test_name": "tsagdskcntr_replace.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdskcntr_replace.tsc - Script for disk controller replacement\n\nHelper script to simulate Disk Controller Replacement",
    "platform": null
  },
  {
    "test_name": "tsagdskcntr_scripts.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdskcntr_scripts.tsc - Common scripts for 85957\n\nThis file contains common scripts used by Disk controller\n      auto recovery tests",
    "platform": null
  },
  {
    "test_name": "tsagdskcntreco.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_failgroup": "failalldbdg",
      "oss_auto_manage_disks": "true",
      "cdb": "true"
    },
    "description": "tsagdskcntreco.tsc - Disk controller Auto recovery Tests\n\nTests for Disk controller auto and manual recovery",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskcntreco2.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_failgroup": "failalldbdg",
      "oss_auto_manage_disks": "true",
      "cdb": "true",
      "test": "tsagdskcntreco_msrestart"
    },
    "description": "tsagdskcntreco2.tsc - Disk Controller Recovery\n\nDisk Controller Recovery Tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskcntreco3.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_failgroup": "failalldbdg",
      "oss_auto_manage_disks": "true",
      "cdb": "true",
      "event_num": "0",
      "test": "^tst_tscname^_test1"
    },
    "description": "tsagdskcntreco3.tsc - Tests for Auto Recover Task\n\nThis is part of Disk Controller Recovery project.\n      Sub Project - Auto recover Task - helps in auto recovery once\n                    disk group is mounted in ASM",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskcntreco4.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_failgroup": "failalldbdg",
      "oss_auto_manage_disks": "true",
      "cdb": "true",
      "event_num": "0",
      "test": "^tst_tscname^_test1"
    },
    "description": "tsagdskcntreco4.tsc - Test for Auto recover Task\n\nThis is part of Disk Controller Recovery project.\n      Sub Project - Auto recover Task - helps in auto recovery once\n                    disk group is mounted in ASM",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskcntreco_common.tsc",
    "setup": null,
    "flags": {
      "msg": "'Failed to recover all grid disks following disk controller failure due to a dismounted disk group'"
    },
    "description": "tsagdskcntreco_common.tsc - Common Steps of Auto recover task tests\n\nHelper script to be used for Auto Recover Task tests\n     Contains steps that are common to all tests\n\nCannot be used standalone - Has to be included in tsagdskcntreco3 and\n       tsagdskcntreco4.tsc",
    "platform": null
  },
  {
    "test_name": "tsagdskde01.tsc",
    "setup": null,
    "flags": {
      "nowarn": "^TST_NOWARN^",
      "MAX_INSTANCE": "2",
      "orcl_sid_temp": "^ORACLE_SID^",
      "md": "0",
      "otemp": "asm_instance^m^",
      "oinst": "^^otemp^"
    },
    "description": "tsagdskde01.tsc - Exadata emulated multi-node diskmon death # 1\n\nWrapper test which setsup ASM/DB and starts running indvidual\n     diskmon death tests\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskde01a.tsc",
    "setup": null,
    "flags": {
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "a"
    },
    "description": "tsagdskde01a.tsc - Exadata emulated multi-node diskmon death # 1\n\nTest # 1:\n     (i) Start workload in two nodes (2 RDBMS/ASM instances)\n     (ii) Kill diskmon in one node\n          - cssd restarts diskmon automatically (new incarnation of diskmon)\n          - cellsrv should connect to new diskmon incaratnation\n          - The surviving diskmon should know about the death of the other\n            diskmon through the cssd communication channel\n     (iii) Workload should finish in both instances without any errors\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskde01b.tsc",
    "setup": null,
    "flags": {
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "^case^"
    },
    "description": "tsagdskde01b.tsc - Exadata emulated multi-node diskmon death # 1\n\nTest # 2:\n     (i) Start workload in two nodes (2 RDBMS/ASM instances)\n     (ii) Kill diskmon in one node\n          - cssd restarts diskmon (new incarnation of diskmon)\n     (iii) Sleep 10 sec\n     (iv) Kill cellsrv (abnormal shutdown)\n          - RS should restart cellsrv\n          - there is a new incarnation of cellsrv which should connect to the\n            new diskmon\n     (v) Workload should finish in both instances without any errors\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskde01c.tsc",
    "setup": null,
    "flags": {
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "c"
    },
    "description": "tsagdskde01c.tsc - Exadata emulated multi-node diskmon death # 1\n\nTest # 3:\n    (i)   Start workload in two nodes (2 RDBMS/ASM instances)\n    (ii)  Kill diskmon in one node\n           - cssd restarts diskmon (new incarnation of diskmon)\n    (iii) Sleep 10 sec\n    (iv)  Shutdown cellsrv through cellcli (normal shutdown)\n           - RS will not restart cellsrv\n    (v)   Sleep for a minute or so before starting cellsrv.\n           - this so that the cell gets evicted. Later when the cell gets\n             started, it will get added automatically.\n    (vi)  Startup cellsrv through cellcli - there is a new incarnation of\n             cellsrv which should connect to the new diskmon\n    (vii) Workload should finish in both instances without any errors.\n           - Workload should be running for atleast a minute after the cell\n             comes up\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskde01d.tsc",
    "setup": null,
    "flags": {
      "orcl_sid_temp": "^ORACLE_SID^",
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "d1",
      "tmp_db_name": "^db_name^",
      "r": "2",
      "otemp": "asm_instance^m^",
      "oinst": "^^otemp^",
      "l": "2",
      "nd": "0"
    },
    "description": "tsagdskde01d.tsc - Exadata emulated multi-node diskmon death # 4\n\nTest # 4:\n     (i)   Start workload in two nodes (2 RDBMS/ASM instances)\n     (ii)  Kill diskmon in one node\n           - cssd restarts diskmon (new incarnation of diskmon)\n     (iii) Shutdown abort ASM inst 2 in the same node where diskmon came down\n           - This will bring down rdbms instance 2 where workload will be\n             aborted\n     (iv)  Sleep 5 sec\n     (v)   Restart ASM 2 and RDBMS 2 and start a new workload\n     (vi) - Should not get any error messages like instance busy etc.\n          - Workload from instance 2 should complete\n          - New workload from instance 1 should complete\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskde01e.tsc",
    "setup": null,
    "flags": {
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "e"
    },
    "description": "tsagdskde01e.tsc - Exadata emulated multi-node diskmon death # 2\n\n(i) Start workload in two nodes (2 RDBMS/ASM instances)\n     (ii) Shutdown abort ASM inst 2\n          - cssd will start doing the fencing through diskmon\n     (iii) Kill diskmon in the same node where ASM went down.\n          - Vijay will give make some code changes which will kill diskmon\n           as soon as it starts receiving fence messeages. The test will\n           instrument this using an underscore parameter.\n          - cssd will start a new incarnation of diskmon\n     (v) Restart ASM 2 and RDBMS 2 and start a new workload\n     (vi) - Should not get any error messages like instance busy etc.\n          - Workload from instance 1 should complete\n          - New workload from instance 2 should complete\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskde01f.tsc",
    "setup": null,
    "flags": {
      "orcl_sid_temp": "^ORACLE_SID^",
      "tmp_db_name": "^db_name^",
      "o_i": "^i^",
      "str": "has_hostname_^o_i^",
      "tmpdskm_pipe": "dskm_pipe^i^",
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "f1",
      "num1": "f1"
    },
    "description": "tsagdskde01f.tsc - Exadata emulated multi-node diskmon death # 2\n\n(i) Start workload in two nodes (2 RDBMS/ASM instances)\n     (ii) Kill diskmons in both nodes\n     (iii) cssd will start new diskmons\n     (iv) Clients should continue the workloads and complete\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskde01g.tsc",
    "setup": null,
    "flags": {
      "orcl_sid_temp": "^ORACLE_SID^",
      "tmp_db_name": "^db_name^",
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "g"
    },
    "description": "tsagdskde01g.tsc - Exadata emulated multi-node diskmon death # 2\n\n(i) Start workload in two nodes (2 RDBMS/ASM instances)\n     (ii) kill -9  of one cssd\n     (iii) workload in node 2 should complete\n     (iv) should see node fence requests from survivng node\n\n      If we invoke the test script with option=2, then we kill cssd in\nstep (ii) with -11 and at the end of the test we look for a\n      fencing message in diskmon.log of node 1.\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskde01h.tsc",
    "setup": null,
    "flags": {
      "orcl_sid_temp": "^ORACLE_SID^",
      "tmp_db_name": "^db_name^",
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "h1",
      "nd": "0",
      "otemp": "asm_instance^m^",
      "oinst": "^^otemp^"
    },
    "description": "tsagdskde01h.tsc - Exadata emulated multi-node diskmon death # 2\n\n(i)  Start workload in two nodes (2 RDBMS/ASM instances)\n     (ii) HOLD cellsrv process for > 5min. This is done by\n          - SIGSTOP (kill SIGSTOP <pid>)\n          When this happens:\n          - diskmon stops communication with cellsrv\n          - cellsrv also figures out that it cannot communicate with diskmon\n          - this will start implicit fencing of the I/O's\n          - ASM dg goes down\n          - RDBMS instance goes down\n     (iii) Remove HOLD on cellsrv:\n          - SIGCONTINUE or exiting gdb on cellsrv - (kill SIGCONT <pid>)\n     (v) Start ASM+RDBMS instances. Start new workload.\n     (iv) Diskmon should reconnect with cellsrv and new workload should go through\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskde01i.tsc",
    "setup": null,
    "flags": {
      "orcl_sid_temp": "^ORACLE_SID^",
      "tmp_db_name": "^db_name^",
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "i",
      "n": "j"
    },
    "description": "tsagdskde01i.tsc - Exadata emulated multi-node diskmon death # 2\n\n-Almost same as tsagdskde01h.tsc except that time to wait is 25 secs.\n     (i) Start workload in two nodes (2 RDBMS/ASM instances)\n     (ii) HOLD cellsrv process for 25 secs. This is done by either of\n          these two methods:\n          - SIGSTOP or gdb cellsrv\n          When this happens:\n          - diskmon stops communication with cellsrv\n          - Since this is less than the timeout it willnot start implicit\n          fencing of the I/O's\n     (iii) Remove HOLD on cellsrv done through\n          - SIGCONTINUE or exiting gbd\n          - ASM dg goes not down\n          - RDBMS instance goes not down and I/O's will finish without any errros.\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskde01j.tsc",
    "setup": null,
    "flags": {
      "orcl_sid_temp": "^ORACLE_SID^",
      "tmp_db_name": "^db_name^",
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "j",
      "n": "j",
      "nd": "0",
      "oinst": "11"
    },
    "description": "tsagdskde01j.tsc - Exadata emulated multi-node diskmon death # 2\n\nAlmost same as tsagdskde01h.tsc except that time to wait after the HOLD is < 5 min.\n     (i) Start workload in two nodes (2 RDBMS/ASM instances)\n     (ii) HOLD one cssd using SIGSTOP process for 120 secs.\n          When this happens:\n          - the surviving cssd recognizes that the other node is\n            not responding and will start fencing the I/O's\n          - Diskmon has been updated so that when it recieves the fece msg\n            from the node not responding it does *not* crash ASM/RDBMS\n            and the test case hits ORA-15311\n          - RDBMS instance from the cssd which is not responding will go down\n            with ORA-15311\n          - the ASM disks are *not* offlined (after the bug fix from 8821490)\n          - The workload from the surviving node is completed\n      (iii) The test case verification is done using the foll:\n          - Make sure alert*log has ORA-15311\n          - ASM disks are not offline\n          - workload from surviving node completes succsessfully\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskde01k.tsc",
    "setup": null,
    "flags": {
      "orcl_sid_temp": "^ORACLE_SID^",
      "tmp_db_name": "^db_name^",
      "pipe1": "^dskm_pipe^",
      "pipe2": "^dskm_pipe^",
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "k1",
      "l": "2",
      "otemp": "asm_instance^l^",
      "oinst": "^^otemp^"
    },
    "description": "tsagdskde01k.tsc - Exadata emulated multi-node diskmon death # 2\n\n(i) Start workload in two nodes (2 RDBMS/ASM instances)\n     (ii) Send a SIGUSR1 to one diskmon to simulate memory exhaustion\n     (iii) This will reboot the node and the instances will come down.\n           The workload in that node will be aborted\n           The workload from the surviving node completes\n     (iv) When diskmon/cssd comes back, the ASM & RDBMS nodes are restarted\n     (v) New workload starts and completes after the node bounce\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskde01l.tsc",
    "setup": null,
    "flags": {
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload3",
      "num": "l",
      "sleep_time": "8"
    },
    "description": "tsagdskde01l.tsc - Exadata emulated multi-node diskmon death # 2\n\n4 emulated nodes. ASM diskgroup created with NORMAL\n       redundancy. Auto mgmt should be ENABLED.\n       ASM and DB instances running on all nodes.\n       On one of the cells run the following in a loop for 50 times.\n       shutdown cellsrv\n       sleep sleep_time_in_seconds\n       startup cellsrv\n       sleep sleep_time_in_seconds\n       where 'sleep_time_in_seconds' is a number between 2 and 8\n       selected randomly for a given test run.\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskde01m.tsc",
    "setup": null,
    "flags": {
      "orcl_sid_temp": "^ORACLE_SID^",
      "tmp_db_name": "^db_name^",
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "i"
    },
    "description": "tsagdskde01m.tsc - Exadata emulated multi-node diskmon death # 2\n\n-Almost same as tsagdskde01h.tsc except that time to wait is 25 secs.\n     (1)  This is from bug26585956 and it requried 2 nodes with 3 cells\n     (2)  Shutdown RS and set debug value in cellinit\n     (3)  HOLD cellsrv process for 25 secs. This is done by either of\n          these two methods:\n          - SIGSTOP  on cellsrv\n          When this happens:\n          - diskmon stops communication with cellsrv\n          - Since this is less than the timeout it willnot start implicit\n          fencing of the I/O's\n     (4)  Use cssfault to simulate failures on node0\n     (5)  Fencing node0\n     (6 ) Remove HOLD on cellsrv done through\n          - SIGCONTINUE on cellsrv\n          - ASM dg goes not down\n          - RDBMS instance goes not down and I/O's will finish without any errros.\n\nUses the emulated multi-node framework",
    "platform": null
  },
  {
    "test_name": "tsagdskderm01.tsc",
    "setup": "tsagnini",
    "flags": {
      "cluster_database": "true",
      "asm_allow_sysdba": "true",
      "creatdev_file": "tkfgrddef"
    },
    "description": "tsagdskderm01.tsc - Setsu for the diskmon death tests\n\nTest # 1:\n     Creates two instance in two nodes. Runs individual diskmon death tests\n\nUses real cluster framework\n     Tests running are:\n      -  tsagdskderm01a\n      -  tsagdskderm01b",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskderm01a.tsc",
    "setup": null,
    "flags": {
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "a"
    },
    "description": "tsagdskderm01a.tsc - Real Cluster diskmon death # 1\n\nTest # 1:\n     (i) Start workload in two nodes (2 RDBMS/ASM instances)\n     (ii) Kill diskmon in one node\n          - cssd restarts diskmon automatically (new incarnation of diskmon)\n          - cellsrv should connect to new diskmon incaratnation\n          - The surviving diskmon should know about the death of the other\n            diskmon through the cssd communication channel\n     (iii) Workload should finish in both instances without any errors\n\nUses the real cluster exadata framework",
    "platform": null
  },
  {
    "test_name": "tsagdskderm01b.tsc",
    "setup": null,
    "flags": {
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload2",
      "num": "b"
    },
    "description": "tsagdskderm01a.tsc - Real Cluster diskmon death # 1\n\nTest # 2:\n     (i) Start workload in two nodes (2 RDBMS/ASM instances)\n     (ii) Kill diskmon in one node\n          - cssd restarts diskmon (new incarnation of diskmon)\n     (iii) Sleep 10 sec\n     (iv) Kill cellsrv (abnormal shutdown)\n          - RS should restart cellsrv\n          - there is a new incarnation of cellsrv which should connect to the\n            new diskmon\n     (v) Workload should finish in both instances without any errors\n\nUses the real cluster exadata framework",
    "platform": null
  },
  {
    "test_name": "tsagdskful.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "dskfuldef",
      "asm_ausize": "1048576"
    },
    "description": "tsagdskful.tsc - SELECT from a table of normal redundancy diskgroup\n\nCreate a Normal redundancy diskgroup and a table of size 1GB on it.\n     Full this table with records so that there is no empty space. Now run concurrent\n     UPDATE and SELECT on the table while the underlying celldisk\n     is being exported and imported. SELECT operation should not fail.\n     The test takes one optional argument 'times' as input (default value 2) and\n     run the forks in a loop for that many number of times.\n     Eg: oratst tsagdskful times=5",
    "platform": null
  },
  {
    "test_name": "tsagdskinj_sparse.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdskinj_sparse.tsc - Disk Injection Tests on SPARSE diskgroup\n\nSimulated GD_READ_ERR and GD_READ_CORRUPTION for sparse griddisk",
    "platform": null
  },
  {
    "test_name": "tsagdskinject.tsc",
    "setup": null,
    "flags": {
      "parent_pdb": "cdb1_pdb1",
      "parent2_pdb": "cdb1_pdb2",
      "child_pdb": "test_snap1",
      "child2_pdb": "test_snap2"
    },
    "description": "tsagdskinject.tsc - GridDisk Error Injection Tests\n\nThese tests uses simulation events to inject GD Error and COrruption",
    "platform": null
  },
  {
    "test_name": "tsagdskiorm.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagaudef",
      "disk": "datafile0"
    },
    "description": "tsagdskiorm.tsc - Diskmon IORM bug 12602161 test case\n\n(I) Setup a single node environment\n     (II) Reconfigure diskmon to freeze IORM requests\n     (III) Create & push a new IORM plan\n     (IV) Bounce cellsrv\n     (V) Check if diskmon in functional by checking alert logs",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdsklooptest.tsc",
    "setup": null,
    "flags": {
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload3",
      "num": "m",
      "diskmon_pid": "^proc_pid^"
    },
    "description": "tsagdsklooptest.tsc - Checks if work load is completed successfully in case of repetitive Diskmon failure\n\nChecks if work load is completed successfully in case of repetitive Diskmon failure\n     Steps:\n     (i)  Start work load on both the nodes (Node 0 & Node 1)\n     (ii) Repeat N times\n            Shutdown Diskmon on both the nodes\n      Startup Diskmon on both the nodes\n     (iii) Check if work load is completed",
    "platform": null
  },
  {
    "test_name": "tsagdskmonhbthreadrcnct.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagdskmonhbthreadrcnc.tsc- Test for diskmon HB thread reconnect\n\nstackup with ASM and DB.\n     set _skgxp_gen_ant_ping_misscount=3600, restart cellsrv\n     update cellinit.ora for \"_reconnect_to_cell_attempts\",\"1000\", kill diskmon so that it respawns.\n     start a workload in the background where short lived sql sessions for insert keep spawning every second for 30 min.\n     waited 10 min and then added cell url to /tmp/cell_hb_fail, touched /tmp/cell_sim_skgxp_hb_fail\n     Sent SIGUSR1 signal to diskmon.\n     Look for \"Heartbeat with diskmon started\" msg in cellsrv, and \"Simulating heartbeat problem to cell\" in diskmon.trc\n\nNeeds tsagdskmonwrkload.tsc to run the workload in background",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskmonwrkload.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdskmonwrkload.tsc - diskmon workload test script\n\nscript forked in tsagbug32133702.tsc for 45 min of sql sessions of few seconds each",
    "platform": null
  },
  {
    "test_name": "tsagdsknodefence.tsc",
    "setup": null,
    "flags": {
      "scr1": "tsagwrkload1",
      "scr2": "tsagwrkload3",
      "num": "n"
    },
    "description": "tsagdsknodefence.tsc - Diskmon Node fence test\n\nIn a multinode environment, if a diskmon in not in a state where it can process a node level fence request,\n     it should bring down the node from where the fence request is coming & should also kill its own node.\n     Contrary to the expected behavior, two nodes fail to go down at the end of test.\n     Tracking bug 13363348 has been raised to examine the bug.",
    "platform": null
  },
  {
    "test_name": "tsagdskoff.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "diskdef",
      "maxinstances": "1"
    },
    "description": "tsagdskoff.tsc - tsagdskoff\n\nDisk offline tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskoraagent.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagaudef",
      "disk": "datafile0",
      "diskmon_pid": "^proc_pid^",
      "oraagent_pid": "^proc_pid^",
      "oraagent_pid_new": "^proc_pid^",
      "check_bug": "1"
    },
    "description": "tsagdskoraagent.tsc - Orarootagent functionality test on real hardware\n\n(I) send a SIGSTOP to diskmon\n     (II) Do a kill -9 of orarootagent for that node (reboot)\n     (III) orarootagent will restart (probably by cssd)\n     (IV) The new orarootagent will kill the existing diskmon which is not responding\n\nReal Hardware test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskpfail.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagdskpfail.tsc - Simulates predictive disk failure\n\nThe test can be outlined as follows :\n * Scenario 1\n          1. create CD on disk A and create GDs on that CD but do not add any of the GDs to ASM diskgroup\n          2. verify that GDs are seen in v$asm_disk and kfod\n          3. simulate predictive failure on disk A\n          4. verify that GDs are not seen in v$asm_disk or kfod\n          5. restart cellsrv and MS\n          6. verify that GDs are not seen in v$asm_disk or kfod\n * Scenario 2\n    1. create CD on disk A and create GDs on that CD and add some of the disks to an ASM diskgroup\n    2. repeat steps 2-6 from scenario 1\n       * Scenario 3\n          1. create CD on disk A and create GDs on that CD and add all of the disks to an ASM diskgroup\n          2. repeat steps 2-6 from scenario 1\n       * Scenario 4\n          1. Keep one of the cells down and verify that kfod runs in\n             parallel without having to wait for unavailable cell to come\n             backup.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskscrub.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "diskdef"
    },
    "description": "tsagdskscrub.tsc - Tests for disk scrubbing project\n\nTests for disk scrubbing project",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskscrub02.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "diskdef02"
    },
    "description": "tsagdskscrub02.tsc - Disk Scrubbing tests\n\nDisk scrubbing tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskscrub03.tsc",
    "setup": "srdbmsini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "differentszfg": "1",
      "creatdev_file": "tsagcaudef"
    },
    "description": "tsagdskscrub03.tsc - Disk scrubbing tests\n\nDisk scrubbing tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskscrub1.tsc",
    "setup": "srdbmsini",
    "flags": {
      "disable_multims": "true"
    },
    "description": "tsagdskscrub1.tsc - Tests for disk scrubbing.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskscrub1a.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagdskscrub1.tsc - Checks if disk scrubbing runs into completion while doing\n\t\t   WBFC resilvering parallely on a different disk.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskscrub1b.tsc",
    "setup": "srdbmsini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "differentszfg": "1"
    },
    "description": "tsagdskscrub1b.tsc - Do disk scrubbing while running ASM disk rebalance",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskscrub1c.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagdskscrub1c.tsc - Do disk scrubbing while bouncing cellsrv on the\n\t\t    cell with corrupt blocks",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskscrub1e.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagdskscrub1e.tsc - Ensures that a trigger disk scrubbing is paused if the partner disk goes offline\n\t\t    and if the partner disk comes back online then the disk scrubbing should resume.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdskstatedump.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagaudef",
      "disk": "datafile0"
    },
    "description": "tsagdskstatedump.tsc - Diskmon State Dump Test\n\nThe objective of this test is to test if orarootagent initiates Diskmon state dump\n   before killing an unresponsive diskmon. This test is in response to Bug 12925823 fix.\n\n   Test flow is outlined below:\n     (1) Setup a single node Environment\n     (2) Force ORAROOT agent to send sigusr2 to diskmon. (It will do this only when diskmon is hung)\n     (3) Check oraroot agent log to see if state dump initiation message is recorded\n     (4) Check diskmon logs to see if state dump has begun\n     (5) Ensure that diskmon is killed after the state dump",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdsofflinerecovery.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagdsofflinerecovery.tsc - Confirm datastore shards status\n\nTo check if datastore shards come online after egs kill",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagdsperdt.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2"
    },
    "description": "tsagdsperdt.tsc - Exadata disk performance detection for HD disks\n\nDetects a bad performing Hard disk by checking $T_WORK/diskstats",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdsperdt2.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'",
      "disk": "FLASH0"
    },
    "description": "tsagdsperdt2.tsc - Exadata disk performance detection for FD\n\nDetects a bad performing Flash Disk by checking $T_WORK/diskstats",
    "platform": null
  },
  {
    "test_name": "tsagdsperdt3.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'",
      "disk": "FLASH0"
    },
    "description": "tsagdsperdt3.tsc - Exadata disk performance detection for FD\n                        for flood control i.e\n                        3 FD disks are made to fail by increasing their\n                        service time (through $T_WORK/diskstats)\n\nDetects a bad performing Flash Disk by checking $T_WORK/diskstats",
    "platform": null
  },
  {
    "test_name": "tsagdsperdt4.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2"
    },
    "description": "tsagdsperdt4.tsc - Exadata disk performance detection for HD\n                        for flood control i.e\n                        Simulate bad disks for 3 disks (through $T_WORK/diskstats)\n\nDetects a bad performing HD by checking $T_WORK/diskstats",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdss.tsc",
    "setup": "tsagnini",
    "flags": {
      "tsagcln": "3",
      "cell_offload_plan_display": "ALWAYS",
      "_bloom_predicate_enabled": "true",
      "oss_memleak_skip_restart": "1"
    },
    "description": "tsagdss.tsc -  Unit tests for Predicates in Storage",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdss2.tsc",
    "setup": null,
    "flags": {
      "oss_testing": "2"
    },
    "description": "tsagdss2.tsc - run tsagdss with multiple servers\n\nRun tsagdss with multiple servers.\n\nThis test takes oss_testing as the number of oss's to\n     run with and port2use as the starting port number to use.",
    "platform": null
  },
  {
    "test_name": "tsagdssfg.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high",
      "creatdev_file": "tsagrddef",
      "dangerousfg": "1",
      "differentszfg": "1",
      "diskstring": "^diskstring^,''^nonsage_path^'*'''",
      "nonsage": "nonsage^i^",
      "tmp_port": "oss_port^i^",
      "nonsage_path": "^T_WORK^^s^^nonsage^^s^",
      "logad": "tsagadfgb^j^",
      "tmp_path": "raw_path^i^"
    },
    "description": "tsagdssfg.tsc - run tsagdss with oss failgroups and mixed disks\n\n1 . run tsagdss with 2 oss servers, 1 failgroup per server and\n       multiple datafile disks per failgroup\n   2 . test rebalance between SAGE and non-SAGE and predicate pushing\n       to mixed disks by adding non-SAGE disks to each of the 2 failgroups\n       and run query\n   3 . drop the non-SAGE disks to get the diskgroups to be SAGE-only\n   4 . repeat 2 and 3 but do the add/drop in parallel with query\n       (with j == 2 in for j loop)\n   5 . repeat 2 and 3 but add the non-SAGE disks to a separate failgroup\n       and drop the SAGE failgroup (done as step 2) then re-add the SAGE\n       failgroups and drop the non-SAGE failgroups to restore SAGE-only\n       (done as step 3)\n       This is to test failover between SAGE and non-SAGE\n       (with j == 3 in for j loop)\n\nYou can specify a larger number of oss's to run with and although\n     the commands will succeed, there will be log difs because I haven't\n     made the comparison general enough.",
    "platform": null
  },
  {
    "test_name": "tsagdsshard.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "saved_ossconf": "^OSSCONF^"
    },
    "description": "tsagdsshard.tsc - EGS offline/online DS Shard\n\nuses a simulation event to delay  DataStoreShard ONLINE Job\n     when the Job is in it's initial state.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagdssql.tsc",
    "setup": null,
    "flags": {
      "file_dest": "'+datafile'",
      "conn": "'sys/knl_test7 as sysdba'"
    },
    "description": "tsagdssql.tsc - run tsagdss.sql\n\nCalled by tsagdss to execute the sql scripts.\n     This was split out from tsagdss.tsc so that it can be invoked\n     after having the database created by another script, say srdbmsini.\n\nNeed to have the database already created, diskmon, oss and asm\n     already running. (These are left running by srdbmsini.)",
    "platform": null
  },
  {
    "test_name": "tsagdumpsqlplan.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagdumpsqlplan.tsc - testcases for checking SQL texts, explain plans,\n                           and client labels in cellsrv/oflsrv incidents\n\nsteps for the test:\n     1: simulate the event to force cellsrv/offload server to crash\n     2: issue smart scan workload and trigger the incident\n     3: look for the incident file and make sure relevant information such\n        as SQL texts, plans, labels are dumped.",
    "platform": null
  },
  {
    "test_name": "tsagdynbuff1.tsc",
    "setup": "tsagnini",
    "flags": {
      "format_long_identifier": "true",
      "compatible": "11.2.0.0"
    },
    "description": "tsagdynbuff1.tsc - add for bug16887605 Test case 1.\n\n1. Set parameter _cell_pred_max_core_exec_threads as 1\n     2. Check in v$cell_state that number of ehcc objects is 1\n     3. Run EHCC scans on a patitioned table with parallel 8\n     4. Verify that total core exec objects > 0\n     5. Reset stats using events and increase core exec objects to 8\n     6. Repeat #2 -  #4\n     7. Decrease core exec objects to 4\n     8. Repeat #2 - #4.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdynbuff2.tsc",
    "setup": "tsagnini",
    "flags": {
      "format_long_identifier": "true",
      "compatible": "11.2.0.0"
    },
    "description": "tsagdynbuff2.tsc - add for bug16887605 Test case 2.\n\n1. Set parameter _cell_pred_max_core_exec_threads as 8\n     2. Check in v$cell_state that number of ehcc objects is 8\n     3. Set simulation event NO_FREE_COREEXEC_OBJ\n     4. Decrease number of core exec objects as 4\n     5. Verify the number of ehcc objects is 8\n     6. Run EHCC scans on a patitioned table with parallel 8\n     7. Verify the number of ehcc objects is 4\n\nRepeat tsagdynbuff1 with simulation event NO_FREE_COREEXEC_OBJ",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdynbuff3.tsc",
    "setup": "tsagnini",
    "flags": {
      "format_long_identifier": "true",
      "compatible": "11.2.0.0"
    },
    "description": "tsagdynbuff3.tsc - add for bug16887605 Test case 3.\n\n1. Set parameter _cell_pred_max_core_exec_threads as 15\n     2. Check in v$cell_state that number of ehcc objects is 15\n     3. For an EHCC scan\n     4. Modify the parameter (increase and decrease) concurrently\n     5. Verify cellsrv did not crash\n\nCheck cellsrv do not crash",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagdynbuff4.tsc",
    "setup": "tsagnini",
    "flags": {
      "format_long_identifier": "true",
      "compatible": "11.2.0.0"
    },
    "description": "tsagdynbuff4.tsc - add for bug16887605 Test case 4.\n\n1. Set parameter _cell_pred_max_core_exec_threads as 4\n     2. Check in v$cell_state that number of ehcc objects is 4\n     3. Set low mem threshold event LOWMEM_TH_FAILURE\n     4. Increase number of core exec objects as 8\n     5. Verify cellsrv did not crash\n     6. Verify the number of ehcc objects is still 4\n     7. Disable low mem threshold event\n\nLow Mem threshold simulation",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagebs2procfence_3cell.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "sage_mirror_mode": "high",
      "setup_blockstore": "true"
    },
    "description": "tsagebs2procfence_3cell.tsc - Test for EBS fencing for 3 cell setup\n\n1. 3 cell setup for blockstore (xrdbmsini with blockstore setup true)\n     2. Check fence alerts after shutting down combination of services\n     3  The cases covered for now are -\n\n        1.a USREDS + SYSEDS\n          b USREDS + BSM\n          c USREDS + ERS\n\n        2.a SYSEDS + ERS + Cellsrv\n          b BSM + ERS + Cellsrv\n\n        3. EGS + Cellsrv",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagebsmultiprocbswfence.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "sage_mirror_mode": "high",
      "setup_blockstore": "true"
    },
    "description": "tsagebsmultiprocbswfence.tsc - Test for BSW fencing for 3 cell setup\n\n1. 3 cell setup for blockstore (xrdbmsini with blockstore setup true)\n     2. Check fence alerts after shutting down combination of services\n     3  The cases covered for now are -\n\n        1. BSW + USREDS\n        2. BSW + BSM\n        3. BSW + ERS + Cellsrv",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagebsprocfence.tsc",
    "setup": null,
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagebsprocfence.tsc - Exascale EBS process fencing tests\n\n1. Single cell setup for blockstore (xblockini)\n     2. Check fence alerts after shutting down services\n     3. Services covered - USREDS, ERS, SYSEDS, BSM, BSW, EGS(restart)",
    "platform": null
  },
  {
    "test_name": "tsagebsprocfence_3cell.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high",
      "setup_blockstore": "true"
    },
    "description": "tsagebsprocfence_3cell.tsc - Test for EBS fencing for 3 cell setup\n\n1. 3 cell setup for blockstore (xrdbmsini with blockstore setup true)\n     2. Check fence alerts after shutting down services\n     3. Services covered - USREDS, ERS, SYSEDS, BSM, BSW, EGS(restart)",
    "platform": null
  },
  {
    "test_name": "tsagebsprocfence_restart_3cell.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high",
      "setup_blockstore": "true"
    },
    "description": "tsagebsprocfence_restart_3cell.tsc - Test for EBS fencing for 3 cell setup\n\n1. 3 cell setup for blockstore (xrdbmsini with blockstore setup true)\n     2. Check fence alerts after restarting service\n     3. Services covered - CELLSRV",
    "platform": null
  },
  {
    "test_name": "tsagebsstartuptimer_test.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagebsstartuptimer_test.tsc - Test to verify the order of\n                  various initializations using EbsStartupTimer\n\nAdditional: Test to verify the order of various initializations using EbsStartupTimer",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagebug36416262.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagebug36416262.tsc - <Test for bug fix >\n\n<This test added for bug 36416262 , A vault is created and 3000 files are added inside vault >\n     <After files have been added , we check ls --filter name=\"f*1\" --detail inside vault>",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagebug36416262_2.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagebug36416262_2.tsc - <bugfix test for bug 36416262>\n\n<This test is successor to tsagebug36416262.tsc with two filetypes in vault>",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagebug37103673.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagebug37103673.tsc - Test for bug37103673 with parallel log creation\n\nThis test creates two 2000G and 2G files in parallel\n     Then compare times and blockio values to check results\n     Finally dropping log groups created at end of test",
    "platform": null
  },
  {
    "test_name": "tsagebug37265698.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagebug37265698.tsc - test added in sharedtxn yixiali_bug-37265698\n\nTest for bug37265698, check roce or ib using is_roce.sh and checking msodl.trc",
    "platform": null
  },
  {
    "test_name": "tsagebug37910925.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagebug37910925.tsc - Test to check broken-log.xml once log.xml is broken and alerts are generated",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagecollectionlevel.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagecollectionlevel.tsc - Test collectionLevel for metrics collection\n\nSetting collectionLevel for cell to disable metrics file persistency\n     and purge",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagecstat_space_usage.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagecstat_space_usage.tsc - Functional test for -\n       1. ECStat space breakdown per FC Group\n       2. IO stats per FC Group\n\nThe txn jiahcai_architecture_space_breakdown_db_pdb_cluster added\n    a new table in the ECSTAT tool to display FlashCache usage for\n    each DB, PDB, and Cluster.\n\n    Test plans are documented on the Confluence pages below:\n    (1) Functional tests for ECStat space breakdown per FCGroup -\n        https://confluence.oraclecorp.com/confluence/display/EXD/Functional+tests+for+ECStat+space+breakdown+per+FCGroup\n    (2) Functional test for IO stats per FC group -\n        https://confluence.oraclecorp.com/confluence/display/EXD/Functional+test+for+IO+stats+per+FC+group\n\nAdded test in lrgdbconsaclstat",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagecstat_tool.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagecstat_tool.tsc - test for new tool called ecstat\n\nPlease see below",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsageds_ves4k_mdread.tsc",
    "setup": null,
    "flags": null,
    "description": "tsageds_ves4k_mdread.tsc - test for 1 cell VES bootstrap md read\n\nTest for minimising number of MD segments which are identified as\n     corrupted segments by mistake due to 1M MDAC read failure\n\n=== Test Steps ===\n      1. With 1 cell Exascale Setup\n      2. Create a vault DATA2 and file1 inside it\n      3. Set the Config parameters : _cell_server_event and _cell_ves_max_bad_ext_md\n      4. Restart cell and read to all 4K MDSeg reads should succeed\n      5. No bootstrap failure as all 4K MDSeg reads succeed\n      6. Remove the Config parameters added to cellinit.ora",
    "platform": null
  },
  {
    "test_name": "tsageds_ves_extent_leak.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsageds_ves_extent_leak.tsc - To avoid leakage of VES extents\n\nThis test validates the fix for below problem statement\n      1.Client#1 opens an EXC file and starts doing IOs\n      2.Usreds restarts (and loses info that the file is opened)\n      3.Client#2 deletes the file. EDS does not tell cellsrv  to revoke opened file handle (because it lost info about opened file).\n      4.Client #1 can still write data to the file, which results in leakage of VES extents\n\nThis test added in lrgsaexacldeds2",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagedscp.tsc",
    "setup": null,
    "flags": {
      "edscp_dbusrwallet": "^dbusrwallet^",
      "tmp_arg": "arg^argi^",
      "cur_arg": "^^tmp_arg^",
      "cur_value": "^^cur_arg^",
      "edscp_srcarea": "^tmp_srcarea^",
      "edscp_fromfile": "^edscp_srcpath^^edscp_fromfile^",
      "edscp_dstarea": "^tmp_dstarea^",
      "edscp_tofile": "^edscp_dstpath^^edscp_tofile^",
      "fsep": "'/'",
      "areasep": "':'",
      "lcl_chk_area": "FALSE",
      "lcl_updpath": "FALSE",
      "edscp_srcpath": "^^edscp_srcarea^^fsep^",
      "edscp_dstpath": "^^edscp_dstarea^^fsep^",
      "edscp_done": "TRUE"
    },
    "description": "tsagedscp.tsc - Calls cp2eds.sh to copy files in and out of EDS vaults",
    "platform": null
  },
  {
    "test_name": "tsagedsfiledltasync.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagedsfiledltasync.tsc - Functional tests for bug-37010738 (add support to free namespace for async eds file deletion)\n\nDefines tests for the txn which will make EDS file deletion async by default",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagedsimmutablesnap.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagedsimmutablesnap.tsc - First Functional test for EDS immutable snapshot project\n\nThe feature test validates the following functionality added with yujduan_eds_immutable_snap -\n   A. Using escli:\n   1) Set a valid mutable lock on a snapshotfile and ensure it is not deleted\n   2) Set a mutable lock with custom time zone offset\n   3) Update the mutable lock value\n   4) Unlock the mutable lock\n   5) Set an immutable lock which can not be updated\n   6) Filter locks by expiry time\n\n   B. Using edstool\n   1) Snaplock_Expirytime is set and gets listed in edstool info\n   2) Snapshot file cant be dropped if Snaplock_Expirytime is valid",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagedslibglobalfile.tsc",
    "setup": "xrdbmsini",
    "flags": null,
    "description": "tsagedslibglobalfile.tsc - Functional test for chayen_edslib_global_file_close\n\nWe can open an Exascale file in two ways: a \"regular\" open and a \"shared\" open:\n     - We can create a regular open anytime, and we can perform IO to a regular open anytime.\n     - We can only create a shared open when we already have a regular open.\n       (The shared open is \"sharing\" the regular open.)\n     - We can perform IO to a shared open as long as there's at least one regular open.\n     - When we close the last regular open, we can NO LONGER perform IO to the shared open.\n       Even if we create a new regular open, we still cannot use the old shared open for IO.\n\n     The tests spawn two or three sqlplus sessions, then use them to test regular and shared opens.\n     We open files, close files, and perform IO using ksfdx debug functions.\n\n     We should not close a sqlplus session mid-test unless we no longer need it for the rest of\n     that test. File opens are tracked per-session. Closing a session also closes all its open files\n     which will mess up the test.\n\n     We MUST serialize execution between the sessions. File opens, closes, and IO must occur\n     in the correct sequence. To synchronize, all sessions set up pipes via DBMS_PIPE. After\n     executing, a session will wait to receive a message from another session before continuing.\n\n     TEST 1 -\n       Loop 10x:\n         session 1 creates a tablespace + table each iteration.\n         session 2 drops that tablespace each iteration.\n       Before the txn, the tablespace drop would take ~40s. After, the tablespace drop takes ~10s.\n\n     TEST 2 -\n       Alternate opening, closing, and performing IO in two sqlplus sessions using ksfdx debug\n       commands. Each command has a comment which specifies whether it should succeed or fail.\n       The command should output \"success=1\" for a success and \"success=0\" for a failure.\n\n     TEST 3 (RTI 29702835) -\n       Use simulation events to trigger segfaults during file open and file close operations.\n       Make sure latch cleanup works correctly and doesn't lead to any asserts.\n\n     TEST 4 (Bug 37264948) -\n       These steps caused the database's primary edslib process to enter an infinite loop:\n         1. Open a file normally in session A, do shared opens of the file in sessions B and C.\n         2. Exit session A without closing the file (triggering edslib cleanup).\n         3. Close the file in session B.\n         4. Database is now stuck in an infinite loop and cannot open/close new files until\n            session C closes the file.\n       Make sure after step 3 that we can open/close files as expected.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagedsrman.tsc",
    "setup": null,
    "flags": {
      "media_type": "CF",
      "vault_db": "DATA",
      "cdb": "true",
      "db_noarchivelog_mode": "true",
      "num_cdbs": "2",
      "cdb_dd": "pdbs_across_cdbs",
      "num_dbs": "^num_cdbs^"
    },
    "description": "tsagedsrman.tsc - EDS RMAN test\n\nTesting of basic use of log_archive_dest and log_archive_format\n     init.ora parameters.  Plus testing of a simple RMAN backup/restore.\n\nOverall test plan is\n        1) get a database up and running\n        2) Get database into archive log mode, archiving to EDS destination\n        3) Create a tablespace in the database to serve as recovery target\n        4) Create table in the tablespace with some reference rows\n        5) Offline the tablespace\n        6) Use RMAN to perform a full backup of the tablespace.\n        7) \"erm\"/remove the tablespace files from the file system\n        8) bounce the datbase (needed to avoid a bug regarding db not closing\n           files).\n        9) use rman to restore and recover the tablespace\n       10) Online the tablespace and confirm the data is as expected.",
    "platform": null
  },
  {
    "test_name": "tsagedsrmanincr.tsc",
    "setup": null,
    "flags": {
      "vault_db": "DATA",
      "cdb": "true",
      "TDE_ENC_TS": "true",
      "vault_log": "DATA",
      "multisp": "true",
      "validate_multisp": "true",
      "db_noarchivelog_mode": "true"
    },
    "description": "tsagedsrmanincr.tsc - EDS RMAN incremental backup test\n\nTesting of basic backup incremental test\n\nOverall test plan is\n      Create tablespace/table on vault( say @DATA)\n      insert values(2500)  and take incremental backup(level 0)\n      save the scn  (#1)\n      insert few more rows(5000) and take incremental backup(level 1)\n      save the scn again\n#      insert few more rows(7500)\n      - remove(esh rm) the datafile on which we created tablespace\n      - restore and recover the database until say scn 1 or scn 2 or until current",
    "platform": null
  },
  {
    "test_name": "tsagedvsetup.tsc",
    "setup": null,
    "flags": {
      "compuser": "oracle",
      "oraclepasswd": "We1come$",
      "rootuser": "root",
      "rootpasswd": "welcome1",
      "exit_status": "'failed'"
    },
    "description": "tsagedvsetup.tsc - Setup File for DV on EDV\n\nSteps -\n     1. Reimaging and OEDA deployment from step 2 to 11\n     2. Remove pre-existing contents from - /u01/app/oracle/product/21.0.0.0/dbhome_1\n        and create new Directory for 19c DB\n     3. Create Vaults, volume and edvVolumeAttachments\n     4. Set permissions for both EdvAttachments and set permission for oracle user\n     5. Setup ASM on the 1st Guest node\n     6. Check ASM installed successfully or not\n     7. SCP db-klone from tftp server\n     8. Install DBSW\n     9. Install DBCA\n    10. Check if DBCA is installed or not",
    "platform": null
  },
  {
    "test_name": "tsageecstat_groupio.tsc",
    "setup": null,
    "flags": null,
    "description": "tsageecstat_groupio.tsc - Functional test for ECStat group IO stat\n\nThis script tests the correctness of FCGroupIOStat module with orion\n     runs to fake DBs.",
    "platform": null
  },
  {
    "test_name": "tsagegsarbiter.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef",
      "nflint": "1"
    },
    "description": "tsagegsarbiter.tsc - EGS servers arbiter unit tests\n\nEGS servers arbiter unit tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagegscancelcopy1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagegacancelcopy1.tsc - EGS Cancel Copy Test in Sandbox Setup\n\n1. SP created with 3 cells.\n   2. Run \"alter storagepool reconfig\" to add 3 more cells.\n       Once EGS reports that the command is completed, rebalance will start.\n   3. Sleep for a random interval from 1 to 30 seconds.\n   4. Run alter \"alter storagepool reconfig\" to drop 3 cells.\n       This command will interrupt the previous rebalance, resulting in EGS issuing CANCEL.\n       Once EGS reports that the command is completed, rebalance will start.\n   5. Sleep for a random interval from 1 to 30 seconds.\n   6. Run \"alter storagepool reconfig\" to add 3 more cells.\n       This command will interrupt the previous rebalance, resulting in EGS issuing CANCEL.\n   7. Wait for everything to complete.",
    "platform": null
  },
  {
    "test_name": "tsagegscancelcopy2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagegscancelcopy2.tsc - EGS Cancel Copy Test in Sandbox Setup\n\n1. SP created with 3 cells.\n   2. Make disks in Slot 0 on cells - 0, 1 and 2 go into ACCESSIBLE_BAD\n       This will cause EGS to rebalance in order to restore redundancy\n   3. Sleep for a random interval from 1 to 30 seconds\n   4. Make disks in Slot 1 on cell - 0, 1 and 2 go into ACCESSIBLE_BAD\n       This will cause EGS to interrupt the existing COPY operations\n       (and therefore issue CANCEL) and instead start new COPY operations.\n   5. Wait for Rebalance to complete and disks in Slots-0 and 1 to be DROPPED Gracefully.\n   6. Create new disks in Slot 0 on cells - 0, 1 and 2 in ACCESSIBLE_GOOD state.\n       This will cause EGS to rebalance\n   7. Sleep for a random interval from 1 to 30 seconds\n   8. Create new disks in Slot 1 on cells - 0 ,1 and 2 in ACCESSIBLE_GOOD state.\n       This will cause EGS to interrupt the existing COPY operations\n       (and therefore issue CANCEL) and instead start new COPY operations.\n   9. Wait for Rebalance to complete and disks in Slots-0 and 1 to be ADDED back.",
    "platform": null
  },
  {
    "test_name": "tsagegscancelsync1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagegacancelsync1.tsc - EGS Cancel Sync Test in Sandbox Setup\n\n1. SP created with 3 cells.\n   2. In a loop (say 30 iterations):\n       a. Make cell-1 go unreachable\n       b. Disks on cell-1 will go OFFLINE (add sleep of 58 seconds)\n       c. Start cell-1\n           EGS will start SYNC for disks on cell-1\n       d. Sleep for a random time anywhere from 20 second to 50 seconds\n       e. Goto Step (a)",
    "platform": null
  },
  {
    "test_name": "tsagegscancelsync2.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagegacancelsync2.tsc - EGS Cancel Sync Test in Sandbox Setup\n\n1. SP created with 3 cells.\n   2. In a loop (say 30 iterations):\n      a. Make disks in slots 0-3 of cell-1 go INACCESSIBLE_UNKNOWN.\n          These disks will be taken OFFLINE by EGS\n      b. Sleep for 5 seconds.\n   3. Make disks in slots 0-3 of cell-1 go ACCESSIBLE_GOOD.\n       EGS will start SYNC for slots 0-3 disks on cell-1\n   4. Make disks in slots 4-7 of cell-2 go INACCESSIBLE_UNKNOWN.\n       These slots 4-7 disks will be taken OFFLINE by EGS\n       But some of these disks will be Source of the SYNC to the disks\n         in slots 0-3 on cell-1.\n       So, EGS will have to issue CANCEL and then move to Partner Disks\n         to complete the SYNC for slots 0-3 disks on cell-1.\n   5. Sleep for 5 seconds.\n   6. Make disks in slots 4-7 of cell-2 go ACCESSIBLE_GOOD.\n       EGS will start SYNC for slots 4-7 disks on cell-2\n   7. Make disks in slots 8-11 of cell-3 go INACCESSIBLE_UNKNOWN.\n       These slots 8-11 disks will be taken OFFLINE by EGS\n       But some of these disks will be Source of the SYNC to the disks\n         in slots 0-3 on cell-1 as well as slots 4-7 on cell-2.\n       So, EGS will have to issue CANCEL and then move to Partner Disks\n         to complete the SYNC for slots 0-3 disks on cell-1 as well as\n          slots 4-7 disks on cell-2.\n   8. Wait till all the disks go ONLINE.\n   9. Goto Step 2a.",
    "platform": null
  },
  {
    "test_name": "tsagegsinst.tsc",
    "setup": null,
    "flags": {
      "egsinstlog": "egsinst^egsinstnum^_^egsinst^.lst",
      "egsinst_tmplog": "egsinsttmp.log",
      "newconfp": "OSSCONF^switchto_cell^",
      "newconf": "^t_work^/egs^egsinst^",
      "egstrcdir": "^ADR_BASE^^s^diag^s^EXC^s^exc^s^^HOSTNAME^^s^trace^s^"
    },
    "description": "tsagegsinst.tsc - egsinst macro\n\nused as macro -\n    Switches context between different EGS instances\n    Also used to set context to EGS leader alert log\n\n    The correct way to use this is either:\n    1)   egsinst <inst> [setupegs] [only_vars]\n   - Sets up EGS instance <inst> wrt trust store and preparing MS files\n     for Cellcli interaction\n   - setupegs is a special parameter passed by exastackup during initial setup",
    "platform": null
  },
  {
    "test_name": "tsagegsioctltmout.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagegsioctltmout.tsc\n\nif EGS leader broadcasts a fence due to a server OFFLINE, the OFFLINE\n   job will wait for at most 5 minutes\n\ntest added in lrgsaexcmemfence1",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsageinternalsystemcellinit.tsc",
    "setup": null,
    "flags": null,
    "description": "tsageinternalsystemcellinit.tsc\n\nTest execution script for jiayawan_set_internal_system_cellinit",
    "platform": null
  },
  {
    "test_name": "tsagemailattachment.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagemailattachment.tsc - Test diagpack attachment behavior for alert\n                               emails\n\nThis test verifies the behavior introduced in yfliu_email-no-attachment,\n     ensuring that diagpacks are not attached to alert emails by default,\n     that attachments respect the diagPackEmailAttach flag, and that the\n     hidden alertEmailAttachmentSize attribute correctly governs whether\n     diagpacks are included based on size, thereby validating both default and\n     configurable attachment handling.\n\nadded in lrgrhx9sams4.",
    "platform": null
  },
  {
    "test_name": "tsageminterface1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsageminterface1.tsc - Test for cellcli commands on EM interface",
    "platform": null
  },
  {
    "test_name": "tsagempty.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagempty.tsc - Generates to dummy .suc to avoid no suc/dif WRN\n\nGenerates to dummy .suc to avoid no suc/dif WRN\n\nThis test doesn't test any functionality.",
    "platform": null
  },
  {
    "test_name": "tsagemptyvaulttest.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagemptyvaulttest.tsc - A bug fix test with 3 test scenarios to check snap files gets deleted in vault\n                            and vault delete is successful.\n\nA bug fix test with 3 test scenarios to check snap files gets deleted in vault\n                            and vault delete is successful.\n\nTest steps:\n\n  Test scenario 1\n    1. Create a volume\n    2. Create 2 snapshots from that volume\n    3. Create a backup from second snapshot\n    4. do ls to the vault and confirm EDS files existence:\n        1 volume file\n        2 snapshot files\n    5. delete second snapshot\n    6. do ls to the vault, the snap EDS file will remain there\n    7. delete first snapshot\n    8. do ls to the vault and confirm the first snap EDS file is gone\n    9. delete the volume\n    10. do ls to the vault and confirm that snap and volume EDS files are gone (vault will be empty)\n    11. Delete vault\n  Test scenario 2\n    1. Create a volume\n    2. Create 2 snapshots from that volume\n    3. Create a backup from second snapshot\n    4. do ls to the vault and confirm EDS files existence:\n        1 volume file\n        2 snapshot files\n    5. delete the volume created\n    6. confirm volume EDS file was deleted from the vault\n    7. delete second snapshot\n    8. do ls to the vault, the snap EDS file should be deleted\n    9. delete first snapshot\n    10. Do ls to the vault and confirm the first snap EDS file is gone (vault will be empty)\n    11. Delete vault\n  Test scenario 3\n    1. Create a volume\n    2. Create a snapshot from that volume\n    3. Create a backup from snapshot\n    4. confirm volume EDS file was deleted from the vault\n    5. delete snapshot\n    8. do ls to the vault, the snap EDS file should be deleted\n    9. do ls to the vault and confirm the first snap EDS file is gone (vault will be empty)\n    10 Delete vault",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagenc1.tsc",
    "setup": null,
    "flags": {
      "test_block_encrypt": "true",
      "auto_undo_management": "true",
      "enc_wallet_loc": "^t_work^",
      "oslfn1": "dbs^database^_logfile_1_1_fname",
      "oslfn2": "dbs^database^_logfile_1_2_fname",
      "osfilename": "^dbs1_undofile_0_1_fname^"
    },
    "description": "tsagenc1.tsc - Test Kernel Buffer Cache ENCryption test 1, Clone tkbcenc1.tsc\n\nCreate tablespaces for block sizes of 4K, 8K (two) and 16K, each with a\n   different encryption algorithm (3DES168, AES128, AES192, and AES256).\n   While actively doing updates to all tablespaces, kill the instance and\n   start up so recovery happens.  Verify consistency of the data.  Use\n   \"fgrep\" on a unique string in the datafile that was encrypted to verify\n   that it indeed does not appear in clear text in the datafile, log files,\n   or undo tablespace.",
    "platform": null
  },
  {
    "test_name": "tsagenc13.tsc",
    "setup": null,
    "flags": {
      "auto_undo_management": "true",
      "compatible": "^def_compatibility^",
      "test_conn": "'system/manager'",
      "file_prefix": "^cc_mode^_sys"
    },
    "description": "tsagenc13.tsc - Test Kernel Buffer Cache ENCryption test 13 - Clone tkbcenc13.tsc\n\nCreate Table As Select and Alter Table Move\n     Verify that \"create table as select\"  will successfully relocate an\n     encrypted table to a tablespace created with a different encryption_spec.\n     Also verify that the table can be effectively decrypted by doing \"create\n     table as select\" to a non-encrypted tablespace.\n     Verify that \"alter table move\" will successfully relocate an encrypted\n     table to a tablespace created with a different encryption_spec.  Also\n     verify that the table can be effectively decrypted by doing \"alter table\n     move\" to a non-encrypted tablespace.",
    "platform": null
  },
  {
    "test_name": "tsagenc15.tsc",
    "setup": null,
    "flags": {
      "auto_undo_management": "true",
      "test_block_encrypt": "true",
      "compatible": "^def_compatibility^"
    },
    "description": "tsagenc15.tsc - Test Kernel Buffer Cache ENCryption test 15, Clone tkbcenc15.tsc\n\nInteractions with Wallet.\n   Do multiple 'alter system set encryption key identified by \"welcome1\";'\n   commands.  They should not give an error, not rekey datafile keys.\n   Create an encrypted table within a tablespace.  Shut down and start up\n   (without accessing that tablespace).  Open the Wallet, then close it.\n   Access to the table within this tablespace should fail.  Note: this is\n   the same as how TDE functions; it would fail with:\n      \"ORA-28365: wallet is not open\".\n   Do a shutdown abort so that recovery is necessary upon startup.  Close the\n   wallet.  Verify that recovery cannot proceed when database is started\n   (fail with \"ORA-28365\").  Open the wallet and verify that\n   recovery can proceed.",
    "platform": null
  },
  {
    "test_name": "tsagenc17.tsc",
    "setup": null,
    "flags": {
      "test_block_encrypt": "true",
      "auto_undo_management": "true"
    },
    "description": "tsagenc17.tsc - Test Kernel Buffer Cache ENCryption test 17,Clone tkbcenc17.tsc\n\n1a. Underscore parameter _db_writer_coalesce_write_limit adjustment test\n       -- encryption should work even if _db_writer_coalesce_write_limit is\n       set to 0\n    1b. Underscore parameter _db_writer_coalesce_area_size boundry\n       verification test.  The new hard minimum is min(1% buffercache size,\n       1MB).  If not specified, it should be set to min(1% buffercache size,\n       4MB).\n    1c. Underscore parameter _db_writer_nomemcopy_coalesce test.  No-memcopy\n       coalescing is now a separate underscore parameter, and should work\n       for both encryption and no-encryption cases.\n\n    2. (Coverage) Test of DBV on datafile with encrypted blocks\n\n    3. (Coverage) Test of BBED on datafile with encrypted blocks",
    "platform": null
  },
  {
    "test_name": "tsagenc4.tsc",
    "setup": null,
    "flags": {
      "test_block_encrypt": "true",
      "recovery_seeddb": "true",
      "auto_undo_management": "true",
      "compatible": "^max_compatibility^",
      "x": "3  # Number of processes using same rollback seg",
      "y": "3  # Number of rollback segs used in test"
    },
    "description": "tsagenc4.tsc - Test Kernel Buffer Cache ENCryption\n                    save undo test 4, Clone tkbcenc4.tsc\n\nWhile an encrypted tablespace is being updated, take the tablespace\n   offline so that SAVE UNDO will be stored in the SYSTEM tablespace.  Bring\n   the tablespace back online and verify in the encrypted table that all\n   changes are correctly reflected (see tkbcmbs0.tsc for a guideline on this).\n   Also verify that cleartext for 'encryptedstring' does not appear within\n   SYSTEM tablespace (file tbs_01.f in recovery database).\n\n   Fork 9 test scripts which generate DML without committing\n   then they perform a rendezvous with a process that takes the tablespace\n   containing the table offline.  Then all active transactions call rollback.\n   When the tablespace is onlined again, verify that the changes are\n   indeed rolled back and verify through statistics that the multiblock\n   save undo is applied.\n\nBased upon test tkbcmbs1",
    "platform": null
  },
  {
    "test_name": "tsagenc6.tsc",
    "setup": null,
    "flags": {
      "compatible": "^max_compatibility^",
      "auto_undo_management": "true",
      "osfilename": "^dbs1_datafile_6_1_fname^"
    },
    "description": "tsagenc6.tsc - Test Kernel Buffer Cache ENCryption 6, Clone tkbcenc6.tsc\n\nTest to show how existing TDE security weakness can be avoided.\n   Create a scenario in which sort data gets spilled in clear text to a\n   temporary tablespace.  Then encrypt the base table and verify that\n   the same clear text data doesn't appear in the temp tablespace.\n   Then a similar subtest is done with a hash join that spills.\n\nThis test is based upon tkbcmst.tsc",
    "platform": null
  },
  {
    "test_name": "tsagenc9.tsc",
    "setup": null,
    "flags": {
      "test_block_encrypt": "true"
    },
    "description": "tsagenc9.tsc - Test Kernel Buffer Cache ENCryption test 9, Clone tkbcenc9.tsc\n\nCreate a partitioned table that spans tablespaces, including ones\n   encrypted and unencrypted.",
    "platform": null
  },
  {
    "test_name": "tsagend.tsc",
    "setup": null,
    "flags": {
      "tmp_nowarn": "^tst_nowarn^",
      "python_path": "^ADE_VIEW_ROOT^/python/bin",
      "tempnowarn": "^TST_NOWARN^",
      "argzz": "asm_allow_sysdba",
      "dumpmsg": "'WARNING: State dump processing without holding lock'",
      "celltrcdir": "^t_diag^^s^diag^s^asm^s^cell",
      "dbusrwallet": "^ORACLE_BASE^/admin/^ORACLE_SID^/eswallet",
      "clean_gi_flag": "' -c'"
    },
    "description": "tsagend.tsc - cleanup at the end of SAGE lrgs\n\nScript called by rdbmscln to cleanup and do whatever is needed at the\n     end of SAGE lrgs.",
    "platform": null
  },
  {
    "test_name": "tsagentbsini.tsc",
    "setup": null,
    "flags": {
      "db_con": "'system/manager'",
      "tbs_name": "TBS_1",
      "columnar_cache": "1",
      "imcfclvwd": "'cellmemory '^imcfclvwd^",
      "cclvwd": "'compress for query high'"
    },
    "description": "tsagentbsini.tsc - prepare for the CC converting tests in encrption tablespace\n\nprepare for the CC converting tests in encrption tablespace",
    "platform": null
  },
  {
    "test_name": "tsageoedaexascaleunittests.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagerhfndd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagerhfndd.tsc - FNDD RHW TEST for Half rack exascale deployment\n\nThis Test runs on EGS leader , kill cellsrv\n     reboot the egs and find new egs to compare logs for 5042 port\n     Test steps as follows :\n     1)Find EGS LEADER\n     2)Kill cellsrv -> reboot current EGS LEADER node\n     3)find newly elected EGS LEADER\n     4)Compare logs for test criteria of differnce between logs<15 seconds",
    "platform": null
  },
  {
    "test_name": "tsagerhmountcrash.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagerhmountcrash.tsc - crash mount test\n\nTest added for bare metal , KVM , DOM\n\nKVM Host : Crashfile directory is /EXAVMIMAGES/crashfiles\nKVM Guest : Crashfile directory is //crashfiles\nDOM0 : Crashfile directory is /EXAVMIMAGES/crashfiles\nDOMU : Crashfile directory is /u01/crashfiles\nBare Metal Compute : Crashfile directory is /u01/crashfiles\nBare Metal Cell : Crashfile directory is /var/log/crashfiles",
    "platform": null
  },
  {
    "test_name": "tsagerror_tracker.tsc",
    "setup": null,
    "flags": {
      "SLEEP_TIME": "600",
      "TRACE_FILE": "^t_work^/tsagerror_tracker_tracefile.dat"
    },
    "description": "tsagerror_tracker.tsc - driver script for error tracking\n                             and sending mail or aborting lrg\n\nThis is the driver script for tracking specific errors in\n     lrgs using tsagexastackup and either send mail and abort the\n     lrg or just send the notification mail incase the error is\n     encountered. The script runs in the background and\n     automatically ends when lrg execution is finished.\n\nUsage : farm submit <lrg> -config \"TRACK_ERROR=true;\n              ERROR_MSG=error1|error2|error3;MODE=ABORT/MAIL;\n              TO_MAIL=<email_address>\"\n     TRACK_ERROR=true : to enable this function\n     ERROR_MSG=error1|error2|error3 : error messages to track\n               different messages need to be seperated by |\n     MODE=MAIL for email alert only, abort for email alert and\n               ending of lrg.\n     TO_MAIL=<email_address> :address where the notif will be sent\n\n     All of these parameters are necessary for the tracking to work\n\n     Currently the files being tracked are mentioned in\n     tsagerror_tracker_tracefile.dat. Please add full location of\n     any other file which you want to track.",
    "platform": null
  },
  {
    "test_name": "tsagerrsimoff.tsc",
    "setup": null,
    "flags": {
      "semicolon": "';'",
      "colon": "':'",
      "substr": "'evarg1='",
      "evargs": "'evarg1=none'"
    },
    "description": "tsagerrsimoff.tsc - turn off error simulation\n\nInvoked by tsagflashio.tsc to turn off error simulation.",
    "platform": null
  },
  {
    "test_name": "tsagerrsimon.tsc",
    "setup": null,
    "flags": {
      "errFrequency": "1000",
      "errCount": "5",
      "semicolon": ";",
      "colon": ":",
      "thisevargs": "count=^errCount^",
      "substr": "CORRUPT_DATA"
    },
    "description": "tsagerrsimon.tsc - turn on error simulation\n\nInvoked by tsagflashio.tsc to turn on error simulation event.",
    "platform": null
  },
  {
    "test_name": "tsagersintcall.tsc",
    "setup": null,
    "flags": {
      "ers_call_timeout_threshold": "5"
    },
    "description": "tsagersintcall.tsc - Find count of ERS internal call timeouts\n\nRuns at the end of lrg to find ERS internal timeouts",
    "platform": null
  },
  {
    "test_name": "tsagersreconfnc.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagersreconfnc.tsc â Test for ERS fencing only upon reconnection.\n\nThis test covers the case when EGS leader only fences the services and bumps up the EXC node\n     incarnation if the RECONNECT error was due to the below reasons.\n     \t1. The peer service restarted, which means it has a newer OSS incarnation number.\n     \t2. The peer service is truly in a bad state -- Some IOCTL doesn't complete in 30 minutes.\n     Otherwise, EGS leader will just re-ONLINE the service with the same EXC node incarnation.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagesclichvolume.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagesclichvolume.tsc - Test for chvolume\n\nAdditional: Tests for escli chvolume command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagesclidatefilter.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagesclidatefilter.tsc - functional test for escli date/time filters\n\nTest covers 4 cases\n     1. Using time only\n     2. Using date only\n     3. testing timezone offset\n     4. testing invalid date/time input format",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagesclirmblockdevices.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagesclirmblockdevices.tsc - escli test cases for rmvolumesnapshotattachment, rmvolumesnapshot, rmvolumeattachment, rmvolume, rmvolumebackup",
    "platform": null
  },
  {
    "test_name": "tsagesclivaultfileattr.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagesclivaultfileattr.tsc - ESCLI test for file and vault attributes\n\nThis is an ESCLI test for --attributes option of mkfile+chfile and\n     mkvault+chvault commands.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagesclivolattachment.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagesclivolattachment.tsc - Escli test cases for mkvolumeattachment+lsvolumeattachment",
    "platform": null
  },
  {
    "test_name": "tsagesclivolbackup.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagesclivolbackup.tsc - Test cases for mkvolumebackup+lsvolumebackup\n\nAdditional: Test cases for escli commands mkvolumebackup and lsvolumebackup",
    "platform": null
  },
  {
    "test_name": "tsagesclivolsnapattachment.tsc",
    "setup": null,
    "flags": null,
    "description": "tvolumesnapshotattachment.tsc - Escli test for mkvolumesnapshotattachment+lsvolumesnapshotattachment\n\nAdditional: escli test for mkvolumesnapshotattachment and lsvolumesnapshotattachment",
    "platform": null
  },
  {
    "test_name": "tsagesclivolsnapshot.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagesclivolsnapshot.tsc - Test cases for mkvolumesnapshot+lsvolumesnapshot\n\nAdditional: Test cases for mkvolumesnapshot and lsvolumesnapshot",
    "platform": null
  },
  {
    "test_name": "tsagesclivolume.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagesclivolume.tsc - Escli test for mkvolume+lsvolume\n\nAdditional: Tests for escli commands mkvolume and lsvolume",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagesnpnodefence.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagesnpnodefence.tsc - test to add fence job for esnp\n\n1. simulate egsevent to start fence job\n     2. look for traces  in egs trace dir\n     3. shutdown esnp & looks for traces",
    "platform": null
  },
  {
    "test_name": "tsageventreg.tsc",
    "setup": "tsagnini",
    "flags": {
      "log_file": "tsageventreg.log"
    },
    "description": "tsageventreg.tsc - Event registry and Event Log test\n\n< DESCRIPTION >",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagevictfm.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagevictfm.tsc - Test the functionality to evict a fenced node metadata entry.\n\nAdditional: Test the functionality to evict a fenced node metadata entry in an Exascale environment.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexa_miscdoubfail.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_multims_testing": "true",
      "quorum_fg": "true",
      "uniq_dsknames": "all",
      "oss_auto_manage_disks": "true",
      "sage_mirror_mode": "high",
      "oss_asm_sec": "true",
      "cdb": "true",
      "creatdev_file": "tsagfcgdef"
    },
    "description": "tsagexa_miscdoubfail.tsc - misc double failure test on EXADATA/ASM\n\nADDS 3 TESTS:\n    1- RESYNC + REBAL\n    2- RESYNC + RESILVER\n    3- RESYNC + RESYNC\n    4- REBAL + REBAL\n    5- REBAL + SCRUB\n\nto be added in lrgdbcsaexadoubfail",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagexaclactvrsync.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexaclactvrsync.tsc - Exascale Active client Resync Test\n\nStart IOV in background\n     cellsrv Shutdown/wait for alert\n     cellsrv startup/Wait for alert\n     Verify IOV completed fine till end",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexaclactvrsync2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexaclactvrsync2.tsc - Exascale Active client Resync Test\n\nStart IOV in background\n     cellsrv Shutdown/wait for alert\n     cellsrv startup/Wait for alert\n     Verify IOV completed fine till end",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexaclactvrsync_all.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexaclactvrsync.tsc - Exascale Active client Resync Test\n\nStart IOV in background\n     cellsrv, syseds, usreds, ers and egs leader Shutdown/wait for alert\n     cellsrv, syseds, usreds, ers and egs startup/Wait for alert\n     Verify IOV completed fine till end",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldasm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexacldasm.tsc - Basic DB tests on ExaScale and ASM\n\nSets up ASM and Exascale stack",
    "platform": null
  },
  {
    "test_name": "tsagexacldbsw.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexacldbsw.tsc - block device + Exascale stack\n\nRuns block store I/O stack",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldbsw_curl.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexacldbsw_curl.tsc - Blockstore tests using curl commands\n\nTests various blockstore functions like create volume, attach volume,\n    delete volume, etc using curl scripts to add ERS coverage",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldcert.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexacldcert.tsc - Arbiter Test for trusted certificates\n\nSetup tstores, use orapki for certificate removals and verify arbiter fixes them\n\nAdded to lrgsaexacldeds",
    "platform": null
  },
  {
    "test_name": "tsagexacldclsrvloop.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexacldclsrvloop.tsc - Exascale test for cellsrv online/offline disk alerts\n\nTest verifies disk online/offline alerts using cellsrv shutdown/sartup\n1.Setup with sage_mirror_mode=high\n2.Case1 - Check alerts for 1 cellsrv instance shutdown/startup\n3.Case2 - Check alerts for 1 cellsrv instance shutdown/startup in loop\n4.Case3 - Check alerts for 2 cellsrv instance shutdown/startup\n5.Case4 - Check alerts for 3 cellsrv instance shutdown/startup",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldclsrvnowaitup.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexacldclsrvnowaitup.tsc - Exascale test for cellsrv\n\t\t\tstartup/shutdown without wait\n\n1.Setup\n    \t2.Shutdown cellsrv, dont' wait for alerts\n    \t3.Startup cellsrv rightaway\n    \t4.Check for no ORA-600 alerts",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexaclddb.tsc",
    "setup": null,
    "flags": {
      "trace": "true",
      "AUTO_LOCAL_UNDO": "true",
      "skip_set_file_dest": "true",
      "dsk_size": "large",
      "testlog": "^tst_tscname^_cdb_3cells"
    },
    "description": "tsagexaclddb.tsc - Basic DB tests on ExaScale\n\nSets up DB on Exascale stack",
    "platform": null
  },
  {
    "test_name": "tsagexaclddbworkload.tsc",
    "setup": null,
    "flags": {
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true",
      "cdb": "true",
      "sage_mirror_mode": "high",
      "num_cells": "^num_cells^",
      "maxtimeout": "10800"
    },
    "description": "tsagexaclddbworkload.tsc - db workload script for exascale\n\nThis script setups db on exascale and create tablespace/table\n\ntest to be added in lrgdbcsaexacldrebal2_db",
    "platform": null
  },
  {
    "test_name": "tsagexacldedv.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexacldedv.tsc - EDV + Exascale Stack\n\nRuns EDV I/O Stack",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexaclders.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexaclders.tsc - Tests for ERS\n\nThis suite contains tests for ERS in Exascale",
    "platform": null
  },
  {
    "test_name": "tsagexacldersfail.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexacldersfail.tsc - Test for ERS commands in a loop\n\nTest runs a bunch of escli commands with 4 different users in loop simultaneously.",
    "platform": null
  },
  {
    "test_name": "tsagexacldesh.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexacldesh.tsc - Test for xsh tool\n\nThe test checks for different functionalities like :\n     xsh cp,xsh cat,xsh dd,od,strings",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldheap.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexacldheap.tsc - SGA HEAP size of Exascale processes\n\nPrints current Heap size of cellsrv, EGS, syseds, usreds, ERS, bsw, bsm",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldiscsiedv.tsc",
    "setup": null,
    "flags": {
      "TKFV_BUILD_EDVMOD": "1",
      "dsk_size": "xl",
      "egs_deplmode": "sharedCloud",
      "test_for_bug34754787": "false",
      "tmp_nowarn": "^tst_nowarn^"
    },
    "description": "tsagexacldiscsiedv.tsc - Test volume operations with EDV setup\n\nTest volume operations with EDV setup",
    "platform": null
  },
  {
    "test_name": "tsagexacldmircorpt.tsc",
    "setup": null,
    "flags": {
      "vault_db": "DATA",
      "dsk_size": "large",
      "cdb": "true",
      "db_noarchivelog_mode": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexacldmircorpt.tsc - Test to be added in lrgdbcsaexacldcrptmir\n\ntest for mirror side read/write support on Exascale file system.",
    "platform": null
  },
  {
    "test_name": "tsagexacldrebaldb1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexacldrebaldb1.tsc - rebal test with db workload\n\nActive client disk pull/push health status check test\n\ntest to be added in lrgdbcsaexacldrebal2_db",
    "platform": null
  },
  {
    "test_name": "tsagexacldrebaldb2.tsc",
    "setup": null,
    "flags": {
      "media_type": "HC,EF",
      "NUM_FLASH_PER_CELL": "4"
    },
    "description": "tsagexacldrebaldb2.tsc - rebal test with active db test\n\nActive client disk fail/drop/add rebalance test\n\ntest to be added in dbcsaexacldrebal2_db",
    "platform": null
  },
  {
    "test_name": "tsagexacldrebaldb3.tsc",
    "setup": null,
    "flags": {
      "media_type": "HC,EF",
      "NUM_FLASH_PER_CELL": "4"
    },
    "description": "tsagexacldrebaldb3.tsc - rebal test with active db workload\n\nActive client graceful disk drop test\n\ntest to be added in lrgdbcsaexacldrebal2_db",
    "platform": null
  },
  {
    "test_name": "tsagexacldrebaldb4.tsc",
    "setup": null,
    "flags": {
      "media_type": "HC,EF",
      "NUM_FLASH_PER_CELL": "4"
    },
    "description": "tsagexacldrebaldb4.tsc - disk inactive/active + cell offline/online test\n                             with active db workload and _egs_server_disk_expiry_minutes=5\n\nTest steps:\n   Test set _egs_server_disk_expiry_minutes=5 timeout for below cases\n\n 1. Shutdown a cellsrv. Wait for Egs to offline the griddisks on it.\n\n 2. Then wait for the timeout  set, like 5mins and check if Egs has\n    force dropped the disks.\n\n 3. Start up cellsrv. Egs will add the griddisks back and trigger rebalance.\n    Wait till the griddisks are online.\n\n     Test 2-:\n\n    alter griddisk xxx inactive and online the disk in step3 with alter griddisk xxx active\n\ntest to be added in lrgdbcsaexacldrebal2_db",
    "platform": null
  },
  {
    "test_name": "tsagexacldresilver.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high",
      "delta_sync": "true",
      "flash": "true",
      "cmd_mirror_mode": "normal",
      "phase_dur": "40",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagexacldresilver.tsc - exascale test for resilver\n\n- setup 3 cells\n     - run iov workload on eds file\n     - fail flash\n     - wait for resilvering to complete\n     - make sure md5sum matches for all the mirrors",
    "platform": null
  },
  {
    "test_name": "tsagexacldresilver_snap.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high",
      "delta_sync": "true",
      "flash": "true",
      "cmd_mirror_mode": "normal",
      "phase_dur": "30",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagexacldresilver_snap.tsc - exascale test for resilver for snapshots xmls\n\n- setup 3 cells\n     - run iov workload on eds file\n     - fail flash\n     - wait for resilvering to complete\n     - make sure md5sum matches for all the mirrors",
    "platform": null
  },
  {
    "test_name": "tsagexacldrest.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "egs_trace": "highest",
      "egs_trace_file_rolling": "true"
    },
    "description": "tsagexacldrest.tsc - Added in  lrgsaexacldeds :Test for EbsRest\n\n# bring the stack up\n   # set the following parameters in cellinit.ora :\n     _exc_phasestats_timer_mode=1\n     _ebts_server_event=trace[Exc_EbsRest.*] disk=high\n    start ebstestsrv\n    \"ebs_simevent[TESTRES_REST] count=1,evarg1='https://www.oracle.com/index.html\"\n      check for traces HTTP establishment traces",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldresyncdb1.tsc",
    "setup": null,
    "flags": {
      "media_type": "HC,EF",
      "NUM_FLASH_PER_CELL": "4",
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on",
      "admin_wallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexacldresyncdb1.tsc - test for resync feature with active db\n\ntest shutdowns/startup all cell services while db wkload is running.\n\nTest to be added in lrgdbcsaexacldresync2_db",
    "platform": null
  },
  {
    "test_name": "tsagexacldresyncdb1_qlc.tsc",
    "setup": null,
    "flags": {
      "NUM_FLASH_PER_CELL": "4",
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on",
      "admin_wallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexacldresyncdb1_qlc.tsc - test for resync feature with active db\n\ntest shutdowns/startup all cell services while db wkload is running.\n\nTest to be added in lrgdbcsaexcresyncdb_qlc\n   this test is same as tsagexacldresyncdb1.tsc but with QLC DISKS",
    "platform": null
  },
  {
    "test_name": "tsagexacldresyncdb2.tsc",
    "setup": null,
    "flags": {
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on"
    },
    "description": "tsagexacldresyncdb2.tsc - test for resync operation with active db\n\nwhile db workload is running , so alter griddisk inactive\n#   (make atleast 2-3 pooldisks inactive)+ active.\n\ntest to be added in lrgdbcsaexacldresync2_db",
    "platform": null
  },
  {
    "test_name": "tsagexacldresyncdb3.tsc",
    "setup": null,
    "flags": {
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on"
    },
    "description": "tsagexacldresyncdb3.tsc - resync operation with db workload\n\nwhile db workload is running, offline/online a pooldisk from\n#    escli - chpooldisk <disk id> --offline\n\nTest to be added in lrgdbcsaexacldresync2_db",
    "platform": null
  },
  {
    "test_name": "tsagexacldresyncdb4.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "media_type": "HC,EF",
      "NUM_FLASH_PER_CELL": "4",
      "mixed_workload": "true",
      "maxpdb": "2",
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on",
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexacldresyncdb4.tsc - source cell reconnect during resync test\n\nPls see below.\n\nTest to be added in lrgdbcsaexacldresync2_db3",
    "platform": null
  },
  {
    "test_name": "tsagexacldrman.tsc",
    "setup": null,
    "flags": {
      "dsk_size": "large",
      "vault_db": "DATA",
      "cdb": "true",
      "db_noarchivelog_mode": "true"
    },
    "description": "tsagexacldrman.tsc - test various rman operations with exascale\n\nIncludes testcases like TSPITR,flashback recovery,duplicate db\n\ntest added in lrgdbcsaexacldrman3",
    "platform": null
  },
  {
    "test_name": "tsagexacldrmanresfservice.tsc",
    "setup": null,
    "flags": {
      "media_type": "HC,EF",
      "NUM_FLASH_PER_CELL": "6",
      "PWFILE_ON_EXC": "false",
      "dsk_size": "large",
      "vault_db": "DATA",
      "cdb": "true",
      "db_noarchivelog_mode": "true",
      "cmd_mirror_mode": "normal",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexacldrmanresfservice.tsc - Create a physical standby using restore from service\n\n1. Create physical standby using restore from service\n    2. Primary and standby uses separate wallets and vaults, i.e.,\n       primary can't write to standby's vault and vice-versa.\n    3. Setup runs with Local UNDO\n\nTest added in lrgdbcsaexacldrman4 and lrgdbcsaexacldrman5",
    "platform": null
  },
  {
    "test_name": "tsagexacldropusr.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "dsk_size": "medium"
    },
    "description": "tsagexacldropusr.tsc - Test to verify user drop functionality\n\n1. Test-1: verifying drop user functionality\n    2. Test-2: Drop Users Garbage Collection test\n    3. Test-3: RAFT nodes test",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldrsvractvclnt.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "delta_sync": "true",
      "flash": "true",
      "cmd_mirror_mode": "normal",
      "phase_dur": "120",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagexacldrsvractvclnt.tsc - exascale test for resilver with active client\n\n- setup 3 cells\n     - run iov workload in background on eds file\n     - fail flash\n     - wait for resilvering to complete\n     - make sure md5sum matches for all the mirrors",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldrsvractvclnt_snap.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "delta_sync": "true",
      "flash": "true",
      "cmd_mirror_mode": "normal",
      "phase_dur": "30",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagexacldrsvractvclnt_snap.tsc - exascale test for resilver for snapshots xmls with active client\n\n- setup 3 cells\n     - run iov workload in background on eds file\n     - fail flash\n     - wait for resilvering to complete\n     - make sure md5sum matches for all the mirrors",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldsearch.tsc",
    "setup": null,
    "flags": {
      "egs_deplmode": "exacc",
      "vault_db": "DATA",
      "dsk_size": "large",
      "cdb": "true",
      "db_noarchivelog_mode": "true"
    },
    "description": "tsagexacldsearch.tsc - test for search feature to  find all\n     files that match the specified pattern(optionally recursive search)\n      support on Exascale file system.\n\ndetails below\n\nadded in lrgdbcsaexacldsearch",
    "platform": null
  },
  {
    "test_name": "tsagexacldsprautomnt5.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4",
      "media_type": "HC,EF",
      "devdir1": "^T_WORK^/raw^oss_port^",
      "devdir2": "^T_WORK^/raw^oss_port2^",
      "devdir3": "^T_WORK^/raw^oss_port3^",
      "devdir4": "^T_WORK^/raw^oss_port4^"
    },
    "description": "1. create HC storagepool with 4 cells\n     2. create EF storagepool with those same 4 cells\n     3. OFFLINE cell-1\n     4. OFFLINE cell-2\n        NORMAL Redundancy of @data and @reco for both HC and EF should go OFFLINE\n     5. OFFLINE cell-3\n  HIGH Redundancy of @data and @reco for both HC and EF should go OFFLINE\n     6. At this point @containermd Ring should also go OFFLINE\n     7. Switch on all cells in randomized order and expect POODisks and StoragepoolRings -HIGH, NORMAL    #         come back ONLINE.",
    "platform": null
  },
  {
    "test_name": "tsagexacldsprmnt1.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "multisp": "true"
    },
    "description": "tsagexacldsprmnt1.tsc - SP automount Test 1\n\nTests that StoragePoolRing is offlined/Online when disks\n      that form a partnerhip group are all offlined/Onlined\n\n3 cell setup",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldsprmnt2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "multisp": "true",
      "ctrl": "localhost:'^nginx_https_port^"
    },
    "description": "tsagexacldsprmnt2.tsc - SPRing Auto mount test#2\n\nSPRing Mount Test\n     3 disks of 1 partnetship group  are offline\n     1 disk from another partner group is offline\n     online operation of this disk is cancelled as @data is offline\n     Online all other disks 1 by 1 so that @data is mounted/online again\n     The 4th disk should now be ONLINE.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldsprmnt3a.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "multisp": "true",
      "ctrl": "'--ctrl localhost:'^nginx_https_port^"
    },
    "description": "tsagexacldsprmnt3a.tsc - Shutting down all cellsrv one by one\n\nDismounts and Remounts StoragePoolRings by shutting down all cells\n     one by one",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldsprmnt3b.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "multisp": "true"
    },
    "description": "tsagexacldsprmnt3b.tsc - SP test\n\nTest Steps -\n      1. Setup exascale stack with 3 cells\n      2. Shutdown all services except EGS on all the 3 cells\n      3. Wait until all the pooldisks and SP rings are offline\n      4. Restart all 3 EGS Servers\n      5. After the cluster is Operational, check that no SP\n            rings were onlined",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldsprmnt4.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "disk_offline_hang": "0",
      "sp_online_fail": "0",
      "sp_online_cancel": "0"
    },
    "description": "tsagexacldsprmnt4.tsc - SP test\n\nTest Steps -\n      1. In a multiple cell env, offline the disks one by one which will\n         make the StoragePoolRing to offline.\n      2. Wait for StoragePoolRing to go offline.\n      3. Then push the last set of disks so that StoragePoolRing will go",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacldsrg.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1",
      "creatdev_file": "tsagddef"
    },
    "description": "tsagexacldsrg.tsc - Basic sanity test for EBS testsrv",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagexacldsuite.tsc",
    "setup": null,
    "flags": {
      "egsopidoffset": "-10000",
      "cdb": "true",
      "FLASH_SIZE": "2000M",
      "PMEM_SIZE": "128M",
      "setup_blockstore": "true",
      "no_gd_with_prefix": "true",
      "keep_pmemlog_disabled_exc": "true",
      "RUN_DISK_TRANSITION": "true",
      "TDE_ENC_TS": "true",
      "tsag_skip_drl": "true",
      "inc_ers_timeout": "true",
      "leader_kill": "true",
      "testcase_set": "2",
      "failure": "cell_unreachable",
      "run_selected_scenarios": "12,21"
    },
    "description": "tsagexacldsuite.tsc - Exacloud tests suite",
    "platform": null
  },
  {
    "test_name": "tsagexacldvaultspace.tsc",
    "setup": null,
    "flags": {
      "PHASE_DUR": "600",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagexacldvaultspace.tsc - testfor vault space\n\ntest checks if space used is greater than space provisioned after iov\n\ntest  to be added in lrgsaexacldvaultspace",
    "platform": null
  },
  {
    "test_name": "tsagexacldxsh.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexacldxsh.tsc - Test for xsh tool\n\nThe test checks for different xsh functionalities like :\n     cp,cat,dd,od,strings,ls,mkvault,rmvault,xattr",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexaclflcrtclsrv.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexaclflcrtclsrv.tsc - Exascale test for disk offline/online alerts\n\nTest disk online/offline alerts for 3 cell setup using\n      cellsrv shutdown/startup around file creation wrkload\n              1.Setup stack with sage_mirror_mode=high\n              2.Shutdown cellsrv for cell1, wait for alerts\n              3.Run file creation workload in background\n              4.Startup cellsrv for cell1, wait for alerts\n              5.Verify file dumps after iov run ends, all should be same",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexacli_ipv6.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagexacli_ipv6.tsc - Exacli on ipv6 system, ipv6 parsing only\n\nTest ipv6 hostname parsing for exacli.\n\nNone",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagexacli_sparc.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagexacli_sparc.tsc - testing exacli on solaris sparc\n\nThis setsup remote view for exacli testing on solaris sparc",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagexaclibatch1.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagexacliovclsrv.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexacliovclsrv.tsc - Exacloud test for cellsrv up/down with iov wrkld\n\nTest disk online/offline alerts for 3 cell setup\nusing cellsrv shutdown/startup during iov workload\n\t1.Setup stack with sage_mirror_mode=high\n\t2.Run iov workload in background\n\t3.Shutdown cellsrv for cell1, wait for alerts\n\t4.Startup cellsrv for cell1, wait for alerts\n\t5.Verify file dumps after iov run ends, all should be same",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexaclnmulticlu.tsc",
    "setup": null,
    "flags": {
      "exascale_cluster_num": "2"
    },
    "description": "tsagexaclnmulticlu.tsc - cleanup multiple exascale clusters\n\ncleanup multiple exascale clusters\n\ncleanup multiple exascale clusters",
    "platform": null
  },
  {
    "test_name": "tsagexaclnonactvrsync.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexaclnonactvrsync.tsc - Exascale Non-Active client Resync Test\n\nShutdown cellsrv /wait for alert\n     Start IOV workload in background\n     Wait for IOV workload to complete\n     Verify IOV completed fine till end\n     cellsrv startup/Wait for alert\n     Verify data consistency for all file dumps using md5sum check",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexaclnonactvrsync_qlc.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexaclnonactvrsync_qlc.tsc - Exascale Non-Active client Resync Test with QLC DISKS\n\nShutdown cellsrv /wait for alert\n     Start IOV workload in background\n     Wait for IOV workload to complete\n     Verify IOV completed fine till end\n     cellsrv startup/Wait for alert\n     Verify data consistency for all file dumps using md5sum check\n\ntest is same as tsagexaclnonactvrsync.tsc but with QLC DISKS",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexaclonedbtest.tsc",
    "setup": null,
    "flags": {
      "sqlnet_string": "'BEQUEATH_DETACH=yes'"
    },
    "description": "tsagexaclonedbtest.tsc - Functional test for exaclonedb tool\n\nFunctional test for exaclonedb tool",
    "platform": null
  },
  {
    "test_name": "tsagexaconfig.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cell_3_few_disks": "true"
    },
    "description": "tsagexaconfig.tsc - Storage Pool Config Test\n\nSP is created with 3 cells where the 3rd cell has only 11 disks",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexadatalogger_perl.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexalogger_perl.tsc - oratst description file for testing exaLogger.pm\n\nCopies tsperllogger_ancestor.pm, tsperllogger_parent.pm, tsperllogger_child.pm,\n     tsperllogger_file and tsperllogger.pl from oss/test/tsage/sosd directory to the\n     test system, and copies exaLogger.pm from /opt/oracle.cellos/lib/perl directory.\n     Then runs the test, captures the output from the logger and streams them into\n     tsperllogger_fresh_generated.log, and remove all the time stamp information\n     and compares it to the reference log at oss/test/tsage/log/tsagexalogger_perl.log.\n\nMake sure exaLogger.pm exists on the test system at /opt/oracle.cellos/lib/perl",
    "platform": null
  },
  {
    "test_name": "tsagexadcli.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagexadcli.tsc - exadcli test\n\nIt tests exadcli with multiple MS running",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagexadoopinit.tsc",
    "setup": null,
    "flags": {
      "tmp_nowarn": "^tst_nowarn^",
      "MY_SKGXP_DYNAMIC_PROTOCOL": "2",
      "rmt_tst_exec": "^TST_STATUS^",
      "CELL_WITH_FLASH_CACHE": "^tmpvar^",
      "server_event": "'\"trace[CELLSRV_Flash_Cache_Layer.*] disk=low,memory=highest\"'"
    },
    "description": "tsagexadoopinit.tsc - Initialization for Exadoop services\n\nCreates cellinit.ora, sets up variables to use Exadoop services.\n\nSet for starting exadoop services",
    "platform": null
  },
  {
    "test_name": "tsagexadropcont.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "egs_trace": "highest",
      "egs_trace_file_rolling": "true"
    },
    "description": "tsagexadropcont.tsc - test for dropping files and container\n\nThe test steps are given below.\n\nTest added in lrgsaexacldeds",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexadsmartall.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexadsmartall.tsc - Test for smart ALL adverb\n\nCurrently, when \"ALL\" is specified without other diskType qualifiers,\n      the command would target at all types of cell disks for grid disk\n      operations.\n      In the transaction - yifanch_bug-35034249, \"ALL\" is made\n      smarter and deterministic. It will filter out cell disks of the\n      primary media type on a real hardware or when\n      \"_cell_griddisk_cmd_smart_all_sim_sys\" is explicitly configured\n      (0:HC, 1:EF w/ TLC only-EF sys before X10, 2:EF w/ TLC and QLC-EF X10).",
    "platform": null
  },
  {
    "test_name": "tsagexaegs_loop.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "lcounter": "50",
      "configured_egs": "3"
    },
    "description": "tsagexaegs_loop.tsc - Elevator Test for EGS Cluster\n\nLoop Test for EGS - starts 3 EGS and kills 1 randomly",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexaersconf.tsc",
    "setup": null,
    "flags": {
      "reconfig": "false",
      "local_run": "true",
      "nginx_log": "^logfile^",
      "tsagexaersconf_log": "^logfile^",
      "T_WORK_ORG": "^T_WORK^",
      "T_WORK": "^T_WORK_ORG^",
      "mesg10": "'ERS .* is in ONLINE state'",
      "NGINX_INSTALLED": "true"
    },
    "description": "tsagexaersconf.tsc - Configures ERS in the stack\n\nInstalls nginx and then starts up ERS",
    "platform": null
  },
  {
    "test_name": "tsagexahardcheck.tsc",
    "setup": null,
    "flags": {
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on",
      "egs_trace": "highest",
      "egs_trace_file_rolling": "true"
    },
    "description": "tsagexahardcheck.tsc - Test for hard check failures for eds\n\n1) create container and file for testing\n     2) simulate read failure\n     3) try to list details , should fail\n     4) unsimulate failure\n     5) try to list again , should pass\n\ntest will run in lrgsaexacldeds",
    "platform": null
  },
  {
    "test_name": "tsagexaioswitchctx.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexaioswitchctx.tsc - Switch context to OSS view during exascale runs\n\nSwitches context to OSS view during 2-view / MDB runs by changing\n     some ENV to point to the OSS view.\n     This helps to switch context to OSS view for performing cell side\n     operations when current context is RDBMS.\n\nFor changing or restoring context to / from OSS view\n     include tsagexaioswitchctx switch/restore",
    "platform": null
  },
  {
    "test_name": "tsagexaloadgen_workload.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexaloadgen_workload.tsc - write all the workloads for exaloadgen tool\n\nhttps://confluence.oraclecorp.com/confluence/display/~sweksha.sinha@oracle.com/Exaloadgen+test+plan",
    "platform": null
  },
  {
    "test_name": "tsagexalogger.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexalogger.tsc - oratst description file for testing exalogger.py\n\nCopies exalogger_test.py from oss/test/tsage/sosd directory to the\n     test system at /opt/oracle.cellos/\n     Then runs the test and captures the output from all log and trace\n     files generated into exalogger_test.log and compares it to the\n     reference log at oss/test/tsage/log/tsagexalogger.log\n\nMake sure exalogger.py is copied onto the test system at /opt/oracle.cellos",
    "platform": null
  },
  {
    "test_name": "tsagexalscluster.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "admin_wallet": "^T_WORK^/esadmin_wallet"
    },
    "description": "tsagexalscluster.tsc - Test for escli lscluster command\n\nThis file include testcase for escli lscluster command which has following testcases:\n(1) lscluster without argument\n(2) lscluster -l\n(3) lscluster --detail\n(4) lscluster --attributes\n(5) lscluster --filter\n(6) lscluster --backup\n(7) lscluster --volume\n(8) lscluster --count\n(9) lscluster --sort",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexaperformance.tsc",
    "setup": "xblockini",
    "flags": {
      "DISABLE_DKWR_THROTTLE": "true"
    },
    "description": "tsagexaperformance.tsc - escli commands performance test\n\nTest various escli commands like mkvault,mkfile,mkvolume,user operation,rmfile,lsfile,rmvault",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexaportmon_non_secure.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexaportmon_non_secure.tsc - Test for ExaPorton services for\n\t\t\t      for non secure fabric cell\n\nThis test include following testcases:\n    1. Happy path\n    2. covering rescue and recovery code paths\n    3. covering failure code path\n    4. covering rescue, failure, and recovery code paths\n    5. concurrent re-IP\n    6. wait for 2 IPs before kicking in\n    7. clear previous alerts upon bootup\n    8. take no action when remote node is being rebooted\n\n Detailed test steps mentioned in this confluence page:\n https://confluence.oraclecorp.com/confluence/display/EXD/ExaPortMon+Testing",
    "platform": null
  },
  {
    "test_name": "tsagexaportmon_secure_fabric_cell.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexaportmon_secure_fabric_cell.tsc - ExaPortMon Test for secure fabric cells\n\nThis test include following testcases:\n\t1. Happy path\n\t\t2. covering rescue and recovery code paths\n\t\t3. covering failure code path\n\t\t4. covering rescue, failure, and recovery code paths\n\t\t5. concurrent re-IP\n\t\t6. wait for 2 IPs before kicking in\n\t7. clear previous alerts upon bootup\n\t\t8. take no action when remote node is being rebooted\n\n\tDetailed test steps mentioned in this confluence page:\n\t\thttps://confluence.oraclecorp.com/confluence/display/EXD/ExaPortMon+Testing",
    "platform": null
  },
  {
    "test_name": "tsagexaraftiov.tsc",
    "setup": null,
    "flags": {
      "rafttest": "true"
    },
    "description": "tsagexaraftiov.tsc - raft iov test\n\nplease see below\n\ntest has been added in lrgsaexacldreschk",
    "platform": null
  },
  {
    "test_name": "tsagexareconfig.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cell_4_few_disks": "true"
    },
    "description": "tsagexareconfig.tsc - Storage Pool Reconfig Test\n\nSP is created with 3 cells, later reconfigured by adding\n     cell with only 11 disks",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexaresloop.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagexaresloop.tsc - Restart Exadata in loop\n\nRestarts whole Exadata Stack in a loop",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagexasnapdelete.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexasnapdelete.tsc - Test for deleting snapshot file while IOV running\n\nTest runs base ves xmls and for deleting snapshot file while IOV running in\n     the background\n\nThis lrg runs with encryption by default, see\n     tsage/data/lrglist_cloudservice.dat",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexasparsedisk.tsc",
    "setup": "tsaginit",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "creatdev_file": "tsagddef"
    },
    "description": "tsagexasparsedisk.tsc - ALTER SPARSE DISKGROUP\n\nTest for ALTER SPARSE DISKGROUP",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagexastack_event.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "egs_trace_file_rolling": "true"
    },
    "description": "tsagexastack_event.tsc - EGS/EDS Events Test\n\nThis test will contain test events for EGS/EDS servers",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexastackdn.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexastackdn.tsc - Brings down EDS, ERS, EGS\n\nShuts down Exascale stack between tests",
    "platform": null
  },
  {
    "test_name": "tsagexastackup.tsc",
    "setup": "tsaginit",
    "flags": {
      "CELL_WITH_QLC_DISK": "1",
      "NUM_FLASH_PER_CELL": "2",
      "FLASH_SIZE": "256M",
      "oss_testing": "3",
      "media_type": "ALL",
      "media_type_vault": "ALL",
      "CELL_WITH_FLASH_CACHE": "^tmpvar^",
      "egsrs_stop_timeout": "60",
      "dsk_size": "regular",
      "RECON_CELL_ATTEMPT_COUNT": "12",
      "dsksz": "8192     ## 8.0 G each so 96G total",
      "creatdev_file": "tsagexastackup_pooldiskdef",
      "nflint": "1",
      "ves_preinit": "false",
      "max_rpm": "2000",
      "egs_test": "true",
      "ers_test": "true",
      "egs_cert_dur_in_mins": "40",
      "egs_dsk_trcval": "highest",
      "egs_mem_trcval": "highest",
      "oss_large_exc_config": "true",
      "num_cells": "3",
      "setup5egs": "true",
      "sage_mirror_mode": "high",
      "max_cmd_rpm": "1000",
      "create_template_with_normal_redund": "true",
      "num_exc_servers": "^oss_testing^",
      "num_egs_servers": "3",
      "num_cell_servers": "^oss_testing^",
      "create_template_with_high_redund": "true",
      "egscnt": "3",
      "egs_servers": "^T_WORK^'/egs1 '^CELL_TWORK2^'/egs2 '^CELL_TWORK2^'/egs3'",
      "disable_multims": "true",
      "egs_nw_mon_recon_time": "90",
      "_cell_state_dump_timeout_in_sec": "10",
      "enable_cmd": "false",
      "num_cmd_mirrors": "3",
      "delta_resync": "false",
      "egs_opid_offset": "^egsopidoffset^",
      "enable_reco": "false",
      "def_maxrpm": "true",
      "def_maxcmdrpm": "true",
      "egs_inactive_timeout": "true",
      "exc_mdb_or_interop": "true",
      "egslib_trace": "l,m",
      "osslib_trace": "l,m",
      "oss_exascale_testing": "true",
      "vesgds": "true",
      "ossviewroot": "ADE_VIEW_ROOT_CV",
      "use_predefined_rsa_key": "false",
      "privkey": "priv.pem",
      "pubkey": "pub.pem",
      "reflogname": "tsagexastackup_noegs",
      "tstlogname": "^logfile^"
    },
    "description": "tsagexastackup.tsc - Brings up the full ExaScale stack\n\noratst -d tsagexastackup.tsc [egs=false] [nginx_reconfig=true] \\\n             [ers=false] [sage_mirror_mode=high] \\\n             [data_only=true] [max_rpm=<numRPMs>] \\\n             [max_cmd_rpm=<numRPMs>] [cmd_mirror_mode=normal|high] \\\n  \t       [delta_sync=true] [eds_trace=h,h] [privkey=<file>]\n             [pubkey=<file>] [num_cells=<n>]\n             [exc_cloud_user_ers_port=<port>]\n             [egs_deplmode=<egs deployment mode>]\n  \t       [media_type=<XT,EF,HC|ALL> ] [disable_si_cc=true]\n             [media_type_vault=<XT,EF,HC|ALL]\n             [multisp=true]\n             [setup5egs=true]\n             [setup1egs=true]\n             [egsopidoffset=<offset>]\n\n       - sets up ExaScale Stack in view\n - if egs=false , it skips EGS setup and uses rpmmap.ora\n       - if egs_trace=highest, highest level of disk and memory tracing\n              will be enabled for EGS (Default is disk=low, memory=high)\n       - if nginx_reconfig=true, nginx will be forcefully reconfigured\n       - if ers=false, does not sets up ERS in the stack\n       - if sage_mirror_mode=high, or oss_testing=3, 3 cellsrvs are setup\n       - if sage_mirror_mode=normal, 3 cellsrvs are setup and many filetypes\n            are created with normal redundancy\n       - if num_cells=4, 4 cells will be setup - used for rebalance tests\n       - if data_only=true, then only DATA StoragePoolRing will\n              created. RECO (and EC) Rings will not be created.\n            Default - Create DATA and RECO SPRings\n       - If max_rpm is set, then the value specified will be used to\n              configure the max. number of RPMs for the StoragePoolRings\n              This value must be a multiple of 1000 and can be\n              something >= 3000 and <= 512000\n       - If max_cmd_rpm is set, then the value specified will be used to\n              configure the max. number of RPMs for the container metadata\n              Ring. This value must be a multiple of 1000 and can be\n              something >= 3000 and <= 512000\n         A special value of 4294967296 means, Container MetaData Ring will\n              be disabled.\n       - If cmd_mirror_mode is set to normal, container metadata uses\n              3 mirrors. If it sets to high, container metadata uses\n              6 mirrors. Default is high.\n       - If delta_sync is set to true, then delta disks sync testing will\n         \tbe enabled. Default is true.\n       - if egsopidoffset is set to a non-zero interger, then EGS operation ID\n           will set the operation ID to UB4MAXVAL + offset.\n       - If dskoffline is set then script with exit after creating\n                the rpmmap.ora.It is used by lrgsaexacldresync1.\n       - If eds_trace is set, then add _eds_server_event for enabling tracing\n       eds_trace=l,h means disk=low, memory=highest\n       - If privkey and pubkey are passed, they should contain private key file\n            and public key file corresponding to the generated RSA Key pair\n          These options are useful if setup needs to use predefined keys\n       - If diskSizeInMB is passed, use this size for pooldisk\n                      (value Must be in MB, for ex - diskSizeInMB=2000)\n       - If egs_autogenerate_mode is true, the automatic generation of\n\t\t  vault names will happen.\n\t\t   - if exc_normal is disable, normal redundancy ring operations are short-circuited\n       - If raft_timeout_level is set to high, higher than normal values\n            will be used in egs excloudinit.ora for raft timeouts\n            Valid values for raft_timeout_level = regular (default), or high\n      exc_cloud_user_ers_port=port\n       - For cloud testing we want to use standard port of 5052 for ERS\n       - If egs_deplmode is passed, the value will be used to deploy\n            EGS cluster in the test. The valid values are:\n            cloudService, cloudAtCustomer, onPrem (default), custom\n       - media_type = Any combination of EF,HC,XT can be passed. Pass ALL\n                      to create all 3 media types. Default is HC.\n       - media_type_vault = Any media type which should be subset of media_type.\n                      This will providion space on reqd media types for the default vault.\n                      Default - Will use provision space on all available media types\n       - multisp = true, This will mean - media_type=ALL, media_type_vault=ALL\n                   Will create 3 Storage Pools, and default Vault will have\n                   all 3 media types\n       - If disable_si_cc is passed, then disable storage index and cc\n       - setup5egs = If set, It will configure 5 EGS servers\n       - setup1egs = If set, It will configure only 1 EGS servers\n       - set SAVE_CELLSRVSTAT to save cellsrv related stats.\n\nTo run an lrg in a specific EGS deployment mode: pass a config parameter\n       farm submit <lrg> -config \"EGS_MODE=<mode>\"\n                 where mode = ExaCS (to run in cloudService deployment),\n                              ExaCC (cloudAtCustomer deployment)\n\n     To track errors in real time and get email notification,\n     Please use the following config parameters:\n       farm submit <lrg> -config \"TRACK_ERROR=true;\n              ERROR_MSG=error1|error2|error3;MODE=ABORT/MAIL;\n              TO_MAIL=<email_address>\"\n       Currently the files being tracked are mentioned in\n       tsagerror_tracker_tracefile.dat. Please add full location of\n       any other file which you want to track.\n       Example : farm submit lrgsaexacldegs5 -config \"TRACK_ERROR=true;ERROR_MSG=ORA-00600: internal\n       error code, arguments: [EgsMain:InitNetworkConnect]|ORA-00700: soft internal error,\n       arguments: [EdsRemoteIO::checkServiceThreadHung:egslib_hung];MODE=MAIL;\n       TO_MAIL=xyz@oracle.com\"\n       For more details about parameters, please refer to file tsagerror_tracker.tsc\n\n    To collect cellsrv stat periodically in farm runs\n    Please use the following config parameters\n      farm submit <lrgname> -config \"EXADATA_STAT_COLLECTION=true;EXADATA_STAT_NAME=CELLSRVSTAT;INTERVAL=10\"\n      Explanation of parameters :\n      EXADATA_STAT_COLLECTION=true : Turn on stats collection\n      EXADATA_STAT_NAME=CELLSRVSTAT/ECSTAT\n      INTERVAL=10 : specify the interval of stat collection in seconds, for example-\n                    INTERVAL=10 will invoke and save cellsrv output every 10 seconds.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagexastackup_edstool.tsc",
    "setup": null,
    "flags": {
      "reflogname": "tsagexastackup_edstool",
      "tstlogname": "^logfile^"
    },
    "description": "tsagexastackup_edstool.tsc - creates vault using edstool\n\nHelper script to tsagexastackup",
    "platform": null
  },
  {
    "test_name": "tsagexastackup_startegs.tsc",
    "setup": null,
    "flags": {
      "sav_osstest": "^num_egs_servers^"
    },
    "description": "tsagexastackup_startegs.tsc - Sets up EGS in Cell's OSSCONF\n\nCalled from tsagexastackup to configure ExaRoot in 3 cell env\n    where EGS is also configured in each Cell and can be operated by CellCLI",
    "platform": null
  },
  {
    "test_name": "tsagexastackupmulticlu.tsc",
    "setup": null,
    "flags": {
      "exascale_setup_script": "xblockini"
    },
    "description": "tsagexastackupmulticlu.tsc - setup multiple exascale clusters\n\nsetup multiple exascale clusters, for 3 cells steup, it requires to enable\n     multiple MS instances by default:\n        oratst -d tsagexastackupmulticlu sage_mirror_mode=high\n\n     For single cell steup, we use oss_multi_exascale_1cell to support multiple\n     MS instances across Exascale clusters:\n        oratst -d tsagexastackupmulticlu\n\n     new paramter exascale_setup_script can be used for user define the setup script\n     The default script is tsagexastackup\n        oratst -d tsagexastackupmulticlu  exascale_setup_script=xblockini\n\nsetup multiple exascale clusters",
    "platform": null
  },
  {
    "test_name": "tsagexastackupremote.tsc",
    "setup": null,
    "flags": {
      "logfilel": "tsagcleanremote"
    },
    "description": "tsagexastackupremote.tsc - exascale stackup on another box\n\nThis will call tsagexastackup to be called on another box\n\nIn the interop mode, this will be executed on the rdbms node, but\n     tsagexastackup will run on the OSS node",
    "platform": null
  },
  {
    "test_name": "tsagexatelemetry.tsc",
    "setup": null,
    "flags": {
      "cdb": "true",
      "sage_mirror_mode": "high",
      "standalone": "true"
    },
    "description": "tsagexatelemetry.tsc - MS -> cloud service communication telemetry test\n\nThe test covers MS -> cloud service communication test for telemetry.\n    Step 1 - Setup the Rest Subscriber with required Parameters\n    Step 2 - Validate if Rest Subscriber is setup or not\n    Step 3 - Start the tcpdump and set listener port as 80 (HTTP)\n    Step 4.1 - Generate Hardware alerts (Test 1) and store the dumps in file.\n    Step 4.2 - Fail The Flash Disk to generate alerts.\n    Step 4.3 - Validate the alerts have been generated for hardware\n    Step 4.4 - Stop the failure simulation of the flash disk\n    Step 5.1 - Generate ADR Alerts (Test 2) and store dumps in a file.\n    Step 5.2 - Kill Cellsrv to generate an alert in alerthistory\n    Step 5.3 - Validate the alerts have been generated for ADR",
    "platform": null
  },
  {
    "test_name": "tsagexawatcher.tsc",
    "setup": null,
    "flags": {
      "cellconnstr": "^dbnodeconnstr^",
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagexawatcher.tsc - ExaWatcher tests for Exadata Cell\n\nExecutes scripts for verifying execution of ExaWatcher processes",
    "platform": null
  },
  {
    "test_name": "tsagexawmetric.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1"
    },
    "description": "tsagexawmetric.tsc - Test to verify ExaWatcher metrics in cells\n\nThis test verifies the working of newly added EXW metrics from\n     the txn ccgervasi_metrics_stream_exawdata - for cells\n     We reboot the cell at the beginning to sync the time on the cell\n     Back to the hardware clock. After the whole test, time is synced\n     again to the farm machine.",
    "platform": null
  },
  {
    "test_name": "tsagexawmetric_db.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "dbnodeconnstr": "root@^compute_node_f^"
    },
    "description": "tsagexawmetric_db.tsc - Test to verify ExaWatcher metrics in dbnodes\n\nThis test verifies the working of newly added EXW metrics from\n     the txn ccgervasi_metrics_stream_exawdata - for dbnodes\n     We reboot the dbnode at the beginning to sync the time\n     Back to the hardware clock. After the whole test, time is synced\n     again to the farm machine.",
    "platform": null
  },
  {
    "test_name": "tsagexc35019894.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal"
    },
    "description": "tsagexc35019894.tsc -\n\nplease see below\n\nto be added in cellupgrade5",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexc4cell_dbdown.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagexc4cell_dbdown.tsc - DB down in 4 cell env\n\nIn a 4 cell env, shutdown all services on 1 cell\n      then Shutdown the DB and try to start the DB instance\n     There should be no hang in the startup\n\nBug - 33789401",
    "platform": null
  },
  {
    "test_name": "tsagexc4cellcreatesp.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexc4cellcreatesp.tsc  - Adding 4th cell after dropping and creating HC Storagepool\n\nTests removes HC storagepool then creating the same storagepool after removal\n     Waits for all relevant messages in EGS Leader alert log\n     while EGS processes rmstoragepool & adding 4th cell to the environment",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexc4thcellfail.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high",
      "media_type": "HC,EF"
    },
    "description": "tsagexc4thcellfail.tsc - Mixed media type env failure in 4 cell setup\n\nThis script test the stablity of cell in 4 cell setup if 1 EF disk\n     and 1 HC disk fails/shutdown.\n\n     1) Create a vault for EF media in EF-storagepool\n     2) Assign access control to new EF vault\n     3) Create EF template\n     4) Create some tablesapce and insert table\n     5) Start DB workload on EF-vault\n     6) Fail one EF disk and wait for rebalance\n     7) Fail one HC disk and wait for rebalance\n     8) Bring back online EF disk and wait for rebalance\n     9) Bring back online HC disk and wait for rebalance\n     10)Stop the workload",
    "platform": null
  },
  {
    "test_name": "tsagexc4thcellfail1.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high",
      "media_type": "HC,EF"
    },
    "description": "tsagexc4thcellfail1.tsc - Mixed media type env failure in 4 cell setup\n     test-2\n\nThis script test the stablity of cell in 4 cell setup if 1 EF disk\n     and some HC disk fails/shutdown.\n\n     1) Setup 3 cell fake hardware\n     2) Start DB workload\n     3) Fail some HC physicaldisks\n     4) Add 4th EF-media cell\n     5) Create a vault for EF media in EF-storagepool\n     6) Assign access control to new EF vault\n     7) Create EF template\n     8) Create some tablesapce and insert table\n     9) Fail-Unfail-Fail-unfail a EF cell physicaldisk\n    10) bring back failed HC physicaldisks online\n    11) Stop the workload",
    "platform": null
  },
  {
    "test_name": "tsagexc5egsloop0.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "setup5egs": "true",
      "lcounter": "30",
      "ndownegs": "'0'"
    },
    "description": "tsagexc5egsloop0.tsc - 5 EGS Server loop test\n\nConfigured 5 EGS Servers and does an EGS leader restart loop test",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexc_addpdb.tsc",
    "setup": null,
    "flags": {
      "srcvault": "^vault_db^",
      "pdbvault": "^vault_1^"
    },
    "description": "tsagexc_addpdb.tsc - Add New PDB\n\nCreate a PDB on a given vault & from a source vault files\n     parameters to be passed:\n        pdbname=<pdb name>\n        pdbvault=<new vault for PDB>\n        srcvault= <existing vault_name to use as source >",
    "platform": null
  },
  {
    "test_name": "tsagexc_aep.tsc",
    "setup": null,
    "flags": {
      "oss_disable_aep": "true",
      "oss_multims_testing": "true",
      "num_cells": "3",
      "egsrs_stop_timeout": "2",
      "oss_devdir1": "^T_WORK^/raw^oss_port^",
      "oss_devdir2": "^T_WORK^/raw^oss_port2^",
      "oss_devdir3": "^T_WORK^/raw^oss_port3^"
    },
    "description": "tsagexc_aep.tsc - Wrapper tsc file to execute tsagexc_aep.sh\n\nCovers functional testing of Automatic Exascale Process Placement (AEP) in 1cell env\n\nLits of lrgs/test which run through tsagexc_aep.tsc\n     \t1. lrgsaexcaep\n\t\t - tsagexc_aep.sh - Functional test for Automatic Exascale Process Placement (AEP) with normal cases\n  \t2. lrgsaexcaepincftr\n\t\t - tsagexc_fhw_aep_inclusion_filte.sh - Functional test for Automatic Exascale Process Placement (AEP)\n\t\t   with inclusion filter\n\t3. lrgsaexcaep4\n\t\t - tsagexc_multims_aep.sh - Functional test for Automatic Exascale Process Placement (AEP) with 3 cell multims setup\n\t4. lrgsaexcaep5\n\t\t - tsagexc_multims_aep_parallel.sh - Functional test for Automatic Exascale Process Placement (AEP)\n\t\t   with 3 cell multims setup with parallel execution & DB workload\n\t5. lrgsaexcaep6\n\t\t - tsagexc_aep_offline_maintenance.sh - Functional test for OFFLINE_MAINTENANCE\n  \t\t   with AEP - thasingh_make_aep_respond_to_upgrades_immediately\n\t6. lrgsaexcaep7\n\t\t - Functional test for Automatic Exascale Process Placement (AEP)\n\t\t\t   with 4 cell multims setup for OFFLINE_MAINTENANCE\n\t7. lrgsaexcaep8\n\t\t - Functional test for Automatic Exascale Process Placement (AEP)\n\t\t\t   with 3 cell multims setup for OFFLINE_MAINTENANCE with DB workload\n      8. lrgsaexcaep9\n           - Functional test for Automatic Exascale Process Placement (AEP)\n             with 3 cell multims with _exc_egs_aep_ms_to_wait_before_gc_offline_processes\n      9. lrgsaexcaep10\n           - BUG 38178358 : SOME EXASCALE SERVICES DID NOT COME UP AFTER CELL ROLLING UPGRADE TO 25.2.0.0.0.250708\n      10. lrgsaexcaep11\n           - Functional Test for escli chcluster --resetAutoServicePlacement\" with simulation of 3 and 2 cells",
    "platform": null
  },
  {
    "test_name": "tsagexc_aep_bug38178358.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexc_aep_bug38178358.tsc - Functional test for BUG-38178358\n\n38178358 - SOME EXASCALE SERVICES DID NOT COME UP AFTER CELL ROLLING UPGRADE TO 25.2.0.0.0.250708\n     Test steps  :\n        1. Setup Exascale 3 cell multims with AEP disabled\n        2. List lscluster & lsservice before enabling AEP\n        3. Enable AEP using chcluster --attributes servicePlacement=auto\n        4. Make sure AEP configure the services as per the intial replica count configs which is 1\n        5. Set _exc_egs_aep_ms_to_wait_before_gc_offline_processes=100000 in excloudinit.ora in 3 cells\n        6. Run 'ebs_simevent[AEP_REPLICA_COUNTS_NORMAL]' in all 3 cells\n        7. Wait for all services replica counts to 3 in dynamic loop wait\n        8. Kill bsw, egs, syseds, usreds and ers on a cell which is not EGS leader\n        9. Sleep for 65 mins, we giving more time for AEP to make placement decsion of replica counts to 2\n        10. Wait for all services replica counts to 2 in dynamic loop wait\n        11. Run 'ebs_simevent[AEP_CRASH_ON_CELL_CNT_CHANGE]' in EGS leader cell\n        12. Wait for all services replica counts to 3 in dynamic loop wait",
    "platform": null
  },
  {
    "test_name": "tsagexc_aep_db_wkload.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexc_aep_db_wkload.tsc - Test setup DB workload in background and runs AEP test cases\n\nAEP with 3cell multims with workload & shutdown/disable services parallelly with helper scripts",
    "platform": null
  },
  {
    "test_name": "tsagexc_bswreg_incar.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexc_bswreg_incar.tsc\n\nTest to verify the fix locally by disabling bsw worker then start it. Bsw should\n     register successfully with smaller incarnation.\n\n     Test based on txn : kukuchen_bug-36717911\n\n     More details : https://orareview.us.oracle.com/146079095\n\nthe test is added to lrgsaexacldbswincar",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexc_cancel.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "mixed_workload": "true",
      "maxpdb": "1",
      "dsk_size": "medium"
    },
    "description": "tsagexc_cancel.tsc - end to end cancel support tests\n\n3 testcases have been added for cancel support. Description can be\n    found at starting of each test.\n\ntest to be added in lrgdbcsaexcegscancel1",
    "platform": null
  },
  {
    "test_name": "tsagexc_cert_update.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexc_cert_update.tsc - Testcase for updating security certificates\n\nTxn verifies fix added to address security issue with cellcli and dbmcli\n     Cellcli and dbmcli should not be able to update security certificates",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexc_conf_rebal.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true"
    },
    "description": "tsagexc_conf_rebal.tsc - double failure case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\n     iii.) - On cell 1- simulate confinement on hard disk\n\n     iv.)  -On cell 2; simulate failure on a hard disk (will trigger rebal)\n\n     v.) -On cell 2; reenable the failed hard disk\n\n     vi.) -On cell 1; reenable the confined disk\n\n endloop\n\n vii.) make sure db workload ran fine\n\nwill be added to lrgdbcsaexcfailc5",
    "platform": null
  },
  {
    "test_name": "tsagexc_conf_rebal_egs.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on",
      "mixed_workload": "true",
      "maxpdb": "1"
    },
    "description": "tsagexc_conf_rebal_egs.tsc - double failure case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\n     iii.) - On cell 1- simulate confinement on hard disk\n\n     iv.)  -On cell 2; simulate failure on a hard disk (will trigger rebal)\n\n     restart egs leader and find the new leader\n\n     v.) -On cell 2; reenable the failed hard disk\n\n     vi.) -On cell 1; reenable the confined disk\n\n endloop\n\n vii.) make sure db workload ran fine",
    "platform": null
  },
  {
    "test_name": "tsagexc_conf_resy_egs.tsc",
    "setup": null,
    "flags": {
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on",
      "create_template_with_high_redund": "true",
      "delta_sync": "true",
      "mixed_workload": "true",
      "maxpdb": "1"
    },
    "description": "tsagexc_conf_resy_egs.tsc - double failure test case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\n     iii.) - On cell 1- simulate confinement on hard disk\n\n     iv.)  -On cell 2; shutdown all services\n\n       restart egs leader and find the new leader\n\n     v.) -On cell 2; restart all services\n\n     vi.) -On cell 1; reenable the confined disk\n\n endloop\n\n vii.) make sure db workload ran fine\n\nto be added in dbcsaexcfailc11",
    "platform": null
  },
  {
    "test_name": "tsagexc_conf_resync.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "delta_sync": "true",
      "maxpdb": "1"
    },
    "description": "tsagexc_conf_resync.tsc - double failure test case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\n     iii.) - On cell 1- simulate confinement on hard disk\n\n     iv.)  -On cell 2; shutdown all services\n\n     v.) -On cell 2; restart all services\n\n     vi.) -On cell 1; reenable the confined disk\n\n endloop\n\n vii.) make sure db workload ran fine\n\nto be added in dbcsaexcfailc3",
    "platform": null
  },
  {
    "test_name": "tsagexc_conf_rslv.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true"
    },
    "description": "tsagexc_conf_rslv.tsc - double failure case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\n     iii.) - On cell 1- simulate confinement on hard disk\n\n     iv.)  -On cell 2; simulate flash failure\n\n     v.) -On cell 2; reenable the failed flash disk\n\n     vi.) -On cell 1; reenable the confined disk\n\n endloop\n\n vii.) make sure db workload ran fine\n\nto be added in lrgdbcsaexcfailc4",
    "platform": null
  },
  {
    "test_name": "tsagexc_conf_rslv_egs.tsc",
    "setup": null,
    "flags": {
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on",
      "create_template_with_high_redund": "true",
      "mixed_workload": "true",
      "maxpdb": "2"
    },
    "description": "tsagexc_conf_rslv_egs.tsc - double failure case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\n     iii.) - On cell 1- simulate confinement on hard disk\n\n     iv.)  -On cell 2; simulate flash failure\n\n       restart egs leader and find the new leader\n\n     v.) -On cell 2; reenable the failed flash disk\n\n     vi.) -On cell 1; reenable the confined disk\n\n endloop\n\n vii.) make sure db workload ran fine\n\nto be added in lrgdbcsaexcfailc14",
    "platform": null
  },
  {
    "test_name": "tsagexc_cosscv.tsc",
    "setup": null,
    "flags": {
      "dsk_size": "xl",
      "num_vols": "7",
      "def_vol_size": "4G",
      "do_not_setup_db": "true",
      "skip_gi": "1",
      "use_edv_as_asmdsk": "true",
      "use_excvol_as_asmdsk": "true"
    },
    "description": "tsagexc_cosscv.tsc - Current View Setup for interop of older DB on ES/EDV volumes\n\nSets up env in current view (OSS) by creating ES/EDV volumes which will be used as\n      ASM disks by lower version ASM",
    "platform": null
  },
  {
    "test_name": "tsagexc_cosslv.tsc",
    "setup": null,
    "flags": {
      "nflint": "1",
      "use_edv_as_asmdsk": "true",
      "asm_diskstring": "''/dev/sd'*'''",
      "use_excvol_as_asmdsk": "true",
      "asmdisks_created": "2",
      "saved_ossconf": "^OSSCONF^"
    },
    "description": "tsagexc_cosslv.tsc - Interop - Lower Version setup (RDBMS) on ES volumes\n\nThis tsc sets up env in lower version view after Exascale is up in\n     higher view (OSS) and ES volumes have been created.\n     It will copy Seed DB and just run the driver of the base lrg needed to run\n     on ES volumes",
    "platform": null
  },
  {
    "test_name": "tsagexc_dbonattsnapvol.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagexc_dbonattsnapvol.tsc - setup asm + exascale + blockstore + edv\n\nThis test does the following:\n      1. setup 2 dbs, db 1 on exascale and db 2 on asm + blockstore + edv\n      2. create a volume and edv attachment of it\n      3. create asm diskgroup BACKUP from volume attachment device\n      4. create tablespace and table in asm db\n      5. add records to table for sanity check\n      6. take backup on BACKUP dg using rman\n      7. after taking backup create a snapshot and thin clone of the volume\n      8. rename BACKUP dg to BACKUP_DB2_SNAP and use the asm diskstring of new cloned volume\n      9. create a new database from backed up data on BACKUP_DB2_SNAP diskgroup\n     10. validate functionality of new and asm db. Both dbs should be up and running\n     11. cleanup\n         11.1 delete tables, tablespaces and shutdown both databases\n         11.2 drop diskgroups BACKUP and BACKUP_DB2_SNAP\n         11.3 shutdown asm\n         11.4 delete blockstore entities",
    "platform": null
  },
  {
    "test_name": "tsagexc_dropadd_conf.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "num_cells": "4",
      "mixed_workload": "true",
      "maxpdb": "1"
    },
    "description": "tsagexc_dropadd_conf.tsc - double failure case\n\nTest steps:\n\n i.) - setup 4 cell env with db\n\n ii.) - - Start  db workload in background\n\n In a loop (count 2); do\n\n     iii.) - On cell 1- drop all griddisks on cell 1\n\n     iv.)  -On cell 2; confine a disk on cell 2\n\n     v.) -On cell 2; bring back all gds on cell 1\n\n     vi.) -On cell 1; reenable the confined disk\n\n endloop\n\n vii.) make sure db workload ran fine\n\nwill be added to lrgdbcsaexcfailc*",
    "platform": null
  },
  {
    "test_name": "tsagexc_dropadd_rebal.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "num_cells": "4",
      "mixed_workload": "true",
      "maxpdb": "1"
    },
    "description": "tsagexc_dropadd_rebal.tsc - double failure case\n\nTest steps:\n\n i.) - setup 4 cell env with db\n\n ii.) - - Start  db workload in background\n\n In a loop (count 2); do\n\n     iii.) - On cell 1- drop all gds on cell 1\n\n     iv.)  -On cell 2; simulate failure on a hard disk (will trigger rebal)\n\n     v.) -On cell 2; reenable the failed hard disk\n\n     vi.) -On cell 1; bring back all gds on cell 1\n\n endloop\n\n vii.) make sure db workload ran fine\n\nwill be added to lrgdbcsaexcfailc*",
    "platform": null
  },
  {
    "test_name": "tsagexc_dropadd_resync.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "delta_sync": "true",
      "num_cells": "4",
      "mixed_workload": "true",
      "maxpdb": "1"
    },
    "description": "tsagexc_dropadd_resync.tsc - double failure case\n\nTest steps:\n\n i.) - setup 4 cell env with db\n\n ii.) - - Start  db workload in background\n\n In a loop (count 2); do\n\n     iii.) - On cell 1- drop all gds on cell 1\n\n     iv.)  -On cell 2; shutdown all services on cell 2\n\n     v.) -On cell 2; startup all services\n\n     vi.) -On cell 1; bring back all gds\n\n endloop\n\n vii.) make sure db workload ran fine\n\nwill be added to lrgdbcsaexcfailc*",
    "platform": null
  },
  {
    "test_name": "tsagexc_dropadd_rslv.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "num_cells": "4",
      "mixed_workload": "true",
      "maxpdb": "1"
    },
    "description": "tsagexc_dropadd_rslv.tsc - double failure case\n\nTest steps:\n\n i.) - setup 4 cell env with db\n\n ii.) - - Start  db workload in background\n\n In a loop (count 2); do\n\n     iii.) - On cell 1- drop all gds on cell 1\n\n     iv.)  -On cell 2; simulate failure on a flash disk (trigger resilvering)\n\n     v.) -On cell 2; reenable the failed flash disk\n\n     vi.) -On cell 1; bring back all gds\n\n endloop\n\n vii.) make sure db workload ran fine\n\nwill be added to lrgdbcsaexcfailc*",
    "platform": null
  },
  {
    "test_name": "tsagexc_dropadd_scrub.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "num_cells": "4",
      "mixed_workload": "true",
      "maxpdb": "1"
    },
    "description": "tsagexc_dropadd_scrub.tsc - double failure case\n\nTest steps:\n\n i.) - setup 4 cell env with db\n\n ii.) - - Start  db workload in background\n\n In a loop (count 2); do\n\n     iii.) - On cell 1- drop all gds on cell 1\n\n     iv.)  -On cell 2; simulate read error and start scrubbing\n\n     v.) -On cell 2; unsimulate the read error\n\n     vi.) -On cell 1; bring back all gds\n\n endloop\n\n vii.) make sure db workload ran fine\n\nwill be added to lrgdbcsaexcfailc*",
    "platform": null
  },
  {
    "test_name": "tsagexc_failca_drl.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexc_failca_drl.tsc - double failure test case with DRL enabled\n\nTest steps:\n     setup 3 cell environment without DB\n   start iov workload in background on DRL and NON DRL file\n   simulate failure on cell1\n   simulate failure on cell 2\n   reenable failures on both cell1 and cell2\n    make sure iov workload ran fine\n\nto be added in failc*_drl lrgs",
    "platform": null
  },
  {
    "test_name": "tsagexc_failcb_drl.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexc_failcb_drl.tsc - double failure test case with DRL enabled\n\nTest steps:\n     setup 3 cell environment without DB\n   start iov workload in background on DRL and NON DRL file\n   start flash failure/reenable in loop on cell 1 in background\n   shutdown/startup all services on cell 2\n    make sure iov workload ran fine\n\nto be added in failcb_drl lrgs",
    "platform": null
  },
  {
    "test_name": "tsagexc_functests.tsc",
    "setup": null,
    "flags": {
      "dbnode1": "^compnode1^",
      "MACH_NAME": "^dbnode1^",
      "MACH_PASSWD": "welcome1",
      "dbnodeconnstr1": "root@^MACH_NAME^",
      "node_f": "^guest^",
      "guestconnstr": "root@^guest^",
      "cell_node": "cell^i^"
    },
    "description": "tsagexc_functests.tsc - Functional test suite for Exascale hybrid lrg\n\nContains various functional tests to be executed on Exascale\n     real hardware half rack Hybrid deployment",
    "platform": null
  },
  {
    "test_name": "tsagexc_io_write_xsh.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexc_io_write_xsh.tsc\n\norereview details :  hequ_xsh_dd_sparse_write\n     https://orareview.us.oracle.com/153733667\n\n   Test steps\n\n   Create volume\n   $ escli mkvolume 100m --vault myvault1\n\n   Get volume path and id\n   $ escli lsvolume\n\n   Write data to volume\n   $ xsh dd --if=/dev/urandom --of=@myvault1/vol.55fd57937d4b4d768a2e6dc8ecd588f8 --bs=1M --count=20\n   to get initiator:\n   $ escli lsinitiator\n\n   Create edv volume\n   $ mkvolumeattachment 3:349ec636d6aa4ae9b895900f265974b2 myvol --attributes initiator=d9a7c88a-4f90-78ac-d9a7-c88a4f9078ac\n\n   Get edv devicePath:\n   $ escli lsvolumeattachment âdetail\n\n   Test dd on edv device, and create a backup volume file\n   $ /usr/local/packages/aime/install/run_as_root 'xsh dd --if=/dev/exc/myvol --of=@test/myvol --status=progress'\n\n   Test md5sum of original volume and backup volume file\n   $ xsh cat @test/myvol | md5sum\n   48a558ad0ec012cef75fbbb551eea9c2  -\n\n   $ xsh cat @myvault1/vol.55fd57937d4b4d768a2e6dc8ecd588f8 | md5sum\n   48a558ad0ec012cef75fbbb551eea9c2  -",
    "platform": null
  },
  {
    "test_name": "tsagexc_ksfdx_cc.tsc",
    "setup": null,
    "flags": {
      "vault_db": "DATA",
      "cdb": "true"
    },
    "description": "tsagexc_ksfdx_cc.tsc - Code coverage tests for ksfdx.c\n\nCode coverage tests for few modules in ksfdx.c",
    "platform": null
  },
  {
    "test_name": "tsagexc_lscell_xtmedia.tsc",
    "setup": null,
    "flags": null,
    "description": "Functional test list xt mediatype details.\n     in a 3 cell multi ms setup",
    "platform": null
  },
  {
    "test_name": "tsagexc_lsfeature_cluster_ops.tsc",
    "setup": null,
    "flags": {
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexc_lsfeature_cluster_ops.tsc - lsfeature for users with cluster privileges\n\nThis test checks if users with cluster privileges (monitors and operators) are able to list\n     features using \"escli lsfeature\" command which was restricted to admins only.",
    "platform": null
  },
  {
    "test_name": "tsagexc_parallel_cells.tsc",
    "setup": null,
    "flags": {
      "outfile": "tsagexastackup_setupcell^cell^.log"
    },
    "description": "tsagexc_parallel_cells.tsc - Setup cell in Exascale cluster\n\nSets up a cell in an Exascale cluster",
    "platform": null
  },
  {
    "test_name": "tsagexc_rdsreset.tsc",
    "setup": null,
    "flags": {
      "node_1": "^node1^.us.oracle.com",
      "node_2": "^node2^.us.oracle.com"
    },
    "description": "tsagexc_rdsreset.tsc - Wrapper script file for RDS-Reset Exawatcher\n\nTest cases for RDS-Reset Exawatcher and cover below cases\n         1. Normal case\n            - Check if RDS-connection is established\n            - Make sure the \"Count:\" in the first line is 30\n         2. RDS hang case\n            - Additional sysinfo* tar file will be present in post-reset collection(netdiag tar file)\n            - Look for diagnostic traces and make sure the QP number in the post-reset collection (netdiag tar file) is different from the previous one\n            - There should be 12 files under QP_INFO directory in total for IB Nodes\n            - There should be 7 files under QP_INFO directory in total for RoCE Nodes\n         3. Customized command normal case\n            - Replace RDSinfo command in /opt/oracle.ExaWatcher/ExaWatcher.conf as below and restart exawatcher\n                 - '/opt/oracle.ExaWatcher/RDSinfoExaWatcher.sh --no-reset 2>/dev/null'\n            - Check if RDS-connection is established\n            - Make sure the \"Count:\" in the first line is 30\n         4. Customized command RDS hang case\n            - Replace RDSinfo command in /opt/oracle.ExaWatcher/ExaWatcher.conf as below and restart exawatcher\n                 - '/opt/oracle.ExaWatcher/RDSinfoExaWatcher.sh --no-reset 2>/dev/null'\n            - Check if RDS-connection is established\n            - Look for diagnostic traces and make sure the QP number in the post-reset collection (netdiag tar file) is same from the previous one\n         5. Invalid command normal case\n            - Replace RDSinfo command in /opt/oracle.ExaWatcher/ExaWatcher.conf as below and restart exawatcher\n                 - '/opt/oracle.ExaWatcher/RDSinfoExaWatcher.sh aaa bbb 2>/dev/null'\n            - Check if RDS-connection is established\n            - Make sure the \"Count:\" in the first line is 30\n         6. Invalid command RDS hang case\n            - Replace RDSinfo command in /opt/oracle.ExaWatcher/ExaWatcher.conf as below and restart exawatcher\n                 - '/opt/oracle.ExaWatcher/RDSinfoExaWatcher.sh aaa bbb 2>/dev/null'\n            - Check if RDS-connection is established\n            - Additional sysinfo* tar file will be present in post-reset collection(netdiag tar file)\n            - Look for diagnostic traces and make sure the QP number in the post-reset collection (netdiag tar file) is different from the previous one\n            - There should be 12 files under QP_INFO directory in total for IB Nodes\n            - There should be 7 files under QP_INFO directory in total for RoCE Nodes\n         7. Peer reboot case\n            - Reboot the peer node\n            - Make sure that QP number becomes -1 for the established connection",
    "platform": null
  },
  {
    "test_name": "tsagexc_readonlymode_parentiops.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexc_readonlymode_parentiops.tsc\n\nChildren clones will take a lock on every volume clone inheriting iops value from its parent.\n      if parent is busy, the it will wait retry and end with ORA errors.\n\n      this is fixed in txn : aembarca_bug-36836859\n      https://orareview.us.oracle.com/145327147",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexc_rebal_rebal.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "mixed_workload": "true",
      "maxpdb": "1"
    },
    "description": "tsagexc_rebal_rebal.tsc - double failure test case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\n     iii.) - On cell 1- fail a disk\n\n     iv.)  -On cell 2- fail a hard disk\n\n     v.) -On cell 2; reenable failed hard disk\n     vi.) on cell 1; reenable failed hard disk\n\n endloop\n\n vii.) make sure db workload ran fine",
    "platform": null
  },
  {
    "test_name": "tsagexc_rebal_rslv.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "media_type": "HC,EF",
      "NUM_FLASH_PER_CELL": "4",
      "mixed_workload": "true",
      "maxpdb": "1"
    },
    "description": "tsagexc_rebal_rslv.tsc - Double failure test case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\nwill be added in lrgdbcsaexcfailc2",
    "platform": null
  },
  {
    "test_name": "tsagexc_rebal_rslv_2.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4",
      "devdir1": "^T_WORK^/raw^oss_port^",
      "devdir2": "^T_WORK^/raw^oss_port2^",
      "devdir3": "^T_WORK^/raw^oss_port3^"
    },
    "description": "tsagexc_rebal_rslv_2.tsc - Rebal+Resync\n\nDouble Failure Test - Rebal+Resync",
    "platform": null
  },
  {
    "test_name": "tsagexc_rebal_rslv_egs.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "mixed_workload": "true",
      "maxpdb": "1"
    },
    "description": "tsagexc_rebal_rslv_egs.tsc - Double failure test case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\nwill be added in lrgdbcsaexcfailc14",
    "platform": null
  },
  {
    "test_name": "tsagexc_rebal_scrub.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true"
    },
    "description": "tsagexc_rebal_scrub.tsc - double failure test case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\n     iii.) - On cell 1- fail a hard disk\n\n     iv.)  -On cell 2- trigger scrub\n\n     v.) -On cell 2; unsimulate scrub event\n     vi.) on cell 1; reenable the failed hard disk\n\n endloop\n\n vii.) make sure db workload ran fine",
    "platform": null
  },
  {
    "test_name": "tsagexc_reconfig_conf.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true"
    },
    "description": "tsagexc_reconfig_conf.tsc - double failure test\n\npls see below\n\nto be added in failc40",
    "platform": null
  },
  {
    "test_name": "tsagexc_reconfig_rebal.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true"
    },
    "description": "tsagexc_reconfig_rebal.tsc - double failure cases\n\npls see below\n\nto be added in dbcsaexcfailc39",
    "platform": null
  },
  {
    "test_name": "tsagexc_reconfig_resync.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true"
    },
    "description": "tsagexc_reconfig_resync.tsc - double failure test\n\npls see below\n\nto be added in dbcsaexcfailc38",
    "platform": null
  },
  {
    "test_name": "tsagexc_reconfig_rslv.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true"
    },
    "description": "tsagexc_reconfig_rslv.tsc - double failure test\n\nPLS SEE BELOW:\n\nto be added in lrgdbcsaexcfailc37",
    "platform": null
  },
  {
    "test_name": "tsagexc_reconfig_scrub.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true"
    },
    "description": "tsagexc_reconfig_scrub.tsc - double failure test\n\npls see below\n\nto be added in lrgdbcsaexcfailc36",
    "platform": null
  },
  {
    "test_name": "tsagexc_reset_aep_replica_count.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexc_reset_aep_replica_count.tsc - Test for resetAutoServicePlacement with simulation of 2 & 3 cells\n\n1.  Setup Exascale 3 cell multims with AEP disabled\n     2.  List lscluster & lsservice before enabling AEP\n     3.  Enable AEP using chcluster --attributes servicePlacement=auto\n     4.  Make sure AEP configure the services as per the intial replica count configs which is 1\n     5.  Set _exc_egs_aep_ms_to_wait_before_gc_offline_processes=100000 in excloudinit.ora in 3 cells\n     6.  Run âebs_simevent[AEP_REPLICA_COUNTS_NORMAL]â in all 3 cells\n     7.  Wait for all services replica counts to 3 in dynamic loop wait\n     8.  Modify one of the replica count to 2 (chcluster for bsm and bsw, test for both of them individually\n          so that the whole test repeats for both kinds of services)\n     9.  Remove a cell by putting one of the cells in offline-maintenance of non egs leader\n     10. Wait for 5mins\n     11. Expect no changes to lscluster\n     12. Execute the resetaep command\n     13. Wait 2 minutes.\n     14. AEP should setup replica counts (lscluster values) suitable for a 2-cell setup\n     15. Modify one of the replica counts to 3 (syseds, usreds, bsm and bsw : repeat the test for all of these so that we have\n         comprehensive coverage)\n     16. Bring up the offline-maintenance of non egs leader\n     17. Wait for 3 minutes.\n     18. AEP should not make any changes to replica counts (lscluster)\n     19. Execute the resetaep command\n     20. Wait for 2 minutes\n     30. AEP should setup replica counts (lscluster values) suitable for a 3-cell setup.",
    "platform": null
  },
  {
    "test_name": "tsagexc_restart_service_simultaneous.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "egsrs_stop_timeout": "2",
      "oss_multims_testing": "true",
      "sage_mirror_mode": "high",
      "setup_blockstore": "true"
    },
    "description": "tsagexc_restart_service_simultaneous.tsc - Restart services with db workload running in background\n\nConcurrent ESNP, EGS-Leader -and- Cellsrv, sysEDS, usrEDS, BSM, BSW restart scenarios. (Include ESNP and EGS - Leader)\n     - The test which restarts random services (2 or 3) in the background in a 3-cell with multims setup while the DB workload is going on and\n       validates that the DB workload completes successfully.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexc_resy_rebal_egs.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "mixed_workload": "true",
      "maxpdb": "1"
    },
    "description": "tsagexc_resy_rebal_egs.tsc - double failure test case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\n     iii.) - On cell 1- shutdown all services\n\n     iv.)  -On cell 2; simulate failure on a disk(rebal will trigger)\n\n     v.) restart egs leader and find the new later\n     vi.) -On cell 2; reenable the failed hard disk\n\n     vii.) -On cell 1; restart all services\n\n endloop\n\n viii.) make sure db workload ran fine\n\nwill be run as part of lrgdbcsaexcfailc15",
    "platform": null
  },
  {
    "test_name": "tsagexc_resync_rebal.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "disable_multims": "true"
    },
    "description": "tsagexc_resync_rebal.tsc - double failure test case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\n     iii.) - On cell 1- shutdown all services\n\n     iv.)  -On cell 2; simulate failure on a disk(rebal will trigger)\n\n     v.) -On cell 2; reenable the failed hard disk\n\n     vi.) -On cell 1; restart all services\n\n endloop\n\n vii.) make sure db workload ran fine\n\nwill be run as part of lrgdbcsaexcfailc1",
    "platform": null
  },
  {
    "test_name": "tsagexc_resync_resync.tsc",
    "setup": null,
    "flags": {
      "oss_multims_testing": "true",
      "oss_disable_aep": "true",
      "create_template_with_high_redund": "true",
      "delta_sync": "true"
    },
    "description": "tsagexc_resync_resync.tsc - double failure test case\n\nTest steps:\n\n i.) - setup 4 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\n     iii.) - On cell 1- shutdown all services\n\n     iv.)  -On cell 3; shutdown all services\n\n     v.) -On cell 3; restart all services\n\n     vi.) -On cell 1; restart all services\n\n endloop\n\n vii.) make sure db workload ran fine\n\nwill be run as part of lrgdbcsaexcfailc8",
    "platform": null
  },
  {
    "test_name": "tsagexc_resync_rslv.tsc",
    "setup": null,
    "flags": {
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on",
      "create_template_with_high_redund": "true",
      "delta_sync": "true"
    },
    "description": "tsagexc_resync_rslv.tsc - double failure test case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\n     iii.)  - On cell 1- shutdown cellsrv\n     iv. )  - start cellsrv and pause the resync simulation with the event\n     v.  )  -On cell 2; simulate failure on a flash disk(resilver will trigger)\n     vi. )  - on cell 1 unpause the resync operation\n     vii.)  -On cell 2; reenable the failed flash disk\n\n\n endloop\n\n viii.) make sure db workload ran fine",
    "platform": null
  },
  {
    "test_name": "tsagexc_resync_scrub.tsc",
    "setup": null,
    "flags": {
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on",
      "create_template_with_high_redund": "true",
      "delta_sync": "true"
    },
    "description": "tsagexc_resync_scrub.tsc - double failure test case\n\nTest steps:\n\n i.) - setup 3 cell env with db\n\n ii.) - - Start a db workload in background\n\n In a loop (count 3); do\n\n     iii.) - On cell 1- shutdown all services\n\n     iv.)  -On cell 2- trigger scrub\n\n     v.) -On cell 2; unsimulate scrub event\n     vi.) on cell 1; restart all services\n\n endloop\n\n vii.) make sure db workload ran fine\n\nwill be run as part of lrgdbcsaexcfailc10",
    "platform": null
  },
  {
    "test_name": "tsagexc_srcfail.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexc_srcfail.tsc - turn on the simevent EGSCONCUR_JOB_HANG and pull out a disk\n\nSimulate source disk failure (disk pull) during rebalance.",
    "platform": null
  },
  {
    "test_name": "tsagexc_stalefile_fix.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexc_stalefile_fix.tsc - Tests stale file handler issue in blockstore\n\nTest steps:\n       1. Setup xblockini with one bsw and two iscsi vips.\n       2. Create one additional volume.\n       3. Then create one snapshot and one backup of each volume.\n       4. Run the script to setup config for another process BSW.\n       5. Start a new bsw process\n       6. Remove and recreate snapshot, backup for each volume.\n       7. Stop new bsw process\n       8. There should not be ORA-00600: internal error code, arguments: [EbsMetaFileInitialize unexpected EDS error code]",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexc_volsnap_metadata.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexc_volsnap_metadata.tsc - Test for the fix of missing volume snapshot metadata\n\nTest Steps:\n       1. Create a volume (1:.*), take its snapshot (1.1:.*) and create its clone/backup\n       2. Set crash on BSWDELETESNAPJOB_DELETE_SNAP_SUBMIT (job 136, state 6)\n       3. Delete the snapshot\n       4. Wait for restore\n       5. Check if metadata remains -\n          a. lsvolumesnapshot should not return output (1.1:.*)\n          b. Create a new snapshot. Its id should be 1.2:.* since 1.1:.* metadata still remains\n       6. Delete backup/clone and remaining snapshot (1.2:.*)\n\nTest should be run for 3 cases -\n       1. Create clone from snapshot\n       2. Create backup from snapshot\n       3. Create both clone and backup from snapshot",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexc_wrkldpdb.tsc",
    "setup": null,
    "flags": {
      "tbsname": "tbs_^pdbname^",
      "start_num_rows": "^start_rows^",
      "max_workload_secs": "^max_time^"
    },
    "description": "tsagexc_wrkldpdb.tsc - Prepare to run workload on new PDB\n\nThis is part of new workload. It sets up workload script for the\n    new PDB created on a new Vault\n     Parameters to be passed:\n        pdbname=<pdb name> max_time=<max time for workload>",
    "platform": null
  },
  {
    "test_name": "tsagexcaclcmd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcaclcmd.tsc - Test for positive and negative cases for\n                         chacl and lsacl commands\n\nTest for positive and negative cases for chacl and lsacl commands\n\nTest cases mentioned inline.",
    "platform": null
  },
  {
    "test_name": "tsagexcaclfilecmd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcaclfilecmd.tsc - Test for user-level privilege , vault level ACL\n                             and file level ACL combination for file level\n                             commands.\n\nTest for positive and negative cases for user-level priv , vault level\n     ACL and file level ACL combination for file level commands.\n\nTest cases mentioned inline.",
    "platform": null
  },
  {
    "test_name": "tsagexcaclvaultcmd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcaclvaultcmd.tsc - Test for user-level priv and vault level ACL\n                              combination for vault level commands.\n\nTest for positive and negative cases for user-level priv and vault level #      ACL combination for vault level commands.\n\nTest cases mentioned inline.",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk10.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcactdsk10.tsc - Test for Exascale activedisk project\n\nExascale Active disks tests for slow flash disk along with\n     dead & predfail disk through following events:\n     Slow disk:\n       alter cell events = \"cellsrv_simevent[SLOW_DISK_INJ]\n       err_type=internal, frequency=1, count=1, evarg1=c9exdisk0,\n       evarg2=436, evarg3=3\"\n     pred fail and disk fail by updating cellinit.ora\n\ntest to be added in lrgdbconsaactd07_excld",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk11.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcactdsk11.tsc - Test for Exascale activedisk project\n\nFlash disks into confine active state.\n     test marks flash disks on cell1 as dead and marks flash disks\n     on cell2 while running db workload and makes sure workload is\n     completed without any issue.\n\ntest to be added in lrgdbconsaactd02_excld",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk12.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "high",
      "oss_testing": "3"
    },
    "description": "tsagexcactdsk12.tsc - Test for Exascale activedisk project\n\nHard disks into confine active state\n     test marks hard disks on cell1 as dead and marks hard disks\n     on cell2 while running db workload and makes sure workload is\n     completed without any issue.\n\ntest to be added in lrgdbconsaactd08_excld",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk13.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcactdsk13.tsc - Test for Exascale activedisk project\n\nSimulate slow hard disk across EGS instance reboot\n     Test case 1 - confineoffline to normal\n       Confine disk which comes back to normal on its own\n       Restart EGS while this confinement is in progress\n       disk should behave as if EGS was never restarted.\n\n     Test case 2 - confineoffline to predfail\n       Confine disk which moves to pred fail state\n       Restart EGS while this confinement is in progress\n       disk should behave as if EGS was never restarted.\n\n       EGS should come back to normal after a restart\n\ntest to be added in lrgdbconsaactd12_excld",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk14.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcactdsk14.tsc - Test for Exascale activedisk project\n\nSimulate slow hard disk for a single disk\n     1. Simulate slow hard disk (confine tests outcome is not forced)\n     2. Check alert logs to confirm confinement\n     3. Since the outcome is not forced, check for status and\n        list or enable lun as per the need.\n\ntest to be added in lrgdbconsaactd09_excld",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk17.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcactdsk17.tsc - Test for Exascale activedisk project\n\nRepeatedly put a disk into confine state.\n      3rd attempt should go into proactive fail.\n\ntest to be added in lrgdbconsaactd09_excld",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk3.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcactdsk3.tsc - Exascale Active disks tests for slow flash disks\n\nExascale Active disks tests for slow flash disks\n   test performs following cases :\n   X= 2.1. confinedOnline->confinedOffline->GOOD (evarg3=0)\n   X= 2.2. confinedOnline->confinedOffline->FAIL (evarg3=6)  needs reenable\n   X= 2.3. confinedOffline->FAIL ( evarg2=431,evarg3=1 ) needs reenable\n   X= 2.4. confinedOffline->FAIL ( evarg2=433,evarg3=7 )  needs reenable",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk4.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcactdsk4.tsc - Test for Exascale activedisk project\n\nSimulate slow flash disk for multiple disk with MS restart.\n     Make sure max 8 disks are confined.\n\ntest to be added in lrgdbconsaactd03_excld",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk5.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcactdsk5.tsc - Test for Exascale activedisk project\n\nTEST 1 :\n       Simulate slow flash disk for a single disk - check flashcache status\n     TEST 2 :\n       Simulate confined to fail for another flash disk - check flashcache\n       status\n\ntest to be added in lrgdbconsaactd04_excld",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk7.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcactdsk7.tsc - Tests for Exascale Active Disk project\n\nRestart Cellsrv + MS across slow hard disk or flash disk using event\n     SLOW_DISK_INJ on cell 2 while running db workload and makes sure workload #      is completed without any issue.",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk7a.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcactdsk7a.tsc - Tests for Exascale Active Disk project\n\nRestart Cellsrv + MS across slow hard disk or flash disk using event\n     SLOW_DISK_INJ on cell 2 while running db workload and makes sure workload #      is completed without any issue.",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk8.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcactdsk8.tsc - Test for Exascale activedisk project\n\nSimulate a slow hard disk with some failed hard disks\n     Fail 4 disks by adding them to _cell_disk_fail_list in\n     cellinit.ora, then simulate slow disk on 5th disk with event\n     alter cell events = \"cellsrv_simevent[SLOW_DISK_INJ] err_type=internal\n     , frequency=1, count=1, evarg1=c9exdisk3, evarg2=434, evarg3=3\"\n\ntest to be added in lrgdbconsaactd06_excld",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk9.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcactdsk9.tsc - Test for Exascale activedisk project\n\nSimulate a slow hard disk with some predfailed hard disks\n     Pred fail 4 disks by adding them to _cell_disk_predfail_list in\n     cellinit.ora, then simulate slow disk on 5th disk with event\n     alter cell events = \"cellsrv_simevent[SLOW_DISK_INJ] err_type=internal\n     , frequency=1, count=1, evarg1=c9exdisk3, evarg2=435, evarg3=3\"\n\ntest to be added in lrgdbconsaactd06_excld",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk_fd_gd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcactdsk_fd_gd.tsc - Test for Exascale activedisk project\n\nTest case to reproduce bug: 32900180\n     Test finds a GD FD pair where FD is cacheing the GD.\n     It then confines the GD first waits for it to turn to\n     HEALTH_BAD_OFFLINE and then fails the FD which was caching it.\n\nTest to be added in lrgdbconsaactd01_excld",
    "platform": null
  },
  {
    "test_name": "tsagexcactdsk_loop.tsc",
    "setup": null,
    "flags": {
      "maxtimeout": "11700"
    },
    "description": "tsagexcactdsk_loop.tsc - Test for Exascale activedisk project\n\nTest to run multiple confinements on Hard disks and Flashdisks in\n     a loop running for max 2.5-3 hours. We update threshold in cdpolicy\n     to be able to fail the disk more that 2 times within a short duration\n     of 3 hours\n\nTest to be added in lrgdbconsaactd13_excld",
    "platform": null
  },
  {
    "test_name": "tsagexcactdskdb.tsc",
    "setup": null,
    "flags": {
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true",
      "SAGE_MIRROR_MODE": "high",
      "creatdev_file": "tsagaudef",
      "nflint": "1",
      "vault_db": "DATA",
      "cdb": "true"
    },
    "description": "tsagexcactdskdb.tsc - active disk confinement test with db\n\nThe test simulates different confinemnet on Vesgds.\n   the test checks the active confinement behavior with active db workload/\n   test 1 - multiple hard disks marked slow performing\n\n   test 2- performs following cases :\n   X= 2.1. confinedOnline->confinedOffline->GOOD (evarg3=0)\n   X= 2.2. confinedOnline->confinedOffline->FAIL (evarg3=6)  needs reenable\n   X= 2.3. confinedOffline->FAIL ( evarg2=431,evarg3=1 ) needs reenable\n   X= 2.4. confinedOffline->FAIL ( evarg2=433,evarg3=7 )  needs reenable\n\ntest to be added in lrgdbconsaactd01_excld",
    "platform": null
  },
  {
    "test_name": "tsagexcactvdskhlth.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "no_gd_with_prefix": "true"
    },
    "description": "tsagexcactvdskhlth.tsc - Active client pull/push disk health test",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcactvdskhlth_rsync.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "no_gd_with_prefix": "true"
    },
    "description": "tsagexcactvdskhlth_rsync.tsc - Active client pull/push disk health test with resync",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcactvrebal.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "no_gd_with_prefix": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexcactvrebal.tsc - active client rebalance test for Exascale\n\n1. Setup 3 cell exastack for exascale with Debugcli\n      2. Start the loop for each xml in iovworkload\n      3. In the loop, Run IOV workload in the background\n      4. select and Fail a disk based on rpmentries from ves dump\n      5. Verify alerts - Being Dropped/Rebalance starting/Dropped/Rebalance\n         finished\n      6. Drop and replace the failed disk\n      7. Verify alerts - Being Added/Rebalance starting/Added/Rebalance\n         finished\n      8. Check that the IOV is killed now\n      9. dump file and Check md5sum\n      10. End loop once all xmls are covered and save all logs",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcactvrebal2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexcactvrebal2.tsc - active client rebalance test for Exascale\n\n1. Setup 3 cell exastack for exascale with Debugcli\n      2. Start the loop for each xml in iovworkload\n      3. In the loop, Run IOV workload in the background\n      4. select and Fail a disk based on rpmentries from ves dump\n      5. Verify alerts - Being Dropped/Rebalance starting/Dropped/Rebalance\n         finished\n      6. Drop and replace the failed disk\n      7. Verify alerts - Being Added/Rebalance starting/Added/Rebalance\n         finished\n      8. Check that the IOV is killed now\n      9. dump file and Check md5sum\n      10. End loop once all xmls are covered and save all logs",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcactvreballoop.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexcactvreballoop.tsc - active client rebalance test  in loop for Exascale\n\n1. Setup 3 cell exastack for exascale with Debugcli\n     2. Run IOV workload in the background\n     3. Start the loop of 20\n     3a. Fail a disk using using status - Failed\n     3b. Verify alerts - Being Dropped/Rebalance starting/Dropped/Rebalance\n         finished\n     3c. turn off cellsrv_simevent[BLOCKIO_WRITE_ERR]\n     3d. Drop and replace the failed disk\n     3e. Verify alerts - Being Added/Rebalance starting/Added/Rebalance\n         finished\n     4. End the loop\n     5. Check that the IOV is killed now\n     6. dump file and Check md5sum",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcactvreballoop1.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexcactvreballoop1.tsc - active client rebalance test  in loop for Exascale\n\n1. Setup 3 cell exastack for exascale with Debugcli\n     2. Run IOV workload in the background\n     3. Select a disk to drop and add\n     4. Start the loop of 20\n     4a. Fail the disk selected in step 3 again and again.\n     4b. Trigger event cellsrv_simevent[BLOCKIO_WRITE_ERR]\n     4c. Verify alerts - Being Dropped/Rebalance starting/Dropped/Rebalance\n         finished\n     4d. Turn off cellsrv_simevent[BLOCKIO_WRITE_ERR]\n     4e. Drop and replace the failed disk\n     4f. Verify alerts - Being Added/Rebalance starting/Added/Rebalance\n         finished\n     5. End the loop\n     6. Check that the IOV is killed now\n     7. Dump file and Check md5sum",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcactvreballoop2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "no_gd_with_prefix": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexcactvreballoop2.tsc - active client rebalance test  in loop for Exascale\n\n1. Setup 3 cell exastack for exascale with Debugcli\n     2. Run IOV workload in the background\n     3. Select two disks to drop and add in parallel\n     4. Start the loop of 5\n     4a. Fail disks and trigger cellsrv_simevent[BLOCKIO_WRITE_ERR]\n     4b. Verify alerts - Being Dropped/Rebalance starting/Dropped/Rebalance finished of both the disks\n     4c. Turn off cellsrv_simevent[BLOCKIO_WRITE_ERR]\n     4d. Drop and replace the failed disks\n     4e. Verify alerts - Being Added/Rebalance starting/Added/Rebalance finished of both the disks\n     5. End the loop\n     6. Check that the IOV is killed now\n     7. dump file and Check md5sum",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcactvrsyncdelta.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexcactvrsyncdelta.tsc - Exascale Active client Resync Test\n\nSet delta_sync\n     Start IOV in background\n     cellsrv Shutdown/wait for alert\n     cellsrv startup/Wait for alert\n     Verify IOV completed fine till end",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcactvrsyncdelta2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexcactvrsyncdelta2.tsc - Exascale Active client Resync Test\n\nSet delta_sync\n     Start IOV in background\n     cellsrv Shutdown/wait for alert\n     cellsrv startup/Wait for alert\n     Verify IOV completed fine till end",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcadd_cell.tsc",
    "setup": null,
    "flags": {
      "exst_media_type": "^media_type^",
      "media_type": "^media_type_new^",
      "CELL_WITH_PMEM_CACHE": "0",
      "spreconfig": "true",
      "creatdev_file": "tsagqlcdef",
      "NUM_QLC_PER_CELL": "6",
      "NUM_FLASH_PER_CELL": "4"
    },
    "description": "tsagexcadd_cell.tsc - script to add 4th cell in a 3 cell setup\n\npls see below\n\nto be  used only as macro\n -----  in order to use this macro, add  the following in a tsc and run it.\n  include tsagmacro.tsc\n  let sage_mirror_mode high\n  include tsagexastackup.tsc\n  add_cell",
    "platform": null
  },
  {
    "test_name": "tsagexcadd_samecell.tsc",
    "setup": null,
    "flags": {
      "reflogname": "tsagexcadd_samecell",
      "tstlogname": "^reflogname^^logsuffix^.log"
    },
    "description": "tsagexcadd_samecell.tsc - Adds a cell back to Exascale cluster\n\nIt has steps to add a cell back to Exascale SP. This cell was earlier\n    dropped from Exascale cluster (tsagexcdrop_cell)",
    "platform": null
  },
  {
    "test_name": "tsagexcaddcell_cf.tsc",
    "setup": null,
    "flags": {
      "media_type": "EF",
      "sage_mirror_mode": "high",
      "NUM_FLASH_PER_CELL": "6"
    },
    "description": "tsagexcaddcell_cf.tsc - Add X10EF cell to existing 3 cell HC+EF\n\nExisting Exascale cluster has 3 cells with HC + EF from same 3 cells\n     Add a new cell that is X10EF - with QLC\n     It will create a new StoragePool for CapacityFlash",
    "platform": null
  },
  {
    "test_name": "tsagexcaddincredata1.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagexcaddincredata1.tsc - Add random data to volume V1 and create a new bkp of the V1, repeat this for 50 iterations",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcaddincredata2.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagexcaddincredata2.tsc - Add random data to volume V1 and create a new clone of the V1, repeat this for 50 iterations",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcaddincredata3.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagexcaddincredata3.tsc - Add random data to clone Cn and create a new clone Cn+1 , repeat this for 50 iterations",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcasmhome_onvol.tsc",
    "setup": null,
    "flags": {
      "dsk_size": "large",
      "def_vol_size": "20G",
      "do_not_setup_db": "true",
      "mountpoint": "^ADE_VIEW_ROOT^/asmhome",
      "asmhome_on_excvol": "true",
      "redund": "external"
    },
    "description": "tsagexcasmhome_onvol.tsc - Test for using mounted volume for ASM tests\n\nCreates an Exascale volume, mount it and use it for RAC ASM test setup\n     Runs lrgdbcon9o42 on this setup",
    "platform": null
  },
  {
    "test_name": "tsagexcatomicrename.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcatomicrename.tsc - Test for verifying atomic rename functionality\n\nPlease see test_atomic_rename.sh script for test details\n\nadded in lrgsaexacldeds",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcautoqrntvlt.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcautoqrntvlt.tsc - vault auto quarantine test for Exascale\n\nAutomatic quarantining of vaults upon trigeering usreds crash event\n     upon file creation and testing of new added flags.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcautoqrntvlt_3cell.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcautoqrntvlt_3cell.tsc\n\nAutomatic quarantining of vault after setting usreds crash event\n     in usreds/excloudinit.ora upon file creation in the vault.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcbufferfail.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcbufferfail.tsc - Buffer allocation failure test for blockstore operations\n\nThis is a buffer allocation failure test where we simulate buffer allocation\n     failure in blockstore operations using cellcli events. The blockstore\n     operations being tested are:\n     1. Volume creation and deletion\n     2. Volume attachment creation and deletion\n     3. Volume snapshot creation and deletion\n     4. Volume snapshot attachment creation and deletion\n     5. Volume backup creation and deletion\n     6. Modifying volume attributes\n     7. VIP Creation and Deletion\n     8. Listing volume attributes\n     9. Test for bug-38102097",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcbug30646935.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcbug30646935.tsc - Test for Bug 30646935\n\nVerifies that one cannot start an Exascale Service - ERS, *EDS\n     by directly invoking the binary and RS won't shutdown the\n     currently running service.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcbug33191184.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcbug33191184.tsc - 33191184 - ISSUE EXASCALE ALERT WHEN SPACE TO\n#                  SUPPORT STORAGE POOL REBALANCE IS NOT AVAILABLE\n\nPlease see below\n\nto be added in lrgsaexcdefrag",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcbug35437158.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcbug35437158.tsc - Bug 35437158 - 50% LOWER X10 XRMEMCACHE SELECT IOPS\n#                               IN EXASCALE BECAUSE RDMA IS MOSTLY NOT USED\n\nrun tsagexcbug35437158.xml and make sure we see RDMA reads\n\nto be added in lrgsaexacldreschk",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcbug36023199.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcbug36023199.tsc - Test for BUG 36023199\n\nThis txn kaiz_bug-36023199 allows ESNP to garbage collect the pervious instance in-memory objects, when its incarnation is bumped up by EGS leader.\n     The test steps includes :\n      1. Setup xrdbmsini\n      2. cellcli > alter cell egsEvents=\"ebs_simevent[EGS_BUMP_UP_ESNP_INC_SIM] count=1\" to EGS leader\n      3. Shutdown DB\n      4. Restart ESNP\n      5. Check and make sure the new ESNP garbage collected the previous instances. This can be verified by searching the below pattern in ESNP traces:[EgsNPFenceMgr] cleanUpInstHashTable: delete inst *",
    "platform": null
  },
  {
    "test_name": "tsagexcbug36609150.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "vault_db": "DATA",
      "cdb": "true"
    },
    "description": "tsagexcbug36609150.tsc - 36609150 - ADD ABILITY TO DO READ ALL 3 MIRRORS\n                      AND VALIDATE AFTER EVERY WRITE AND DUMP CLIENT SIDE DIAG\n\npls see below\n\nadded in lrgdbconsaexacldcov1",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcbug36837336.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcbug36837336.tsc- Bug 36837336 - EDS SNAPSHOT CREATION\n           BEING BLOCKED BY OTHER FILE DELETION IN SAME CONTAINER\n\nPlease see below",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcbug36877926.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcbug36877926.tsc - Bug 36877926 - USREDS CRASHED WITH\n#      ORA-600 [EDSSSTREE::PROCESSADDNODE_NOPARENTBRANCH] DURING DATASTORE\n           SHARD OFFLINE/ONLINE LOOP TEST\n\npls see below\n\ntest added in lrgsaexacldeds",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcbug37004624.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal"
    },
    "description": "tsagexcbug37004624.tsc - Bug 37004624 - EXASCALE - PATCHMGR FAILED\n#              DURING PATCH FINALIZATION AND REBOOT STEP LEAVING CELL IN\n#                MAINTENANCE MODE INSIDE EGS.\n\npls see below\n\nto be added in cellupgrade lrg",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcbug37284751.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcbug37284751.tsc - TEST FOR 37284751 - ESCS: MS NOT CLEARING THE\n       STATEFUL ALERTS IN MULTIPLE SITES AFTER UNDERLYING ISSUES ARE RESOLVED\n\nplease see below\n\nto be added in lrgsaexcdefrag",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcbug37362959.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "nostorage": "true"
    },
    "description": "tsagexcbug37362959.tsc - Bug 37362959 - storagepool creation with failed\n                              disks.\n\npls see below\n\nto be added in lrgsaexcmiscbugsnondb",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcbug37593422.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcbug37593422.tsc - test for bug 37593422\n\npls see below\n\nadded in lrgsaexcdefrag",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcbug37788743.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcbug37788743.tsc - Test for bug 37788743\n\nIn EXC, every smart scan IO needs to pass/successfully complete a XLN\n     phase (via PredicateDiskXln).  This is because of bugs 34707983, 35485022\n     where VES doesn't perform RPM checks for XRMEM smart scan IOs.  As a\n     result, we need to fail the smart scan IO even before issuing the IO if\n     XLN fails.  When XLN fails, in PredicateDisk::createPredicateMapElems(),\n     we mark the IO with an error and start attempting to return the\n     unprocessed back to RDBMS for retry.  This is done without breaking the\n     IO into smart scan's usual 1MB units.\n\n     In bug 37788743, a smart scan IO failed XLN phase and was added to an\n     output buffer, ready to be returned to RDBMS.  However, there were some\n     buffer pressure and cellsrv decided to evict an output buffer with this\n     failed IO.  When evicting an output buffer and requeuing the IO for\n     processing, PredicateDisk checks that the IO was broken up into smart\n     scan's usual 1MB units.  This assert triggers for smart scans that failed\n     XLN earlier, leading to:\n\n     ORA-00600: internal error code, arguments:\n       [PD::recreateIORequestFromEvictedFilteredMap_1], [8388608], [8388608], [1048576]\n\n     The fix is to check whether the IO should be retried/requeued in cellsrv\n     or sent back to RDBMS.\n\n     Added a new test event to simulate XLN failure and output buffer eviction.\n       alter cell events =\"cellsrv_simevent[PRED_XLN_FAIL_EVICT] frequency = 1\";\n\nTest for bug 37788743",
    "platform": null
  },
  {
    "test_name": "tsagexcbug37984707.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "oss_multims_testing": "true",
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal"
    },
    "description": "tsagexcbug37984707.tsc - test for Bug 37984707 - POOLDISK MAY SKIP FORMAT UPON DISK REUSE\n\npls see below\n\nto be added in lrgsaexcmiscbugsnondb_3cell",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcbug38108274.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcbug38108274.tsc - Test for bug 38108274\n\nDuring latch recovery, CLMN/PMON tries to restore the another process's\n     edslibctx to a consistent state so that it can be cleaned up and freed.\n     Since edslibctx being recovered is from another process, we should not\n     access anything in edslibctx that belong to the other process's private\n     memory, which includes edsblictx->gp_edslib_ctx,\n     edslibctx->diagctx_edslib_ctx, and edslibctx->membucket_edslib_ctx.\n\n     In edslibmsg, latch recovery called into function that had diag traces\n     and asserts that use these private edslibctx members, leading to\n     the following asserts:\n\n     ORA-07445: exception encountered: core dump [edslibmsg_remove_msg_from_free()+76]\n       [SIGSEGV] [ADDR:0x7FC2B9A0873C] [PC:0x5578BFC] [Address not mapped to object]\n\n     Fix is to remove diag traces/asserts from latch recovery code.\n\n     New test events are added to cause a SEGV while holding edslibmsg latch:\n       alter session set events='27606 trace name context forever, level 0x00800000';\n       alter session set events='27606 trace name context forever, level 0x01000000';\n       alter session set events='27606 trace name context forever, level 0x02000000';\n\n    Without the fix, the above assert can be observed in this test.\n    With the fix, this test runs clean.\n\nTest for bug 38108274",
    "platform": null
  },
  {
    "test_name": "tsagexcbug38327208.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "oss_multims_testing": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcbug38327208.tsc - Bug 38327208 - EXASCALE: SMART REBALANCE\n      REPORTING INSUFFICIENT FREE SPACE WHEN WE ONLY HAVE 70T USED IN 147T STORAGEPOOL\n\npls see below\n\nto be added in lrgsaexcdefrag",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexccancel_4cell.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "num_cells": "4",
      "cmd_mirror_mode": "normal",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagexccancel_4cell.tsc - cancel operation test with 4 cell setup\n\nplease see below.\n\nto be added in lrgsaexccancel_4cell",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexccdbid_eds.tsc",
    "setup": null,
    "flags": {
      "cdb": "true",
      "cdb_dd": "pdbs_across_cdbs",
      "num_cdbs": "2",
      "num_pdbs": "2",
      "vault_db": "DATA",
      "PWFILE_ON_EXC": "false",
      "VF_ON_EXC": "true"
    },
    "description": "tsagexccdbid_eds.tsc - Test for pdbid,cdbid check from eds, db\n\nWhen the database opens a file through USREDS, it passes information (ids/names)\n   about itself that USREDS then saves in that file's metadata. We want to ensure\n   these ids/names are saved correctly the first time a file is opened and are\n   updated correctly when they change.\n\nto be added in lrgdbcsaexcedscdbid",
    "platform": null
  },
  {
    "test_name": "tsagexccellcli.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexccellcli.tsc - Test for new cellcli command to deliver SIGUSR2\n                          to CELLSRV/EBS services.\n\nTest for new cellcli command to deliver SIGUSR2 to CELLSRV/EBS services.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexccelloffline.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexccelloffline.tsc - cell offline test\n\ncell offline test\n     1. Shutdown all EGS servers.\n     2. Shutdown one cellsrv.\n     3. Startup all EGS servers.\n     4. After about 30 seconds since elected as leader, EGS should take the cell and disks offline.\n     5. Startup cellsrv\n     6. EGS should take the cell and disks back online.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexccellsrvredund.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high",
      "setup_blockstore": "true"
    },
    "description": "tsagexccellsrvredund.tsc - Test for redundancy check after cellsrv\n                                shutdown\n\nTest steps for pure exascale env:\n     - Bring up Exascale rack with high redundancy with all services running\n       (SYS-EDS, USR-EDS, ERS, BSM, BSW, EGS).\n     - Shutdown cellsrv check:\n     1. Shutdown cellsrv on cell 1\n     2. Make sure cellsrv is gone using ps\n     3. Wait for 12 \"ExaScale PoolDisk.*is in OFFLINE_MAINTENANCE\" messages in\n        leader egs alert log\n     4. Switch to cell 2 and try to shutdown cellsrv. It should fail.\n     5. look for \"EgsStoragePoolMgmtJob.*removed from OFFLINE_MAINTENANCE list\n        due to PartnerDisk is OFFLINE or will be taken OFFLINE on cell\n        EDSCELL2\" in EGS traces\n     6. Switch back to first cell and startup cellsrv\n     7. Wait for 12 \"ExaScale PoolDisk.*is ONLINE\" messages in leader egs\n        alert log\n     - Shutdown all services check:\n       Repeat the same steps mentioned above, except in step (4). In step (4),\n       instead of shutting down only cellsrv, shut down all services. Here,\n       the cellsrv shutdown should fail.\n\n     Test steps for hybrid (EXC + ASM) env:\n     1. Shutdown cellsrv on cell 1\n     2. Make sure cellsrv is gone using ps\n     3. Wait for 12 \"ExaScale PoolDisk.*is in OFFLINE_MAINTENANCE\" messages in\n        leader egs alert log\n     4. Switch to cell 2 and try to shutdown cellsrv. It should fail.\n     5. Switch back to first cell and startup cellsrv\n     6. Wait for 12 \"ExaScale PoolDisk.*is ONLINE\" messages in leader egs\n        alert log\n     7. Inactivate a griddisk on cell 1\n     8. Switch to cell 2 and try to shutdown cellsrv. It should fail.\n     9. Switch back to first cell and activate the griddisk\n    10. Inactivate a pooldisk on cell 1\n    11. Switch to cell 2 and try to shutdown cellsrv. It should fail.\n     9. Switch back to first cell and activate the pooldisk\n\nFor pure exascale - lrgdbcsaexcsrvredund1\n     For hybrid exascale - lrgdbcsaexchybrid5",
    "platform": null
  },
  {
    "test_name": "tsagexccellsrvstat.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexccellsrvstat.tsc -\n\nperform create, alter, delete operation on BST objects(volume, snap, bkp, clone, iscsi attachment),\n     and verify if it is reflected in the stats shown by cellsrvstat -bsm",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexccellsrvstaters.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexccellsrvstaters.tsc - test for cellsrvstat -ers,egs,syseds,usreds\n\nTest for cellsrvstat -ers,egs,syseds,usreds\n\n1. Run cellsrvstat command\n     2. Run some escli commands to trigger change in stats\n     3. Verify change in stats",
    "platform": null
  },
  {
    "test_name": "tsagexcchkmetadata.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcchkmetadata.tsc - Testcase for celldisk metadata corruption\n\nCheck for celldisk metadata corruption",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcchmodemultibsw.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagexcchmodemultibsw.tsc - test to change iscsi mode in a multi bsw setup\n\n1.setup bsw in iscsi mode - 2 bsm, 3 bsw\n     2.kill bsw3.\n     3.change bsw1 to non iscsi mode. try mkvolume - mkvolume fails, we get \"Volume creation attempt in mixed mode\"\n     4.chnage bsw2 to non iscsi mode. try mkvolumec - mkvolume is success\n     5.change bsw1 back to iscsi mode. try mkvolume - mkvolume is success\n     6.change bsw2 back to iscsi mode. try mkvolume - mkvolume will give \"BSM ioctl op code 18001\"\n     7. restart bsm and bsw. try mkvolume.\n\n     changing is mode is done by adding/removing the iscsi lies.\n     adding iscsi lines to excloudinit.ora, to change mode from non iscsi to iscsi.\n     removing iscsi line from excloudinit.ora, to change mode from iscsi to non iscsi.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcchmodetoiscsi.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcchmodetoiscsi.tsc - changing cluster mode from non iscsi mode to iscsi mode\n\n1. Setup blockstore in non iscsi mode\n     2. Add iscsi lines to excloudinit.ora\n     3. Shutdown bsw using cellcli and start bsw using startBSW.sh -\n         BSW does not restart, as a volume is already present and cluster mode is set\n     4. Undo the checnges done to excloudinit.ora, shutdown bsm and bsw using cellcli\n     5. Run cleanup.sh\n     6. Start BSM and BSW using cellcli commands.\n     7. Add iscsi lines to excloudinit.ora\n     8. Shutdown bsw using cellcli and start bsw using startBSW.sh\n     9. mkvolume cmd fails with - \"Error code: 10026 - No valid VIP for volume create\"\n        we started the setup in iscsi mode(where mk commands work without VIPs)\n        now the setup is changed to iscsi mode, thus the requirement to create VIPs,\n        before using mk commands\n     10. Create VIPs using createVip.sh\n     11. mkVolume command succeeds, volume is created.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcchmodetononiscsi.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagexcchmodetononiscsi.tsc - changing cluster mode from iscsi mode to non iscsi mode\n\n1. Setup blockstore in iscsi mode\n     2. Remove iscsi lines from excloudinit.ora\n     3. Stop BSW service by using stopBSW.sh and Start BSW by using cellcli\n        BSW does not start, as a volume is already present, cluster mode is set.\n        mode change is not allowed.\n     4. Adding iscsi lines to excloudinit.ora\n     5. Shut down BSM and BSW service using cellcli commands\n     6. Cleanup using cleanup.sh\n     7. Start BSM service using cellcli and BSW service using startBSW.sh\n     8. Remove iscs ilines from excloudinit.ora (to convert the setup to non iscsi mode)\n     9. Stop BSW using stopBSW.sh and start BSW service using cellcli\n     10. Run mkvolume, volume gets created.\n         Here, we are now in non iscsi mode setup, thus VIP creatioin is not required.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcchpooldisk.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high",
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcchstoragepool.tsc - Test for chpooldisk escli command\n\nTest for chpooldisk escli command",
    "platform": null
  },
  {
    "test_name": "tsagexcchstoragepool.tsc",
    "setup": null,
    "flags": {
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcchstoragepool.tsc - Test for chpooldisk and chstoragepool\n                                escli commands.\n\nTest for chpooldisk and chstoragepool escli commands.",
    "platform": null
  },
  {
    "test_name": "tsagexcchwallet_cert.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcchwallet_cert.tsc - Test for --trusted-cert-file option\n\nchwallet can now add trust to a wallet from a file.\n\n     @>chwallet --wallet /ade/sidatta_exc12a/oss/deploy/config/eswallet\n     /cwallet.sso --trusted-cert-file /tmp/tx1 --trusted-cert-file /tmp/tx2\n     --trusted-cert-file /tmp/tx3 --clear-old-trusted-certs",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexccloneofaclone.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexccloneofaclone.tsc - test to crash bsm/bsw while creating a clone,\n                                form an existing clone\n\nSimulates bsm/bsw crash at different job states during\n     the creation clones from existing clone",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexccloneofaedsvolclone.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexccloneofaedsvolclone.tsc\n\nCreate a volume from an EDS file then create a clone of it.\n     Crash BSW/BSM while while creating a clone of another clone and validate if it succeeds.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexccloneofavolbkpclone.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexccloneofaclone.tsc - test to crash bsm/bsw while creating a clone C2,\n                                form an existing clone C1.\n                                V1-->S1-->B1-->V2-->S2-->C1-->S3-->BSM/BSW CRASH-->C2\n\nRestore volume from a backup then create clone of it.\n     Simulates bsm/bsw crash at different job states duringcreating a clone of another clone and validate if it succeeds.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcclonerollback.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcclonerollback.tsc - check volume clone creation and bkp restore with BSM failover\n\nSimulates bsm crash at certain jobstates,\n     during clone creation or bkp restore from a non existent parent",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexccommoniov.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "num_cells": "4",
      "cmd_mirror_mode": "normal",
      "phase_dur": "60",
      "vault_db": "DATA",
      "vault_log": "DATA",
      "oss_devdir1": "^T_WORK^/raw^oss_port^",
      "oss_devdir2": "^T_WORK^/raw^oss_port2^"
    },
    "description": "tsagexccommoniov.tsc - common script to be used in saexacldvesdropadd1/2\n\nHelper script to setup environment and have scripts to use",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcconnstrs2l.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "cell_conn1": "o^s^^cellip^:^oss_port1^",
      "cell_opt1": "'-p \"'^cell_conn1^'\"'",
      "cell_conn2": "o^s^^cellip^:^oss_port1^/^disk^",
      "cell_opt2": "'-p \"'^cell_conn2^'\"'"
    },
    "description": "tsagexcconnstrs2l.tsc - Exascale connection stress tool test\n\nTest for connection stress tool on EXASCALE.\n     The test runs cellconnstress three times, one for each eligible type of\n     client for an Exascale environment: 'exc_enroll', 'exc_conn' and\n     'exc_open'. The duration of the run is fixed to 3 iterations, with 1\n     second interval between consecutive iterations. The tool is instructed\n     to use two clients (processes) during the run.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexccurlstress.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexccurlstress.tsc - curlstress\n\nThis test runs 100 plus concurrent curl sessions to stress the rest interface",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdatacopybkp.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexc_dataCopy_bkp.tsc\n\nMore details can be found at : https://orareview.us.oracle.com/146921825\n   aembarca_datacopy.bkp\n\n   --- test 1 : data integrity---\n   --- test 3 :  mixing types ---\n   --- test 5 : ACL update when volume ownership changes ---\n   --- test 6: contentType support ---\n\n   Added test for iyangy_bug-38065528 txn,\n   bug 38065528 test, to operate concurrently on different backups of the same volume and when bsm are restarted\n   more details can be found at : https://orareview.us.oracle.com/157574089",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdatacopybkp2.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcdatacopybkp2.tsc\n\nMore details can be found at : https://orareview.us.oracle.com/146921825\n   aembarca_datacopy.bkpi\n\n   --- test 4 : several backups on same volume ---\n   test on both EDS destination and linux destination on data copy backups",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdatacopybkpfailrecovery.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcdatacopybkpfailrecovery.tsc\n\nMore details can be found at : https://orareview.us.oracle.com/146921825\n   aembarca_datacopy.bkp\n\n  --- test 2 : failure and recovery modeâ--\n  the test steps are run for EDS dataCopy bkps\n  failure test on the below states while creating bkp",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdatacopybkpfailrecovery2.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcdatacopybkpfailrecovery2.tsc\n\nMore details can be found at : https://orareview.us.oracle.com/146921825\n   aembarca_datacopy.bkp\n\n   --- test 2 : failure and recovery modeâ--\n   for Linux dataCopy bkps\n\nthe test is added to lrgsaexacldbswincar",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdbclonevlt_template.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "vault_db": "DATA",
      "compatible": "^def_compatibility^",
      "media_type": "HC,EF",
      "NUM_FLASH_PER_CELL": "4",
      "eswallet": "^t_work^/esadmin_wallet",
      "dbusrwallet": "^ORACLE_BASE^/admin/^ORACLE_SID^/eswallet"
    },
    "description": "tsagexcdbclonevlt_template.tsc - Tests exaclonedb with vault template\n\nTest for bug-34723472 where exaclonedb fails when db has vault template",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdbdock.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagexcdbdock.tsc - Test DB backup and recovery on exascale docker\n                         setup\n\n1. Perform a backup of database running on exascale\n     2. Try doing a restore preview validate to make sure it works",
    "platform": null
  },
  {
    "test_name": "tsagexcdbonvol.tsc",
    "setup": null,
    "flags": {
      "def_vol_size": "11G",
      "maxiops": "25000",
      "change_vault_iops": "100000",
      "mountpoint": "^T_WORK^/tmpdir1",
      "maxinstances": "1",
      "cluster_database": "true",
      "TKFV_RAC_RUN": "1",
      "egs_csmode": "false",
      "PWFILE_ON_EXC": "false",
      "oss_testing": "0",
      "asm_raw_dir": "raw",
      "force": "yes",
      "asm_diskstring_dir": "/dev/exc/",
      "asm_diskstring": "''/dev/exc/edv'*'''",
      "asmdisks_created": "2",
      "TKFV_ASM_DISKSTRING": "^asm_diskstring^",
      "cdb": "true"
    },
    "description": "tsagexcdbonvol.tsc - Init File for preparing volume to setup a DB\n\nThis file should be included in a test if the purpose is to setup a\n    legacy DB on an Exascale Volume or on an EDV\n\n    It will create an Exascale volume using xblockini (or an EDV using\n    tkueedvinit).\n\nFlags used:\nuse_excvol_as_asmdsk - Exascale Volume is the ASM disk itself\nuse_edv_as_asmdsk - EDV is the ASM disk itself\nasmhome_on_excvol - setup asmhome on the Exascale mounted volume",
    "platform": null
  },
  {
    "test_name": "tsagexcdbrmsp.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "egs_trace": "highest",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcdbrmsp.tsc - Dropping Storagepool with DB setup\n\nTests removal of db and storagepool then creating a new storagepool after removal\n     Waits for all relevant messages in EGS Leader alert log\n     while EGS processes rmstoragepool",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdbuser.tsc",
    "setup": null,
    "flags": {
      "dbuser_privkeyfile": "^t_work^/excdbusrpriv.pem",
      "dbuser_pubkeyfile": "^t_work^/excdbusrpub.pem",
      "dbuser_uid": "excdbusr",
      "dbusrwallet": "^ORACLE_BASE^/admin/^ORACLE_SID^/eswallet"
    },
    "description": "tsagexcdbuser.tsc - Creates DB user for Exascale\n\nSets up a new user - excdbusr which will be the user for DB",
    "platform": null
  },
  {
    "test_name": "tsagexcdcnsp.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcdcnsp.tsc - Drop sp and create sp more than 10 times and make sure top 10 heap values less than 10MB\n\nTest follows the below sequence\n    1- Create an ExaScale stack with 3 cells.\n    2- Drop the current storage pool.\n    3- Get heap dump from the EGS leader:\n        CellCLI> alter cell egsevents=\"immediate excsrv.ebs_dump(sgaheapsummary, 1, 0)\"\n    4- Repeat the following steps 10 times:\n        (a) Create a storage pool with a new name.\n        (b) Offline and online pool disks of cell-1.\n        (c) Drop the storage pool.\n    5- Get heap dump from the EGS leader, again:\n        CellCLI> alter cell egsevents=\"immediate excsrv.ebs_dump(sgaheapsummary, 1, 0)\"\n    6- Make sure the top 10 memory users in the heap dump have not increased significantly (say the difference should be less than 50 MB)",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdefrag1.tsc",
    "setup": null,
    "flags": {
      "vault_db": "DATA",
      "oss_multims_testing": "true"
    },
    "description": "tsagexcdefrag1.tsc - Griddisk defragmentation test\n\ngd defrag test with resync and rebal operation",
    "platform": null
  },
  {
    "test_name": "tsagexcdeplmode.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "admin_wallet": "^T_WORK^/esadmin_wallet"
    },
    "description": "tsagexcdeplmode.tsc - chcluster - deployment Mode tests\n\nTests - escli lscluster commands and chcluster command with focus\n             on deployment modes\n           escli chcluster --attributes deploymentMode=[ cloudService | cloudAtCustomer\n                                                 | onPrem | custom ]\n           Also tests attributes autoFileEncryption\n            autoFileEncryption: This is a boolean setting that dictates whether\n                          files are created by EDS encrypted or not.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdgonvol.tsc",
    "setup": null,
    "flags": {
      "TKUE_ASM_COMPAT": "19.0.0.0.0",
      "TKUE_RDBMS_COMPAT": "19.0.0.0.0"
    },
    "description": "tsagexcdgonvol.tsc - Create ASM diskgroups using Exascale volumes\n\nASM DG creation script - uses Exascale volumes /dev/sd* as disks",
    "platform": null
  },
  {
    "test_name": "tsagexcdisableserv.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcdisableserv.tsc - Test to disable a service using chservice\n\nAdds Test for chservice --disable --force, in which case\n    ERS unregisters the service\n    with EGS even when the cell for the service is not available.",
    "platform": null
  },
  {
    "test_name": "tsagexcdiskdiscovery.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcdiskdiscovery.tsc - test for ExaScale disk discovery when VES\n                                disk bootstrap is slow\n\npls see below:\n\ntest to be added in lrgsaexacldegs3",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdiskofflinetimeout.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "vault_db": "DATA",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcdiskofflinetimeout.tsc - Disk Offline timeout\n\nThe test includes the following steps\n    1.  Start a non-rdbms stack with a 3 cell setup\n    2.  Run a IOV workload to populate XRMEM cache\n    3.  Set Disk Offline timer to 0 (diskOfflineTimerInMins=0)\n    4.  Dump XRMEM lru: (using event cli for 'dumplru',0,3,3)\n    5.  Offline any griddisk\n    5.  Wait for rebalance to finish\n    6.  Online the griddisk\n    7.  Wait for the format to finish\n    8.  Look again for XRMEM population after format (using dumplru)\n    9.  Grep for the pattern to note the differences in step 4 & 8\n\n    Trace files to look for and examine:\n    1. EGS Alert logs for state transitions in rebalance and format\n    2. Cell alert log to translate griddisk number to its name\n    3. IOV workload trace file\n    4. dumplru trace files from steps 4 & 8\n\n    How to execute steps:\n    escli --wallet $T_WORK/esadmin_wallet/ chstoragepool OPC_HC_STORAGE --attributes diskOfflineTimerInMins=0\n    alter cell events = \"immediate cellsrv.cellsrv_nvcache('dumplru',0,3,3);\n    alter griddisk HC_cell1_CD_exdisk6_EDSCELL1 inactive/active\n    Extract a griddisk_num from 1st dumplru trace file:\n     - grep \"cnt.*dloc\" <dumplru_trc1> | grep -v \"dloc 0\" | head -1 | sed -e 's/.*dloc \\([1-9].*\\)/\\1/g' -e 's/ .*//g'\n    Get the line count for active cache headers for that disk in dumplru trace files:\n     - grep \"grep \"cnt.*dloc <gridisk_num>\" <dumplru_trc1/2> | wc -l",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdiskoutspace.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcdiskoutspace.tsc - disk out of space error test\n\ncreate a large file and write/fill data in it with exciogen,\n#       should see out of space error\n\nruns in lrgsaexacldegs3",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdiskresync_drop.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagexcdiskresync_drop.tsc - double failure test\n\nPlease see below\n\nto be added in lrgdbcsaexcfailc44",
    "platform": null
  },
  {
    "test_name": "tsagexcdiskresync_unreachable.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcdiskresync_unreachable.tsc - double failure tests\n\npls see below\n\nto be added in failc45",
    "platform": null
  },
  {
    "test_name": "tsagexcdiskstate.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagexcdiskstate.tsc - test storagepool reconfig with missing/confined/failed disk.\n\nTest case 1: cell drop with missing/confined/failed disk\n    Test case 2: cell add with missing/confined/failed disk\n\nto be added in lrgdbcsaexcdiskstate_4cell",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdockersetup.tsc",
    "setup": null,
    "flags": {
      "vault_db": "DB^ORA_SID_UPPER^",
      "vault_log": "^vault_db^",
      "dockerdir": "^OSS_HOME^/src/tools/docker",
      "lrgmode": "1",
      "shlrgmode": "--lrgmode",
      "mysleep": "200",
      "tsaglogappend": "0",
      "tsagdockercell": "1",
      "diffname": "^logname^^logsuffix^"
    },
    "description": "tsagexcdockersetup.tsc - Brings up exascale on Docker\n\nSets up ExaScale stack to simulate a real hardware. You get\n    three cellsrv, EGS and EDS in a docker\n\n    oratst tsagexcdockersetup.tsc\n       -- Regular way of running it, which will cleanup docker and\n       -- bring up new exascale docker setup\n\n       userperl\n         -- Uses the tsagexcdocker.pl instead of the shell scripts\n\n       nolrgmode\n         -- Wont set any lrg specific parameters in the cells\n\n       memory\n         -- Mention how much memory docker should use\n\n       onlysetupdocker\n         -- Only sets up docker and exits\n\nIf the test succeeds, or fails there will always be traces in the\n     form of cell*.tar.gz.lst for all 3 cells in the work directory",
    "platform": null
  },
  {
    "test_name": "tsagexcdoscell.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcdoscell.tsc - Simulate DoS attack on cellsrv\n\nTest for Denial of Service attack on cellsrv\n\nBug 38495244 - UNAUTHENTICATED DENIAL OF SERVICE AGAINST CELLSRV",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdoubleencrypt.tsc",
    "setup": null,
    "flags": {
      "eds_encryption_mode": "on",
      "egs_deplmode": "sharedcloud",
      "cdb": "true",
      "auto_local_undo": "true",
      "dsk_size": "medium"
    },
    "description": "tsagexcdoubleencrypt.tsc - test for BUG 34270596 - RMAN BACKUP ON\n#                                EXASCALE SHOULD AVOID DOUBLE ENCRYPTION\n\npls see below.\n\nto be added in lrgdbcsadblencrypt",
    "platform": null
  },
  {
    "test_name": "tsagexcdoublefail_4cell.tsc",
    "setup": null,
    "flags": {
      "dsk_size": "medium",
      "num_cells": "4"
    },
    "description": "tsagexcmiscfailure3.tsc - misc double failure test\n\npls see  below\n\nto be added in lrgdbcsaexcfailc34",
    "platform": null
  },
  {
    "test_name": "tsagexcdoubleresl_db.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcdoubleresl_db.tsc - double flash failure test\n\nPlease see below\n\ntest to be added in lrgdbcsaexcresilver_dou",
    "platform": null
  },
  {
    "test_name": "tsagexcdrop_add_disk.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagexcdrop_add_disk.tsc - first failing 4 disks and reenabling,then failing next 4 disks and reenabling\n\nTest steps:\n\n i.) - setup 4 cell env with db\n\n ii.) - - Run oltp and redolog workload\n\n In a loop (count 2); do\n\n     iii.) - simulate 4 disks failure on cell3, rebalance finalized.\n\n     iv.)  - add 4 disks back on cell3, while rebalance running, simulate\n\n      v.)  - next 4 disks failure on the same cell node cell3, rebalance finalized.\n\n      vi.) - then add 4 disks back on cell3, check for rebalance.\n\n endloop\n\n vii.) make sure oltp and redolog workload ran fine",
    "platform": null
  },
  {
    "test_name": "tsagexcdrop_cell.tsc",
    "setup": null,
    "flags": {
      "cell_to_drop": "4",
      "ctrl1": "'--ctrl localhost:'^nginx_https_port^",
      "ctrl2": "'--ctrl localhost:'^nginx_https_port2^",
      "ctrl": "^ctrl1^",
      "reflogname": "tsagexcdrop_cell",
      "tstlogname": "^reflogname^^logsuffix^.log"
    },
    "description": "tsagexcdrop_cell.tsc - test to add drop_cell macro\n\nto be  used only as macro\n -----  in order to use this macro, add  the following in a tsc and run it.\n  include tsagmacro.tsc\n   let cell_to_drop (2/3/4)\n  let num_cells 4\n  include tsagexastackup.tsc\n  drop_cell",
    "platform": null
  },
  {
    "test_name": "tsagexcdrop_restart.tsc",
    "setup": null,
    "flags": {
      "WITH_EGS_RESTART": "false",
      "num_cells": "4"
    },
    "description": "tsagexcdrop_restart.tsc - double failure test\n\npls see below\n\nto be added in lrgdbcsaexcfailc31",
    "platform": null
  },
  {
    "test_name": "tsagexcdropadd_db.tsc",
    "setup": null,
    "flags": {
      "delta_sync": "true"
    },
    "description": "tsagexcdropadd_db.tsc - drop/ add cell operation with active db workload\n\n1- in a  4 cell setup, start db workload in background\n     2- drop  cell 1 and wait for rebalance\n     3- create cell 1 and wait for rebalance\n     4- stop the workload and make sure there is no db crash\n\ntest to be added in lrgdbcsaexaclddropadd2_db",
    "platform": null
  },
  {
    "test_name": "tsagexcdropaddcell_loop.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagexcdropaddcell_loop.tsc - drop/add cell to Exascale cluster\n\nContains a loop test to drop/add cell to Exascale cluster\n    Cell can be added back at same port or a new port altogether",
    "platform": null
  },
  {
    "test_name": "tsagexcdropaddcell_loop2.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagexcdropaddcell_loop2.tsc - drop/add cell to Exascale cluster\n\nplease see below",
    "platform": null
  },
  {
    "test_name": "tsagexcdropcell_perm.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "num_cells": "4",
      "standalone": "true",
      "nlogfile": "^tst_tscname^.log"
    },
    "description": "tsagexcdropcell_perm.tsc - Drop a cell permanently in 4 cell setup.\n\nCheck there is no asserts after dropping a cell permanently.\n     Steps:\n     1. Setup 4 cell\n     2. Start db-workload\n     3. Fail all physical disk of cell1\n     4. Wait for Rebalance\n     5. Check alerts for physicaldisk permanently dropped\n     6. Stop the workload",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdropdeadesnp.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcdropdeadesnp.tsc - Drop dead ESNP RAFT/fence metadata\n\nThis test covers the case when ESNP is evicted, all its associated\n     RAFT/fence metadata should get cleared.\n\nSteps in the test are as follows :\n     1. Check intially ESNP should exist and store it's id\n     2. Shutdown ESNP and sleep for 60 seconds\n     3. Disable the dead ESNP\n     4. Check for dead ESNP alerts in egs traces\n     5. Check there is no ESNP service running now\n     6. Startup ESNP\n     7. Check ESNP is now started with a new id",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdsoff1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcdsoff1.tsc - Offline/online cell services in loop",
    "platform": null
  },
  {
    "test_name": "tsagexcdsoffon.tsc",
    "setup": null,
    "flags": {
      "oss_multims_testing": "true",
      "sage_mirror_mode": "high",
      "setup_blockstore": "true",
      "iscsi": "true",
      "egs_deplmode": "sharedcloud",
      "mixed_workload": "true",
      "maxpdb": "1",
      "pdbname": "pdb^i^"
    },
    "description": "tsagexcdsoffon.tsc - Data Store shard offline/online\n\nThis test sets up a 3 cell env in sharedcloud mode\n     with DB and Blockstore workload running\n     In parallel, shuts down syseds/usreds/BSM on 2 cells\n       Bring them up in parallel on 2 cells\n    The idea is to stress out DataStore shards offline/online\n    We run this in loop of 100",
    "platform": null
  },
  {
    "test_name": "tsagexcdsp_rebal.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcdsp_rebal.tsc - Drop sp and trigger rebal and make sure no ORA issues seen\n\nTest follows the below sequence\n     1.Create 3 cell setup env\n     2.Drop OPC_HC_STORAGE\n     3.Wait for ExaScale StoragePool OPC_HC_STORAGE is Being DROPPED force\n     4.Make disk to fail using - alter physicaldisk disk simulate failuretype=failed\n     5.Wait for rm storagepool OPC_HC_STORAGE done",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdss.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagexcdss.tsc - Basic Smart scan test on Exascale\n\nEnables Smart scan using an event on Exascale, runs some queries\n     on EHCC and non EHCC table and check cellsrvstat",
    "platform": null
  },
  {
    "test_name": "tsagexcdstatedbandcell.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "sage_mirror_mode": "high",
      "cluster_database": "true",
      "setup_blockstore": "true",
      "iscsi": "true"
    },
    "description": "tsagexcdstatedbandcell.tsc - Test for putting processes in d-state\n\nThis test adds DB and CELL processes in d-state with workload running in background.\n\nCASE 1 : PMON1 in D-state\n     CASE 2 : PMON1, CELLSRV1, SYSEDS1 and USREDS1 in D-state\n     CASE 3 : EGS LEADER in D-state and 2 followers cells shutdown\n     CASE 4 : BLOCKSTORE SERVICES OF 2 CELLS IN D-STATE\n     CASE 5 : PMON1 and BLOCKSTORE SERVICES OF 2 CELLS IN D-STATE\n     CASE 6 : PMON1 and leader cell EGS,CELLSRV,SYSEDS,USREDS in D-state",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdstatehybrid.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcdstatehybrid.tsc\n\nTest to cover D-state cases in 3 cell hybrid environment\n\nCases covered in this test:\n\n     1. PMON (ExaScale) in D-state\n     2. PMON (Exadata) in D-state\n     3. EGS of leader cell in D-state and\n        cellsrv of follower egs cell shutdown",
    "platform": null
  },
  {
    "test_name": "tsagexcdualasmp.tsc",
    "setup": null,
    "flags": {
      "tsaglogappend": "1"
    },
    "description": "tsagexcdualasmp.tsc - DUAL asm and exascale setup password file test\n\n1. Makes sure that the password files really work in the dual setup of\n        ASM and exascale on same view\n     2. Check if both databases can be controlled by srvctl",
    "platform": null
  },
  {
    "test_name": "tsagexcedscntopt.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true",
      "test_name": "^tst_tscname^"
    },
    "description": "tsagexcedscntopt.tsc - Test for EdsCmdCntrAlterSize job optimization\n\nTxn - liqiqin_edscntraltersize_optimize contains added counters\n     (called file stats) in EdsDir such that whenever bsm creates/drops\n      volumes, we would update the corresponding counter.\n      The test makes sure the counter is working as expected",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcedscrash_md.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcedscrash_md.tsc - EDS crash test\n\nforce EDS multi-page metadata updates + EDS crash,\n     test MD recovery code at EDS restart\n\nThis test directly uses eds_kvs_recover1.sh script",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcedsdataset.tsc",
    "setup": "xrdbmsini",
    "flags": null,
    "description": "tsagexcedsdataset.tsc - Test for eds dataset\n\nThis test eds dataset which includes:\n    (1)  Get all dataset in vault\n    (2)  Check extra dumping method\n    (3)  Filter dataset in vault\n    (4)  Show space used of datasets\n    (5)  Create and drop tablespace in dataset\n    (6)  Create new vault, new file in vault and drop these files and vault\n     Tests for the dbid info + dataset integration include:\n    (7)  Datasets created by default should have the correct names\n    (8)  Datasets' children and files should increase and decrease appropriately as files are added and removed\n    (9)  Empty datasets should automatically be deleted\n    (10) List files within a dataset and a dataset's descendants\n    (11) Bug 38572515: Removing offshooting branch clone causing wrong space\n         used reported.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcedsfail2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on",
      "sage_mirror_mode": "high",
      "tstlogfile": "^tst_tscname^_edsfail.log"
    },
    "description": "tsagexcedsfail2.tsc - ESCLI loop test while restarting eds in background\n\nEdsfailover test where shutting syseds/usreds while running escli\n     workload in the background.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcedsfail_dropvlt.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcedsfail_dropvlt.tsc - This test crashes eds while tring to drop\n                                  a vault containing files.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcedsfstool.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcedsfstool.tsc - Tests for edsfstool\n\nTest dump any EDS dir file from cellsrv parse/check dumped eds dir file and report any abnormal page",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcedskill_db.tsc",
    "setup": null,
    "flags": {
      "cmd_mirror_mode": "normal",
      "sage_mirror_mode": "high",
      "multisp": "true"
    },
    "description": "tsagexcedskill_db.tsc - Test to kill syseds/usreds randomly while\n                             db workload running in the background.\n\nTest to kill syseds/usreds randomly",
    "platform": null
  },
  {
    "test_name": "tsagexcedslegacyrcvry.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "media_type": "ALL"
    },
    "description": "tsagexcedslegacyrcvry.tsc - Test EDS recovery job for those legacy\n                                 files created\n\nTest EDS recovery job for those legacy files created before EDS\n     space usage code was merged so file attributes could be correctly\n     populated and persisted.\n\n     Test Steps:\n      1. Setup tsagexastackup.tsc;\n      2. cleanups exist vault\n      3. Set the new simulation event to create legacy file and its clonei\n      4. Check the output. The media type, redundancy, and parent size\n         are not populated\n      5. Check the vault dataset\n      6. Restart usrEDS\n      7. check the file metadata attributes\n      8. check vault dataset",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcedsmdata.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "tstlogfile": "^tst_tscname^.log"
    },
    "description": "tsagexcedsmdata.tsc - test_case for EDS metadata backup and recovery\n\ntest_case for EDS metadata backup and recovery",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcedsmdcorrupt.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcedsmdcorrupt.tsc - Test for EDS metadata corruption on some (but not all) mirrors\n\nPlease see below\n\ntest to be run in lrgsaexacldeds",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcedsmdrepair.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcedsmdrepair.tsc - Test for 1 cell extent repair\n\n###############################################################\n     ###  Test for corrupted extent repair in 1 Cell Setup\n    1. Exascale setup\n    2. Drop Flash Cache and set the configuration\n       cellcli -e drop flashcache all; echo \"_cell_ves_max_bad_ext_md=-1\" >> $T_WORK/cellinit.ora;\n    3. Create a Vault DATA1 and file file1 inside it.\n    4. Corrupt the metadata for file1 and exciogen transalation to get the extent and disk\n       exciogenÂ -file @DATA1/file1Â -type trans -offset 0 -size 8192\n    5. Use extent id and disk from exciogen to get\n       priCDOffset and secCDOffset from ves_dump cellcli output\n    6. Corrupt the metadata by using the disk and priCDOffset and secCDOffset\n        dd if=/dev/zero of=^raw^/$cell_disk count=1 bs=2048 oflag=seek_bytes seek=$pricdoffset conv=notrunc\n        dd if=/dev/zero of=^raw^/$cell_disk count=1 bs=2048 oflag=seek_bytes seek=$seccdoffset conv=notrunc\n    7. Restart the cell and read the file @DATA1/file1.\n       Reading @DATA1/file1 should return error.\n        Error message: XSH-11: Cannot discover Exascale storage\n    8. Check for \"VES_RepairCorruptedExtJob\" and \"cannot find forward target (error 22)\" in trace\n    9. Reading again file1 should return same error\n############################################################\n\nExtent Repair test for 1 cell setup\n      LRG: lrgsaexacldeds2",
    "platform": null
  },
  {
    "test_name": "tsagexcedsmdrepair2.tsc",
    "setup": null,
    "flags": {
      "oss_multims_testing": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcedsmdrepair2.tsc - Test for unavailable extent repair in 3 Cell\n                          using VES_BulkMoveJob before successful repair extent\n\n###############################################################\n     ###  Test for corrupted extent repair in 3 Cell VES_BulkMoveJob\n    1. Exascale 3 Cell setup\n    2. Create a Vault DATA1 and file file1 inside it.\n    3. Use exciogen transalation to get cell and swithch Cell con text\n    4. Drop Flash Cache and set the configuration\n       cellcli -e drop flashcache all; echo \"_cell_ves_max_bad_ext_md=-1\" >> $T_WORK/cellinit.ora;\n    5. Use exciogen to perform transalation on file1 to get extent id and disk number.\n    6. Use extent id and disk from exciogen to get\n       priCDOffset and secCDOffset from ves_dump cellcli output\n    7. Corrupt the metadata  and change config parameter\n        dd if=/dev/zero of=$OSS_DEVDIR/$cell_disk2 count=1 bs=200 oflag=seek_bytes seek=$pricdoffset2 conv=notrunc\n        dd if=/dev/zero of=$OSS_DEVDIR/$cell_disk2 count=1 bs=200 oflag=seek_bytes seek=$seccdoffset2 conv=notrunc\n        cellsrv.cellsrv_setparam(_cell_ves_simulation, 32)\n    8. Restart the cell and read the file @DATA1/file1.\n       Reading @DATA1/file1 should return error.\n        Error message: XSH-11: Cannot discover Exascale storage\n    9. Check for message: \"ves_read operates on a corrupted extent\" in trace\n    10. Wait for 35 second and read file1 again, it should return more lines than step 9\n    11. Change the configuration and read file1 after 40 second.\n        It should return same numbers of lines as in Step 10  in trace.\n    12. Write to corrupted extent should succeed and return same number of message in trace\n    13. Restart the Cell and read file1 again, it should return same number of message in trace\n###############################################################\n\nTest for successful extent repair using VES_BulkMoveJob\n    LRG: lrgsaexacldeds3",
    "platform": null
  },
  {
    "test_name": "tsagexcedsmdrepair2_batchio.tsc",
    "setup": null,
    "flags": {
      "oss_multims_testing": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcedsmdrepair2_batchio.tsc - Test for successful extent repair 3 cell using Batch IO\n\n###############################################################\n     ###  Test for corrupted extent repair in 3 Cell using Batch IO\n    1. Exascale 3 Cell setup\n    2. Create a Vault DATA1 and file file1 inside it.\n    3. Use exciogen transalation to get cell and swithch Cell con text\n    4. Drop Flash Cache and set the configuration\n       cellcli -e drop flashcache all; echo \"_cell_ves_max_bad_ext_md=-1\" >> $T_WORK/cellinit.ora;\n    5. Use exciogen to perform transalation on file1 to get extent id and disk number.\n    6. Use extent id and disk from exciogen to get\n       priCDOffset and secCDOffset from ves_dump cellcli output\n    7. Corrupt the metadata\n        dd if=/dev/zero of=$OSS_DEVDIR/$cell_disk2 count=1 bs=200 oflag=seek_bytes seek=$pricdoffset2 conv=notrunc\n        dd if=/dev/zero of=$OSS_DEVDIR/$cell_disk2 count=1 bs=200 oflag=seek_bytes seek=$seccdoffset2 conv=notrunc\n    8. Change the config parameter\n       cellsrv.cellsrv_setparam(_cell_ves_simulation, 128)\n    9. Restart the cell and read the file @DATA1/file1.\n       Reading @DATA1/file1 should return error.\n        Error message: XSH-11: Cannot discover Exascale storage\n    10. Check for message: \"ves_read operates on a corrupted extent\" in\n        \"EgsCacheGetJob operates on a corrupted extent\"  trace\n    11. Wait for 35 second and read file1 again, it should return more lines than step 10\n    12. Change the configuration and read file1 after 40 second.\n        It should return same numbers of lines as in Step 11  in trace.\n    13. Write to corrupted extent should succeed and return same number of message in trace\n    14. Restart the Cell and read file1 again, it should return same number of message in trace\n###############################################################\n\nTest for successful extent repair using Batch_IO\n    LRG: lrgsaexacldeds3",
    "platform": null
  },
  {
    "test_name": "tsagexcedsmdrepair2_blockio.tsc",
    "setup": null,
    "flags": {
      "oss_multims_testing": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcedsmdrepair2_blockio.tsc - test BLOCKIO_READ_ERR with coruption\n\n#     1. Exascale 3 Cell setup\n    2. Create a Vault DATA1 and file file1 inside it.\n    3. Use exciogen transalation to get cell and swithch Cell con text\n    4. Drop Flash Cache and set the configuration\n       cellcli -e drop flashcache all; echo \"_cell_ves_max_bad_ext_md=-1\" >> $T_WORK/cellinit.ora;\n    5. Use exciogen to perform transalation on file1 to get extent id and disk number.\n    6. Use extent id and disk from exciogen to get\n       priCDOffset and secCDOffset from ves_dump cellcli output\n    7- simulate read error on secondary mirror using BLOCKIO_READ_ERR\n    8- corrupt metadata of primary mirror\n    9- check relevant traces\n\nadded in lrgsaexacldeds3",
    "platform": null
  },
  {
    "test_name": "tsagexcedsmdrepair2_resilver.tsc",
    "setup": null,
    "flags": {
      "oss_multims_testing": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcedsmdrepair2_resilver.tsc -\n\nCreate test file  make vault, create file, write 4K data, and verify with cat.\n\nDetect primary cell  run exciogen on the file and source the matching cell context script.\n\nReset flashcache  flush and drop flashcache, then set _cell_ves_max_bad_ext_md=-1 in cellinit.ora.\n\nBlock repairs  inject VES_JOB_HANG event to hang upcoming repair jobs.\n\nCorrupt secondary SAP  zero out the secondary SAP block on disk.\n\nTrigger resilvering  recreate flashcache and fire ves_corrupt_extent_sap event, then confirm hang in logs.\n\nRestart cell services  reboot cellsrv and wait ~5 minutes to see repair job entries in trace logs.\n\nValidate data  read file from all mirrors (--imirror=0/1/2) and ensure data matches.",
    "platform": null
  },
  {
    "test_name": "tsagexcedssnap.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcedssnap.tsc - Tests for EDS snapshot-related functions\n\nAdditional: Tests for EDS snapshot related functions using batch files in edstool",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcedsspaceused.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcedsspaceused.tsc - Space used calculation tests on EDS dataset\n\nThis test consists following testcases:\n    1. Check that the datasets correctly calculate the sizes of\n        the auto-generated files after startup\n    2. Check that space calculation = provisioned space and\n       that provisioned space is correct for the media types\n    3. Does the provisioned size correctly scale with the redundancy?\n    4. Test adding, resizing, and deleting clones and snapshots\n    5. Calculating space used when both allocated size and\n       provisioned size are present. Different cases are:\n     i. Do we take the larger of provisioned size and allocated size for a single file?\n    ii. Do clones share provisioned size cushion as allocated size grows?\n   iii. Do clones share provisioned size as provisioned size grows?\n    iv. Does allocated size update correctly as we delete clones?\n     v. Does allocated size update correctly when we delete invisible nodes and snapshots?\n    vi. Does allocated size update correctly if we move nodes to a different dataset?\n   vii. Does provisioned + allocated size update correctly when nodes in different datasets are deleted?\n    6. Space allocated update could be blocked for a long time",
    "platform": null
  },
  {
    "test_name": "tsagexcedstool.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcedstool.tsc - Tests for edstool\n\nTests edstool utility to create/list/alter containers/files",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcedvacfs.tsc",
    "setup": null,
    "flags": {
      "python_path": "^ADE_VIEW_ROOT^/python/bin",
      "edvsetup": "1",
      "make_acfs": "1"
    },
    "description": "tsagexcedvacfs.tsc - Sets up cluster EDV with ACFS on it\n\nCluster EDV + ACFS\n\nCluster EDV + ACFS",
    "platform": null
  },
  {
    "test_name": "tsagexcedvacfsclean.tsc",
    "setup": null,
    "flags": {
      "cleanup_acfs": "1"
    },
    "description": "tsagexcedvacfsclean.tsc - Cleanup for tsagexcedvacfs.tsc\n\nCleanup for tsagexcedvacfs.tsc\n\nCleanup for tsagexcedvacfs.tsc",
    "platform": null
  },
  {
    "test_name": "tsagexcedvdrl.tsc",
    "setup": "xblockini",
    "flags": {
      "python_path": "^ADE_VIEW_ROOT^/python/bin",
      "ledvsetup": "1",
      "edvcleanup_only": "1",
      "sage_mirror_mode": "high",
      "oss_multims_testing": "true",
      "cell_with_xrmem_cache": "0",
      "egs_cert_dur_in_mins": "20"
    },
    "description": "tsagexcedvdrl.tsc - Test for DRL v1.1\n\nTest verifies enable/disable DRL v1.1 on a live system\n     while IO workload is running on block-store volumes",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcedviscsibsfail.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcedviscsibsfail.tsc - BSM/BSW restart in EDV + ISCSI volume env\n\nBSM/BSW restart in EDV + ISCSI volume env",
    "platform": null
  },
  {
    "test_name": "tsagexcedviscsicon.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcedviscsicon.tsc - EDV + ISCSI concurrency test\n\nEDV + ISCSI concurrency test",
    "platform": null
  },
  {
    "test_name": "tsagexcedvsetup.tsc",
    "setup": null,
    "flags": {
      "debugstr": "\"edvsetup::debug::\"",
      "HAS_FLEXC_GNS_CONF": "false",
      "TKFV_USE_PCW_RSC": "1"
    },
    "description": "tsagexcedvsetup.tsc - Stack up script for block device services with EDV\n\nThis script creates a new user oracle and sets up EDV with\n     that user. This script is called from with in xblockini\n     when xblockini is run with edvsetup flag.\n\n     To run xblockini with edvsetup run the following scripts for prerun cleanup\n     1. cd oss\n     2. sh src/tools/edv/edvcleanup.sh\n     3. oratst -d xblockini edvsetup",
    "platform": null
  },
  {
    "test_name": "tsagexcedvsetupiscsi.tsc",
    "setup": null,
    "flags": {
      "debugstr": "\"edvsetup::debug::\"",
      "HAS_FLEXC_GNS_CONF": "false",
      "TKFV_USE_PCW_RSC": "1"
    },
    "description": "tsagexcedvsetupiscsi.tsc - Stack up script for block device services with EDV\n\nThis script creates a new user oracle and sets up EDV with\n     that user.",
    "platform": null
  },
  {
    "test_name": "tsagexcefmediaspcr.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagexcefmediaspcr.tsc - mixed EF media SP\n\nCreates SP on 2 cells with QLC disks + 1 cell with non QLC\n\nCovers Bug 37659085",
    "platform": null
  },
  {
    "test_name": "tsagexcegs_coldstart.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcegs_coldstart.tsc - Test for Cold Start mechanism in EGS\n\nFunctional test for new cold start mechanism\n     Implementation of cold start mechanism - the EGS leader will wait for\n      all the online CELLs before the cold start to come back online,\n      unless a skip waiting parameter is set\n     Cold start is implemented if\n     (1) EGS has been down for more than a threshold (default 5 minutes),\n     (2) the cluster was shut down by chcluster --shutdown last time,\n     (3) there is a simulated cold start event.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcegs_coldstart1.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "oss_multims_testing": "true"
    },
    "description": "tsagexcegs_coldstart1.tsc - Test for Cold Start mechanism in EGS\n\nFunctional test for new cold start mechanism\n     Implementation of cold start mechanism - the EGS leader will wait for\n      all the online CELLs before the cold start to come back online,\n      unless a skip waiting parameter is set\n     Cold start is implemented if\n     (1) EGS has been down for more than a threshold (default 5 minutes),\n     (2) the cluster was shut down by chcluster --shutdown last time,\n     (3) there is a simulated cold start event.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcegs_diskdscv.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcegs_diskdscv.tsc - Test for Disk Discovery in EGS\n\nAdditional: Test for Disk Discovery in EGS when cell crashes using a simulation event on a cell - DDSCVR_CELLSRV_DEATH",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcegscrash.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcegscrash.tsc - Simulates EGS crash by shutting down cellsrv\n\nNone\n\nNone",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcegsdiskdiscovery.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcegsdiskdiscovery.tsc - Test for disk discovery\n\nDrop and create storagepool with hang \"EGS_DD_BEF_PERS_HANG\"\n\nTest Steps\n     1. Drop the OPC_HC_STORAGE storage pool.\n     2. Find which Cell the EGS leader is running on and switch to that Cell instance.\n     3. Simulate the Cell event that will cause the EGS leader to hang for 5 minutes\n         when running the create SP command before running disk discovery\n         cellcli -e 'alter cell egsEvents=\"ebs_simevent[EGS_DD_BEF_PERS_HANG] count=1\"'.\n     4. Create Storage Pool command.\n     5. Wait for 1 minute.\n     6. Cancel the hang simulation event on the Cell instance where the current EGS leader is running\n      cellcli -e 'alter cell egsEvents=\"ebs_simevent[EGS_DD_BEF_PERS_HANG] off\"'\n     7. Run cellcli -e \"alter physicaldisk '''<disk_name>''' simulate failuretype=failed\"\n     8. Kill the EGS leader to trigger leader re-election\n     9. After step 4 finishes, run the ESCLI command \"lsstoragepool --detail\" to\n         check that the Storage Pool OPC_HC_STORAGE is successfully created.\n     10. Grep for \"OPC_HC_STORAGE is Operational\" from the egs alert_*.log files.\n     11. Cancel the simulated disk failure\n     12. Startup the previously killed EGS leader",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcegsdiskdiscovery2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcegsdiskdiscovery2.tsc - Test for disk discovery\n\nDrop and create storagepool with hang \"EGS_DD_PERS_HANG\"\n\nTest Steps\n     1. Drop the OPC_HC_STORAGE storage pool.\n     2. Find which Cell the EGS leader is running on and switch to that Cell instance.\n     3. Simulate the Cell event that will cause the EGS leader to hang for 5 minutes\n         when running the create SP command before running disk discovery\n         cellcli -e 'alter cell egsEvents=\"ebs_simevent[EGS_DD_PERS_HANG] count=1\"'.\n     4. Create Storage Pool command.\n     5. Wait for 1 minute.\n     6. Cancel the hang simulation event on the Cell instance where the current EGS leader is running\n      cellcli -e 'alter cell egsEvents=\"ebs_simevent[EGS_DD_PERS_HANG] off\"'\n     7. Run cellcli -e \"alter physicaldisk '''<disk_name>''' simulate failuretype=failed\"\n     8. Kill the EGS leader to trigger leader re-election\n     9. After step 4 finishes, run the ESCLI command \"lsstoragepool --detail\" to\n         check that the Storage Pool OPC_HC_STORAGE is successfully created.\n     10. Grep for \"OPC_HC_STORAGE is Operational\" from the egs alert_*.log files.\n     11. Cancel the simulated disk failure\n     12. Startup the previously killed EGS leader",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcegsdiskdiscovery3.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcegsdiskdiscovery3.tsc - Test for disk discovery\n\nDrop and create storagepool with hang \"EGS_DD_AFT_PERS_HANG\"\n\nTest Steps\n     1. Drop the OPC_HC_STORAGE storage pool.\n     2. Find which Cell the EGS leader is running on and switch to that Cell instance.\n     3. Simulate the Cell event that will cause the EGS leader to hang for 5 minutes\n         when running the create SP command before running disk discovery\n         cellcli -e 'alter cell egsEvents=\"ebs_simevent[EGS_DD_AFT_PERS_HANG] count=1\"'.\n     4. Create Storage Pool command.\n     5. Wait for 1 minute.\n     6. Cancel the hang simulation event on the Cell instance where the current EGS leader is running\n      cellcli -e 'alter cell egsEvents=\"ebs_simevent[EGS_DD_AFT_PERS_HANG] off\"'\n     7. Run cellcli -e \"alter physicaldisk '''<disk_name>''' simulate failuretype=failed\"\n     8. Kill the EGS leader to trigger leader re-election\n     9. After step 4 finishes, run the ESCLI command \"lsstoragepool --detail\" to\n         check that the Storage Pool OPC_HC_STORAGE is successfully created.\n     10. Grep for \"OPC_HC_STORAGE is Operational\" from the egs alert_*.log files.\n     11. Cancel the simulated disk failure\n     12. Startup the previously killed EGS leader",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcegsdiskdiscovery4.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "num_cells": "4",
      "media_type": "HC,EF",
      "delta_sync": "true"
    },
    "description": "tsagexcegsdiskdiscovery4.tsc - Test for disk discovery\n\nDrop and create HC and EF storagepool with hang \"EGS_DD_AFT_PERS_HANG\"\n\nTest Steps\n     1.  Drop the OPC_HC_STORAGE and OPC_EF_STORAGE storage pools.\n     2.  Create the OPC_HC_STORAGE storage pool in the background. After waiting\n          for 30 seconds, also create the OPC_EF_STORAGE storage pool in the\n          background.\n     3.  Wait for more than 1 minute and validate the creation of both storage pools.\n          Run the ESCLI command \"lsstoragepool --detail\" to check that the\n          OPC_HC_STORAGE and OPC_EF_STORAGE are successfully created.\n          Grep for \"OPC_HC_STORAGE is Operational\" and \"OPC_EF_STORAGE is Operational\"\n          from the egs alert_*.log files.\n     4.  Find which Cell the EGS leader is running on and switch to a Cell instance\n         where the EGS leader is not running on.\n     5.  Run the command cellcli -e drop griddisk all prepare with the argument\n          prefix equal to \"HC_cell\" and \"EF_cell\". This marks all the HC and EF\n          griddisks on that Cell instance for drop during alter SP reconfig.\n     6.  Switch to the Cell instance where the EGS leader is running.\n     7.  Simulate the Cell event that will cause the EGS leader to hang for 5 minutes\n          when running the alter SP reconfig command before running disk discovery\n          cellcli -e 'alter cell egsEvents=\"ebs_simevent[EGS_DD_AFT_PERS_HANG] count=1\"'.\n     8.  Run alter storagepool --reconfig command for both OPC_HC_STORAGE and\n          OPC_EF_STORAGE in the background at the same time.\n     9.  Wait for longer than 1 minute.\n     10. Cancel the hang simulation event on the Cell instance where the current EGS leader is running\n          cellcli -e 'alter cell egsEvents=\"ebs_simevent[EGS_DD_AFT_PERS_HANG] off\"'\n     11. Run cellcli -e \"alter physicaldisk '''<disk_name>''' simulate failuretype=failed\"\n          to fail 1 HC disk and 1 EF disk.\n     12. Kill the EGS leader to trigger leader re-election\n     13. After step 8 finishes, run the ESCLI command \"lsstoragepool --detail\" to\n          check the state of OPC_HC_STORAGE and OPC_EF_STORAGE.\n     14. Grep for \"alter storagepool OPC_HC_STORAGE resize.*successful\" from the egs alert_*.log files.\n     15. Cancel the simulated disk failures\n     16. Wait for 2 minutes and grep for \"ExaScale PoolDisk.*DROPPED permanently\"\n          from the egs alert_*.log files to check for number of disks dropped.\n     17. Startup the previously killed EGS leader",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcegsdynamicevtprocess.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagexcegsdynamicevtprocess.tsc - test to check the dynamic processing of of events\n                                       added to cells\n\nAdditional: EGS Sandbox test to check the dynamic processing of of events added to cells",
    "platform": null
  },
  {
    "test_name": "tsagexcegsfail.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcegsfail.tsc - EGS Failure Test\n\nTest Steps -\n     1- Get a healthy ExaScale stack with 3 cells.\n     2- Shutdown EGS leader and one CELLSRV\n     3- Wait one minute. During this delay, the new leader is\n         elected and it will offline the CELLSRV in step 2.\n     4- Startup EGS and CELLSRV from step 2, again.\n     5- Wait until resync is completed and all pooldisks are online",
    "platform": null
  },
  {
    "test_name": "tsagexcegsfail1.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "update_jwt_secret_refresh_time": "true"
    },
    "description": "tsagexcegsfail1.tsc - Test scenario where EGS hang was noticed\n\nTest Steps-\n     1- shutdown one cell and wait until pooldisks are offline.\n     2- shutdown all EGS services.\n     3- Startup the cell again.\n     4- startup all EGS services.\n     5- EGS leader bootstrap should be successful\n\n    Test Steps for BUG - 33070157\n     1. Add \"_egs_ers_secret_expiry_minutes=5\" in excloudinit.ora of ALL\n           EGSs BEFORE starting up EGS for the first time.\n     2. Stackup.\n     3. Wait for 10 minutes, to ensure JWT secret was refreshed at least once.\n     4. Restart all EGSs and confirm that the restart finished\n           successfully",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcegsfail2.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcegsfail2.tsc - Test scenario to verify the fix added for the\n                            issues seen in docker lrgs\n\nAdditional: Miscellaneous EGS Failure tests",
    "platform": null
  },
  {
    "test_name": "tsagexcegsfaildb.tsc",
    "setup": null,
    "flags": {
      "media_type": "HC,EF",
      "NUM_FLASH_PER_CELL": "4"
    },
    "description": "tsagexcegsfaildb.tsc - EGS Failure Test\n\nTest Steps-\n     1- Get a healthy ExaScale stack with 3 cells.\n     2- Shutdown EGS leader and one CELLSRV\n     3- Wait one minute. During this delay, the new leader is\n         elected and it will offline the CELLSRV in step 2.\n     4- Startup EGS and CELLSRV from step 2, again.\n     5- Wait until resync is completed and all pooldisks are online\n\n    The test is run with active clients and non-active clients",
    "platform": null
  },
  {
    "test_name": "tsagexcegsfaildb_qlc.tsc",
    "setup": null,
    "flags": {
      "NUM_FLASH_PER_CELL": "4"
    },
    "description": "tsagexcegsfaildb_qlc.tsc - EGS Failure Test with QLC DISKS setup\n\nTest Steps-\n     1- Get a healthy ExaScale stack with 3 cells.\n     2- Shutdown EGS leader and one CELLSRV\n     3- Wait one minute. During this delay, the new leader is\n         elected and it will offline the CELLSRV in step 2.\n     4- Startup EGS and CELLSRV from step 2, again.\n     5- Wait until resync is completed and all pooldisks are online\n\n    The test is run with active clients and non-active clients\n\nTEST IS SAME AS tsagexcegsfaildb.tsc but WITH QLC DISKS",
    "platform": null
  },
  {
    "test_name": "tsagexcegslistleaderurl.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcegslistleaderurl.tsc\n\nTest to check `list escluster attributes leaderUrl`\n   command should not return an empty string despite the EGS Cluster not undergoing any\n   leadership changes.\n\n   check the port from _exc_egs_listener_port from $T_WORK/cellinit.ora\n   Find the leader and obtain the port number from the URL\n   verify that these 2 ports match.\n\n   set event to RAFT_GET_LEADER_URL_ERR,\n   This will trigger a 5 second delay in handling append entries\n\n   now create a user, esadmin_wallet mkuser usera and\n   cellcli> list escluster attributes leaderUr, at the same time.\n   The command should work fine and return the leaderUrl",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcegsrecover.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "exc_feature_manager_init_delay": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcegsrecover.tsc - EGS LEADER RECOVERY TEST\n   DESCRIPTION:\n    Test for avoiding hang in EGS leader during cell recover\n    TEST STEPS:\n    (1) create storagepool with 3 cells\n    (2) stop cellsrv-1\n    (3) stop cellsrv-2\n    (4) set simulation event to crash EGS-LEADER when it attempts to OFFLINE a disk.\n    (5) stop one of the FOLLOWER egssrv\n    (6) stop cellsrv-3\n    (7) As part of OFFLINE of disks in cell-3,EGS-LEADER will die due to the simulation event.\n    (8) Shutdown the lone egssrv as well.\n    (9) start all 3 egssrvs -- but keep all the 3 cellsrvs down\n\nTest to be added in lrgsaexacldegs5",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcegssandbox1.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssb1.tsc - EGS Sandbox Tests\n\nTest scenarios included -\n     The environment is created with 3 cells followed by storagepool creation\n     The 3 cells have different sized disks.\n     1. cell_0 has 1 TB disks, cell_1 has 2 TB disks and cell_2 has 15 TB\n     2. cell_0 has 3 TB disks, cell_1 has 2 TB disks and cell_2 has 1 TB\n     3. cell_0 has 1 TB disks, cell_1 has 2 TB disks and cell_2 has 3 TB",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox10.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox10.tsc - EGS Sandbox tests\n\nEGS Sandbox tests with Cancel Scenarios and EGS Leader restart in between",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox11.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox11.tsc - EGS Sandbox tests\n\nEGS Sandbox tests where SP is altered multiple times with failure\n       in between",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox12.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox12.tsc - EGS Sandbox tests\n\nEGS Sandbox tests with Cancel Scenarios and EGS Leader restart in between",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox13.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox13.tsc - EGS Sandbox Tests\n\nTest Scenarios Included -\n      Test is added based on the scenario in Bug - 33495791\n\n     Scenario 1 -\n      1. Set disk expiry timer to 2 mins.\n      2. Set diskstate of disk on cell-0 to inaccessible_unknown\n      3. EGS will offline the disk\n      4. After 2 minutes, EGS will force drop the disk.\n      5. Sleep for a random time, so that test choses to wait or not wait\n          until rebalance is complete\n      6. Set disk state to accessible_good, EGS will force add the disk\n      7. While rebalance is going on, set disk state to inaccessible_unknown\n      8. After 2 mins, EGS will force drop the disk\n      9. Again, sleep for a random time, so that test choses to wait or not\n          wait until rebalance is complete\n     10. Set disk state to accessible_good, EGS will force add the disk\n     11. Wait for rebalance to complete\n\n     Scenario 2 -\n      Same as scenario 1, but in Step 7 disk state is set to dead\n      and the disk will be force dropped immediately",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox14.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox14.tsc - EGS Sandbox Tests\n\nThis txn addresses disk restrictions during upgrade. Previously, the\n     restrictions on disks going down for maintenance were overly strict\n     which forced cells to upgrade in sequence, instead of in parallel.\n     This txn loosens the restrictions, so that cells from different\n     groups can do down in parallel (but cells in the same group cannot)\n\n     UPDATE: Now, in cell ring model, we check for parallel upgradations\n       as follows:\n       1. Request for 3 cells maintenance, of which 2 cells will be in same\n          group, and 3rd cell's group will not overlap with first two cells.\n       2. 3rd cell and one of the cell from 1st group will go into maintenance.\n       3. Bring back the cell froe first group that went into\n          maintenance.\n       4. Now, the other cell should go into maintenance along with the\n          3rd cell, which was already in maintenance.",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox15.tsc",
    "setup": null,
    "flags": {
      "run_test": "false",
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox15.tsc - EGS Sandbox tests\n\nEGS Sandbox tests where SP is altered multiple times with failure\n       in between",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox16.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagexcegssandbox16.tsc - EGS Sandbox Tests\n\nTest scenarios included -\n          Test makes use of the dynamic event adding utility in sandbox\n          1. Create SP with 4 cells\n          2. Offline a cell and attempt to add 2 new cells //Reconfig fails\n          3. Online the cells and again reconfig SP //Should pass",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox2.tsc",
    "setup": null,
    "flags": {
      "bug32672448": "FIXED",
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox2.tsc - EGS Sandbox Tests\n\nTest scenarios included -\n     The environment is created with N cells followed by storagepool creation\n     Different cells have different sized disks",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox3.tsc",
    "setup": null,
    "flags": {
      "bug32746488": "FIXED",
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox3.tsc - EGS Sandbox Tests\n\nTest Cases included -\n       1. Start env with 3 cells - after 4/5 mins add 4 more cells -\n           SP reconfig\n       2. Start env with 3 cells - after 4 mins add 3 more cells -\n           SP reconfig - after couple more mins - drop 1 cell\n       3. Start env with 3 cells - after 5 mins - start adding more\n           cells upto 50 and resize SP",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox4.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox4.tsc - EGS Sandbox Tests\n\nTest Scenarios Included- SP Reconfig tests with EGS Leader Restart\n      1. Start with 3 cells - create SP - After 2 mins restart\n          EGS Leader - Once new leader is elected - alter SP reconfig\n      2. Start with 4 cells - create SP - drop 1 cell's disks and wait\n          for rebalance to finish - Restart EGS Leader -\n          Once new EGS Leader is elected - alter SP reconfig",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox5.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox5.tsc - EGS Sandbox Tests\n\nTest Scenarios Included -\n      SP Resize tests due to change in disk states or add/drop of cells\n      1. Start env with 3 cells, two disks on a cell go to accessible_needs_triage state\n         Check if cells are force dropped, if Rebalance is done and\n          bring back the disks to available state\n         Again make the same disks go to needs triage state and check if rebalance is done\n      2. Start with 6 cells - after 5 mins - grow upto 14 cells -\n          alter SP reconfig - 2 mins after rebalance is done -\n          drop 2 cells - alter SP reconfig and wait for rebalance\n      3. Start with 6 cells - after 5 mins all the cells go for\n          maintenance one by one",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox6.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox6.tsc - EGS Sandbox Tests\n\nTest Scenarios Included - Multiple Failures\n       1. Two cells request for maintenance at the same time\n\negs_sandbox_setup.sh\n     egs_sandbox_cellevents.sh\n     egs_sandbox_run.sh",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox7.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox7.tsc - EGS Sandbox Tests\n\nTest Scenarios Included -\n       1. Start with 3 cells where cell_0 has 2TB disks, cell_1 has 1TB\n            disks and cell_2 has 15TB disks\n          cell2's 1 disk is failed and replaced between 5 to 7 mins\n          cell 1 reconnects at 7th min and cell0 reconnects at 8th min\n          cell1's 1 disk is failed and replaced between 9 to 11 mins\n          cell 0 reconnects at 11th min and cell2 reconnects at 12th min\n          cell0's 1 disk is failed and replaced between 13 to 15 mins\n          cell 2 reconnects at 15th min and cell1 reconnects at 16th min",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox8.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox8.tsc - EGS Sandbox Tests\n\nTest Scenarios Included -\n       1. Start with 7 cells, 5 minutes after SP creation all cells go\n           unreachable and come back alive one by one\n       2. Start with 3 cells, after 5 minutes 4 more cells are added\n           and SP is reconfigured. After rebalance is finalized restart\n           EGS Leader\n       3. Start with 6 cells - 3 mins after SP creation, cell_5 goes\n           unreachable and all disks are force dropped - wait for\n           rebalance over - restart EGS Leader",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox9.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox9.tsc - EGS Sandbox Tests (Multiple Failures)\n\nTest Cases included -\n      1. SP is created with 26 cells\n         A couple of disk failures on cell 0 and cell_18 going offline at\n          same time -> disk comes back (cell_18 remains down) -> wait for\n          rebalance -> cell_5 goes for maintenance -> cell_18 comes back\n          online and is added back -> cell_5 comes out of maintenance\n      2. SP is created with 26 cells\n          cell_3 -> goes offline at 5th min, comes back at 7th min\n          cell_16 -> goes offline at 6th min, comes back at 8th min\n          10 more cells start being added about 30 secs before cell_3\n            goes offline\n      3. SP is created with 26 cells\n          All 26 cells have different sized disks - 3TB, 2TB and 1TB\n          Add 3 more cells having disk sizes 500G, 15TB and 500G\n          4 mins later, multiple cells go unreachable (cell_0, cell_3 and cell_05)\n            for a min and come back, wait for rebalance finalized,\n             again 2 cells go unreachable for 90 secs,\n              wait for rebalance finalized",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox_ef1.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcegssandbox_ef1.tsc - EGS Sandbox tests\n\nTest Scenarios Added are -\n       Test Case -1\n       1. create cells with 4 disks\n       2. create SP with 3 cells\n       3. add a 4th cell; wait for rebalance\n       4. add a 5th cell; wait for rebalance\n       5. add a 6th cell; wait for rebalance\n       Test Case -2\n       Same as testcase-1 but the cells are created with 8 disks.",
    "platform": null
  },
  {
    "test_name": "tsagexcegssandbox_spcreation.tsc",
    "setup": null,
    "flags": {
      "exctrcdir": "^t_diag^'/diag/EXC/exc/'^hostname^'/trace/'",
      "mtype": "^media_type^",
      "disk_size": "^size_of_disks^",
      "disk_num": "^no_of_disks^",
      "starting_cell": "^x^",
      "ending_cell": "^y^",
      "test_name": "^tst_tscname^_^cell_count^"
    },
    "description": "tsagexcegssandbox_spcreation.tsc - EGS Sandbox Tests\n\nTest scenarios included -\n          Test makes use of the dynamic event adding utility in sandbox\n          Create SP with cells from 3 to 100 with varying parameters as passed in the test,\n          with creating and dropping SP in each iteration",
    "platform": null
  },
  {
    "test_name": "tsagexcenableaep.tsc",
    "setup": null,
    "flags": {
      "egs_aep_wait_before_offline": "120000",
      "aep_num_bsm": "0",
      "aep_num_bsw": "0",
      "aep_num_syseds": "5",
      "aep_num_usreds": "5",
      "prefix": "hide"
    },
    "description": "tsagexcenableaep.tsc - Enables AEP in fake hw tests\n\nThis script does the configuration needed to enable AEP\n     (Automated Exascale process placement)",
    "platform": null
  },
  {
    "test_name": "tsagexcenrollfail.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcenrollfail.tsc - Test for unauthorized user trying to enroll using edstool\n\nAfter bringing up exastackup do the following for setup:\n      1. Create a new directory and copy the existing wallet there:\n           cd $T_WORK; mkdir tmpdir; cd tmpdir;\n           cp -r ../eswallet .\n           cp ../*ora .\n      2. Run edstool here once to make sure we have all the required files in the new directory.\n      3. OSSCONF should point to  $T_WORK/tmpdir/\n      4. Change the credentials associated with the user info found in the wallet\n           export OSSCONF=$T_WORK/tmpdir/\n           escli> mkkey --private-key-file privatekey.pem --public-key-file publickey.pem\n           escli> chwallet --wallet eswallet/ --private-key-file privatekey.pem\n      5. Run edstool, having OSSCONF pointing to $T_WORK/tmpdir/\n           This should fail, and in the trace file for edstool\n           found in $ADE_VIEW_ROOT/oracle/diag/asm/use*/host_*/trace\n           there should be a message \"[TLS] Enrollment failed because the user was not authorized to enroll\"",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcenrollfail1.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcenrollfail1.tsc - Test for too many enrollment attempts\n\nTxn adds functionality to fail an enrollment request, if a user has\n     made too many enrollment attempts in the recent past.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcersconcurrency.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "admin_wallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcersconcurrency.tsc - Test for ERS commands in a loop\n\nTest runs a bunch of escli commands with 50 different users in\n     loop simultaneously.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcersdoc.tsc",
    "setup": null,
    "flags": {
      "TKFV_BUILD_EDVMOD": "1",
      "dsk_size": "large"
    },
    "description": "tsagexcersdoc.tsc - Generates ERSDOC\n\nContains a script to generate ERSDOC\n     Runs in lrgsaexacldiscsiedv\n\nThis needs to run in an env with Blockstore + EDV services",
    "platform": null
  },
  {
    "test_name": "tsagexcesclicellcli.tsc",
    "setup": null,
    "flags": {
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcesclicellcli.tsc - Test for running cellcli cmds from escli\n\nTest for running cellcli commands from escli\nSyntax:\n\tcellcli --cell <cell-name> [--xml] [-e] <cellcli-command>\n\nTest cases mentioned inline.",
    "platform": null
  },
  {
    "test_name": "tsagexcesclidescribe.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcesclidescribe.tsc - Test for describe command\n\nTest for describe command. Syntax: describe <resource_name>\n\nTest cases mentioned inline.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcesclihelp.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "eswallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcesclihelp.tsc - Test for escli help commands\n\nScript to test all escli help commands\n\nThis test introduces validation of help strings for\n     the following escli commands:\n      acfsctl cellcli chacl chcluster chfile chpooldisk chresourceprofile\n      chservice chstoragepool chtemplate chuser chvault chvolume\n      chvolumebackup chvolumegroup chvolumehavip chvolumesnapshot chwallet\n      chxattr clonefile dbmcli describe exit extentmap getfile\n      help ls lsacfsfilesystem lsacl lscell lscelldisk lscluster lscomputeserver\n      lsfile lsgriddisk lsinitiator lskey lspooldisk\n      lsresourceprofile lsservice lssnapshots lsstoragepool\n      lstemplate lsuser lsvault lsvolume lsvolumeattachment\n      lsvolumebackup lsvolumegroup lsvolumehavip lsvolumesnapshot lswallet\n      lsxattr mkacfsfilesystem mkfile mkkey mkresourceprofile mkstoragepool\n      mktemplate mkuser mkvault mkvolume mkvolumeattachment\n      mkvolumebackup mkvolumegroup mkvolumegroupsnapshot mkvolumehavip mkvolumesnapshot mkwallet\n      putfile quit rm rmacfsfilesystem rmfile rmresourceprofile rmstoragepool\n      rmtemplate rmuser rmvault rmvolume rmvolumeattachment\n      rmvolumebackup rmvolumegroup rmvolumehavip rmvolumesnapshot rmxattr\n      snapshotfile",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcesclilsdataset.tsc",
    "setup": "xrdbmsini",
    "flags": null,
    "description": "tsagexcesclilsdataset.tsc - Test for lsdataset command\n\nDatasets are a new resource that serve statistics about vault space usage by\n     various databases.\n\n     This test implements the lsdataset command to list datasets and its\n     attributes.\n\nESCli commands implemented:\n     lsdataset : Lists datasets and its attributes\n     lsdataset --files : List files that belong to a dataset.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcesclivaultspace.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcesclivaultspace.tsc - File creation tests\n\nThis test covers the cases :\n     1. CREATING FILES GREATER THAN PROVISIONED SPACE IN VAULT AND CLONING THEM\n     2. MATERIALISING THE FILES AND THEN CLONING THEM\n     3. BUG 35325283 - VAULT PROVISIOINABLE SPACE IS NOT BEING APPLIED\n        CORRECTLY TO SNAPSHOTS AND CLONES\n     4. SPACE QUOTA EXCEEDED ERROR - CASE 1 - MATERIALISING FILE\n     5. SPACE QUOTA EXCEEDED ERROR - CASE 2 - MATERIALISING SNAPSHOTS\n\nThis test covers the following cases:\n     (1) File creation: check if vault has enough space for the new file created. If\n         so, continue the job. Otherwise raise an error and end the job.\n     (2) File resize: check if vault has enough space for the requested file size\n         extension.\n     (3) File snapshot/clone: check if vault has enough space for the\n         stem/trunk/branch file change during snapshot/clone. If not, raise an error and\n         end the job now.\n     (4) Container Resize: check if vault still has enough space after shrink. If\n         not, reject the request.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcesclivip.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcesclivip.tsc - ESCLI test_case for mkvolumeHAVIP, lsvolumeHAVIP , rmvolumeHAVIP\n\nESCLI test_case for mkvolumeHAVIP, lsvolumeHAVIP, rmvolumeHAVIP",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcesnp_gc_fix.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcesnp_gc_fix.tsc - This txn fixed a bug that EGS leader can't garbage collect ESNPs that are shutdown when EGS leader is not available\n\n1. Create exastackup with 3 cell setup\n    2. run escli as admin wallet, and  @>lsservice --filter serviceType=nodeProxies should see 1 entry\n    3. shutdown all EGSs\n    4. shutdown ESNP\n    5. startup all EGSs back\n    6. Update the _exc_egs_listener_port to EGS leader port\n    7. alter cell egsEvents = \"ebs_simevent[EGS_EVICT_ESNP_SIM] count=1\"\n    8. wait for 11 minutes, and check the below traces in EGS leader:\n    [ESNP] evictExpiredESNPEntries:Evicted ESNP(name=phoenix96010_100.70.108.3_53194_10, nodeID=95, nodeInc=1, connTime=Thu Jan  1 00:00:00 1970 0 msec, disConnTime=Thu Sep 28 00:44:48 2023 861 msec)\n    9. run escli as admin wallet, and @>lsservice --filter serviceType=nodeProxies should see nothing\n    10. alter cell egsEvents = \"ebs_simevent[EGS_EVICT_ESNP_SIM] count=off\"\n    11. Startup ESNP",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcesnprestart.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcesnprestart.tsc - Test for restartable ESNP\n\nThe test follows the following steps:\n       1. Check ESNP metadata is properly created\n       2. Restart ESNP twice and check for metadata\n       3. Shutdown ESNP and check fence alerts (new inc. id)",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcesnpup.tsc",
    "setup": null,
    "flags": {
      "reflogname": "tsagexcesnp",
      "tstlogname": "^logfile^"
    },
    "description": "tsagexcesnpup.tsc - Starts up ESNP - EGS Node Proxy\n\nEGS Node Proxy is equivalent to Diskmon. It runs on client(DB) VM",
    "platform": null
  },
  {
    "test_name": "tsagexcexaloadgen.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcexaloadgen.tsc - test for exaloadgen\n\nhttps://confluence.oraclecorp.com/confluence/display/~sweksha.sinha@oracle.com/Exaloadgen+test+plan",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcexpimp.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcexpimp.tsc - Test to export/import a tablespace\n\nTest to export a tablespace, created a snapshot of this datafile\n      and import in other PDB for read only access and read write access\n\nThis is not a standalone test. It needs setup created in tsag4kcolumn.tsc",
    "platform": null
  },
  {
    "test_name": "tsagexcfail_4cell.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "num_cells": "4",
      "cmd_mirror_mode": "normal",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagexcresync_4cell.tsc - resync, rebal and cancel operation test with 4 cell setup\n\nPlease see below.\n\nto be added in lrgsaexcfail_4cell",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcfail_db_bs_edv.tsc",
    "setup": null,
    "flags": {
      "maxtimeout": "20000",
      "max_num_of_snapshots": "20",
      "bs_workload_duration": "900",
      "num_of_pdbs": "2",
      "bs_entity_num": "5",
      "vault_db": "DATA",
      "dsk_size": "large",
      "auto_local_undo": "true",
      "cdb": "true",
      "oss_testing": "3",
      "sage_mirror_mode": "high",
      "iscsi": "true",
      "setup_blockstore": "true",
      "media_type": "EF,HC",
      "oss_multims_testing": "true"
    },
    "description": "tsagexcfail_db_bs_edv.tsc - run db + bs workload parallel to service restart\n\n1. initial Setup\n        setup 3 cell blockstore (multi storage) with following params\n          let oss_testing 3\n          let sage_mirror_mode high\n          let iscsi true\n          let setup_blockstore true\n          let media_type hc,ef\n          let edv setup true (setup cluster + node edv with default user)\n          let oss_multims_testing true\n\n     2. setup db on vault (xrdbmsini) - create two pdbs.\n\n     3. operations with db workload (steps 3.1 to 3.7 will run in parallel)\n      3.1 service restart in parallel to all operations - cellsrv, bsmsrv\n            will enable other service restarts after stabilizing the current setup in integration\n        3.2 start running db workload on both pdbs\n        3.3 will enable pdb carousel workloadafter Bug 36785013 is resolved\n        3.4 run EDV workload (IOV + ACFS)\n        3.5 run blockstore workload with iscsi attachment create 10+ volumes\n            (this number is configurable - half of them on EF and haolf of them on HC),\n            volume attachments (to run IOV workload), snapshots, clones, backups, on same vault\n      3.6 delete volumes and snapshots in async mode\n      3.7 run incremental backup operation on same vault",
    "platform": null
  },
  {
    "test_name": "tsagexcfail_flash.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagexcfail_flash.tsc - failing flash disks in loop\n\nTest steps:\n\n i.) - setup 4 cell env with db\n\n ii.) - - Run oltp and redolog workload\n\n In a loop (count 5); do\n\n     iii.) - On cell 3 - Failing two flash disks\n\n     iv.)  - sleep 180\n\n      v.)  - On cell 3 - Reenabling failed flash disks\n\n endloop\n\n vi.) make sure oltp and redolog workload ran fine",
    "platform": null
  },
  {
    "test_name": "tsagexcfail_restart.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagexcfail_restart.tsc - stimulate disks failure on cell3 then restart cellsrv, then rebooting egs 40 times.\n\nTest steps:\n\n i.) - setup 4 cell env with db\n\n ii.) - - Run oltp and redolog workload\n\n In a loop (count 3); do\n\n     iii.) - On cell 3 - Failing two disks\n\n     iv.)  - On cell 3 - restart cellsrv\n\n      v.)  wait for rebalance to complete\n\n      vi.)  - On cell 3 - Reenabling failed disks\n\n endloop\n\n vii.) reboot egs leader 40 times\n viii.) make sure oltp and redolog workload ran fine",
    "platform": null
  },
  {
    "test_name": "tsagexcfastfile.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "db_noarchivelog_mode": "true",
      "dsk_size": "large",
      "standalone": "true",
      "eswallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagfastfileinit.tsc - functional test for Fast File Initialization on Exascale\n\nfunctional test for Fast File Initialization on Exascale",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcfauxjob.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcfauxjob.tsc - Fauxjob test\n\nPlease see below\n\ntest to be run in lrgsaexacldeds",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcfcoppo.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "dsksz": "300",
      "creatdev_file": "tsagdat"
    },
    "description": "tsagexcfcoppo.tsc - test for opportunistic caching\n\nplease see below\n\nto be added in lrgsaexcfcoppo",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcfenceproc.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcfenceproc.tsc - process fencing test for exascale\n\nKill shadow process while doing direct path IO's\n    Runs in single instance, with exascale setup\n\n- set event to HOLD IO\n     - Fork sql which does an IO\n     - kill shadow process\n     - check if IO fencing happened",
    "platform": null
  },
  {
    "test_name": "tsagexcfilemetrics.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcfilemetrics.tsc - Metrics and File Operations tests\n\nThe test generates read and write traffic on files on different media\n       and ensures that the number of IO and space metrics are as expected.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcfilemgmt.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcfilemgmt.tsc - Test for File management commands.\n\nTest for File management commands.\n\nTest cases mentioned inline.",
    "platform": null
  },
  {
    "test_name": "tsagexcfilter_ls.tsc",
    "setup": null,
    "flags": {
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcfilter_ls.tsc - Test for filter option and its attributes for ls\n\nTest for filter option and its attributes for ls\n\nTest cases mentioned inline.",
    "platform": null
  },
  {
    "test_name": "tsagexcfilter_lssnapshots.tsc",
    "setup": null,
    "flags": {
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcfilter_lssnapshots.tsc - Test for filter option and its\n                                     attributes for lssnapshots\n\nTest for filter option and its attributes for lssnapshots\n\nTest cases mentioned inline.",
    "platform": null
  },
  {
    "test_name": "tsagexcfilter_lstemplate.tsc",
    "setup": null,
    "flags": {
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcfilter_lstemplate.tsc - Test for filter option and its\n                                     attributes for lstemplate\n\nTest for filter option and its attributes for lstemplate\n\nTest cases mentioned inline.",
    "platform": null
  },
  {
    "test_name": "tsagexcfndd.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcfndd.tsc - fndd test for exascale\n\nrestart egs services and check for fndd code path\n\nruns in lrgsaexacldegs2",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcforcedelvault.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexcforcedelvault.tsc - test the behavior on blockstore, if a vault has been deleted with --force flag#                                 then we try to create or delete existent blockstore objects",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcfsaprtcn.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcfsaprtcn.tsc - Test to check memory protection to FSA objects\n\nThe test scenario is to enable protection for all FSAs with object sizes\n     larger than 6000 bytes as well as one specific small FSA, then we try\n     accessing an unallocated object in each of the following FSAs.\n\n  >> FlashLog Write Context: the object size is 224 which is smaller than\n     6000. So, we expect no crashes because it is not protected.\n  >> Cache Put Job Fixed Size: the object size is 6832 which is greater than\n     6000. We expect a crash due to invalid write access to a read-only\n     (protected) object.\n  >> PredCacheGet Job Fixed Size: the object size is 1024 which is smaller\n     than 6000, but we explicitly selected this FSA using\n     _cell_fsa_prot_by_name to protect unallocated objects.\n     Therefore, it should crash.\n\nTXN : mhoseinz_fsa-config",
    "platform": null
  },
  {
    "test_name": "tsagexcgdsmartall.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcgdsmartall.tsc - Test for smart ALL adverb - EXASCALE\n\nCurrently, when \"ALL\" is specified without other diskType qualifiers,\n      the command would target at all types of cell disks for grid disk\n      operations.\n      In the transaction - yifanch_bug-35034249, \"ALL\" is made\n      smarter and deterministic. It will filter out cell disks of the\n      primary media type on a real hardware or when\n      \"_cell_griddisk_cmd_smart_all_sim_sys\" is explicitly configured\n      (0:HC, 1:EF w/ TLC only?EF sys before X10, 2:EF w/ TLC and QLC?EF X10).",
    "platform": null
  },
  {
    "test_name": "tsagexcgraceshut.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcgraceshut.tsc - check graceful shutdown of exascale services\n\n1. shutdowns services using cellcli\n     2. check for exit code of each service in cell traces\n     3. startup services using cellcli",
    "platform": null
  },
  {
    "test_name": "tsagexchandoff.tsc",
    "setup": "xblockini",
    "flags": {
      "delta_sync": "true",
      "egs_deplmode": "onprem",
      "sage_mirror_mode": "high",
      "create_template_with_high_redund": "true"
    },
    "description": "tsagexchandoff.tsc - Test for DRL  handoff stats verification\n\nRuns 3 testcases to trigger handoff stats WITH DELTA RESYNC TRUE\n   1- DATA REBALANCE\n   2- DISK RESYNC\n   3- CELL UPGRADE\n\nTO BE ADDED IN lrgsaexchandoff_drl",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexchandoff_resync_sr.tsc",
    "setup": "xblockini",
    "flags": {
      "egs_deplmode": "onPrem",
      "create_template_with_high_redund": "true",
      "sage_mirror_mode": "high",
      "delta_sync": "false"
    },
    "description": "tsagexchandoff_resync_sr.tsc - Test for handoff stats with SR RESYNC\n\npls see below\n\nto be added in lrgsaexchandoff2",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexchardcheck_sr.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "false"
    },
    "description": "tsagexchardcheck_sr.tsc - VES hardcheck failure test\n\nPlease see below\n\nto be added in lrgveshardcheck_sr",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexchybrid_edvclean.tsc",
    "setup": null,
    "flags": {
      "PCW_NO_BLOWOUT": "true",
      "SKIP_TSAGEND": "true"
    },
    "description": "tsagexchybrid_edvclean.tsc - stop css stack and clenas up edv\n\nstop css stack and clenas up edv uninstalling edv drivers\n\nstop css stack and clenas up edv",
    "platform": null
  },
  {
    "test_name": "tsagexchybridiorm.tsc",
    "setup": null,
    "flags": {
      "oss_exascale_asm_testing": "true",
      "setup_blockstore": "true",
      "vault_db": "DATA"
    },
    "description": "tsagexchybridiorm.tsc - IORM TEST WITH HYBRID ENVIRONMENT\n\npls see below:",
    "platform": null
  },
  {
    "test_name": "tsagexcidempotence.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcidempotence.tsc - Tests idempotency in addition and deletion of\n     resources such as volumes, attachments, snapshots, backups and VIPs.\n\nIn this test, idempotency is tested through the following 3 cases:\n     Case 1: 2 Curl requests with same idemp key are sent for an operation.\n             This case is only for testing creation of resources.\n\n     Case 2: 2 Curl requests with same idemp key are sent for an operation.\n             This is done right after BSM/BSW restart. This case is also for\n             testing creation of resources.\n\n     Case 3: 2 Curl requests with same idemp key are sent for an operation\n             with a little delay in between the requests. This case is for\n             testing both creation and deletion of resources.\n\n     Also tests creation of existing attachments with invalid and incorrect\n     CHAP credentials.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcidemptncy.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcidemptncy.tsc - Test for snapshot and clone creation\n                            idempotency\n\nThis test verify if edslib has to retry a snapshot or clone operation\n     that completed successfully within the last 2 minutes, it would get a\n     successful response instead of EDSLIB_EDSSERV_EXISTS error.\n\n     Steps:\n    1. Create a vault and a file in it\n    2. Simulate EDS_SRV_SNAP_DIE_AFTER_COMMIT\n    3. Create snapshot/clone of the file without any error\n    4. Simulate EBS_SRV_ERR_BEFORE_IOCTL_REPLY\n    5. Create snapshot/clone of the same file, it will throw error\n\nDev's txn : apyatkov_snap_idempotency",
    "platform": null
  },
  {
    "test_name": "tsagexcignmir.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcignmir.tsc - Test for iov flags: <ignore_mirror_mismatch>\n                         and <behavior_on_read_error>\n\nTest for iov flags: <ignore_mirror_mismatch> and <behavior_on_read_error>\n\nFlag: <verify_mirror_match>\n       value: true : verify if mirror is matching, if corrupted throw error.\n              false: ignore mirror mismatch, if any.\n     Flag: <behavior_on_read_error>\n       value: ignore: ignore any read error and continue iov read.\n              throw_read_error: throw error on first read and abort iov read\n              retry: retry reading till iov timer is over, if read\n                     error goes away continue with read for rest of the\n                     iov timer.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcinstall_debug_kernel.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "cell_node": "^cell_node^"
    },
    "description": "tsagexcinstall_debug_kernel.tsc - Install debug kernel in cell nodes\n\nThe test steps :\n      1. Install the test kernel-ueknano\n       a) Comment out \"image_functions_clean_hd_boot\" in /opt/oracle.cellos/validations/init.d/misceachboot\n       b) Back up grub.cfg (/boot/efi/EFI/redhat/grub.cfg)\n       c) Install kernel RPM\n       d) List /boot directory, and get the new entries (Example: vmlinuz-5.4.17-2136.316.7.el8uek.aspen1.ol8.x86_64 and initramfs-5.4.17-2136.316.7.el8uek.aspen1.ol8.x86_64.img)\n       e) Convert back the backup grub.cfg file and modify the high-lighted parts in the first menuentry in /boot/efi/EFI/redhat/grub.cfg  with the new entries in step d.\n       f) If 'mokutil --sb-state' already returns \"SecureBoot disabled\" proceed with step h.\n       g) else 'ubiosconfig export all -x /tmp/enabled_secureboot.xml -f -E' , Edit the /tmp/enabled_secureboot.xml file and change the setting for Secure Boot from Enabled to Disabled. That is, change this line in the file: <Attempt_Secure_Boot>Enabled</Attempt_Secure_Boot> , 'ubiosconfig import all -x /tmp/disabled_secureboot.xml -f -E -y' , then proceed with step h\n       h) Reboot.\n       i) Check if the kernel is installed correctly using uname.\n      2. Place debug-ib-comp-err.sh under /root\n      3. Put rds_rdma.conf under /etc/modprobe.d/\n      4. Reboot node",
    "platform": null
  },
  {
    "test_name": "tsagexcinteroputil.tsc",
    "setup": null,
    "flags": {
      "num_arg": "0",
      "argname": "arg^num_arg^",
      "argvalue": "^^argname^",
      "remotesetup": "1",
      "gettest": "1",
      "p_varname": "^varname^",
      "exitfatal": "FATAL",
      "dbgprfx": "^tst_tscname^':debugg::'",
      "remote_host": "^lhost^"
    },
    "description": "tsagexcinteroputil.tsc - wrapper over tsagexcinteroputil.py\n\nThis has a lot of useful functions\n\n     checkconfig\n     ------------\n     Check if the exascale related config parameters are set\n\n     checkssh\n     ------------\n     check if we can login to the remote box\n\n     checklocalip\n     ------------\n     Check if the local host is accessible in the host configured\n\n     checkremoteip\n     ------------\n     Check if the remote box is accessible on the known ip's from\n     ifconfig\n\n     getfile\n     ------------\n     Gets the file from remote box\n\n     putfile\n     ------------\n     Puts the file to the remote box\n\n     runcommand\n     ------------\n     Runs any command on the remote host\n\n     getrhost\n     ------------\n     Returns remote host name, if not set uses localhost\n\n     remotesetup\n     ------------\n     Does a lot of checks on the remote box for exascale\n\n     gettest\n     ---------\n     Get the oss test name to run",
    "platform": null
  },
  {
    "test_name": "tsagexciormvlt1.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "egs_deplmode": "sharedCloud"
    },
    "description": "tsagexciormvlt1.tsc - Inter Vault IORM Plan\n\nVerifies that inter Vault IORM Plans are propagated to cells\n      on create/update/delete Vaults\n      on create/update/delete resource profiles\n      when relevant storage pool attributes have changed",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexciov_cellrestart.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "num_cells": "4",
      "test_iters": "10"
    },
    "description": "tsagexciov_cellrestart.tsc - IOV test with source and target cell restart\n\nAdditional: Resync test when source or target cell restarts during Resync operation in an 4 cell env.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexciovworkload.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "create_template_with_high_redund": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexciovworkload.tsc - common helper file for drl double failure test\n\nsetup 3 cells and create few common scripts to be used in multiple tests\n\nto be added in failc*_drl lrgs",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexclargewrite.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexclargewrite.tsc - FC LARGE WRITE test for exascale\n\nRun iov and orion workload\n#     check large write stats\n\nto be added in lrgsaexcfclarge_nondb",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcldescli.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcldescli.tsc - test for escli commands.\n\nTesting ls,mkvault,rmvault,putfile,getfile,mkuser,rmfile,mkfile commands",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcleader.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcleader.tsc - Tests EGS Leader election\n\nTests EGS Leader Election happens within 5 secs of Leader dying",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcliattr.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagexcliattr.tsc - privilege test for exacli user on all objects\n\nIn this test we test the access control infrastructure of MS. We verify that\n      commands cannot be executed when no privilege is granted for that command, and\n      the command can be executed when proper privileges are granted. This is tested\n      for all combinations every object, for verbs list, alter, and\n      every attribute the object contains.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagexcliattr2.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagexcliattr2.tsc - testing create on exacli\n\nCreate command with all objects from help create for exacli",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagexcliattr3.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagexcliattr3.tsc - testing drop on exacli\n\nCreate command with all objects from help drop for exacli",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagexclink_parent_child_bkps.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true",
      "multiple_bst": "true # Inorder to setup 2 BSM + 3 BSW instances",
      "setup_blockstore": "true"
    },
    "description": "tsagexclink_parent_child_bkps.tsc\n\nTest to check the fix of race condition betwen BswDeleteBkpJob and BswFindBkpJob\n     details : https://orareview.us.oracle.com/140800706",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcload.tsc",
    "setup": null,
    "flags": {
      "exitfatal": "FATAL"
    },
    "description": "tsagexcload.tsc - wrapper over tsagexcload.py\n\nThis has a lot of useful functions\n\n     loadrds\n     ------------\n     Loads the modules rds and rds_tcp if its not already loaded, if it fails\n     to load it will store the result in resfile\n\n     loadblkstore\n     ------------\n     Loads the modules rds, rds_tcp and target_core_user if its not already\n     loaded, if it fails to load it will store the result in resfile\n\n     loadmod\n     ------------\n     Loads the given module(s) if its not already\n     loaded, if it fails to load it will store the result in resfile\n\n     resfile\n     ------------\n     If called from macro will be blowout.dif\n\n     noexitfatal\n     ------------\n     We will exit fatally thereby aborting entire lrg, if this option is not\n     set",
    "platform": null
  },
  {
    "test_name": "tsagexclogicscrub.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexclogicscrub.tsc - Test for Logical Scrubbing tool\n\nplease see below",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexclogvolume1.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexclogvolume1.tsc - Basic Tests for LOG Volume\n\nTests for LOG Volume - create, snap, backup, restore",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexclscluster.tsc",
    "setup": "xblockini",
    "flags": {
      "admin_wallet": "^T_WORK^/esadmin_wallet"
    },
    "description": "tsagexclscluster.tsc - ESCLI test for lscluster\n\nESCLI test for lscluster lie below\n     1. Volume rate limiting parameters can now be set per media type\n(Hdd/Af/Pmem/Ml).\n\n      2. BSM can now pick up updates to rate limiting params and update its workingstate.\n\n      3. Store backup service params in EGS. However, these don't get used yet.\n\n      4. escli lscluster does not show volume or backup params anymore:\n        > escli --user admin:welcome1 lscluster --detail\n\n      5. To list cluster level volume params, you pass a special switch:\n        > escli --user admin:welcome1 lscluster --volume --detail\n\n      6. To list cluster level backup params, you pass a different special switch:\n\n      7. These cluster attributes are now 'hidden' by default: leaderId, leaderInc,alterServiceTimeoutInSecs, cellCommTimeoutInSecs.\n         To list these, you specify the attributename.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexclssnapshots.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "eswallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexclssnapshots.tsc - test_case for lssnapshot",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexclssrvmgmt_docker.tsc",
    "setup": null,
    "flags": {
      "fulldate1": "^fulldate^",
      "onlysetupdocker": "true",
      "tsagdockercell": "0",
      "escli_ph": "'--user admin:welcome1 --ctrl ers:5052'",
      "fulldate2": "^fulldate^"
    },
    "description": "tsagexclssrvmgmt_docker.tsc - Test for running lscell, lscelldisk,\n\t\t\t     and lsgriddisk commands from docker on 3\n\t\t\t     cell setup.\n\nTest for running lscell, lscelldisk, and lsgriddisk commands from\n     docker on 3 cell setup.",
    "platform": null
  },
  {
    "test_name": "tsagexclsvltwterr.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexclsvltwterr.tsc\n       - Test to quarantine System DataStore Shards in EGS\n       - Test to list vault using escli ls and xsh ls\n         in case of syseds error for one of the shards",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcmailvld.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcmailvld.tsc - Test for a mail validation NPE fix",
    "platform": null
  },
  {
    "test_name": "tsagexcmailvld_db.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcmailvld_db.tsc - Test for a mail validation NPE fix",
    "platform": null
  },
  {
    "test_name": "tsagexcmaterializedspaceusage.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcmaterializedspaceusage.tsc - Test for materialized space usage\n\n1. For volumes spaceUsed is equal to redundancy * size.\n    2. For clones, make sure value of spaceUsed is equal to what EDS returns.\n    3. For snapshots make sure value of spaceUsed is equal to what EDS return.\n\nWe have an attribute named redundancy. We can get it by querying escli: lsvolume --attributes redundancy.\n     spaceUsed = redundancy * size\n      -> When redundancy is high, spaceUsed = 3 * size.\n      -> When redundancy is normal, spaceUsed = 2 * size.\n      -> When redundancy is none, spaceUsed = 1 * size.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcmcrebal.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcmcrebal.tsc - Multi cell oss rebal test",
    "platform": null
  },
  {
    "test_name": "tsagexcmcrebal_fail1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcmcrebal_fail1.tsc - Multi cell oss rebal + egs kill/revive",
    "platform": null
  },
  {
    "test_name": "tsagexcmcrsync.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcmcrsync.tsc - Multi cell oss resync test\n\n1. Set up multi view exascale with database setup in driver file for this test.\n    2. In the test  - run DB workload in the background for the entire duration of test.\n    3. bounce cell2 and cell1 while workload is running.\n    4. Verify workload for successful completion at the end of the test .",
    "platform": null
  },
  {
    "test_name": "tsagexcmcrsync_fail1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcmcrsync_fail1.tsc - Multi cell oss resync + EGS kill test\n\n1. Set up multi view exascale with database setup in driver file for this test.\n    2. In the test  - run DB workload in the background for the entire duration of test.\n    3. Bounce cell1 and kill and revive egs in between, while workload is running.\n    4. Verify workload for successful completion at the end of the test .",
    "platform": null
  },
  {
    "test_name": "tsagexcmemoryoutspace.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcmemoryoutspace.tsc - VES memory out of space error test\n\nPlease see below",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcmetacorruption.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexcmetacorruption.tsc - Intentional Corruption and Disaster Recovery Tests\n\nA) Intentional Corruption Test - This tests the result of incorrectly modifying a restricted field of\n        the metadata structure which can be any flag, magic number, file path etc.The result of this can be\n        anything from a missing volume, snapshot, backup, to service interruption.\n        The test steps are as follows -\n        1. Back up metafiles using the BKP option\n        2. In this test, the LUN metadata binary has been converted to a human readable format to be edited\n        3. Modify the magic number field.\n        4. Convert the JSON file to Binary using teh CVT option of the EXCMETA tool.\n\n     B) Disaster Recovery Test - The tool offers the possibility of recovering from the backup created,\n  in the event of a disaster. The test validates that the tool will return the server to a previous\n        state,the moment the backup was made.\n        The test steps are as follows -\n        1. Create a corrupt file with random data.\n        2. Feed the corrupt file to LUN metadata using the fix 'soft' option of the EXCMETA tool.\n        3. Stop the BSW and BSM services\n        4. Use fix 'Recovery' option of the EXCMETA tool to restore to a previous created backup\n\n     C) Data Consistency Test - The tool The tool has the ability to check the consistency of the data with respect to the current label.\n        By data consistency, we refer to the fields of each metadata structure, in which their equality is verified between the structures\n        declared in the header files in the current label and the structures built from the binary data obtained.\n        If a metadata field from any header file is incorrectly updated, deleted or an unwanted field is added, the tool should throw an error\n        at CVT option while converting from JSON to Binary or Vice versa. (TBD)\n\n\n     CONFLUENCE LINK FOR REFERENCE -\n     https://confluence.oraclecorp.com/confluence/display/~bryan.violante@oracle.com/Excmeta+functional+test",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcmetaenhance.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcmetaenhance.tsc - This test script is to test the enhancement of excmeta tool\n\nThis test script is to test the enhancement of excmeta tool by the following:\n       1. Support the 'excmeta cvt' on newly introduced metadata block BsmMeta_VolumeGroupBlock\n       2. Add the new option (--json) for 'excmeta bkp' to backup the metadata with JSON format.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcmetatool.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexcmetatool.tsc - Tests for EXCMETA tool\n\nThis test transaction introduces ExcMeta tool positive flow testing.\n      The EXCMETA tool is tested for the following options -\n    -help: MAN command for EXCMETA tool command and its various attributes.\n          -bkp:  Backup command to store a backup of EDS Metadata files locally in a folder.\n          -cvt:  Used to convert binary to human readable (JSON) and vice versa.\n          -diff: Used to compare and see diffs between 2 files.\n          -fix:  Used to fix Metadata files back to EDS.\n          -qtm:  Used to quarantine problematic objects.\n      This test includes validating the working and error handling for these commands\n\n      CONFLUENCE LINK FOR REFERENCE -\n      https://confluence.oraclecorp.com/confluence/display/~bryan.violante@oracle.com/Excmeta+functional+test",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcmetricstream.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmetricstream.tsc - Test for the metric streaming functionality\n                            in cells\n\nTest for the metric streaming functionality in cells. Steps:\n     (Initially restart services on the cell)\n\n      1) Metric attributes initialization\n      2) Modifying \"Enabled\" metricdefiniton\n      3) Modifying \"Disabled\" metricdefiniton\n      4) Attempt to modify \"Disallowed\" metricdefiniton\n      5) Altering Stream and Collector Intervals\n      6) Adding Metric Stream tags\n      7) Get metrics from REST Endpoint of cell with metric_collector user\n      8) Get metrics from list metricstream command on cell\n      9) Verify the number of metrics on REST Endpoint and on cell\n     10) Verify that timestamps of metrics are unique\n     11) Verify Metriccurrent timestamp and its value\n     12) Drop Existing Users and Roles\n     13) Add CellDiag role and verify allocation of metric stream privileges\n     14) Verify upload to metric streaming database: Success msg in ms-odl.trc\n     15) Disable metric streaming altering metricStreamIntvlInSec\n     16) Disable metric streaming altering mertricStreamEndPoint\n     17) Altering metric stream endpoints\n     18) Verify file persistence in pending area for fake endpoint\n     19) Verify support for multiple endpoints\n     20) Verify creation of longterm_metric file if MS shuts down\n     21) Verify streaming attribute can be enabled/disabled\n     22) Verify a metric can have different values for finegrained vs. streaming\n     23) Verify fine-grained metric is present in list metriccurrent\n     24) 34980249 - ADD \"RETENTIONPOLICY\" ATTRIBUTE TO METRICDEFINITION",
    "platform": null
  },
  {
    "test_name": "tsagexcmetricstream_db.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcmetricstream_db.tsc - Test for the metric streaming functionality\n                                  in DBNodes\n\nTest for the metric streaming functionality in DBNodes. Steps:\n     Initially, restart all services on DBNode\n\n      1) Metric attributes initialization\n      2) Modifying \"Enabled\" metricdefiniton\n      3) Modifying \"Disabled\" metricdefiniton\n      4) Altering Stream and Collector Intervals\n      5) Adding Metric Stream tags\n      6) Get metrics from REST Endpoint of DBNode with metric_collector user\n      7) Get metrics from list metricstream command on DBNode\n      8) Verify the number of metrics on REST Endpoint and on DBNode\n      9) Verify that timestamps of metrics are unique\n     10) Verify Metriccurrent timestamp and its value\n     11) Drop Existing Users and Roles\n     12) Add CellDiag role and verify allocation of metric stream privileges\n     13) Verify upload to metric streaming database: Success msg in ms-odl.trc\n     14) Disable metric streaming altering metricStreamIntvlInSec\n     15) Disable metric streaming altering mertricStreamEndPoint\n     16) Altering metric stream endpoints\n     17) Verify file persistence in pending area for fake endpoint\n     18) Verify support for multiple endpoints\n     19) Verify creation of longterm_metric file if MS shuts down\n     20) Verify streaming attribute can be enabled/disabled\n     21) Verify a metric can have different values for finegrained vs. streaming\n     22) Verify fine-grained metric is present in list metriccurrent\n     23) 34980249 - ADD \"RETENTIONPOLICY\" ATTRIBUTE TO METRICDEFINITION",
    "platform": null
  },
  {
    "test_name": "tsagexcmirvalid.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcmirvalid.tsc - Test for excmirvalid tool to validate mirrors in\n                           files of a vault.\n\nThe tool so far accepts two types of arguments, -f filename or -v\n     vaultname. If the argument is a vault type, the tool will verify all\n     files inside that vault; Otherwise, the tool will verify the single file.\n     The tool supports other optional arguments, -t (number of IO threads\n     to do validation), -b (base directory to save the validation results)\n\n     Sample usage:\n     excmirvalid -v @TEST -t 8 -b /tmp\n     excmirvalid -f @TEST/file1 -t 4 -b /tmp",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcmiscbugs2_3cell.tsc",
    "setup": null,
    "flags": {
      "oss_multims_testing": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcmiscbugs2_3cell.tsc - Test for MISC BUGS\n\nFunctional Test for BUG 36960421\n     When drive is dropped and online again, drive is not formatted. Egs will try to\n     format the drive first and sends OSS_IOCTL_EGS_CMD_CREATE_SP_OWNER to cell node.\n     This ioctl is failed  with network error due to cell restart operation.\n\n     Current we are not considering network error code in\n     SPOWNER_EgsDiskOnlineJobState and not retrying it. Hence the logic assumes that\n     disk status is expected to be  online status and asserting it.\n     Fix is  handled network error and failconnect by cancelling online job operation\n     with the fix,\n     when sp owner broadcast ioctl of diskonline job fails with network error, we\n     fail the job.\n\n     when cell is reachable again,  egs will try to online all disks. also add the\n     new drive",
    "platform": null
  },
  {
    "test_name": "tsagexcmiscbugs_3cell.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcmiscbugs_3cell.tsc --- contains misc exascale 3 cells bug tests\n\nmiscellaneous exascale bug tests, pls see below",
    "platform": null
  },
  {
    "test_name": "tsagexcmiscdocker.tsc",
    "setup": null,
    "flags": {
      "tsagdockercell": "0"
    },
    "description": "tsagexcmiscdocker.tsc - docker tests\n\nIncludes misc docker test:\n   test 1- check for lscell, lscelldisk, lsgriddisk, lsservice\n   test 2- REST CONFIG upgrade functional test\n   test 3- ers failover test with active escli workload\n   test 4- REST configuration bug test\n   test 5- test for nginx certificate/key upload\n\nTest to be added in lrgsaexacldockertests",
    "platform": null
  },
  {
    "test_name": "tsagexcmissingparent.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexcmissingparent.tsc - Simulate bsm crash after snap deletion and before clone deletion\n\nrun_misparentvoldel_tests.sh -\n     2 tests\n     Simulate bsm crash after snap deletion and before clone deletion - which was previously\n     resulting in Parent snap not found bug 35264480\n\n     Simulate bsm crash after snap deletion and before clone deletion - which was previously\n     resulting in Parent volume not found bug 35231751",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcmkfeature.tsc",
    "setup": null,
    "flags": {
      "eswallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcmkfeature.tsc - Exascale software feature creation test\n\nTest for ExaScale software features with versioning infrastructure\n\nThis test covers the positive as well as negative test cases for\n     feature creation using mkfeature escli command.",
    "platform": null
  },
  {
    "test_name": "tsagexcmkvolclonechiops.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexcmkvolclonechiops.tsc\n\nCreate clone with manipulating iopsProvisioned and iopsInheritedFromParent attributes\n    - while creating clone with these attributes simulate failure (crash BSW/BSM) and validate if it succeeds.\n    with lrg : lrgsaexacldvolclone5, rtest runs in OnPrem mode/ non shared cloud mode.\n    with lrg : lrgsaexacldvolclone5cs, test can run in shared cloud (cs) mode as well.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcmonifdbasic.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcmonifdbasic.tsc - test basic IFD functionality\n\n1. Check and make sure that IFD is setup within EGS\n     2. Ensure that RDMA setup was successful\n\nTests for Bug 34645405",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcmonshutdown.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcmonshutdown.tsc - test race between shutdown and IFD query\n\ntest race between EGS shutdown and IFD query within EGS\n     1. Shutdown all EGS servers.\n     2. Add simulation event to stall IFD query\n     3. Startup all EGS servers.\n     4. Snipe cellsrv to trigger IFD query\n     5. Send SIGUSR1 to one of egssrv that is in the middle of IFD query\n     6. Ensure no issues came up (incidents)\n\nTests for Bug 34290807",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcmountopf.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcmountopf.tsc - mount operation failure\n\nTo test the failure of mount operation",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcmulti_failure.tsc",
    "setup": null,
    "flags": {
      "delta_sync": "true"
    },
    "description": "tsagexcmulti_failure.tsc - Test for Multiple failure scenario\n\n1. In a 4 cell setup, start DB workload in background\n      2. Drop one cell and trigger alter storagepool reconfig which starts REBALANCE\n      3. Make cell3 to OFFLINE when REBALANCE starts\n      4. Make again cell3 to BEGING ONLINE state\n      5. Make cell3 to OFFLINE again\n      6. Repeat step2 to step 5 in a loop of 3 times and no ORA issues/EGS crash should be occured",
    "platform": null
  },
  {
    "test_name": "tsagexcmulti_rmsp.tsc",
    "setup": null,
    "flags": {
      "PWFILE_ON_EXC": "false",
      "sage_mirror_mode": "high",
      "multisp": "true",
      "NUM_FLASH_PER_CELL": "4"
    },
    "description": "tsagexcmulti_rmsp.tsc - Deletion of XT,EF and HC storagepool in multisp environment\n\nTests removes XT,EF  and HC of storagepool,Waits for all relevant messages in EGS Leader alert log while EGS processes rmstoragepool\n     And resetup DB & multisp env that will recreate all SPs with same name",
    "platform": null
  },
  {
    "test_name": "tsagexcnewraft.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcnewraft.tsc - new optimization in raft\n\nTo test the new optimization in raft",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcnewvolattr.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagexcnewvolattr.tsc - test to verify the changes/removel\n                             in lsvolume cmd attributes\n\nTest to check the following changes,\n     hide blockSizeIOPS field from lsvolume output & ?detail\n     removal of numUsedIO\n     rename of fileSystemId to acfsFileSystem\n     rename of provisionedIOPS to iopsProvisioned\n     rename of progress to restoreProgress\n     rename of provisionedMBPS to bandwidthProvisioned\n     rename of redundancyType to redundancy\n     rename of vipId to volumeHAVIP",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcnoencrypt.tsc",
    "setup": null,
    "flags": {
      "eds_encryption_mode": "off",
      "egs_cluster_secmode": "permissive",
      "cdb": "true"
    },
    "description": "tsagexcnoencrypt.tsc - Run xrdbmsini without EDS encryption\n\nRun xrdbmsini without EDS encryption",
    "platform": null
  },
  {
    "test_name": "tsagexcnonactvdskhlth.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "no_gd_with_prefix": "true"
    },
    "description": "tsagexcnonactvdskhlth.tsc - Non Active client pull/push disk health test\n\n1. Setup 3 cell exastack for exascale with Debugcli\n      2. Pull a disk\n      3. Verify alerts - Being Offlined/Offlined\n      4. Run IOV workload and let it finish\n      5. Push back the disk\n      6. Verify alerts - Being Onlined/Onlined\n      7. dump file and Check md5sum",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcnonactvrebal.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true",
      "no_gd_with_prefix": "true"
    },
    "description": "tsagexcnonactvrebal.tsc - Non active client rebalance test for Exascale\n\n1. Setup 3 cell exastack for exascale with Debugcli\n      2. Fail a disk\n      3. Verify alerts - Being Dropped/Rebalance starting/Dropped/Rebalance\n         finished\n      4. Run IOV workload and let it finish\n      5. Drop and replace the failed disk\n      6. Verify alerts - Being Added/Rebalance starting/Added/Rebalance\n         finished\n      7. dump file and Check md5sum",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcnonactvrsyncdelta.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagexcnonactvrsyncdelta.tsc - Exascale non-active client Resync Test\n\nShutdown cellsrv /wait for alert\n     Start IOV workload in background\n     Wait for IOV workload to complete\n     Verify IOV completed fine till end\n     cellsrv startup/Wait for alert\n     Verify data consistency for all file dumps using md5sum check",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcnspc.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "egs_trace": "highest",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcnspc.tsc - Creating new storagepool after deletion\n\nTests removal of storagepool and Creating new storagepool after removal\n     Waits for all relevant messages in EGS Leader alert log\n     while EGS processes rmstoragepool",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcnumspanbkp.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagexcnumspanbkp.tsc - test to check/verify 2 new attribute numSnapshots and numBackups for escli lsvolume.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcondiskmd.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexcondiskmd.tsc - On disk MD compatibility\n\nThis test setup Exascale stack always using the label's binaries.\n    It will then create some objects like vaults + Volumes and then shutdown\n    the Exascale stack.\n    Rebuilts the binaries in the label and then start the stack and runs some\n    escli commands on older objects.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcoptimalbsmleader.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "oss_disable_aep": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcoptimalbsmleader.tsc - Test for optimal bsm leader selection\n\nThe BSM leader and EGS leader consume a significant amount of memory (>10GB).\n     To free the memory for cellsrv on the EGS leader cell, the cell which is not\n     the EGS leader should be prioritized as the candidate for BSM leader during\n     BSM leader failover.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcords1.tsc",
    "setup": null,
    "flags": null,
    "description": "The test file helps to call ORDS install script, ORDS Apis tests and uninstall script",
    "platform": null
  },
  {
    "test_name": "tsagexcords_tests.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcords_tests.tsc\n\nThis file executes GET DB APIs on the ORDS server",
    "platform": null
  },
  {
    "test_name": "tsagexcords_tests2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcords_tests.tsc\n\nThis file executes POST and PATCH method APIs on the ORDS server",
    "platform": null
  },
  {
    "test_name": "tsagexcparallelreconfig.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcparallelreconfig.tsc - Test for Parallel Reconfig\n\nAdditional: Test for Parallel Storagepool Reconfig",
    "platform": null
  },
  {
    "test_name": "tsagexcpartdisk.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcpartdisk.tsc - Test for partner disk inactive\n\nAdditional: EGS Rejecting disk offline request as partner disks are offline themselves.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcpdbcarousel.tsc",
    "setup": null,
    "flags": {
      "egs_deplmode": "exacc",
      "vault_db": "DATA",
      "dsk_size": "xxl",
      "auto_local_undo": "true",
      "cdb": "true",
      "skip_set_file_dest": "true",
      "db_noarchivelog_mode": "true"
    },
    "description": "tsagexcpdbcarousel.tsc - pdb carousel test on exascale\n\nPls see below\n\ntest to be run in lrgdbcsaexcpdbcarousel",
    "platform": null
  },
  {
    "test_name": "tsagexcpdbcarouselworkload.tsc",
    "setup": null,
    "flags": {
      "maxtimeout": "20000",
      "max_num_of_snapshots": "30",
      "workload_duration": "3600",
      "num_of_pdbs": "2",
      "egs_deplmode": "exacc",
      "vault_db": "DATA",
      "dsk_size": "large",
      "auto_local_undo": "true",
      "cdb": "^cdb^",
      "db_noarchivelog_mode": "true",
      "oss_exascale_asm_testing": "true",
      "num_pdbs": "^num_of_pdbs^"
    },
    "description": "tsagexcpdbcarouselworkload.tsc - run snapshot carousel with db workload\n\nThis test creates 10 pdbs, run db workload on them and in parallel\n     create pdb snapshot.\n\n1. create 10 pdbs\n     2. start creating snapshots of all 10 pdbs every minute\n     3. start db workload on all pdbs so that snapshot are created\n        parallel to workload.\n     4. At the end, list snapshots, stop workload and delete snapshots\n     5. Validate if snapshots are properly created for all pdbs.",
    "platform": null
  },
  {
    "test_name": "tsagexcpdbcc2thinclone.tsc",
    "setup": null,
    "flags": {
      "vault_db": "DATA",
      "dsk_size": "xl",
      "auto_local_undo": "true",
      "cdb": "true",
      "skip_set_file_dest": "true"
    },
    "description": "tsagexcpdbcc2thinclone.tsc - Test for sharing CC2 across Exascale clones\n\nPls see below\n\ntest to be run in lrgdbcsaexcpdbcarousel",
    "platform": null
  },
  {
    "test_name": "tsagexcpdbrsfs.tsc",
    "setup": null,
    "flags": {
      "sysstr1": "'sys/knl_test7@inst1 as sysdba;'",
      "pdblogfile": "tsagexcpdbrsfs^ctr^.log"
    },
    "description": "tsagexcpdrsfs.tsc - PDBs setup for restore from services test\n\nThe script sets up pdbs to be used in exascale pdb standby tests for further verifications.\n       1. PDB - read write\n       2. PDB - Read only\n       3. PDB - snapshot copy, source read write\n       4.a. PDB - snapshot copy with source in read only\n       4.b. PDB - snapshot copy with source in read write, final pdb put in read only after copy\n       5. PDB - snapshot copy using snapshot, source read write\n       6. PDB - snapshot copy using snapshot, source in read only",
    "platform": null
  },
  {
    "test_name": "tsagexcpdbsi_snap.tsc",
    "setup": null,
    "flags": {
      "vault_db": "DATA",
      "cdb": "true",
      "eds_encryption_mode": "off",
      "egs_cluster_secmode": "permissive",
      "auto_local_undo": "true"
    },
    "description": "tsagexcpdbsi_snap.tsc - storage index test with pdb snapshot\n\ncontains 2 test\n     1- Test1 - SI with snapshot and insert another half table\n     2- Test2 - SI tests for updating half table on PDB with snapshots",
    "platform": null
  },
  {
    "test_name": "tsagexcpmemcache.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcpmemcache.tsc - full fletched rdma stats check for ves disks\n\niov + orion + failure testcase to check rdma stats\n\ntest to be added in lrgsaexcvespmemrdma",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcpmemsnap.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "create_template_with_high_redund": "true",
      "num_cells": "4",
      "CELL_XRMEM_CACHE_SIZE": "1200",
      "vault_db": "DATA"
    },
    "description": "tsagexcpmemsnap.tsc - Exascale snapshot with xrem cache test\n\nTest runs in drl and non drl mode\n\nThis is intended to provide combinatorial testing with rdma, CRC\n     validation and snapshots as a complete set. It relies on iov as\n     the IO driver, and mirror validation tool as a tool to capture\n     the divergence of data across mirrors and catch corruptions.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcpooldcmd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcpooldcmd.tsc - Pooldisk creation precheck test\n\nPooldisk creation command pre-check Test\n\n     pre-checking is added to both single and group grid disk creation and\n     throw exception in following cases:\n     Rule a:\n     1. a pool disk already exists in the specified storagePool\n        [single GD creation cmd]\n     2. a pool disk with a different prefix already exists in the specified\n         storagePool [group GD creation cmd]\n     Rule b: [both single and group GD creation cmd]\n     1. The specified storagePool has any pre-existing pool disk with a\n        different media type\n     2. Multiple media type of cell disks requested for pool disk creation\n        (when 'ALL' is specified without other disk type qualifier with the\n        changes for smart ALL)",
    "platform": null
  },
  {
    "test_name": "tsagexcpooldsk.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcpooldsk.tsc - Test scenarios for resizing pooldisks.\n\nThis test contains test scenarios 1-8 from the test plan.\n\nTest Plan: https://confluence.oraclecorp.com/confluence/display/~avik.mondal%40oracle.com/Fix+and+Test+Details+on+Bug+30908521",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcpooldsk2.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcpooldsk2.tsc - Pooldisk resize test scenarios\n\nThis test contains pooldisk resize test scenarios 8-13 from the test plan.\n\nTest Plan: https://confluence.oraclecorp.com/confluence/display/~avik.mondal%40oracle.com/Fix+and+Test+Details+on+Bug+30908521",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcpreventparallelops.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "backoff_inc_vol_bkp": "true",
      "setup_blockstore": "true"
    },
    "description": "tsagexcpreventparallelops.tsc\n\nto run run_prevent_parallel_ops_tests.sh - Tests to check,\n     prevent accidental parallel operations like\n     mkvolumebackup from snapshots of the same volume,\n     or mkvolume from backups of the same volume",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcpriorityelect1.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "loop_count": "20",
      "setup5egs": "true",
      "egs_cert_dur_in_mins": "1000"
    },
    "description": "tsagexcpriorityelect1.tsc - Test for EGS Priority Election\n\nTest for EGS Priority Election\n     Details - https://confluence.oraclecorp.com/confluence/display/EXC/Prioritize+RAFT+Leadership+on+Specific+Nodes#PrioritizeRAFTLeadershiponSpecificNodes-TestsforPriorityElection",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcpriorityelect2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "setup5egs": "true",
      "egs_cert_dur_in_mins": "1000"
    },
    "description": "tsagexcpriorityelect2.tsc - Test for EGS Priority Election\n\nTest for EGS Priority Election\n     Details - https://confluence.oraclecorp.com/confluence/display/EXC/Prioritize+RAFT+Leadership+on+Specific+Nodes#PrioritizeRAFTLeadershiponSpecificNodes-TestsforPriorityElection",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcpwfile.tsc",
    "setup": "xrdbmsini",
    "flags": null,
    "description": "tsagexcpwfile.tsc - PW file on EXC tests - coverage\n\nCoverage tests for pwfile on EXC vault storage",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcqlc_cf.tsc",
    "setup": null,
    "flags": {
      "media_type": "CF",
      "num_cells": "4"
    },
    "description": "tsagexcqlc_cf.tsc - 1) cells with different number of disks are\n#                            not allowed in the same storage pool.\n                         2) missing disks\n\nPls see below\n\nto be added in lrgdbcsaexcqlc",
    "platform": null
  },
  {
    "test_name": "tsagexcqrntvlt.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcqrntvlt.tsc - \"Manual vault quarantine tests\"\n\nAdded test cases covering 4 possible cases of quarantiningg vault manually. The cases are :\n\n     1. Quarantining a vault and then trying to perform edstools operations on the quarantined vault.\n     2. Quarantining a non-existent vault\n     3. Quarantining an already quarantined vault.\n     4. Unquarantining an already unquarantined vault.\n     5. Crashing usereds and then performing quarantine operations on the vault.\n\n     Added test case simulating the usreds crash and the quarantining a vault.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcracasm_onedv.tsc",
    "setup": null,
    "flags": {
      "dsk_size": "xl",
      "num_vols": "^TKFV_DISKS_NDISKS^",
      "def_vol_size": "^TKFV_DISKS_SIZE^M",
      "do_not_setup_db": "true",
      "use_edv_as_asmdsk": "true",
      "TKFV_DISKS_NDISKS": "4",
      "TKFV_DISKS_SIZE": "9216",
      "TKFV_SCRIPT_TO_RUN": "tkfvmrg.tsc",
      "TKFV_LRG_NAME": "lrgdbcsaexcracdbadvm",
      "TKFV_RAC_RUN": "1",
      "nflint": "1",
      "tlxCssStart_DISABLED": "1",
      "tkfvcreateDiskFiles_DISABLED": "1",
      "tlxInitDisksUni_DISABLED": "1",
      "tkfvgetDisks_DISABLED": "1",
      "TKFV_DG_REDUNDANCY": "external",
      "tlxLoadUsmDrivers_DISABLED": "1",
      "tlxCssStop_DISABLED": "0",
      "tlxUnloadUsmDrivers_DISABLED": "0"
    },
    "description": "tsagexcracasm_onedv.tsc - Tests EDVs as ASM disks\n\ncreate EDVs, attach and\n      give these volumes to ASM on multiple RAC nodes\n      and then run a DB on this ASM storage.\n      Runs dbcon9o25 on this setup",
    "platform": null
  },
  {
    "test_name": "tsagexcracasm_onvol.tsc",
    "setup": null,
    "flags": {
      "dsk_size": "xl",
      "num_vols": "^TKFV_DISKS_NDISKS^",
      "def_vol_size": "^TKFV_DISKS_SIZE^M",
      "do_not_setup_db": "true",
      "use_excvol_as_asmdsk": "true",
      "TKFV_DISKS_NDISKS": "4",
      "TKFV_DISKS_SIZE": "9216",
      "TKFV_SCRIPT_TO_RUN": "tkfvmrg.tsc",
      "TKFV_LRG_NAME": "lrgdbcsaexcracdbadvm",
      "TKFV_RAC_RUN": "1",
      "nflint": "1",
      "tlxCssStart_DISABLED": "1",
      "tkfvcreateDiskFiles_DISABLED": "1",
      "tlxInitDisksUni_DISABLED": "1",
      "tkfvgetDisks_DISABLED": "1",
      "TKFV_DG_REDUNDANCY": "external",
      "tlxLoadUsmDrivers_DISABLED": "1",
      "tlxCssStop_DISABLED": "0",
      "tlxUnloadUsmDrivers_DISABLED": "0"
    },
    "description": "tsagexcracasm_onvol.tsc - Tests Exascale Volumes as ASM disks\n\ncreate Exascale volumes, attach & login.\n      give these volumes to ASM on multiple RAC nodes\n      and then run a DB on this ASM storage.\n      Runs dbcon9o25 on this setup\n\n      If we've chosen to test ADVM, run a USM LRG\n      instead.",
    "platform": null
  },
  {
    "test_name": "tsagexcracdb.tsc",
    "setup": null,
    "flags": {
      "MAX_INSTANCE": "2",
      "TST_GPNP": "true",
      "nowarn": "^TST_NOWARN^",
      "oss_exascale_testing": "true"
    },
    "description": "tsagexcracdb.tsc - Sets up RAC DB on Emulated nodes\n\nSets up RAC DB on Exascale on emulated nodes",
    "platform": null
  },
  {
    "test_name": "tsagexcracdb_clean.tsc",
    "setup": null,
    "flags": {
      "ainst": "asm_instance^i^"
    },
    "description": "tsagexcracdb_clean.tsc - Cleanup at end - DB on Exc Volume\n\nCleans up the test env after the lrg is done",
    "platform": null
  },
  {
    "test_name": "tsagexcraftlog1.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcraftlog1.tsc - RAFT LOG corruption test\n\nUses event simulation to simulate slow disk by adding\n       15 seconds delay to the write function\n     Test Steps\n       The goal of the test is to verify that RAFT can handle\n       the EGS Leader failover when followers run handleAppendEntries\n       (1) A regular single cell 3 EGS setup\n       (2) Increase the flushing latency using sim event on EGS followers\n       (3) Run an escli mkuser command to create new RAFt entry\n       (4) Kill the EGS leader while above command is running. The delay\n           is handled by the events set earlier\n       (5) Make sure after EGS leader failover, RAFT entries are handled\n           correctly and no EGS crashes.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcrebal4cell_qlc.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "oss_multims_testing": "true",
      "num_cells": "4",
      "cmd_mirror_mode": "normal"
    },
    "description": "tsagexcrebal4cell_qlc.tsc - IOV Test with Source and Target Cellsrv Restart\n\nPlease see below.\n\nto be added in lrgsaexcvesrebal4cell_qlc\n   tets is same as tsagexcrebal_4cell.tsc but with QLC DISKS",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcrebal_4cell.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "num_cells": "4",
      "cmd_mirror_mode": "normal"
    },
    "description": "tsagexcrebal_4cell.tsc - IOV Test with Source and Target Cellsrv Restart\n\nPlease see below.\n\nto be added in lrgsaexacldvesrebal_4cell",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcrebal_4cell_pd_failure.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "num_cells": "4",
      "cmd_mirror_mode": "normal"
    },
    "description": "tsagexcrebal_4cell_pd_failure.tsc - IOV Test with Source and Target Cellsrv Restart\n\nPlease see below.\n\nto be added in lrgsaexcreb_4cellpdfail",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcrebal_db_snap.tsc",
    "setup": null,
    "flags": {
      "oss_multims_testing": "true",
      "cdb": "true",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true",
      "multisp": "true",
      "vault_db": "DB^ORA_SID_UPPER^",
      "num_cells": "4",
      "AUTO_LOCAL_UNDO": "true"
    },
    "description": "tsagexcrebal_db_snap.tsc - rebal + snapshot workload\n\nruns 2 tests:\n   1- rebal operation with snapshot creation/drop\n   2- rebal operation with workload(insert,update,drop) on snapshot\n\nto be added in lrgdbcsaexcrebal2_db_snap",
    "platform": null
  },
  {
    "test_name": "tsagexcrebal_dsp.tsc",
    "setup": null,
    "flags": {
      "delta_sync": "true"
    },
    "description": "tsagexcrebal_dsp.tsc - rebalance & rm SP should not trigger any ORA-0600 issues\n\n1. In a 4 cell setup, start DB workload in background\n      2. Drop cell 1 and wait for REBALANCE to start\n      3. Trigger rmstoragepool and it should be denied\n      4. Wait for REBALANCE to complete",
    "platform": null
  },
  {
    "test_name": "tsagexcrebal_iowrite_err.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcrebal_iowrite_err.tsc - test to try out BLOCKIO_WRITE_ERR during rebal\n\nadds BLOCKIO_WRITE_ERR cellsrv simevent in tsagexacldrebaldb2.tsc\n     simulation done on cell2\n\ntest to be added in dbcsaexcfailc22",
    "platform": null
  },
  {
    "test_name": "tsagexcrebal_iowrite_ves_err.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcrebal_iowrite_ves_err.tsc - test to try out VES_READ_ERR and BLOCKIO_WRITE_ERR during rebal\n\nadds VES_READ_ERR and BLOCKIO_WRITE_ERR cellsrv simevent in tsagexacldrebaldb2.tsc\n    simulation done on cell1 and cell2 respectively\n\ntest to be added in dbcsaexcfailc27",
    "platform": null
  },
  {
    "test_name": "tsagexcrebal_ves_err.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcrebal_ves_err.tsc - test to try out VES_READ_ERR during rebal\n\nadds VES_READ_ERR cellsrv simevent in tsagexacldrebaldb2.tsc\n\ntest to be added in dbcsaexcfailc21",
    "platform": null
  },
  {
    "test_name": "tsagexcresilver2_db.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "mixed_workload": "true",
      "maxpdb": "2",
      "dsk_size": "large"
    },
    "description": "tsagexcresilver2_db.tsc - resilver test with active db workload\n\nTests for resilver added in this lrg are:\n     1. Test case 2: missing RT MS will auto recreate the griddisks\n     2. Test case 3: Simulate VESRepair BLK_ZERO_SCAN hang\n     3. Test case 4: Simulate VESRepair MD_SEG_REPAIR_WRITE hang\n     4. Test case 5: Simulate VESRepair EXT_FIND_TARGET hang for data repair\n     5. Test case 6: test for bug 37266315\n\nThese test are added from existing lrg:lrgdbcsaexacldresilver_db\n     and file: tsagexcresilver_db.tsc to new lrg: lrgdbcsaexacldresilver2_db\n     as daily label runs were taking more than 6 hours to complete.",
    "platform": null
  },
  {
    "test_name": "tsagexcresilver_db.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "mixed_workload": "true",
      "maxpdb": "2",
      "dsk_size": "large"
    },
    "description": "tsagexcresilver_db.tsc - resilver test with active db workload\n\n- setup 3 cells\n     - run db workload in background\n     - fail flash\n     - wait for resilvering to complete\n     - make sure workload ran fine\n\ntest to be added in lrgdbcsaexacldresilver_db",
    "platform": null
  },
  {
    "test_name": "tsagexcresilver_db_snap.tsc",
    "setup": null,
    "flags": {
      "cdb": "true",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true",
      "sage_mirror_mode": "high",
      "media_type": "HC,EF",
      "vault_db": "DB^ORA_SID_UPPER^",
      "AUTO_LOCAL_UNDO": "true"
    },
    "description": "tsagexcresilver_db_snap.tsc - resilver test with active snapshot workload\n\nruns 2 test:\n  1- resilver operation with snapshot creation/deletion workload\n  2- resilver operation with snapshot workload(insert,update,drop)\n\nto be added in lrgdbcsaexcresilver2_db_snap",
    "platform": null
  },
  {
    "test_name": "tsagexcresilver_iohang.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true"
    },
    "description": "tsagexcresilver_iohang.tsc - adds BLOCK_IO_HANG cellsrv simevent in tsagexcresilver_db.tsc\n\nadds BLOCK_IO_HANG cellsrv simevent in tsagexcresilver_db.tsc\n    simulation done on cell2\n\ntest added in lrgdbcsaexcfailc26",
    "platform": null
  },
  {
    "test_name": "tsagexcresilver_slowdisk_ioall.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "mixed_workload": "true",
      "maxpdb": "2",
      "dsk_size": "medium"
    },
    "description": "tsagexcresilver_slowdisk_ioall.tsc - adds BLOCKIO_ALL_ERR and SLOW_DISK_INJ in tsagexcresilver_db.tsc\n\nadds BLOCKIO_ALL_ERR and SLOW_DISK_INJ cellsrv simevent in tsagexcresilver_db.tsc\n    simulation done on cell1 and cell2 respectively\n\ntest added in lrgdbcsaexcfailc29",
    "platform": null
  },
  {
    "test_name": "tsagexcresilver_ves_err.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "mixed_workload": "true",
      "maxpdb": "2"
    },
    "description": "tsagexcresilver_ves_err.tsc - test to try out VES_READ_ERR\n\nadds VES_READ_ERR cellsrv simevent in tsagexcresilver_db.tsc\n    simulation done on cell2\n\ntest added in dbcsaexcfailc25",
    "platform": null
  },
  {
    "test_name": "tsagexcresilverdb_qlc.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "mixed_workload": "true",
      "maxpdb": "2",
      "dsk_size": "large"
    },
    "description": "tsagexcresilverdb_qlc.tsc - resilver test with active db workload\n\n- setup 3 cells\n     - run db workload in background\n     - fail flash\n     - wait for resilvering to complete\n     - make sure workload ran fine\n\ntest to be added in lrgdbcsaexcresilverdb_qlc\n    test is same as tsagexcresilver_db.tsc but with QLC DISKS",
    "platform": null
  },
  {
    "test_name": "tsagexcresizeedsfile.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "no_drl_flag": "'-f 0x200'"
    },
    "description": "tsagexcresizeedsfile.tsc - EDS file resize test with drl and no-drl\n\nCreate EDS file with size 2 GB\n     Create snapshot of the above file\n     Resize the above file to new size = 1 GB",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcresloop.tsc",
    "setup": "xrdbmsini",
    "flags": null,
    "description": "tsagexcresloop.tsc - Restart ES cluster in loop\n\nRestarts whole ES cluster in a loop",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcrestacl.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcrestacl.tsc - Test for Rest ACL\n\nTests rest_vault_client and rest_volume_client privileges",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcrestart_fail.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4"
    },
    "description": "tsagexcrestart_fail.tsc - restart cellsrv on cell 3, then stimulate disks failure on cell 4\n\nTest steps:\n\n i.) - setup 4 cell env with db\n\n ii.) - - Run oltp and redolog workload\n\n In a loop (count 3); do\n\n     iii.) - On cell 3 - Restart cellsrv\n\n     iv.)  - On cell 4 - Failing two disks\n\n      v.)  - On cell 4 - Reenable failed disks\n\n endloop\n\n vi.) make sure oltp and redolog workload ran fine",
    "platform": null
  },
  {
    "test_name": "tsagexcrestartrs.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcrestartrs.tsc - Test for cell RS, db RS restart\n\nThis test monitors pids of cell RS, db RS and all monitors before and after RS restart cases.\n     The following cases are tested:\n       1. Restart cell RS through CLI\n       2. Kill cell RS using SIGKILL\n       3. Restart db RS through CLI\n       4. Kill db RS using SIGKILL",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcresync_dou.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "delta_sync": "true",
      "cmd_mirror_mode": "normal",
      "phase_dur": "120",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagexcresync_dou.tsc - double failure resync test\n\nplease see below\n\ntest to be added in lrgsaexcresync_doubfail",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcresync_ioall_err.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcresync_ioall_err.tsc - Test to try out BLOCKIO_ALL cellsrv simevent\n    during resync operation\n\nTest modified : tsagexacldresyncdb2.tsc\n    while db workload is running , so alter griddisk inactive\n    (make atleast 2-3 pooldisks inactive)+ active.\n    Modifications :\n    1. move to cell2 just after disk is altered\n    2. initiate cellsrv simulation and check for alerts\n    3. move to cell1\n\ntest added in dbcsaexcfailc23",
    "platform": null
  },
  {
    "test_name": "tsagexcresync_ioall_iohang.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcresync_ioall_iohang.tsc - Test to try out BLOCKIO_ALL_ERR and BLOCKIO_ALL_HANG\n    cellsrv simevent during resync operation\n\nTest modified : tsagexacldresyncdb2.tsc\n    while db workload is running , so alter griddisk inactive\n    (make atleast 2-3 pooldisks inactive)+ active.\n    Modifications :\n    1. move to cell2 just after disk is altered\n    2. initiate cellsrv simulation and check for alerts\n    3. move to cell1 and initiate simulation\n\ntest added in dbcsaexcfailc28",
    "platform": null
  },
  {
    "test_name": "tsagexcresync_slowdisk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcresync_slowdisk.tsc - Test to try out SLOW_DISK_INJ cellsrv simevent\n     during resync operation\n\nTest modified : tsagexacldresyncdb2.tsc\n    while db workload is running , so alter griddisk inactive\n    (make atleast 2-3 pooldisks inactive)+ active.\n    Modifications :\n    1. move to cell2 just after disk is altered\n    2. initiate cellsrv simulation and check for alerts\n    3. move to cell1\n\ntest added in dbcsaexcfailc24",
    "platform": null
  },
  {
    "test_name": "tsagexcresyncall_db_snap.tsc",
    "setup": null,
    "flags": {
      "cdb": "true",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true",
      "sage_mirror_mode": "high",
      "multisp": "true",
      "oss_disable_aep": "true",
      "vault_db": "DB^ORA_SID_UPPER^",
      "AUTO_LOCAL_UNDO": "true"
    },
    "description": "tsagexcresyncall_db_snap.tsc - resync test with active snapshot workload\n\nruns 2 test :\n    1- resync operation with snapshot create/open/close/drop workload\n    2- resync operation with workload ( 'insert, update, drop' rows) on snapshot\n\nto be added in lrgdbcsaexcresync2_dball_snap",
    "platform": null
  },
  {
    "test_name": "tsagexcrle.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcrle.tsc - RLE test for flags\n\nSteps for testing '-l' flag:\n     1. Set up Exascale stack\n     2. Create a user using \"escli mkuser\" command\n     4. Change user privileges using \"escli chuser\"\n     5. Run \"rle -l\"\n     6. Check rle_main.trc, it should contain correct privileges\n        for nameNode related to user\n\n     '-s' flag runs RLE in stdout mode.\n     Steps for testing '-s' flag:\n     1. Setup Exascale stack\n     2. Run rle with \"-s\" option, grep + redirect output to a file\n     3. Run rle with \"-v\" option, the output will be printed in rle_main.trc\n     4. Grep for the same string and redirect output to another files\n     5. Compare both the files. The output should be same.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcrmsp.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "egs_trace": "highest",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcrmsp.tsc - drop storagepool tests\n\nTests removal of storagepool\n       Waits for all relevant messages in EGS Leader alert log\n       while EGS processes rmstoragepool",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcrmvault2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on",
      "sage_mirror_mode": "high",
      "tstlogfile": "^tst_tscname^.log"
    },
    "description": "tsagexcrmvault2.tsc - Drop vault testcase\n\n1. Check that two concurrent commands to drop the same vault work OK.\n     2. While dropping a vault is in progress, trigger reassignment of\n        Sys DS Shard from one EDS instance to another instance. Verify that\n        moving the DS Shard between EDS does not have to wait for dropping\n        the vault to complete (is not blocked by 'drop vault' command).",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcrndmrd.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcrndmrd.tsc - IOV test for random read through EGSLIB\n\nTests EGSLib support to read from a random online mirror using IOV",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcrpm_mvmt_tool.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcrpm_mvmt_tool.tsc - Test for RPM movement analyze tool\n\nPlease see below",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcrslv_doufail.tsc",
    "setup": null,
    "flags": {
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true",
      "sage_mirror_mode": "high",
      "cdb": "true"
    },
    "description": "tsagexcrslv_doufail.tsc - double failure resilver test\n\nPls see below\n\ntest to be added in lrgdbcsaexcrslvfail_db",
    "platform": null
  },
  {
    "test_name": "tsagexcrsmon_ershang.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcrsmon_ershang.tsc - RS monitors ERS hang\n\nWith an improvement in monitoring, RS will know if NGINX or ERSSRV hangs\n         and restarts them.\n     The test hangs ERS and nginx and checks if they are getting restarted",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsandbox100disk.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagexcsandbox100disk.tsc - Test for 100 disks per cell support in sandbox\n\nTest: Create storagepool with 6 cells, where each cell has 100 disks\n         Later cell_5 goes into maintenance and disks are dropped\n         then bring back cell_5. After the cell is added back,\n         reconfigure the storagepool and add 3 more cells",
    "platform": null
  },
  {
    "test_name": "tsagexcsandbox_cgsizingmode.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^_a"
    },
    "description": "tsagexcsandbox_cgsizingmode.tsc - Test for cellgroupsizingmode argument\n\nTest cases to switch from one cellgroupsizingmode to another\n     Create testcase with default availability mode and then switch to\n     Performance and balance modes",
    "platform": null
  },
  {
    "test_name": "tsagexcsandbox_cgsizingmode_1.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcsandbox_cgsizingmode_1.tsc - Test for cellgroupsizingmode argument\n\nTest cases to switch from one cellgroupsizingmode to another\n     Create storagepool with Balance mode and then switch to Performance\n      and availability mode",
    "platform": null
  },
  {
    "test_name": "tsagexcsandbox_cgsizingmode_2.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcsandbox_cgsizingmode_2.tsc - Test for cellgroupsizingmode argument\n\nTest cases to switch from one cellgroupsizingmode to another\n     Create storagepool with Performance mode and then switch to Balance\n      and availability mode",
    "platform": null
  },
  {
    "test_name": "tsagexcsandbox_cgsizingmode_3.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagexcsandbox_cgsizingmode_3.tsc - Test for cellgroupsizingmode argument\n\nTest cases to switch from one cellgroupsizingmode to another while\n      adding & dropping cells",
    "platform": null
  },
  {
    "test_name": "tsagexcsandbox_diskstate1.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^",
      "min_cell": "27",
      "max_cell": "27",
      "run_selected_scenarios": "1,82"
    },
    "description": "tsagexcsandbox_diskstate1.tsc - Disk State Transition Tests\n\nAdditional: Disk state transition tests in EGS Sandbox env",
    "platform": null
  },
  {
    "test_name": "tsagexcsandbox_diskstate2.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^b_2",
      "max_iters": "12000",
      "min_cell": "27",
      "max_cell": "27"
    },
    "description": "tsagexcsandbox_diskstate2.tsc - Disk State Transition Tests\n\nDisks from different partnership group traverse the same\n      disk states in parallel",
    "platform": null
  },
  {
    "test_name": "tsagexcsandbox_diskstate2b.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^c"
    },
    "description": "tsagexcsandbox_diskstate2b.tsc - Disk State Transition Tests\n\nAdditional: Disk state transition tests in EGS Sandbox env",
    "platform": null
  },
  {
    "test_name": "tsagexcsandbox_diskstate3.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagexcsandbox_diskstate3.tsc - Disk State Transition Tests\n\nTest where 2 disks from same partnership group traverse the graph",
    "platform": null
  },
  {
    "test_name": "tsagexcsandbox_diskstate4.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^b"
    },
    "description": "tsagexcsandbox_diskstate4.tsc - Disk State Transition Tests\n\nTest where 3 disks from same partnership group traverse the graph",
    "platform": null
  },
  {
    "test_name": "tsagexcsandbox_lastmirror.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagexcsandbox_lasstmirror.tsc - Triple failure Test\n\nAdditional: Triple failure test in EGS Sandbox env",
    "platform": null
  },
  {
    "test_name": "tsagexcsclctrlplane.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcsclctrlplane.tsc - Exascale clould test for setting up control\n                               plane and testing with dataplane\n\nThis sets up the controlplane and also sets up the dataplane and does\n     a bunch of tests\n\n     The controlplane is accessible through two swagger API interfaces\n        adminUIUrl  : http://10.248.121.24:24010/ui\n        clientUIUrl : http://10.248.121.24:24000/ui",
    "platform": null
  },
  {
    "test_name": "tsagexcsclers.tsc",
    "setup": null,
    "flags": {
      "debugfile": "^tst_tscname^_debug.lst"
    },
    "description": "tsagexcsclers.tsc - Exascale clould integration ERS test\n\nThis tests the contract between the Exascale cloud service\n     and ERS.",
    "platform": null
  },
  {
    "test_name": "tsagexcsclvol.tsc",
    "setup": null,
    "flags": {
      "debugfile": "^tst_tscname^_debug.lst"
    },
    "description": "tsagexcsclvol.tsc - Exascale clould integration volume test\n\nThis tests the volume tests from exascale cloud on the LRG environment,\n     thereby making sure that the ERS functionality is not broken",
    "platform": null
  },
  {
    "test_name": "tsagexcscrub.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "sage_mirror_mode": "high",
      "flash": "true",
      "cmd_mirror_mode": "normal",
      "phase_dur": "60",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagexcscrub.tsc - VESGD scrub test with active client\n\n- Run IOV in background\n   - simulate io error\n   - start/finish scrubbing\n   - kill iov ,make sure it ran fine\n   - check md5sum on all mirrors\n\ntest to be added in lrgsaexacldvesscrub",
    "platform": null
  },
  {
    "test_name": "tsagexcscrub_db.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcscrub_db.tsc - VESGD scrub test with active db workload\n\n- start db workload in background\n   - simulate io error\n   - start/finish scrubbing\n   - stop wkload  and make sure workload ran fine\n\ntest to be added in lrgdbcsaexacldvesscrub_db",
    "platform": null
  },
  {
    "test_name": "tsagexcscrubdelay.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cdb": "true",
      "OSS_AUTO_MANAGE_DISKS": "true",
      "sage_mirror_mode": "normal",
      "devdir1": "^T_WORK^/raw^oss_port^",
      "devdir2": "^T_WORK^/raw^oss_port2^"
    },
    "description": "tsagexcscrubdelay.tsc - Check for scrub IO skip\n\nTest to verify that next scrub IO is skipped if multiple\n     slow scrub IOs are encountered in a defined region.\n\nruns in lrgdbconsascbug8",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagexcscrubtest.tsc",
    "setup": "srdbmsini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true",
      "sage_mirror_mode": "normal"
    },
    "description": "tsagexcscrubtest.tsc - Check scrub io back off on workload run\n\nTest for bug-34797913:PROVIDE A NEW METRIC TO DISTINGUISH SCRUB\n       LATENCY. This script tests if disk scrub IOs back down when\n        workload is run.\n\nThis test validates backing of scrub IOs when workload is run\n     Test steps are as follows:\n      1: Start scrub operation on the cell\n      2: Record total number of current scrub IOs -> preWorkloadIo\n      3: Setup orion on the cell node\n      4: Run orion workload on all griddisks on the cell\n          with diskType as harddisk\n      5: Record total number of current scrub IOs -> inWorkloadIo\n     Requirements:\n      1: ASM deployment\n      2: All ASM diskgroups need to be mounted for scrub operation",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagexcsecondaryfcpop1.tsc",
    "setup": "xblockini",
    "flags": {
      "python_path": "^ADE_VIEW_ROOT^/python/bin",
      "ledvsetup": "1",
      "edvcleanup_only": "1",
      "sage_mirror_mode": "high",
      "oss_multims_testing": "true",
      "cell_with_xrmem_cache": "0"
    },
    "description": "tsagexcsecondaryfcpop1.tsc - Test for secondary cache population\n\nTest Script verifies feature - Secondary flash cache\n     population from primary mirror for exascale volume files\n\nTest Details - https://confluence.oraclecorp.com/confluence/display/~neeraj.gandhi@oracle.com/Remote+Cache+Populator+Test+Cases",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsecondaryfcpop2.tsc",
    "setup": "xblockini",
    "flags": {
      "python_path": "^ADE_VIEW_ROOT^/python/bin",
      "ledvsetup": "1",
      "edvcleanup_only": "1",
      "sage_mirror_mode": "high",
      "oss_multims_testing": "true"
    },
    "description": "tsagexcsecondaryfcpop2.tsc - Test for secondary cache population\n     Test case 5: Non-volume tests\n\ntests based on jigguo_secondary_cache_population\n        for more details :\n        https://confluence.oraclecorp.com/confluence/display/~neeraj.gandhi@oracle.com/Remote+Cache+Populator+Test+Cases#RemoteCachePopulatorTestCases-Testcase5:Non-volumetests\n\n     Test Script verifies feature - Secondary flash cache\n     population from primary mirror for exascale volume files",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsecondaryfcpop3.tsc",
    "setup": "xblockini",
    "flags": {
      "python_path": "^ADE_VIEW_ROOT^/python/bin",
      "ledvsetup": "1",
      "edvcleanup_only": "1"
    },
    "description": "tsagexcsecondaryfcpop3.tsc - Test for secondary cache population\n\nTest Script verifies feature - Secondary flash cache\n     population from primary mirror for exascale volume files",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsecuritykey.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "oss_multims_testing": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcsecuritykey.tsc - security certificate test for exacli\n\nThis test checks uploading a security certificate on exacli\n       -   Its extension of test file tsagsecuritykey.tsc\n       -   We added check specific to Exascale 3 times after List security Certificate\n              1. Restart all services using `alter cell restart services all`\n              2. Make sure all Exascale services are online\n              3. Make sure there are no new alerts using `cellcli -e list alerthistory`\n       -   Make sure EGS cluster is down. Please shutdown ERS, SYSEDS, and USREDS using 'alter cell shutdown services ers' only in cell1 and cell2\n       -   Upload public key and private key using\n             - alter cell securityPrivKey=\"file:///^OSS_HOME^/test/tsage/sosd/pkcs8.rootca.key\",securityPubKey=\"file:///^OSS_HOME^/test/tsage/sosd/adc01drc.rootca.cer\"\n       -   Restart MS\n       -   Check to see if this alert is present in alerthistory: \"local security credential update succeeded. But security credential synchronization with Exascale failed with the following error The ExaCTRL server experienced an error. Retry the command.\"\n       -   Revert the upload using cellcli -e alter cell securityCert=?default?\n       -   Restart MS\n       -   Startup EGS cluster\n       -   Redo the public key and private key upload using\n             - alter cell securityPrivKey=\"file:///^OSS_HOME^/test/tsage/sosd/pkcs8.rootca.key\",securityPubKey=\"file:///^OSS_HOME^/test/tsage/sosd/adc01drc.rootca.cer\"\n       -   Restart MS\n       -   Check if the following clear alert is present in alerthistory: \"Security credential synchronization with Exascale succeeded.\"",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsecuritymgmt.tsc",
    "setup": null,
    "flags": {
      "adminwallet": "^t_work^/esadmin_wallet",
      "user1wallet": "^t_work^/user1/esuser1_wallet"
    },
    "description": "tsagexcsecuritymgmt.tsc - Test for security and user management commands\n\nTest for security and user management commands.\n     Sequnce of commands:\n     1. Generatekey\n     2. mkwallet\n     3. fetchtruststore\n     4. mkuser\n     5. lsuser\n     6. chuser\n     7. lswallet\n     8. chwallet\n     9. rmuser",
    "platform": null
  },
  {
    "test_name": "tsagexcserscurl.tsc",
    "setup": null,
    "flags": {
      "debugfile": "^tst_tscname^_debug.lst",
      "excld_ers_selfsigned": "1",
      "exc_cloud_user_ers_port": "5052",
      "egs_deplmode": "sharedCloud"
    },
    "description": "tsagexcserscurl.tsc - Curl request tests on ERS endpoint\n\nThis test executes resource functionality using curl requests on ERS",
    "platform": null
  },
  {
    "test_name": "tsagexcservclsalrt.tsc",
    "setup": "xblockini",
    "flags": {
      "sage_mirror_mode": "high",
      "admin_wallet": "^T_WORK^/esadmin_wallet"
    },
    "description": "tsagexcservclsalrt.tsc - testcase for checking cluster alerts to the EGS services\n\ntestcase for checking cluster alerts to the EGS services",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcservicemgmtcmd.tsc",
    "setup": "xblockini",
    "flags": {
      "sage_mirror_mode": "high",
      "adminwallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcservicemgmtcmd.tsc - Test for cluster and service level commands\n\nTest for cluster and service level commands",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsetdepl.tsc",
    "setup": null,
    "flags": {
      "egs_mode": "exacs",
      "valid_egsmodes": "'onprem, exacs, exacc, custom, dedicatedcloud, sharedcloud,'",
      "sep": ",",
      "egs_deplmode": "onprem",
      "valid_deplmodes": "'onprem, sharedcloud, dedicatedcloud, custom,'",
      "ch_egs_deplmode": "true",
      "egs_autogenerate_mode": "true",
      "egs_cert_dur_in_mins": "40",
      "eds_encrypt": "true",
      "eds_encryption_mode": "off",
      "chcluster_cmd": "'chcluster --attributes '",
      "chcluster_depl": "'deploymentMode='",
      "chcluster_eds": "'autoFileEncryption='",
      "cert_duration": "',certDurationInMins='^egs_cert_dur_in_mins^",
      "chcluster_cmd_final": "'chcluster --attributes certDurationInMins='^egs_cert_dur_in_mins^",
      "only_cellsrv_restart": "true"
    },
    "description": "tsagexcsetdepl.tsc - Change Cluster deployment mode\n\nCheck if test need to change Cluster Attributes:\n     - deploymentMode - deployment Mode can be onPrem (default)\n                  or cloudService, cloudAtCustomer or Custom\n     - If deploymentMode= Custom, test can also set\n      - autoFileEncryption = true/false\n      - egs_cert_dur_in_mins=<time> (certDurationInMins is set to <time> mins)\n         ## default value = 40 mins",
    "platform": null
  },
  {
    "test_name": "tsagexcsetupuser.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "eswallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcsetupuser.tsc - Script to setup 4 user with different privileges\n\nScript to setup 4 user with different privileges\n\nThis script creates 4 users, user1-4 with each having\n     following privileges\n\n     1. user1 - vlt_inspect privilege\n        vaults: usr1vlt1 and usr1vlt2\n        file:   usr1vlt1/usr1file1\n     2. user2 - vlt_read privilege\n        vaults: usr2vlt1 and usr2vlt2\n        file:   usr2vlt1/usr2file1\n     3. user3 - vlt_use privilege\n        vaults: usr3vlt1 and usr3vlt2\n        file:   usr3vlt1/usr3file1\n     4. user4 - vlt_manage privilege\n        vaults: usr4vlt1 and usr4vlt2\n        file:   usr4vlt1/usr4file1",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcshutdown_cellsrv.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "oss_multims_testing": "true"
    },
    "description": "tsagexcshutdown_cellsrv.tsc - Test for Cellsrv Shutdown Redundancy Checks\n\nMore info - https://confluence.oraclecorp.com/confluence/display/~harry.l.li@oracle.com/Cellsrv+Shutdown+Redundancy+Check+Test+Results#CellsrvShutdownRedundancyCheckTestResults-Cellsrvisshutdownoncell1.EGSleaderisrunningonnewversion.Cellsrvoncell2cannotbeshutdownorrestarted.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsic2crelocation.tsc",
    "setup": null,
    "flags": {
      "eds_encryption_mode": "off",
      "egs_cluster_secmode": "permissive",
      "cdb": "true",
      "sage_mirror_mode": "high",
      "egs_dsk_fences_threshold": "true",
      "file_dest": "@^vault_db^"
    },
    "description": "tsagexcsic2crelocation.tsc - Exascale SI C2C relocation\n\ntest_case of Exascale SI C2C relocation",
    "platform": null
  },
  {
    "test_name": "tsagexcsidbonvol.tsc",
    "setup": null,
    "flags": {
      "dsk_size": "large"
    },
    "description": "tsagexcsidbonvol.tsc - Sets up DB on Volume\n\nSets up DB on Exascale Volume\n     This test will create an Exascale volume using xblockini.\n     xblockini partitions it, creates ext3 filesystem and then mounts it\n     to $T_WORK/tmpdir1.\n     The test then sets up a Database on this mounted Volume\n     Note - This database is not an Exascale DB, it is a legacy DB on a FS ,\n      and that filesystem happens to be the Exascale volume created using\n      Exascale block store services",
    "platform": null
  },
  {
    "test_name": "tsagexcsimevent_edserror.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcsimevent_edserror.tsc - Functional test for the newly added eds\n                                    simulation event\n\nPlease see below\n\nadded in lrgsaexacldeds",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsingle_egs.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "setup1egs": "true"
    },
    "description": "tsagexcsingle_egs.tsc - Test for single egs setup",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsloop.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcsloop.tsc - Run certain things in a loop for 5 hours\n\nThe test does the following in a loop\n      01. Create user in exascale\n      02. Create vaults\n      03. Copy database files to vaults\n      04. List files in vaults\n      05. createVolume\n      06. createVolumeAttachment\n      07. loginISCSI\n      08. deleteVolumeAttachment\n      09. deleteVolume\n      10. deleteVault\n      11. List all the vaults\n\nArguments:\n       lrgtime -- Total time required for lrg to run in minutes,\n                  default is 5 hours [5 * 60 * 60, 18000]",
    "platform": null
  },
  {
    "test_name": "tsagexcslowst8dump.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcslowst8dump.tsc - Slow state dump test",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsmartrebalance.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcsmartrebalance.tsc - To check smart rebalance\n\nIf there is less space available in storage pool then don't\n     trigger rebalance\n\nhttps://confluence.oraclecorp.com/confluence/display/EXC/Smart+Rebalance+Low+Space+Experiment",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsnapdbloop.tsc",
    "setup": null,
    "flags": {
      "egs_deplmode": "exacc",
      "cdb": "true",
      "vault_db": "DATA",
      "dsk_size": "large",
      "auto_local_undo": "true",
      "db_noarchivelog_mode": "true",
      "maxtimeout": "10800"
    },
    "description": "tsagexcsnapdbloop.tsc - recursive hot clones test on exascale\n\nTest for Bug 33846770 - SNAPSHOT CLONE HIERARCHY TEST REPORTS CORRUPTION\n\nto be added in lrgdbcsaexcpdbclones_loop",
    "platform": null
  },
  {
    "test_name": "tsagexcsnaptree.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "media_type": "ALL"
    },
    "description": "tsagexcsnaptree.tsc - EdsSrv and EdsMgr Unit Test\n\npls see below:\n\ntest to be added in lrgsaexcedssnaptree",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsnapvoldelparallel.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcsnapvoldelparallel.tsc - Test : do lsvolumesnap in parallel to volume deletion,\n      such that when the snap tries to access teh parent volume data it fails",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsnshtmrrsync.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcsnshtmrrsync.tsc - Test for snapshot mirrors are in sync\n\nUnit test to validate mirror sync code in EDS works:\n       Create a non-DRL file with redundancy=high\n       for 1 to 20\n         Run write workload to the file\n         While workload is running, create clone of the file\n         Validate clone mirror using tsagmigvalid.sh",
    "platform": null
  },
  {
    "test_name": "tsagexcspreconfig_disk.tsc",
    "setup": null,
    "flags": {
      "num_cells": "4",
      "admin_wallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcspreconfig_disk.tsc - double failure tests\n\npls see below\n\nto be added in lrgdbcsaexcfailc30",
    "platform": null
  },
  {
    "test_name": "tsagexcspresurrect.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "egs_trace": "highest",
      "fc_disk_trace_level": "high",
      "vault_db": "DATA"
    },
    "description": "tsagexcspresurrect.tsc - Creating new storagepool after deletion\n\nTests populates a storagepool to ensure the population of XRMEM\n     backing it, then drops it before recreating  a new storagepool\n     using dropped and recreated GrdDisks. Any data that may represent\n     the new GridDisks, including the EDS metadata should not be returned\n     by the XRMem cache when the new storagepool is being created or after.\n     It is difficult to mimic the exact signature with branch and extent\n     information for the EDSmetadata (which reports an ORA-700 in case of\n     stale or bad data being read back) so the test attempts to maximize\n     the XRMem cache population as the best effort. While RDBMS as the\n     workload driver for the same is acceptable, iov allows for easy\n     debugging in case of XRMEM cache issues like mirror mismatches, etc.\n\nInitial goal is to create a simple test case with a single cell\n     which can then be augmented with multiple cells and posible\n     testing combinations. Donor file: tsagexcnspc.tsc",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsrfailover.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "dsk_size": "medium",
      "num_cells": "4"
    },
    "description": "tsagexcsrfailover.tsc - SR failover test\n\nPlease see below\n\nto be added in lrgdbcsaexcsrfailover",
    "platform": null
  },
  {
    "test_name": "tsagexcsrvredund.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high",
      "setup_blockstore": "true",
      "oss_multims_testing": "true"
    },
    "description": "tsagexcsrvredund.tsc - Test for RS Redundancy Checks\n\nTest steps -\n\n      Check for each service - BSM, BSW, ERS, EGS, SYSEDS, USREDS\n1. Find 2 cells on which service is running\n2. Shutdown service on a cell\n3. Make sure service is gone using ps\n\t4. Switch to second cell\n5. Offline same service on second cell\n\t6. The command should immediately fail with error\n7. Restart same service on second cell\n   8. The command should immediately fail with error\n   9. Restart the service with 'ignore redundancy'\n   10. Restart should pass\n   11. Shut down the service with 'ignore redundancy'\n   12. Shutdown should pass\n   13. Restart service for next step\n   14. Restart EGS Leader (except for EGS)\n   15. Switch back to first cell\n   16. Startup service on first cell\n   17. Shutdown the service on the second cell, to confirm that it can go\n       offline\n   18. Start up the service\n\n   Shutdown All Case\n   1. Shut down all services on one cell\n   2. confirm that none of those services can be shut down on a second cell\n   3. check RS status - should be alive\n   4. Restart all the service with 'ignore redundancy'\n   5. Restart should pass\n   6. Restart without ignore redundancy - should fail\n   7. switch back to cell1 and startup all services",
    "platform": null
  },
  {
    "test_name": "tsagexcssst.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcssst.tsc - Smart Scan simulation Tool Test in Exascale Env\n\nTxn tests support for exascale env to the smart scan simulation tool",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsstreedel.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcsstreedel.tsc - test_case for snapshot tree node drop collapse",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcstandby.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "vault_db": "DATA",
      "PWFILE_ON_EXC": "false",
      "compatible": "^def_compatibility^",
      "media_type": "HC,EF",
      "NUM_FLASH_PER_CELL": "6",
      "auto_local_undo": "true",
      "standalone": "true",
      "dsk_size": "xl",
      "TDE_ENC_TS": "true",
      "eswallet": "^t_work^/esadmin_wallet",
      "dbusrwallet": "^ORACLE_BASE^/admin/^ORACLE_SID^/eswallet"
    },
    "description": "tsagexcstandby.tsc - Creates a Standby DB using RMAN duplicate\n\nUses RMAN duplicate to create a standby DB of a primary DB on Exascale",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcstandby2.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "vault_db": "DATA",
      "PWFILE_ON_EXC": "false",
      "compatible": "^def_compatibility^",
      "media_type": "HC,EF",
      "NUM_FLASH_PER_CELL": "4",
      "num_cells": "3",
      "auto_local_undo": "true",
      "TDE_ENC_TS": "true",
      "shared_pool_size": "1G",
      "eswallet": "^t_work^/esadmin_wallet",
      "dbusrwallet": "^ORACLE_BASE^/admin/^ORACLE_SID^/eswallet"
    },
    "description": "tsagexcstandby2.tsc - test standby creation on EF when\n                           primary db is both on EF and HC\n\nTest to check if standby db can be created on EF alone\n     when primary db is on both EF and HC",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcstress.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "eds_stress_test": "true",
      "dsk_size": "medium"
    },
    "description": "tsagexcstress.tsc - edstool stress test\n\nStress workload to test n/w connections\n    starts 110 edstool put commands in parallel\n    keep running (& forking) additional commands to keep the count around 110\n    continue doing this till 140K files are put on the Vault\n\nThis LRG runs on 32G VM and takes about 6 hours",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcsts4curl.tsc",
    "setup": null,
    "flags": {
      "debugfile": "^tst_tscname^_debug.lst",
      "excld_ers_selfsigned": "1",
      "exc_cloud_user_ers_port": "5052",
      "egs_deplmode": "sharedCloud"
    },
    "description": "tsagexcsts4curl.tsc - Test for curl command run directly from DP\n\nTest for curl command run directly from DP",
    "platform": null
  },
  {
    "test_name": "tsagexcsusersecuritytest.tsc",
    "setup": null,
    "flags": {
      "setup_blockstore": "true",
      "iscsi": "true"
    },
    "description": "tsagexcsusersecuritytest.tsc - exascale user security tets\n\ncross user access testing",
    "platform": null
  },
  {
    "test_name": "tsagexcswinstfeaturever.tsc",
    "setup": "xrdbmsini",
    "flags": null,
    "description": "tsagexcswinstfeaturever.tsc - Test cases for instance feature versioning\n\nThis test contains test cases for below mentioned newly added features\n     in ExaScale software feature project.\n\n     1. Add ExScale instance(EGSLIB clients running on compute VMs,\n        including GI, DB, EDV, tools, etc) feature support.\n     2. Implementation of feature deferred update policy.\n     3. Adding support for service release version control based on\n        the latest release version definition.\n\nThe test covers the below mentioned scenarios:\n\n     1. Test to add a fake clusterware feature\n     2. Release Version control tests\n     3. ESNP feature reconfig tests\n     4. ESNP instance tests\n     5. EGS feature list tests\n     6. Feature Deferred Update Policy Tests",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexctcpmon.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high"
    },
    "description": "tsagexctcpmon.tsc - TCP Monitor test\n\ntests TCP Monitor thread and verifies that EGS detects cellsrv down",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexctemplateescli.tsc",
    "setup": "xblockini",
    "flags": {
      "eswallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexctemplateescli.tsc - Test for escli template commands\n\nTest for escli template commands",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcthincloneofaclone.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexcthincloneofaclone.tsc - test to crash bsm/bsw while creating a thin clone,\n                                form an existing clone\n\nSimulates bsm/bsw crash at different job states during\n     the creation of thin clones from existing clone",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcthincloneofavolbkpclone.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexcthincloneofavolbkpclone.tsc - test to crash bsm/bsw while creating a thin clone C2,\n                                from an existing clone C1.\n\nRestore volume from a backup then create thin clone of it.\n     Simulates bsm/bsw crash at different job states during creating a clone of another clone and validate if it succeeds.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexctwoownersvolume.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagexctwoownersvolume.tsc - test to check,\n                                  2 users as owners of each volume is now allowed\n     more details : https://orareview.us.oracle.com/transaction/oxu_chvolume_user\n\nAdditional: More than 1 owners of a volume.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcuncomgc.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcuncomgc1.tsc - Test for file level TIME-TO-LIVE(TTL) for uncommitted files\n\nThis change introduces a per-file time-to-live (TTL, in seconds) for entries in\n     the on-disk uncommitted file hash table. This requirement is realized to support\n     Object Store PUT API which creates and writes to temp files before renaming on\n     commit. If the API server crashes mid-PUT, these temp files may linger,\n     consuming storage and impacting billing, space availability, and list-object API\n     performance. This change enables timely garbage collection of such uncommitted\n     files.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcunlimvault.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "multisp": "true"
    },
    "description": "tsagexcunlimvault.tsc - Tests on vaults with unlimited resources",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcunlimvoliops.tsc",
    "setup": "xblockini",
    "flags": {
      "multisp": "true"
    },
    "description": "tsagexcunlimvoliops.tsc - Test for volumes with unlimited iops\n\nIn this test, volumes are created with/without provisioning iops and\n    IORM plans are verified for chvolume operations.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcupdatevolsnapacl.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "false"
    },
    "description": "tsagexcupdatevolsnapacl.tsc\n\nTest to add/remove new owner to volume,\n     create vol,snap,clone.use the ls cmd and check the acl details\n     before and after making the ownership changes.\n\n     more details can be found here,\n     aembarca_bug-37280235 : https://orareview.us.oracle.com/149083086",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcuploadfiles.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcuploadfiles.tsc - Functional test for uploading large files\n\nThis test uploads files around 5G in size and larger through escli\n     and then downloads them and validates the files.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcusersetupstress.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcusersetupstress.tsc - User Setup Stress Test\n\nRun 100 concurrent sessions of creating users and setting up wallets",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcusrpriv.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "eswallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcusrpriv.tsc - ExaScale User Privilege Tests\n\nTests User Privileges in Exascale\n     Details at - https://confluence.oraclecorp.com/confluence/display/~rajeev.k.jain@oracle.com/Test+Plan+for+User+Privileges",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvaultattr_ef.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "media_type": "EF"
    },
    "description": "tsagexcvaultattr_ef.tsc - ESCLI Test for pmemCache attributes for media_type EF only.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvaultescli.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcvaultescli.tsc - Test for escli vault commands\n\nTest for escli vault commands",
    "platform": null
  },
  {
    "test_name": "tsagexcvaultloop.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "tstlogfile": "^tst_tscname^_eds.log"
    },
    "description": "tsagexcvaultloop.tsc - loop test for vault and file restarting eds in background",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvaultmetrics.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "cmd_mirror_mode": "normal",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagexcvaultmetrics.tsc - Curl test for vault metrics\n\nAdditional: test for Vault metrics",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvaultmetrics_2.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "cmd_mirror_mode": "normal",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcvaultmetrics_2.tsc - Vault Metrics tests\n\nChecks if user with privilage cl_monitor is able to list vault metrics data.\n     Test runs in a 3 Cell env with a DB setup\n     Test runs with active DB workload in onPrem mode\n     EDS crashes are also included in the test",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvaultoperation.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcvaultoperation.tsc - vault operation helper scripts\n\ncreates/deletes vault/file/snapshot/clone in a loop\n\nadded in lrgdbcsaexcfailc8 and lrgsaexacldvesresync3",
    "platform": null
  },
  {
    "test_name": "tsagexcvaultspaceexhaust.tsc",
    "setup": "xblockini",
    "flags": {
      "dsk_size": "xxl"
    },
    "description": "tsagexcvaultspaceexhaust.tsc\n\nCreate vautl, create blockstore objects in it, and check/validate,\n     space used/ available attributs",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvaultspaceusage.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcvaultspaceusage.tsc - Functional test for Space usage\n\n1. Create vault of size 100X\n   2. Create file/volume of size > 100X (fails)\n   3. Create file/volume of size <= 100X (success)\n   4. Create files/volumes of total size > 100X (the files after total size > 100X, fail)\n   5. Create files/volumes of total size <= 100X (all are successfully created)\n   6. Create files/volumes of total size = 90X, create clone for the files (will use 10% i.e 9X size, successful)\n   7. Create files/volumes of total size = 99X, create clone for the files (will use 9.9X, total 108.9X, fails)\n   8. Create files/volumes of total size = 80X, create clone for the files (will use 10% i.e 8X size, successful)\n   9. Create files/volumes of total size = 80X, create clone for the files, start filling the clone with > 20X (Total size > 100X, fails)\n   10. Delete few files/volumes from 100X full vault, recreate with sizes less than deleted files (success)\n   11. Delete few files/volumes from 100X full vault, recreate with sizes more than deleted files (fails)\n   12. Delete few files/volumes from 100X full vault, fill the remaining clones with sizes less than deleted files ( success)\n   13. Delete few files/volumes from 100X full vault, fill the remaining clones with sizes less than deleted files ( fails).\n   14. xsh vault space usage commands\n       1. xsh version (software version and label date in one line)\n       2. xsh ls -sh (h for human readable size)\n       3. xsh ls -s\n       4. Space used metrics of one vault:\n             xsh ls -sh @DATA\n       5. Details of multiple vaults:\n             xsh ls -sh\n       6. Details of multiple files:\n             xsh ls -dh @DATA/\n       7. Details of one file:\n             xsh ls -dh @DATA/file\n       8.Added negative test cases",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvaulttablespace.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "eswallet": "^t_work^/esadmin_wallet",
      "dbusrwallet": "^ORACLE_BASE^/admin/^ORACLE_SID^/eswallet"
    },
    "description": "tsagexcvaulttablespace.tsc - test_case of creating tablespace in vaults",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvector_common.tsc",
    "setup": null,
    "flags": {
      "shared_pool_size": "10G",
      "max_string_size": "extended",
      "pga_aggregate_limit": "10G",
      "flash_size": "5000",
      "vault_db": "DATA",
      "dsk_size": "medium"
    },
    "description": "tsagexcvector_common.tsc - helper script for vector db workload\n\nCOMMON FLAGS FOR VECTOR DB WORKLOAD",
    "platform": null
  },
  {
    "test_name": "tsagexcves_lastmirrordebugcli.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "oss_multims_testing": "true",
      "max_rpm": "10000",
      "delta_sync": "true",
      "debugcli": "1",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcves_lastmirrordebugcli.tsc - Exascale triple failure test\n\nTest case 0 - storagepool offline/online\n     Test case 1 - triple failure\n     Test case 2 - triple failure with disk restore\n     Test case 3 - triple failure with disk force add",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcves_lastmirrordebugclinormaldisabled.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "oss_multims_testing": "true",
      "max_rpm": "10000",
      "delta_sync": "true",
      "debugcli": "1",
      "exc_normal": "disable",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcves_lastmirrordebugclinormaldisabled.tsc - Exascale triple failure test with normal redundancy disabled\n\nTest case 0 - storagepool offline/online\n     Test case 1 - triple failure\n     Test case 2 - triple failure with disk restore\n     Test case 3 - triple failure with disk force add",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvesactvpredfail.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "no_gd_with_prefix": "true"
    },
    "description": "tsagexcvesactvpredfail.tsc - Active client test for graceful drop rebalance",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvesdelmd.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "ves_sim": "8",
      "sage_mirror_mode": "high",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagexcvesdelmd.tsc - VES metadata deletion test using exciogen tool\n\nPlease see below\n\ntest to be added in lrgsaexcvesdeletemd",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvesdeltarebal.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal"
    },
    "description": "tsagexcvesrpmrebal.tsc - Active client test rpm rebalance\n\n1. Setup 3 cell exastack for exascale delta resync false\n      2. Create a file @TEST/IOVTESTFILE -r 3 -f 0x200 -s 1g and keep IOV\n         running using ves_base_long_rw.xml.\n      3. Use exciogen to perform a translate on the IOV file, Save the rpm\n         Id 6005b2 removing the prefix 'c0'.\n      4. Offline the griddisk via \"cellcli -e alter griddisk <diskname> inactive\",\n         wait for EGS to offline it Wait for 60s, keep IOV workload running\n      5. Do a state dump via cellcli -e \"alter cell events='immediate\n         cellsrv.cellsrv_statedump(0)'\"\n      6. Use escli command \"escli --wallet $T_WORK/esadmin_wallet lspooldisk 74\"\n         to get disk\n      7. Simulate predictive failure on it and wait for rebalance to complete\n      8. On each cell, do a cellsrv state dump, then grep -A1 \"Printing ongoing\",\n         it should look like following without additional message:\n      9. Cancel the failure\n      10. Active the inactive disk via alter griddisk xxx active",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvesgddefrag.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "vault_db": "DATA",
      "sage_mirror_mode": "high"
    },
    "description": "tsagexcvesgddefrag.tsc - Test for griddisk defragmentation\n\nTest for a new feature - VES griddisk defragmentation\n      It combines small 8k allocations into 64k and above large\n       allocations.\n      Test Scenario -\n       1. create a file and create a snapshot of it\n       2. write 7 8k blocks to the file using xsh\n       3. Do ves allocation dump, we should see seven smallAlloc on snap 1\n          and isDefragNeeded is 0 for that extent\n       4. After writing the 8th 8k block, isDefragNeeded should become 1\n       5. Trigger defrag through md_resync command\n       6. After defrag, we should see a 64k large allocation on snap 1\n\nTest is added to lrgsaexcedssnaptree",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcveshardchk.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true"
    },
    "description": "tsagexcveshardchk.tsc - VES Hardcheck failure test\n\nSet stackup with 3 cells\n     edstool create @TEST/IOVTESTFILE -s 144m -r 3 -b 8k\n     shut down the cellsrv\n     run iov on @TEST/IOVTESTFILE with read/write\n     Restart the cellsrv\n     Simulate 1000-count retry\n     Grep for hardChecks failed, insert into BadOffsetExtList\n     edstool create @TEST/IOVTESTFILE2 -s 64m -r 3 -b 512\n     shut down the cellsrv\n     xsh dd --if=/dev/urandom --out=@TEST/IOVTESTFILE2 --bs=1M --count=64\n     run iov on @TEST/IOVTESTFILE2 with read/write\n     Restart the cellsrv\n     Simulate 1-count retry\n     Grep for HARD checks failed: retries: 0, hardchk: .*, hardftyp: .*, hardblksz: 512, cgtyp: .*\n     edstool create @TEST/IOVTESTFILE3 -s 64m -r 3 -b 16384\n     shut down the cellsrv\n     xsh dd --if=/dev/urandom --out=@TEST/IOVTESTFILE3 --bs=1M --count=64\n     run iov on @TEST/IOVTESTFILE3 with read/write\n     Restart the cellsrv\n     Simulate 1000-count retry\n     Grep for HARD checks failed: retries: 1, hardchk: .*, hardftyp: .*, hardblksz: 16384, cgtyp: .*",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvesnonactvpredfail.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "no_gd_with_prefix": "true"
    },
    "description": "tsagexcvesnonactvpredfail.tsc - Non-active test for graceful drop rebalance\n\n1. Setup 3 cell exastack for exascale with Debugcli\n      2. Gracefully drop a disk ( pred fail)\n      3. Verify alerts - Being gracefully dropped/rebalance started\n         dropped/rebalance finalised\n      4. Run IOV workload and let it finish\n      5. replace the disk\n      6. Verify alerts - Being Added/Rebalance started\n         Added/Rebalance finalised\n      7. dump file and Check md5sum",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvesrpmrebal1.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "devdir1": "^T_WORK^/raw^oss_port^",
      "devdir2": "^T_WORK^/raw^oss_port2^",
      "devdir3": "^T_WORK^/raw^oss_port3^"
    },
    "description": "tsagexcvesrpmrebal1.tsc - Active client test rpm rebalance\n\n1. Setup 3 cell exastack for exascale delta resync true\n      2. Create a file @TEST/IOVTESTFILE -r 3 -f 0x200 -s 1g and keep IOV\n         running using ves_base_long_rw.xml.\n      3. Use exciogen to perform a translate on the IOV file, Save the rpm\n         Id 6005b2 removing the prefix 'c0'.\n      4. Offline the griddisk via \"cellcli -e alter griddisk <diskname> inactive\",\n         wait for EGS to offline it Wait for 60s, keep IOV workload running\n      5. Do a state dump via cellcli -e \"alter cell events='immediate\n         cellsrv.cellsrv_statedump(0)'\"\n      6. Use escli command \"escli --wallet $T_WORK/esadmin_wallet lspooldisk 74\"\n         to get disk\n      7. Simulate predictive failure on it and wait for rebalance to complete\n      8. On each cell, do a cellsrv state dump, then grep -A1 \"Printing ongoing\",\n         it should look like following without additional message:\n      9. Cancel the failure\n      10. Active the inactive disk via alter griddisk xxx active",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvesrpmrebal2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "debugcli": "1",
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "devdir1": "^T_WORK^/raw^oss_port^",
      "devdir2": "^T_WORK^/raw^oss_port2^",
      "devdir3": "^T_WORK^/raw^oss_port3^"
    },
    "description": "tsagexcvesrpmrebal2.tsc - Active client test rpm rebalance\n\n1. Setup 3 cell exastack for exascale delta resync true\n      2. Create a file @TEST/IOVTESTFILE -r 3 -f 0x200 -s 1g and keep IOV\n         running using eds_snap_*xml.\n      3. Use exciogen to perform a translate on the IOV file, Save the rpm\n         Id 6005b2 removing the prefix 'c0'.\n      4. Offline the griddisk via \"cellcli -e alter griddisk <diskname> inactive\",\n         wait for EGS to offline it Wait for 60s, keep IOV workload running\n      5. Do a state dump via cellcli -e \"alter cell events='immediate\n         cellsrv.cellsrv_statedump(0)'\"\n      6. Use escli command \"escli --wallet $T_WORK/esadmin_wallet lspooldisk 74\"\n         to get disk\n      7. Simulate predictive failure on it and wait for rebalance to complete\n      8. On each cell, do a cellsrv state dump, then grep -A1 \"Printing ongoing\",\n         it should look like following without additional message:\n      9. Cancel the failure\n      10. Active the inactive disk via alter griddisk xxx active",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvipmigclearconf.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true",
      "multiple_bst": "true # Inorder to setup 2 BSM + 3 BSW instances",
      "setup_blockstore": "true"
    },
    "description": "tsagexcvipmigclearconf.tsc - Test the migration of vip from bsw1 to bsw2 , from bsw2 to bsw3.\n     functional test for fix bug 36213160.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvolgrptypes.tsc",
    "setup": "xblockini",
    "flags": {
      "python_path": "^ADE_VIEW_ROOT^/python/bin",
      "ledvniscsi": "1",
      "edvcleanup_only": "1"
    },
    "description": "tsagexcvolgrptypes.tsc -Multiple Volume Groups and Volume Group Type\n\nmore details at:\n     https://confluence.oraclecorp.com/confluence/display/EXC/Multiple+Volume+Groups+and+Volume+Group+Type",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvolloop_curl.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcvolloop_curl.tsc - This test adds block store volume operations using only curl calls\n\nBlock store operations using curl - Loop test",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvolopr.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcvolopr.tsc - This test adds iscsi volume operation + edv volume\n\nTest volume operations",
    "platform": null
  },
  {
    "test_name": "tsagexcvolrace.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcvolrace.tsc - functional test for lsvolume race condition\n\n1. drop xblockini volumes\n     2. make 3 new volumes\n     3. initiate bsmevent simulation\n     4. run lsvolume (should be stuck for 1 min)\n     5. remove one volume, lsvolume should return two volumes",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvolresize.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcvolresize.tsc - Volume resize test filesystem on it\n\nVolume resize test",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvolresizeloop.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcvolresizeloop.tsc - Volume resize test filesystem on it in loop\n\nVolume resize test in loop",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvolumeattachmentios.tsc",
    "setup": "xblockini",
    "flags": {
      "eswallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcvolumeattachmentios.tsc - test case for checking that data in volumeattachments of same volume\n     but created using different initiatorids, is same.\n\ntest case for checking that data in volumeattachments of same volume but created using\n     different initiatorids, is same.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvolumegrp2.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcvolumegrp2.tsc - test the volume group feature\n\ntest the newly added volume group features\n     Advance tests from this test doc\n     https://confluence.oraclecorp.com/confluence/display/~kuan-ju.chen@oracle.com/Volume+Group+Tests",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxattrescli.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "eswallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcxattrescli.tsc - Test for extended attribute commands\n\nTest for extended attribute commands: chxattr, lsxattr and rmxattr",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxrmwotpm.tsc",
    "setup": null,
    "flags": {
      "cell_with_pmem_disk": "false"
    },
    "description": "tsagexcxrmwotpm.tsc - Test for xrmem and pmem conversion without pmemcache\n\nTestcases includeded :\n     1) list xrmemCache and list pmemCache output should be same\n     2) After dropping  xrmemCache and creating pmemCache, it should\n        create xrmemCache\n     3) Drop pmemCache and create xrmemCache, it should again create\n        xrmemCache\n     4) Output for create pmemCache and create xrmemCache should be same\n     5) pmemCache is not treated as xrmemCache if xrmemCacheEnabled=false\n     6) Auto-Add xrmemcache via xrmemCacheEnabled=true\n     7) Auto-drop xrmemCache via xrmemCacheEnabled=false",
    "platform": null
  },
  {
    "test_name": "tsagexcxrmwpm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcxrmwpm.tsc - Test for xrmem and pmem conversion with pmemCache\n\n1) list xrmemCache and list pmemCache output should be same\n     2) After dropping  pmemcache and creating xrmemCache, it should\n        create pmemCache\n     3) Drop xrmemCache and create pmemCache, it should again create\n        pmemCache\n     4) Output for create pmemCache and create xrmemCache should be same\n     5) xrmemCache is not treated as pmemCache if xrmemCacheEnabled=true",
    "platform": null
  },
  {
    "test_name": "tsagexcxshcat.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshcat.tsc - Test for xsh cat command\n\nTest for xsh cat command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshchacl.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshchacl.tsc - Test for xsh chacl command\n\nTest for xsh chacl command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshconcurrency.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "admin_wallet": "^t_work^/esadmin_wallet"
    },
    "description": "tsagexcxshconcurrency.tsc - Test for xsh commands in a loop\n\nTest runs a bunch of xsh commands with 50 different users in\n     loop simultaneously.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshcp.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshcp.tsc - Test for xsh cp command\n\nTest for xsh cp command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshdd.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshdd.tsc - Test for xsh dd command\n\nTest for xsh dd command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshls.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshls.tsc - Test for xsh ls command\n\nTest for xsh ls command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshlsacl.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexcxshlsacl.tsc - Test for xsh lsacl command\n\nTest for xsh lsacl command",
    "platform": null
  },
  {
    "test_name": "tsagexcxshlsclusterguid.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcxshlsclusterguid.tsc - Tests for xsh lsclusterguid command\n\nThe peirao_add_xsh_lsclusterguid txn introduced a new command xsh\n     lsclusterguid, which shows the list of cluster info acknowledged by the\n     EGS leader.\n\n     Positive Case 1: lsclusterguid without arguments\n     Positive Case 2: lsclusterguid with valid wallet path\n     Positive Case 3: lsclusterguid with --json flag\n\n     Negative Case 4: lsclusterguid with invalid argument\n     Negative Case 5: lsclusterguid with invalid wallet path\n\nAdded in lrgsaexcxshperf",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshlsdataset.tsc",
    "setup": "xblockini",
    "flags": {
      "PWFILE_ON_EXC": "true",
      "VF_ON_EXC": "true",
      "vault_db": "DATA",
      "media_type": "HC,EF"
    },
    "description": "tsagexcxshlsdataset.tsc - Test for xsh lsdataset command\n\nTest for xsh lsdataset command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshlsfeature.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcxshlsfeature.tsc - tests for xsh lsfeature command\n\nPositive Case 1: lsfeature without arguments\nPositive Case 2: lsfeature with wallet option\nPositive Case 3: lsfeature with feature name ($FEATURE_NAME)\nPositive Case 4: lsfeature JSON output\n\nNegative Case 5: invalid wallet path\nNegative Case 6: invalid option\nNegative Case 7: invalid feature name\n\nto be added in lrgsaexcxshperf",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshman.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshman.tsc - Test for xsh man command\n\nTest for xsh man command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshmkvault.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshmkvault.tsc - Test for xsh mkvault command\n\nTest for xsh mkvault command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshmv.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshmv.tsc - Test for xsh mv command\n\nTest for xsh mv command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshod.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshod.tsc - Test for xsh od command\n\nTest for xsh od command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshperformance.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcxshperformance.tsc - Test for xsh performance.\n\nThis test is to check xsh performance by:\n    1. Check 1000 vault creation duration\n    2. Creating 2500 files in vault 1\n    3. Cloning 2500 files in vault 1\n    4. Deleting cloned files from vault 1\n    5. Deleting base files from vault 1\n    6. Deleting all 1000 vaults",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshresourceprofile.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshresourceprofile.tsc - Test for xsh resourceprofile command\n\nTest for xsh resourceprofile command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshrm.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshrm.tsc - Test for xsh rm command\n\nTest for xsh rm command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshrmvault.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshrmvault.tsc - Test for xsh rmvault command\n\nTest for xsh rmvault command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshstrings.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshstrings.tsc - Test for xsh strings command\n\nTest for xsh strings command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshtemplate.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshtemplate.tsc - Test for xsh template command\n\nTest for xsh template command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshtrace.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshtrace.tsc - Test for trace option in xsh\n\nThis test is related to traces of xsh command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcxshxattr.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagexcxshxattr.tsc - Test for xsh xattr command\n\nTest for xsh xattr command",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexczdlra.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagexczdlra.tsc - Setup script for ZDLRA on exascale, sets up\n                        zdlra and exascale and also does backup/restore\n\nWorkarounds:\n       For any workarounds, use the tag Workaround::: so that they can be\n       searched and removed later",
    "platform": null
  },
  {
    "test_name": "tsagexczdlrasetup.tsc",
    "setup": null,
    "flags": {
      "cdb": "true",
      "ORACLE_SID_ORIG": "^ORACLE_SID^",
      "logfile_main": "^logfile^",
      "logname_main": "^logname^",
      "logfile_temp": "tsagexczdlrasetup_temp.log",
      "vault_log": "^ORA_SID_UPPER^",
      "vault_db": "^ORA_SID_UPPER^"
    },
    "description": "tsagexczdlrasetup.tsc - Replacement of tkrmset for tkrmrrset when\n                             run in exascale mode\n\nWorkarounds:\n       For any workarounds, use the tag Workaround::: so that they can be\n       searched and removed later",
    "platform": null
  },
  {
    "test_name": "tsagexectest1.tsc",
    "setup": null,
    "flags": {
      "tmp": "CELL^envar^_IP"
    },
    "description": "tsagexectest1.tsc - Basic rds-ping\n\nBasic rds-ping\n\nBasic rds-ping",
    "platform": null
  },
  {
    "test_name": "tsagexectest2.tsc",
    "setup": null,
    "flags": {
      "bwpattern": "'[1-9]\\.[0-9][0-9]*'"
    },
    "description": "tsagexectest2.tsc - basic qperf testing with large buffer sizes\n\ntasgexectest2.tsc tests the qperf with large buffer sizes from dbnode to cell and vice versa.\nFirst, it starts the qperf utility on dbnode and executets bandwidth test on cell. After execution of the\nbandwidth test kills the qperf utility on dbnode. Next to that same process is repeated from cell to dbnode.",
    "platform": null
  },
  {
    "test_name": "tsagexp.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexp.tsc - export import CellDisks\n\nThis tests the export import celldisk feature using CellCli",
    "platform": null
  },
  {
    "test_name": "tsagexp_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagexp_nls.tsc - export import CellDisks\n\nTests Export Import Celldisk commands of cellcli",
    "platform": null
  },
  {
    "test_name": "tsagexpimp.tsc",
    "setup": null,
    "flags": {
      "oss_testing": "2",
      "creatdev_file": "tsagddef",
      "tmp_port": "^oss_port^",
      "raw2": "^raw^/",
      "raw1": "^raw^/",
      "raw_path": "^raw1^"
    },
    "description": "tsagexpimp.tsc - Export Import Celldisk Test\n\nTests the export and import of celldisks from 1 cell to second cell",
    "platform": null
  },
  {
    "test_name": "tsagextstor_arbiter.tsc",
    "setup": "srdbmsini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagextstor_arbiter.tsc - Unit test for cell to cell rebalance in ExaCloud\n\n(1) rebalance from 1 disk to other on same cell\n    (2) rebalance from 1 disk on cell 1 to 1 disk on cell 2\n    Uses IOV and Arbiter for unit testing",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfail.tsc",
    "setup": null,
    "flags": {
      "tsagfailLog": "^logfile^",
      "tsagfailLogRef": "tsagfail.log",
      "errbasename": "^outbasename^"
    },
    "description": "tsagfail.tsc - test a failure scenario while DB workload is going on\n\nIf this is run standalone, there is no failure scenario.\n     This can be invoked by a parent script which should set myFailAct\n     to command to execute to simulate failure (see tsagflashrm.tsc).",
    "platform": null
  },
  {
    "test_name": "tsagfailhd.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagfailini.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagfailini.tsc - set defaults for tsagfail.tsc\n\nSet defaults to be used by tsagfail.tsc.\n     Calling script should set dbusr to DB login, e.g. scott/tiger,\n     and set workloadtbl to table to be used for workload.",
    "platform": null
  },
  {
    "test_name": "tsagfailsimulation.tsc",
    "setup": "tsaginit",
    "flags": {
      "MACH_NAME": "slcc13cel14",
      "cellconnstr": "root@slcc13cel14",
      "cell": "slcc13cel14",
      "MACH_PASSWD": "welcome1"
    },
    "description": "tsagbugsimulation.tsc - Test for BUG 16363462: NEED A MECHANISM TO SIMULATE COMMAND EXECUTION AND IOCTL FAILURES/TIMEOUT/ERRORS\n\nThe test adds a new underscore parameter to cellinit.ora called _cmd_failure_simulation_pattern to simulate failures.\n\nThe test file will add a new underscore parameter to cellinit.ora called _cmd_failure_simulation_pattern\nwith a pattern and a return value.\nIf CommandExecutionUtil finds a match, then it will not execute that command.\nIt will instead return the retval specified in the underscore parameter.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfakeiberrsim.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagfakeiberrsim.tsc - adding failure simulation in fakeib\n\nadding failure simulation in fakeib, so that IPCDAT RDMA error code-paths can be exercised over fake hardware.\n\ntest cases present:\n1. The test can set the following parameter in fakeib.prm _fakeib_error_sim=create_cq exclude=cellsrv retcode=22 count=100 interval=1. Setting this parameter will result in fakeib simulating errors while creating\n  RDMA completion queues with a 50% probability. IPCDAT context creation can fail when error simulation is enabled for create_cq. That means RDMA operations will\n  not be performed.So, the test can take that into account. The database should still successfully startup and workload should complete.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfakeiberrsim_destroy_qp.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagfakeiberrsim_destroy_qp.tsc\n\nadding failure simulation in fakeib using parameter fakeib_error_sim during RDMA QueuePair destruction.\n\nThe test set the following parameter in fakeib.prm _fakeib_error_sim: type=destroy_qp count=250 retcode=16.\n     The error simulation using above parameter will exercise the lesser taken code paths in the QP's life-cycle\n     and will target all modules that directly/in-directly create QPs such as IFD, PMemLog (both client and cellsrv), PMemCache etc.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfastmetadata.tsc",
    "setup": null,
    "flags": {
      "oss_failgroup": "failalldbdg",
      "OSS_ENABLE_FC_PERSISTENCE": "off",
      "OSS_ENABLE_NC_PERSISTENCE": "writeback",
      "exa_cdb": "true",
      "processes": "200",
      "sessions": "220",
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagfastmetadata.tsc - faster metadata test\n\nThis test validates update and persistence of celldisk metadata\n     to disk in case of flashcache/pmemcache failure. Now after xqliu_flashfailureopt\n     transaction metadata persistence would be speed up by temporarily\n     persisting celldisk metadata to system SSD. This temporal write would\n     only be triggered when celldisk is busy and a system SSD is available.So there\n     could be a significant speed up for flash cache failure and resilver\n     on x7 or later systems with a proper threshold value.\n\n     Normally the temporal metadata would be deleted after celldisk metadata\n     write, but if cellsrv crashes before deleting it, the metadata would be\n     read during next bootstrap, then be deleted.\n\nto be added in lrgdbcsafastermetadata",
    "platform": null
  },
  {
    "test_name": "tsagfault.tsc",
    "setup": "srdbmsini",
    "flags": {
      "testlogfile": "tsagqnt.log",
      "pidlogfile": "tsagqmpid.log",
      "asm_cluster": "^cur_cluster_guid^':'",
      "creatdev_file": "tsagr2def",
      "_rdbms_internal_fplib_enabled": "false",
      "cell_offload_plan_display": "ALWAYS"
    },
    "description": "tsagfault.tsc - Fault Isolation tests for Exadata\n\nWith fault isolation, cellsrv will try to detect which entity\n     previously caused a cell crash. Once cellsrv detects such an entity,\n     it will quarantine it. When the same entity comes again, cellsrv\n     won't do \"predicate-push\" for it.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfault1.tsc",
    "setup": "srdbmsini",
    "flags": {
      "pidlogfile": "tsagqmpid1.log",
      "alerthistoryfile": "tsagfault1_list_alert_manual_qnt.log",
      "asm_cluster": "',clusterName=ASMClusterName'",
      "asm_cluster_name": "^cur_cluster_guid^':'"
    },
    "description": "tsagfault1.tsc - Exadata Quarantine Manager Test 2\n\ni) Test Case 1.1:Select count from user_tables with railroad crash enabled.\n     ii) Test Case 1.2:Run 3 queries with railroad crashes enabled.\n     iii) Test Case 1.3: a) Negative test case for Manual database quarantine creation\n                          b) Positive test case for Manual DISK REGION  and SQL PLAN quarantine creation\n                          c) Negative test case for Manual DISK REGION  and SQL PLAN quarantine creation\n     iv) Test Case 1.4:Create Cell Offload quarantine and  run queries-should not go through smart scan\n     v) Test Case 1.5:Create SQLID quarantine and Run queries-should not go through smart scan\n     vi) Test Case 2.1:Test Manual SQLID Quarantines creation and deletion\n     vii) Test Case 2.2:Test alter quarantine with attribute comment.Also test list quarantine SQLID.\n\n     The test also tests list alerthistory.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfault2.tsc",
    "setup": "srdbmsini",
    "flags": {
      "pidlogfile": "tsagqmpid2.log",
      "quarantine_count_threshold": "41"
    },
    "description": "tsagfault2.tsc - Exadata Quarantine Manager Test 3\n\ni) Test _cell_qm_db_quarantine_time_threshold\n     ii) Test _cell_qm_db_quarantine_threshold",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfaultcli.tsc",
    "setup": null,
    "flags": {
      "asm_cluster": "^cur_cluster_guid^':'",
      "db1_name": "^qm_db_name_1^"
    },
    "description": "tsagfaultcli.tsc - Quarantine Manager CLI test",
    "platform": null
  },
  {
    "test_name": "tsagfbattst.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagfbattst.tsc - BBU tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfbattstcv.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagfbattstcv.tsc - Cache Vault tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfc_xrmc_resiliency.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagfc_xrmc_resiliency.tsc - Test for resiliency of FlashCache and XRMemCache\n                                  by running I/O workloads, recreating cache, restarting cell services\n\n1. Copy Orion from the local directory to the cell node using SSH as root and place it in the target directory\n   2. In OSSCONF, create cellip ora with cell equals ipaddress:5042, using ipaddress one or ipaddress two from cellinit ora\n   3. Create one gigabyte griddisks for all celldisks with the command create griddisk DATA CD zero zero hostname celldisk equals CD zero zero hostname offset equals thirty two megabytes size equals one GB\n   4. Create the gds lun file and populate its contents\n   5. Ensure FlashCacheMode is set to WriteBack and use cellcli commands to check or set\n   6. Ensure FlashCache and XRMemCache exist and create them with cellcli if needed\n   7. Wait for griddisks to show cachedBy as updated or sleep for two minutes\n   8. Start the Orion workload using the appropriate advanced random command\n   9. Run ecstat with count four and observe FlashCache writes and disk reads\n  10. Stop the Orion workload after the test by finding its process ID and killing the process\n  11. Recreate FlashCache with cellcli by flushing dropping creating and listing\n  12. Repeat validation after cache re creation\n  13. Kill cellsrv and run ecstat with count five until cellsrv restarts disk read IO per second should be zero",
    "platform": null
  },
  {
    "test_name": "tsagfcchksum.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "conn1": "'sys/knl_test7 as sysasm'",
      "creatdev_file": "tsagcaudef",
      "compatible": "11.2.0.0",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback"
    },
    "description": "tsagfcchksum.tsc - FC checksum test\n\nFC user data checksum to be switched from SCNDBA to CRC and back",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfccomp.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagfccompdef",
      "log_file": "tsagfccompini.log"
    },
    "description": "tsagfccomp.tsc - Dynamic Compressed FlashCache Tests on Real hardware\n\nBasic Orion FlashCache Compression testcases for Project Dynamic Compressed FlashCache",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfccomp1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagfccomp1.tsc - Dynamic Compressed FlashCache\n\nBasic FlashCache Compression testcases for Project Dynamic Compressed FlashCache\n\nThese tests just run on Fake Exadata",
    "platform": null
  },
  {
    "test_name": "tsagfcdiskstat.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_testing": "1"
    },
    "description": "tsagfcdiskstat.tsc - Basic functional test of FC disk stats\n\nFunction test for FCStats\n     Runs different types of workloads with Orion\n     and checks to make sure that the expected FCStats are incremented",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcdiskstat_x10ef.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_testing": "1",
      "flashsize": "512M",
      "creatdev_file": "tsagqlcdef",
      "CELL_WITH_QLC_DISK": "1",
      "NUM_QLC_PER_CELL": "8",
      "QLC_DISK_SIZE_MB": "2048"
    },
    "description": "tsagfcdiskstat_x10ef.tsc - replicate tsagfcdiskstat test on X10EF setup\n\nThis new test replicates tsagfcdiskstat.tsc on a simulated X10EF setup.\n     This will be very helpful to catch potential issues in ECStat or\n     even new code in FlashCache before running it on real HW.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcgroupmon.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0"
    },
    "description": "tsagfcgroupmon.tsc - Test case for bug 32764765\n\nTest case for bug 32764765\n\nTest steps for below bug:\n     Bug# 32764765 - STABLE_E2POD:FLASHCACHE ENHANCEMENT,MONITOR FC GROUP PERIODICALLY AND PROACTIVELY REMOVE EMPTY GROUPS\n\n     1. No asm-db is needed. This will be a pure cell test. Create writeback FC, disable ramcache and pmemcache.\n\n     2. Use the IORM debug action to fake several DBs.\n     alter cell events = 'cellsrv.cellsrv_iorm_ctl[FAKE_DBS] opval=10'\n\n     3. Enable IORM plan for database \"TEST5\".\n     alter iormplan dbplan=((name=test5, share=1, flashcache=on))\n     list iormplan detail\n\n     4. Run orion workload on dbid=5 to populate flashcache. 100% read test would be fine.\n     $ADE_VIEW_ROOT/rdbms/bin/orion -run advanced -type rand -matrix point -testname test1 -size_small 8 -num_large 0 -num_small 8\n      -write 0 -duration 60 -dbid 5\n\n     5. After the workload, query flashcachecontent. The content should all cached data are from dbid=5 (TEST5).\n     list flashcachecontent detail\n\n     6. Perform a FC group dump. And grep \"FC GROUPS COUNT\" in fcgroup dump. There would be 2 groups.\n        One is for system default group, the other is for TEST5.\n     alter cell events=\"immediate cellsrv.cellsrv_flashcache(dumpgroups, 0, 0, 1)\";\n\n     7. Set IORM plan for database TEST6.\n     alter iormplan dbplan=((name=test6, share=1, flashcache=on))\n     list iormplan detail\n     list flashcachecontent where dbID=6 detail\n\n     8. Run orion workload on dbid=6 to populate TEST6 data into FC, which would evict TEST5 cached data.\n     $ADE_VIEW_ROOT/rdbms/bin/orion -run advanced -type rand -matrix point -testname test1 -size_small 8 -num_large 0\n      -num_small 8 -write 0 -duration 120 -dbid 6\n\n     9. After the workload, query flashcachecontent. The content should all cached data are from dbid=6 (TEST6).\n     list flashcachecontent detail\n\n     10. Perform a FC group dump. And grep \"FC GROUPS COUNT\" in fcgroup dump. There would be 3 groups.\n         One is for system default group, one is for TEST5, and the other is for TEST6.\n     alter cell events=\"immediate cellsrv.cellsrv_flashcache(dumpgroups, 0, 0, 1)\";\n\n     11. Set new cell parameter _cell_fc_group_gc_threshold_in_sec to 30 seconds. And sleep 30 seconds.\n     alter cell events = \"immediate cellsrv.cellsrv_setparam('_cell_fc_group_gc_threshold_in_sec', 30)\";\n     alter cell events = \"immediate cellsrv.cellsrv_getparam('_cell_fc_group_gc_threshold_in_sec')\";\n     sleep 30\n\n     12. Run FC debug action to trigger the FC monitor.\n     alter cell events=\"immediate cellsrv.cellsrv_flashcache(monitor, 0, 0, 1)\" ;\n\n     13. Perform a FC group dump. And grep \"FC GROUPS COUNT\" in fcgroup dump. There would be 2 groups.\n         One is for system default group, one is for TEST6. And there is no more group for TEST5.\n     alter cell events=\"immediate cellsrv.cellsrv_flashcache(dumpgroups, 0, 0, 1)\";",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcgroupmon2.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef"
    },
    "description": "tsagfcgroupmon2.tsc - Test case for bug 34028097\n\nTest case for bug 34028097",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcgroupmove.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0"
    },
    "description": "tsagfcgroupmove.tsc - test case for bug 33387010\n\nTest case for bug 33387010.\n\nTest steps:\n     (1) Create 2 test DBS (TEST5 & TEST6) and set default IORM plan.\n     (2) Run orion read workload on 300MB disk area to populate all data\n         into TEST6.\n     (3) Run orion read workload on 150MB disk area to switch cached\n         TEST6 content to TEST5 content. All reads should get cache hit.\n     (4) Set hardMax=200MB for TEST6.\n     (5) Run orion read workload on 300MB disk area again to simulate\n         database change of the cached data. All reads should get cache\n         hit, even when cachedSize of TEST6 has reached hardMax limit.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcgroupstests.tsc",
    "setup": "srdbmsini",
    "flags": {
      "compatible": "12.0.0.0",
      "num_dbs": "4 #number of databases to be created",
      "creatdev_file": "tsagfcgdef"
    },
    "description": "tsagfcgroupstests.tsc - tests for flash cache groups\n\nTests for flash cache groups\n\nTest 1: Create four databases, and create an inter-db plan on cell side for them.\n             Make sure the plan has been successfully altered.\n     Test 2: Make sure that all flashcache groups have been created, as per they mentioned in\n             inter-db plan.\n     Test 3: List the flashcache size used by each database and the total FC size.\n     Test 4: Populate databbase1 databaseb2 and databaseb3 that they fall in theree categories:\n             database 1: less than min\n             database 2: greater than max\n             database 3: between min and max sizes.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcgrp_common.tsc",
    "setup": null,
    "flags": {
      "file_dest1": "'+datafile'"
    },
    "description": "tsagfcgrp_common.tsc - Common Set of scripts for lrgsafcgroups%",
    "platform": null
  },
  {
    "test_name": "tsagfchrdgrp.tsc",
    "setup": "srdbmsini",
    "flags": {
      "num_dbs": "2",
      "debugcli": "true",
      "num_flash_per_cell": "4",
      "asm_ausize": "1048576"
    },
    "description": "tsagfchrdgrp.tsc - Test for FC Hard Groups\n\nVarious Tests for FlashCache Hard Groups",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfchrdgrp2.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagccdef"
    },
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagfchrdtest1.tsc",
    "setup": null,
    "flags": {
      "fcminsize": "256",
      "loopcount": "^db1rows_half^"
    },
    "description": "tsagfchrdtest1.tsc - Test Case 1 for lrgsafcgroups2\n\nFlashCache Soft Group testing. Steps:\n     a) create flashcache SOFT MAX group with min (X) and MAX(Y) sizes\n         where Y is the size of the FlashCache.\n     b) populate the group to make sure that it is getting\n         populated on empty cache\n     c) now add another softmax group and populate it make sure that\n         first group gives up cache headers but not below MIN levels.",
    "platform": null
  },
  {
    "test_name": "tsagfchrdtest2.tsc",
    "setup": null,
    "flags": {
      "loopcount": "^db1rows_half^"
    },
    "description": "tsagfchrdtest2.tsc - Test case 2 for Flashcache Hard max groups\n\nSanity Test for Flashcache Max groups\n      a) create flashcache HARD MAX group with size X (size from 576M to 4G)\n      b) populate the group beyond the group's max size and amake sure\n         it does not get populated beyond its max size.",
    "platform": null
  },
  {
    "test_name": "tsagfchugepage.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "cell": "^cell_node^"
    },
    "description": "tsagfchugepage.tsc - Flash Cache Hugepage Memory Test\n\nChecks for Huge Page Support and checks if data is cached after running the workload",
    "platform": null
  },
  {
    "test_name": "tsagfcinit.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagfcinit.tsc - Dynamic Compressed FlashCache\n\nFormat the device file and recreate FlashCache",
    "platform": null
  },
  {
    "test_name": "tsagfcioruntc.tsc",
    "setup": null,
    "flags": {
      "num_dbs": "1",
      "cur_cdb_prefix": "cdb^cur_cdb^_",
      "cur_pdb_name_var": "^cur_cdb_prefix^pdb^cur_pdb_num^",
      "cur_pdb_action_var": "^cur_cdb_prefix^pdb^cur_pdb_num^action",
      "cur_pdb_wlcount_var": "^cur_cdb_prefix^pdb^cur_pdb_num^wlcount",
      "cur_pdb_shares_var": "^cur_cdb_prefix^pdb^cur_pdb_num^shares"
    },
    "description": "tsagfcioruntc.tsc - Exadata CDB Flash Cache/DiskIO Run Test Case\n\nThis file runs a Flash Cache/Disk IO test case using the specified\n     parameters.\n\nParameters:\n        wl_type: Type of workload, can be 'fc' or 'io'\n\n        wl_time: Workload running time in seconds.\n\n        cdbn_pdbm: The name of the container where each workload will run,\n                   where n is the CDB number and m is the PDB number, and\n                   both numbers can be up to 10.\n\n        cdbn_pdbmwlcount: The workload count for the specified cdbn_pdbm\n                          container. I.e. a wlcount of 3 will be three times\n                          as intensive as a wlcount of 1.\n\n        cdbn_pdbmshares: The expected resource shares consumed by the\n                         container. Used for verification.\n\n        cdbn_pdbmaction: The action to be performed on the container before\n                         and after the test case is run. Only supported value\n                         is 'close', which closes the PDB before starting the\n                         test case, and reopens the container after the test\n                         case is done.",
    "platform": null
  },
  {
    "test_name": "tsagfciov.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_asm_sec": "false",
      "num_gd": "12",
      "flash_size": "96",
      "configset": "fciov_set4_direct_read",
      "threads": "5",
      "gdsz": "368                                                   # size in Mb",
      "duration": "36000                                          # max time to run",
      "area_sz": "64",
      "sleep_btw_test": "120",
      "max_areas": "10",
      "think_time": "500",
      "clusterid": "'ASMClusterName'",
      "cfgdir": "tsagfciov.sav",
      "semicolon": "';'",
      "c": "1",
      "temp_nowarn": "^tst_nowarn^",
      "tsagfciovscript": "tsagfciov1",
      "num_cases": "1",
      "asmsec_lrg": "_asmsec"
    },
    "description": "tsagfciov.tsc - Flash Cache tests with IOV\n\nFlash Cache tests with IOV\n     Usage: run tsagfciov [configset=<iov_config_set> [duration=<#_sec>]\n            [num_gd=<#_data_disks>] [gdsz=<griddisk_size_in_Mb>]\n            [test_cases=<csv_list_of_test_cases>]\n     - if configset is not specified, config files will be generated\n       with random values\n     - iov_config_set can be one of the fciov*.tgz file in tsage/sosd,\n       without the .tgz suffix\n     - other parameters like num_flash_per_cell (# of FDOM's),\n       flash_size (size of each FDOM), ...\n       can also be specified\n       e.g. oratst tsagfciov configset=fciov_set4_direct_read",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfciov1.tsc",
    "setup": null,
    "flags": {
      "reflogsuffix": "w",
      "mslog": "^celltrcdir^ms-odl.log"
    },
    "description": "tsagfciov1.tsc - Flash Cache IOV tests\n\nCalled by tsagfciov to run failure cases.",
    "platform": null
  },
  {
    "test_name": "tsagfciovhardminmax.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagxddef"
    },
    "description": "tsagfciovhardminmax.tsc - Test added in lrgsafciovgroup3\n\nFunctional Test for Bug 29029289 - TRACKING\n   BUG FOR ADW FC HARDMIN AND HARDMAX POPLICY SUPPORT",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfciovini.tsc",
    "setup": null,
    "flags": {
      "errbasename": "^outbasename^"
    },
    "description": "tsagfciovini.tsc - initializations for safciov*",
    "platform": null
  },
  {
    "test_name": "tsagfciovl.tsc",
    "setup": null,
    "flags": {
      "mslog": "^celltrcdir^ms-odl.log"
    },
    "description": "tsagfciovl.tsc - flash cache logger IOV tests\n\nCalled by oss_safciovl to test using flashcache as a logger for\n     temporarily failed devices.",
    "platform": null
  },
  {
    "test_name": "tsagfclarge1.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagfclarge1.tsc - Test to check large flashcache with ASM rebalance\n\n1.) set up normal redundancy ASM diskgroup\n    2.) trigger a rebalance by dropping disk from the asm side\n    3.) check that ASM writes are being cached in FC (NRW size should be non-zero).",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfclarge1_hwd.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "cell": "scas15celadm14",
      "MACH_NAME": "^cell^",
      "MACH_PASSWD": "welcome1",
      "max_areas": "2  # max number of areas on each device",
      "duration": "600  # run IOV for 15 Mins",
      "blk_szlst": "'256K,512K,1024K'",
      "io_szlst": "'256K,512K,1024K'       # to set the num of blocks",
      "threads": "3",
      "offset": "8  # start offset to be 8M aligned",
      "hardftyp_oss_iorm": "2",
      "area_sz": "6"
    },
    "description": "tsagfclarge1_hwd.tsc - Test for flash wear detection\n\nLarge fc write check test for lrgrhx5safclarge1\n\nhigh capacity X5 machine has been used for testing",
    "platform": null
  },
  {
    "test_name": "tsagfclarge2.tsc",
    "setup": "tsaginit",
    "flags": {
      "max_areas": "2  # max number of areas on each device",
      "gd": "'datafile3'    # sparse griddisks",
      "duration": "300  # run IOV for 5 Mins",
      "blk_szlst": "'256K,512K,1024K'",
      "io_szlst": "'256K,512K,1024K'       # to set the num of blocks",
      "threads": "3",
      "offset": "8  # start offset to be 8M aligned",
      "hardftyp_oss_iorm": "4",
      "area_sz": "6",
      "creatdev_file": "tsagrddef",
      "cfgdir": "tsagfciov.sav"
    },
    "description": "tsagfclarge2.tsc - Test to check large flash cache write with IOV workload\n\n1) set _cell_iorm_lw_util_threshold=0 to disable the IORM disk busy heuristic on\n    fake hardware (so all LWs should be eligible for caching).\n  2) run IOV to generate cache-eligible large writes that will go into the \"NRW\" list.\n  3) dump some stats about the large writes.\n  4) check that NRW size should be non-zero.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfclarge3.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagfclarge3.tsc - Test for large write FLASHCACHE\n\nWe will be testing following scenarios :\n\n   1- Case 1: LW with 'KEEP' objects.\n   2- case 2: LW with default OLTP small read IOV workload(IOV workload size is around the FC size).\n   3- case 3: LW with default OLTP small write IOV workload(IOV workload size is around the FC size)\n   4- case 4: LW with default OLTP small read IOV workload(IOV workload size is 80% of the FC size)\n   5- case 5: LW with default OLTP small read IOV workload(IOV workload size is 120% of the FC size)",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfclarge4.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagfclarge4.tsc - Flash Cache LW test with toggled limits\n\nThis test toggles FC Group limits between HardMax and SoftMax\n   while workload is running.\n   The expectation is that the toggling should be safe and not cause\n   any side effects.\n   Intended to catch bug 34765796 where toggling was causing\n   global limit to conitunally deplete until LW were no longer\n   able to be cached\n\ntest steps are below",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfclarge_bugs.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagfclarge_bugs.tsc - adds couple of bug tests\n\nBUG 35770007 - PREVENT LARGE WRITE DATA FROM GETTING TRAPPED IN THE FLASH CACHE\n    BUG 35769892 - ADD A PARAMETER TO CONTROL FLASH CACHING BEHAVIORS BASED ON CLIENT I/O HINT\n\nto be added in lrgdbconsafclarge1\nmore details in :\nhttps://confluence.oraclecorp.com/confluence/display/~rajeev.k.jain@oracle.com/BUGS+FOR+FC+LARGE+WRITE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfclargeiorm.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "cellconnstr": "root@^cell_name^"
    },
    "description": "tsagfclargeiorm.tsc - test for large fc write for IORM\n\nThis test will verify 2 different testcase:\n    1- Testing IORM heuristic when hard  disks are busy\n    2- Test that LW does not cache by default on X2",
    "platform": null
  },
  {
    "test_name": "tsagfclogdbini.tsc",
    "setup": null,
    "flags": {
      "rawdsk": "bigdatafile5",
      "celdsk": "c9bigdatafile5",
      "grddsk": "exdisk1",
      "rawdsk2": "bigdatafile6",
      "celdsk2": "c9bigdatafile6",
      "grddsk2": "bigdatafile6",
      "file_dest": "'+bigdatafile'",
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'",
      "lunsize": "2621500",
      "tblspacesize": "1600"
    },
    "description": "tsagfclogdbini.tsc - generate initialization/helper scripts\n                          for tsagcdbfclogger.tsc\n\ngenerate initialization/helper scripts for tsagcdbfclogger.tsc\n\ngenerate initialization/helper scripts for tsagcdbfclogger.tsc",
    "platform": null
  },
  {
    "test_name": "tsagfclogini.tsc",
    "setup": null,
    "flags": {
      "errFrequency": "1                                         # make every IO hang",
      "errCount": "0 # don't specify count, let errors happen till hang is recognized",
      "simevent": "BLOCKIO_ALL_HANG"
    },
    "description": "tsagfclogini.tsc - create scripts needed by fc logger tests\n\nScripts used by tsagfciovl.tsc, tsagffilp.tsc",
    "platform": null
  },
  {
    "test_name": "tsagfcmdsp.tsc",
    "setup": null,
    "flags": {
      "asm_ausize": "1048576",
      "errbasename": "^tst_tscname^tmp.log"
    },
    "description": "tsagflashmdsp.tsc - flashcache metadata shadow paging\n\nRun a workload in the background while restarting cellsvr after setting\n     these error simulation events:\n     - FLASHCACHE_PRIM_MD_CKSM_ERR\n       Simulates on-flash corruption to the primary metadata page.\n     - FLASHCACHE_SHAD_MD_CKSM_ERR\n       Simulates on-flash corruption for the shadow page.\n     - FLASHCACHE_DUAL_MD_CKSM_ERR\n       Simulates on-flash corruption for both pages.\n     - FLASHCACHE_MD_VERS_MISM_ERR\n       Simulates the error case when the primary has an older version.\n\n     Also test with shadow metadata disabled, with events\n     - FLASHCACHE_MD_VERS_MISM_ERR\n     - FLASHCACHE_PRIM_MD_CKSM_ERR",
    "platform": null
  },
  {
    "test_name": "tsagfcmdsync.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0"
    },
    "description": "tsagfcmdsync.tsc - functional test for FC MD sync during clean shutdown\n\nfunctional test for FC MD sync during clean shutdown\n\nfunctional test for FC MD sync during clean shutdown\n\n     (1) Create 1GB WriteBack FlashCache with 2 flash stores.\n     (2) Set cell parameter _cell_fc_toresilver_limit_mb to 1.\n         This parameter limits the size of dirty cached data to resilver\n         in flashcache to 1MB, and forces datasync to happen in background.\n         The parameter can be dynamically set via:\n         alter cell events = \"immediate cellsrv.cellsrv_setparam(' _cell_fc_toresilver_limit_mb', 1)\";\n     (3) Create 2 GridDisk with total size 800MB or greater.\n         Wait till grid disks are cached by FlashCache.\n     (4) Run orion 100% write workload on the entire grid disks for 2 mins\n         to generate all the dirty cachelines.\n         orion -run advanced -type rand -write 100 -testname orion \\\n           -size_small 1024 -num_small 16 -size_large 64 -num_large 0 \\\n           -matrix point -is_advm -duration 120\n     (5) Get a FC stat dump for reference.\n         alter cell events=\"immediate cellsrv.cellsrv_flashcache(dumpstats, 0, 0, 1)\";\n     (6) Get FC_BY_DIRTY metric . It should be non-zero.\n         list metriccurrent attributes name, metricObjectName, metricValue where name like \"FC_BY_DIRTY\"\n     (7) Sleep 120 seconds to allow background to do more datasync.\n         For determinstic test result, check cellsrvstat periodically\n         until timeout or \"FlashCache synced dirty data (KB)\" no longer\n         changes.\n     (8) Redirect the cellsrvstat output to a file for reference and check.\n         The expected behavior is that there should be non-zero synced\n         dirty data in FlashCache. We are interested in the following stats.\n         Number of disk writer datasync writes\n         Total size for disk writer datasync writes (KB)\n         Cachesize(KB)\n         FlashCache synced dirty data (KB)\n         FlashCache unsynced dirty data (KB)\n     (9) Restart cellsrv.\n         alter cell restart services cellsrv;\n     (10) Get a FC stat dump for reference.\n     (11) Get FC_BY_DIRTY for reference. The expected behavior is that\n          MD sync during clean shutdown should have converted the synced\n          dirty cachelines to clean cachelines, so the real dirty data\n          is <= 1MB after cellsrv restart.\n     (12) Save cellsrvstat result for reference. Check cellsrvstat.\n          The expected behavior is that there is no synced\n          dirty data and the unsycned dirty data is <= 1MB.\n          Cachesize(KB)                         --> non-zero, close to 800MB.\n          FlashCache synced dirty data (KB)     --> should be 0\n          FlashCache unsynced dirty data (KB)   --> should be <= 1MB",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcmode.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_no_asmdb": "true",
      "errbasename": "^tst_tscname^o.log"
    },
    "description": "tsagfcmode.tsc - test flashcachemode change\n\nTest flashcachemode change in misc scenarios, with or without\n     cellsrv running.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcomp_dev.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "sgrcel2",
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagfcomp_dev.tsc - Wrapper for fcomptests.sh\n\nThis is a wrapper for fcomptests.sh to execute device level tests for\n     flash compression.",
    "platform": null
  },
  {
    "test_name": "tsagfcompms.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "sgrcel2",
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagfcompms.tsc - Dynamic compression Tests for MS",
    "platform": null
  },
  {
    "test_name": "tsagfcoppo.tsc",
    "setup": "tsaginit",
    "flags": {
      "cell_with_xrmem_cache": "0",
      "CELL_WITH_FLASH_CACHE": "0",
      "creatdev_file": "tsagrdmadef"
    },
    "description": "tsagfcoppo.tsc - opportunistic caching test\n\nhttps://confluence.oraclecorp.com/confluence/display/EXD/Opportunistic+Flash+Caching",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcorion.tsc",
    "setup": null,
    "flags": {
      "nlarge": "10",
      "runtype": "seq",
      "ndisks": "1",
      "nsmall": "0",
      "size_large": "64",
      "size_small": "8",
      "dur": "90",
      "write": "100",
      "thold": "30.00",
      "cache_size": "0",
      "data_input": "'-datainput '^data_input^",
      "huge_no_need": "'-hugenotneeded'",
      "orion_testname": "tsag_orion_^orion_prefix^",
      "data_input_tmp": "^data_input^"
    },
    "description": "tsagfcorion.tsc - Use Orion to write IO into flashcache\n\nUse Orion to write IO into flashcache",
    "platform": null
  },
  {
    "test_name": "tsagfcr.tsc",
    "setup": null,
    "flags": {
      "locally_managed": "true",
      "locally_managed_db": "true",
      "auto_undo_management": "true",
      "creatdev_file": "tkfgrddef",
      "dnmusers": "6",
      "bnmusers": "6",
      "asm_ausize": "65536",
      "compatible": "^def_compatibility^",
      "sysstr": "'sys/knl_test7 as sysdba",
      "dgstmt": "'alter diskgroup bambam offline disks in failgroup F1;'"
    },
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagfcre.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "asm_ausize": "4194304",
      "creatdev_file": "tsagfcrez"
    },
    "description": "tsagfcre.tsc - File creation optimization on SAGE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcremap.tsc",
    "setup": "tsaginit",
    "flags": {
      "num_flash_per_cell": "2",
      "oss_enable_fc_persistence": "writeback",
      "flash_size": "256M",
      "nflint": "1",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagfcremap.tsc - Reduction in FC remapping\n\nTests the reduction in the FC remap time for certain scenarios\n        (flash failure and cancel flush)\n       The remapping now happens within 1 sec, but for fake hw tests\n       we relax the check and verify that it happens within 10 sec.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcreplace.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_testing": "1",
      "flash_size": "576",
      "num_flash_per_cell": "1",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0"
    },
    "description": "tsagfcreplace.tsc - Test case for cachelines replacement of flashcache\n\nTest case for cachelines replacement of flashcache\n\nPayPal ran into a problem where they were not able to pre-warm their IOT\n     tables into flash cache via KEEP (due to some bug possibly in the PQ layer).\n\n     Also cover the IORM bug:\n  Bug 33403239 - ENABLE PDB LEVEL SETTINGS WITHOUT THE CDB PLAN FOR CDBS WITH SINGLE PDB\n\n     Below is the PayPal problem:\n\n       1). They have 5 DBs sharing the Exadata cells.  None of them have any flash\n           cache min/limit/size set initially.The first 4 DBs have occupied the flash\n           cache completely.  The 5th DB is a newly migrated DB, i.e. a new comer\n           trying to get its blocks cached into flash cache.\n       2). They can set the FlashCacheMin for the 5th DB (the newly migrated DB) to\n            some reasonable value (say 3TB per cell), so that we can set aside\n            at least 3TB per cell for caching this new DB.\n       3). They can then do the pre-warm full table scans of key objects in the\n           5th table.  Even though KEEP is not set, because the FC size for\n           the 5th DB is not yet at the FlashCacheMin, all of the scans should\n           get into flash cache as long as we can free up enough cachelines in\n           flash cache to keep up. We may require repeated scans to get a table\n           fully scanned, as flash cache may need some time to  find all the\n           replacement cachelines for the new DB as it's already full.\n\n     Test steps for fake dbs:\n\n     (1) Let's create 2 data grid disks. Datafile0 is 512MB, which will be used\n         for Database TEST5. Dadatafile1 is 512MB, which will be used for Database\n          TEST6. Create FC 512 MB.\n     (2) Similar to the current test, we first use orion to populate FC using TEST5.\n         Let use the entire 512MB grid disk region for orion. We expect that FC is\n          fully filled with TEST5 data. This is to simulate the scenario in\n          customer's env where FC got full with existing databases data.\n     (3) Set flashCacheMin limit for TEST6, say 200MB.\n     (4) We need to introduce concurrent orion workloads on both TEST5 and TEST6\n         to simulate the workload in customer env, i.e. both databases are\n         active and performance critical.\n         * This time, TEST5 need to have a smaller working set, say 200MB griddisk\n            area. So that we can simulate  the scenario where some cache data from\n            TEST5 can become cold, while some cached data are still hot.\n         * TEST6 needs to have a working set larger than its flashCacheMin,\n           say 500MB griddisk area.\n     (5) After some time, we're expecting that cachedSize for TEST6 should be\n         more than its flashCacheMin 200MB.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcrsz.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagfcrsz.tsc - test create/resize/drop of flashcache",
    "platform": null
  },
  {
    "test_name": "tsagfcrszcalc.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagfcrszcalc.tsc - Helper script for tsagfcrsz calculation\n\nHelper script for calculation in tsagfcrsz.tsc",
    "platform": null
  },
  {
    "test_name": "tsagfcsecondarypop.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagfcsecondarypop.tsc - TEST FOR flash cache secondary population\n                              deduplication\n\nPlease see below\n\nadded in lrgsafcpopiov_2tierfc and lrgdbconsafcpopiov",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcset.tsc",
    "setup": "tsaginit",
    "flags": {
      "tmp_port": "^oss_port^"
    },
    "description": "tsagfcset.tsc - Flashcache setup file\n\nThis sets up two instances of cellsrv managing a different set of disks",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcset_debugcli.tsc",
    "setup": "tsaginit",
    "flags": {
      "tmp_port": "^oss_port^"
    },
    "description": "tsagfcset_debugcli.tsc - Flashcache setup file using Debugcli\n\nThis sets up two instances of cellsrv managing a different set of disks",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcspaura2.tsc",
    "setup": "tsaginit",
    "flags": {
      "logmsg": "'Add AURA2 card then disable AURA2 shadow paging'"
    },
    "description": "tsagfcspaura2.tsc - enable/disable flashcache shadow paging\n\nTest different combinations of shadow paging for AURA1 and AURA2",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcspauraimpl.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagdebugclidef3"
    },
    "description": "tsagfcspauraimpl.tsc - This script is called by tsagfcspaura2.tsc\n\nAdd AURA card and enable/disable shadow paging.",
    "platform": null
  },
  {
    "test_name": "tsagfcstair.tsc",
    "setup": "tsaginit",
    "flags": {
      "cell_with_pmem_cache": "1",
      "cell_with_xrmem_cache": "0",
      "asm_disk_def": "tsagrdmadef"
    },
    "description": "tsagfcstair.tsc - Flashcache/PMEMCache stair down/up tests\n\nFlashcache/PMEMCache stair down/up tests\n\nFlashcache/PMEMCache stair down/up tests\n     below is test steps:\n\n       1. create 2 grid disks datafile0 and datafile1 (512MB each).\n       2. create flashcache 600MB\n       3. create iorm fake_db test5 by using the following cellcli command.\n          Cellcli> alter cell events = \"cellsrv.cellsrv_iorm_ctl[FAKE_DBS] opval=6\"\n       4. set iorm plan for fake DB test5 to flashcache=off\n          Cellcli> alter iormplan dbplan=((name=test5, flashcache=off, share=1))\n       5. create a orion.lun for orion test.\n          o/xxx/datafile0\n          o/xxx/datafile1\n       6. start orion all-read workload (duration 15 minutes) on DB test5.\n          $ADE_VIEW_ROOT/rdbms/bin/orion -run advanced -type rand -matrix point\n                 -testname orion -size_small 8 -num_large 0 -num_small 8 -write 0\n       \t\t  -duration 900 -dbid 5\n\n       7. run the following stair up test in a loop.\n         fclimit=0\n         for i in 1..5 loop\n           fcLimit += 100M\n           alter iormplan dbplan=((name=test5, flashcache=on, flashcachesize=<fcLimit>))\n           sleep 2 minutes\n           # get DB_FC_BY_ALLOCATED for DB test5\n           list metriccurrent attributes metricValue where name='DB_FC_BY_ALLOCATED'\n       \t     and metricObjectName='TEST5';\n            # perform check -- the metricValue should match fcLimit\n           <check>\n         end loop\n       8. run stair down test in a loop. Similar to step 7, but with fcLimit\n          decremented back to 0 gradually.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfcstat.tsc",
    "setup": null,
    "flags": {
      "errbasename": "^outbasename^"
    },
    "description": "tsagfcstat.tsc - Basic functional test of FC metric/stats",
    "platform": null
  },
  {
    "test_name": "tsagfcstatcommon.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagfcstatcommon.tsc - Common Celldisk fcstat exports\n\nfcstats module for collecting IO statistics\n\n     exports:\n     fcstat_start.sh    - Start collecting fcstats\n     fcstat_collect.sh  - Stop collecting fcstats and validate\n     FCS_*              - macros to help grep for specific stats\n\nGeneral Usage:\n     include tsagfcstatcommon.tsc\n     fcstat_start.sh ^tst_tscname^\n       run IO workload (cellsrv should not be rebooted)\n     fcstat_collect.sh ^tst_tscname^ ^FCS_BGPOP_WRITE^'|'^FCS_POP_MISS^",
    "platform": null
  },
  {
    "test_name": "tsagfcstatioreasons.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagfcstatioreasons.tsc - test to verify ioreasons in fcstats\n\nTest to verify data in cellsrvstat -stat=cd_fc_stats\n     The test sums the number of ios and bytes from ioreason\n     and stats portion of exported json data and verifies that\n     both of them are equal",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagfcunpinonerr.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "conn1": "'sys/knl_test7 as sysasm'",
      "creatdev_file": "tsagcaudef",
      "compatible": "11.2.0.0",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback"
    },
    "description": "tsagfcunpinonerr.tsc - FC_UNPIN_ON_ERR test\n\nTest that FC layer shall not unpin with FC_UNPIN_ONER\nFC keepSize doesn't decrease even with FC io buffer allocation failures.\nuse PRED_DA_NULL_BUF simulation with scan in loop",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfdflush.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "lstfile": "^tst_tscname^.lst"
    },
    "description": "tsagfdflush.tsc\n\nIf an harddisk or a flash physicaldisk supporting WriteBack FlashCache is\ndropped for replacement with maintain redundancy option, then turn on the\nDoNotService LED till the flash cache is flushed. Once the flash cache is\nflushed (for flash disks) and the system job of monitoring the DO-NOT-SERVICE LED\n      wakes up and determines the node is safe to shutdown then turn off the DO-NOT-SERVICE LED.",
    "platform": null
  },
  {
    "test_name": "tsagfdlsscsi.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_no_asmdb": "true",
      "testname": "tsagfdlsscsi",
      "test_part": "2"
    },
    "description": "tsagfdlsscsi.tsc - FD population + HD failure combo\n\nIntroduce an error in lsscsi output by removing 1 flash disk and then\n  introducting failure in a hard disk. Failure should be detected.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfeaturedowngrade.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagfeaturedowngrade.tsc\n\nSW feature downgrade test\n\nThis test verifies 3 cases related to feature downgrade:\n     CASE 1 : Downgrade a feature\n     CASE 2 : Disable a feature\n     CASE 3 : Upgrade feature from DISABLED to ACTIVE with Persistence Impact",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagfeatureupgrade.tsc",
    "setup": "xblockini",
    "flags": {
      "egs_cert_dur_in_mins": "20"
    },
    "description": "tsagfeatureupgrade.tsc - Exascale software feature upgrade test\n\nTest for ExaScale software features with versioning infrastructure\n\nThis test covers the below metioned cases where all services\n     are restarted after making any change in feature.conf\n\n      0a. POSITIVE CASE - Verification of default feature AEP\n      0b. POSITIVE CASE - Verifying feature SP_OP_TRACK\n      1. POSITIVE CASE - Adding a feature\n      CASE - DISABLING USREDS IN 1 CELL AND THEN ADDING FEATURE\n      1a.POSITIVE CASE - Adding a feature having EGS as a participant\n      1b.POSITIVE CASE - Adding a feature having multiple participants having EGS as mandatory one (bug-36339781)\n      2. POSITIVE CASE - Adding faeture in disabled state and then enabling it\n      3. NEGATIVE CASE - Downgrade feature version to immediate lower minor bit\n      4. NEGATIVE CASE - Downgrade feature version to immediate lower major bit\n      5. NEGATIVE CASE - Downgrade feature version to any lower major bit\n      6. POSITIVE CASE - Rolling Upgrade feature version to next sequential minor bit\n      7. POSITIVE CASE - Upgrade feature version to next sequential major bit\n      8. NEGATIVE CASE - Upgrade feature version to non-sequential minor bit\n      9. POSITIVE CASE - Upgrade feature version to non-sequential minor bit\n      10. POSITIVE CASE - Upgrade feature version to non-sequential major bit\n      11. POSITIVE CASE - Upgrade feature version to non-sequential [current_majorbit+1].0\n      12. NEGATIVE CASE - Upgrade feature version to non-sequential [majorbit].[any_minorbit]\n      13. POSITIVE CASE - Upgrade feature version to non-sequential [majorbit].[any_minorbit]\n      14. POSITIVE CASE - Upgrading feature version to current version\n      15. POSITIVE CASE - Disabling a feature manually and then upgrading its version\n      2a. POSITIVE CASE - Adding a new feature in blackout mode which is in disabled state\n      2b. POSITIVE CASE - Adding a new feature in blackout mode which is in enabled state\n      2c. POSITIVE CASE - Adding a new participant in already existing feature via feature.conf\n      16. NEGATIVE CASE - Removing feature.conf completely - Commented out\n      17. POSITIVE CASE - Using empty feature.conf - Commented out",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagfeatureupgrade_non_uniform.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagfeatureupgrade_non_uniform.tsc - Feature creation using non-uniforem feature.conf files\n\nFeature creation using non-uniforem feature.conf files\n\nThis test covers 4 cases listed below:\n\n     1. Feature with different versions but same participants in 3 cells\n     2. Feature with different modes in 3 cells\n     3. Feature with same versions but different participants in 3 cells\n     4. Feature with different feature ids in each cell",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagfeatureupgrade_part_restart.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagfeatureupgrade_part_restart.tsc - Exascale software feature upgrade test\n\nTest for ExaScale software features with versioning infrastructure\n\nThis test covers the the impact of partial restarts\n     of services upon making changes in feature.conf.\n\n     4 types of restarts are tested:\n\n     1. In case of multiple participants in a feature, only one participant\n        is restarted in all 3 cells\n     2. In case of multiple participants in a feature, all participants are\n        restarted in only one cell\n     3. In case of multiple participants in a feature, one type of participant\n        is restarted in only one cell and in other 2 cells some other participant\n        is restarted\n     4. Relevant restarts i.e., only all the participnats are restarted in all cells",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagfeatureupgrade_part_restart_trim.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagfeatureupgrade_part_restart_trim.tsc - Exascale software feature upgrade test\n\nTest for ExaScale software features with versioning infrastructure\n\nThis test is a subpart of test tsagfeatureupgrade_part_restart.tsc\n     because there were around 200 cell switches in the base test. This\n     test also covers the the impact of partial restarts of services\n     upon making changes in feature.conf\n\n     4 types of restarts are tested:\n\n     1. In case of multiple participants in a feature, only one participant\n        is restarted in all 3 cells\n     2. In case of multiple participants in a feature, all participants are\n        restarted in only one cell\n     3. In case of multiple participants in a feature, one type of participant\n        is restarted in only one cell and in other 2 cells some other participant\n        is restarted\n     4. Relevant restarts i.e., only all the participnats are restarted in all cells",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagfencauth.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagfencauth.tsc - Test for ExaScale fencing authorization mechanism\n\nWhen a server receives a fence request, it will proceed only if the peer\n     is authorized. This lrg contains the unit tests for fence request\n     authorization.\n     Test cases:\n       1. Set up the ExaScale cluster.\n       2. Restart ERS to trigger a node level fencing. The fence should\n          complete successfully since it is sent from EGS leader.\n       3. Run edstool list command to trigger a member level fencing. The\n          fence should complete successfully since it is sent from the ESNP\n          who has the same node ID as edstool.\n       4. Use Arbiter to simulate a node level fencing. The fence should\n          fail, because Arbiter's cert doesn't have EGS bit.\n       5. Use Arbiter to simulate a member level fencing for a different node.\n          The fences should fail, because Arbiter's TLS cert has the same\n          node ID as the ESNP, that it is not allowed to fence a different\n          node's member.\n       6. Use Arbiter to simulate a member level fencing for the same node.\n          The fence should complete successfully since it is sent from the\n          Arbiter who has the same node ID as ESNP.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagfetchconfig.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagfetchconfig.tsc - Fetch Configs from Exadata config database\n\nFetch Configs from Exadata config database\n\nWe setup a mysql service on Exadata APP VM:\n       phoenix228912.dev3sub3phx.databasede3phx.oraclevcn.com\n     and create exadata database on it\n     This tsc is going to fetch all configs in exadata.t_config, we can use\n     below system to maintain it in System -> config Menu :\n  URL: http://phoenix228912.dev3sub3phx.databasede3phx.oraclevcn.com/exadata/admin/login.vm\n  User/pwd: root/root\n     You can also use any mysql client to maintain the table: t_config\n         host: phoenix228912.dev3sub3phx.databasede3phx.oraclevcn.com\n         port: 3306\n         db: exadata\n         user : exadata\n         password: exadata\n\n    below is an example to use mysql\n      mysql -u exadata -h phoenix228912.dev3sub3phx.databasede3phx.oraclevcn.com -p\n      mysql> use exadata\n      mysql> select * from t_config;",
    "platform": null
  },
  {
    "test_name": "tsagffilp.tsc",
    "setup": "tsaginit",
    "flags": {
      "errbasename": "^outbasename^",
      "diskstring_dir1": "^sage_diskstring_dir^",
      "dsk0": "FLASH2_0",
      "dsk1": "FLASH2_1"
    },
    "description": "tsagffilp.tsc - FFI db/asm verification loop test\n\nFFI db/asm verification loop test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagffini.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagffini.tsc - common initializations for FFI tests",
    "platform": null
  },
  {
    "test_name": "tsagffiov.tsc",
    "setup": "tsaginit",
    "flags": {
      "tmp_nowarn": "^tst_nowarn^",
      "num_gd": "12",
      "flash_size": "128",
      "configset": "fciov_set4_direct_read",
      "threads": "5",
      "gdsz": "368                                                   # size in Mb",
      "duration": "900  # run IOV for 15 minutes",
      "area_sz": "6",
      "max_areas": "5",
      "sleep_btw_test": "120",
      "blk_szlst": "'512,1K,2K,4K,8K,16K,512K'",
      "io_szlst": "'1K,2K,4K,8K,16K,512K'",
      "mmtd_dsk": "box/mdmgrtestdisk",
      "cfgdir": "tsagfciov.sav"
    },
    "description": "tsagffiov.tsc - Fast file initialization test\n\nFast file initialization test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagffiov2.tsc",
    "setup": null,
    "flags": {
      "startime": "^TST_EXE_RESULT^"
    },
    "description": "tsagffiov2.tsc - Fast file initialization test\n\nFast file initialization test",
    "platform": null
  },
  {
    "test_name": "tsagffiovf.tsc",
    "setup": "tsaginit",
    "flags": {
      "num_gd": "12  # number of griddisks to create and use",
      "flash_size": "128",
      "configset": "fciov_set4_direct_read",
      "threads": "10  # number of threads per each IOV process",
      "gdsz": "368                                                # gd size in Mb",
      "duration": "5400                 # run IOV for 1.5 hours for most test cases",
      "area_sz": "20            # size of each area on gd for an IOV process, in Mb",
      "max_areas": "5                          # max number of areas on each device",
      "pct_read": "40                # percentage of read threads in an IOV process",
      "blk_szlst": "'512,1K,2K,4K,8K,16K,512K'           # possible block size list",
      "offset": "4               # start offset to be 4M aligned for FFI to kick in",
      "cfgdir": "tsagfciov.sav"
    },
    "description": "tsagffiovf.tsc - FFI with IOV test, flashcache failure\n\nFFI with IOV test, flashcache failure",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagffiwkld.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "creatdev_file": "tsagffiwkldd",
      "asm_ausize": "4194304  # au size should be 4M for FFI to kick in",
      "dgattr": "'attribute '''compatible.asm'''='''^scompatible.asm^''','''compatible.rdbms'''='''^scompatible.rdbms^''','''cell.smart_scan_capable'''='''true''",
      "errbasename": "^outbasename^"
    },
    "description": "tsagffiwkld.tsc - Fast File Init test with DB workload\n\nFast File Init tests with DB workload",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfgini.tsc",
    "setup": null,
    "flags": {
      "tmp_port": "5042",
      "tag": "'cellsrv[2-'^oss_testing^'] tsagfgini'",
      "uniq_dsknames": "all",
      "devfile": "^devfilei^",
      "devfilei": "^devfile^"
    },
    "description": "tsagfgini.tsc - start up oss servers for failgroups\n\nCreate datafile disks for each of the oss servers and make them\n     failgroups for the datafile diskgroup.\n     Each oss server will have the same number of datafile disks, which\n     will be of the same size as specified by the disk definition file\n     (creatdev_file).\n     Only the last server created will have all types of disks,\n     including controlfile, logfile, standby.",
    "platform": null
  },
  {
    "test_name": "tsagfgini_tmp.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagfgini.tsc - Modified for remote interop\n\nSets up multiple OSS on remote OSS node",
    "platform": null
  },
  {
    "test_name": "tsagfilt.tsc",
    "setup": "tsagnini",
    "flags": {
      "tsagcln": "3",
      "file_dest": "'+datafile'"
    },
    "description": "tsagfilt.tsc - Filtering & a simple Arbiter test\n\nEnsures Bytes read and filtered & returned makes sense and filter\n     efficiency is within acceptable limits.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfips.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cdb_lv1": "^cdb^",
      "cdb_lv2": "^cdb^",
      "sqlnet_string": "'SQLNET.WALLET_OVERRIDE=TRUE'",
      "upper_val": "2",
      "log_file": "tsagfipsini1_^cc_mode^.log"
    },
    "description": "tsagfips.tsc - FIPS tests\n\nWhen run these tests in CC1 mode, all cell columnar cache stats should not get bumped up\n    When run these tests in CC mode, all cell columnar cache stats should get bumped up",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfirmupdate.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagfirmupdate.tsc - test for system disk firm failure\n\nThe test checks when a disk is replaced, MS triggers the firmware\n   update and if the firmware update fails then MS will drop the lun\n   and reject the disk with firmware update failed. And MS will\n   do the same for the system disk. But if this is the only system\n   disk then MS should not delete the lun (as a final resort to rescue\n    the system).",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflashcachemode.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_no_asmdb": "true",
      "errbasename": "^tst_tscname^o.log"
    },
    "description": "tsagflashcachemode.tsc - test case for new attribute added: flashCacheMode\n\nThis attribute is used to display and set the current value of\n    _cell_fc_persistence_state parameter in cellinit.ora.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflashdg.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflashdg.tsc - test having tablespace on flash disk groups",
    "platform": null
  },
  {
    "test_name": "tsagflashevt.tsc",
    "setup": null,
    "flags": {
      "testdesc": "'Loop to set flashcache events for code coverage and flush'",
      "evt1": "'immediate cellsrv.cellsrv_flashcache(backuplistfile, 0,0,0)'",
      "evt2": "'immediate cellsrv.cellsrv_dump(start_time, 1)'",
      "evt3": "'immediate cellsrv.cellsrv_flashcache(dumplru, 0,0,3)'",
      "evt4": "'immediate cellsrv.cellsrv_flashcache(removeflashid_fc, 123456,0,0)'",
      "evt5": "'immediate cellsrv.cellsrv_flashcache(removegridid_fc, 233432234,0,0)'",
      "evt6": "'immediate cellsrv.cellsrv_flashcache(dumpflashchecks, 0,0,0)'",
      "evt7": "'immediate cellsrv.cellsrv_flashcache(dumpobj, 0,0,225)'",
      "evt8": "'immediate cellsrv.cellsrv_flashcache(dumpgridhdrs, 0,0,225)'",
      "nevt": "8",
      "evt": "evt^i^"
    },
    "description": "tsagflashevt.tsc - Simulate some error while a workload is going on",
    "platform": null
  },
  {
    "test_name": "tsagflashfaillp.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflashfaillp.tsc - flashcache failure loop test",
    "platform": null
  },
  {
    "test_name": "tsagflashio.tsc",
    "setup": null,
    "flags": {
      "db_cache_size": "16",
      "processes": "200                          # need this if using parallel query",
      "asm_ausize": "1048576",
      "enabledegraded": "# tell tsagerrsimoff to reenable any degraded flash disk",
      "errbasename": "^tst_tscname^tmp.log"
    },
    "description": "tsagflashio.tsc -- concurrent I/O's\n\n1. Create 5 tables of 1000 rows each - one on FLASH griddisk,\n     the others on FLINT griddisks. Two FLINT tables are created with\n     cell_flash_cache \"DEFAULT\", one with cell_flash_cache \"KEEP\", one\n     with cell_flash_cache \"NONE\".\n     Make one of the \"DEFAULT\" tables partitioned, with 10 partitions.\n     1a. Change the \"KEEP\" table to \"NONE\" and the \"NONE\" table to \"KEEP\"\n     then continue with next step to test partial write.\n     2. If err_simulation1 is specfied, enable it. Then kick off sqlplus\n     sessions to add ^addrows^ rows to each FLINT table and to update\n     them concurrently. Also repeatedly drop and recreate the partitioned\n     table and corresponding tablespace.\n     3. Query the tables to make sure they are updated correctly.\n     4. If working with a small flashcache (<=32M, or <=64M in case of\n     writeback), add rows to \"KEEP\" table until it takes up more than 70%\n     of flashcache. Then select from other tables to verify that they do\n     not displace \"KEEP\" cache to the point that cachedKeepSize is less\n     than 70% of flashcache.\n     5. Disable err_simulation1 and if err_simulation2 is specfied,\n     enable it. Then kick off sqlplus sessions to delete rows from each\n     FLINT table, leaving only 500 rows, and to query them concurrently.\n     6. Query the tables to make sure they are updated correctly.\n\n     If testing read or write persistence, bounce the cell between steps\n     2 and 3. Also bounce the cell in steps 2 and 5 if disks are mirrored.",
    "platform": null
  },
  {
    "test_name": "tsagflashloop.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_no_asmdb": "true"
    },
    "description": "tsagflashloop.tsc - Flash devices offline-online loop test\n\nTakes a Randome Flash device offline and online\n      Test continues for 3 hours",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflashrestart.tsc",
    "setup": null,
    "flags": {
      "filesize": "^TST_EXE_RESULT^",
      "cacheKeepSizePreShutdown": "^TST_EXE_RESULT^",
      "cacheSizePreShutdown": "^TST_EXE_RESULT^",
      "errFrequency": "1",
      "errCount": "1",
      "evstr": "'['^bootstrap_err^'] count='^errCount^', frequency='^errFrequency^",
      "nextPersistMode": "off",
      "oldsuffix": "^logsuffix^",
      "kill_cellsrv": "^tst_exe_result^",
      "cacheKeepSizePostShutdown": "^TST_EXE_RESULT^",
      "cacheSizePostShutdown": "^TST_EXE_RESULT^",
      "persistoff": "1",
      "nobootstrap": "1",
      "restartcell": "^tst_exe_status^",
      "errmsg": "'cacheSizePostShutdown < 70% of cacheSizePreShutdown'"
    },
    "description": "tsagflashrestart.tsc - restart cell to test flash persistence\n\nThis script is invoked by tsagflashio.tsc to shutdown DB, ASM,\n     cell and restart them. It gets flashcachecontent before and afterwards.\n     If restartMode == killcell, kill cellsrv with -9 then restart it.\n     (This is done only if not running with RS. If running with RS then\n     cellsrv is shutdown by RS.)\n     If restartMode == allservices, shutdown all cell services then\n     restart them. (This is done only if running with RS. If not running\n     with RS then cellsrv is stopped via SIGUSR1 then restarted.)\n     celldowntime can be specified to sleep the desired number of seconds\n     before restarting.",
    "platform": null
  },
  {
    "test_name": "tsagflashrm.tsc",
    "setup": null,
    "flags": {
      "errbasename": "^tst_tscname^tmp.log"
    },
    "description": "tsagflashrm.tsc - Simulate various failure cases\n\ntsagflashrm.tsc - Simulate various failure cases\n     . flash griddisk data corruption\n     . testcase=flashcachelst\n       - loss of flashcache.lst\n       - corruption of flashcache.lst\n       - loss of flashcache.lst and cell_dsk_config.xml*\n     . testcase=flashdisk\n       - temporary loss of FLASH disk\n       - hot replacement of FLASH disk\n         (while cell is up - not possible on real hw)\n       - cold replacement of FLASH disk\n         (while cell is down)\n\n     You can specify what type of failure to test. For example,\n     \"oratst oss_saflashrmw testcase=flashcachelst\"\n     or\n     \"oratst oss_saflashrmr testcase=flashdisk\"",
    "platform": null
  },
  {
    "test_name": "tsagflashrsz.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflashrsz.tsc - resize flash cache\n\nInternally, resize is done by dropping and re-creating flash cache.",
    "platform": null
  },
  {
    "test_name": "tsagflashstat.tsc",
    "setup": null,
    "flags": {
      "cacheobj": "flashcachecontent",
      "pipefile_tmp": "pipefile^cell2shutdown^",
      "tmppfile": "^pipefile^"
    },
    "description": "tsagflashstat.tsc - get flash cache stats",
    "platform": null
  },
  {
    "test_name": "tsagflashwear.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "cell": "scas15celadm14",
      "MACH_NAME": "^cell^",
      "MACH_PASSWD": "welcome1"
    },
    "description": "tsagflashwear.tsc - test for flash wear detection\n\nPls see below\n\nto be aded in lrgrhx5safclarge1",
    "platform": null
  },
  {
    "test_name": "tsagflashwearorion.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflashwearorion.tsc - Real HW test for verifying flash wear checks\n\nTest steps:\n       i.) - Get device name and Vendor id\n      ii.) - Get cell ip\n     iii.) - check resetting of timed_workload_media_wear is needed or not\n                 if yes -- reset value of timed_workload_media_wear (it takes around an hour time)\n                 else no need of reset\n      iv.) - Before running Orion Workload, checking value of timed_workload_media_wear\n       v.) - Copy orion to cell node\n      vi.) - Setup cellip.ora and .luns to run orion\n   In a loop ( count 30 ); do\n        vii.) - Run orion workload for 2 min\n       viii.) - Comparing new value with old value\n                 if newvalue>oldvalue ----> test passes\n                 loop break;\n                 else test fails\n   endloop\n        xi.) - verifying flash wear ticks or not\n\ntest is added in lrgrhx7saipcdat",
    "platform": null
  },
  {
    "test_name": "tsagflcache1.tsc",
    "setup": null,
    "flags": {
      "errbasename": "^outbasename^"
    },
    "description": "tsagflcache1.tsc - Test flashcache object",
    "platform": null
  },
  {
    "test_name": "tsagflcache2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflcache2.tsc - Tests flashcache - alter, drop",
    "platform": null
  },
  {
    "test_name": "tsagflcache3.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflcache3.tsc - Flashcache Tests part 3\n\nThis tests the create/alter/drop all \"FLASHDISK\" and some miscellaneous tets",
    "platform": null
  },
  {
    "test_name": "tsagflcachepopiov.tsc",
    "setup": "tsaginit",
    "flags": {
      "max_areas": "5  # max number of areas on each device",
      "gd": "'datafile0,datafile1'    # datafile griddisks",
      "duration": "600  # run IOV for 10mins",
      "blk_szlst": "'32K'",
      "io_szlst": "'512,1K, 1.5K, 2K, 2.5K, 3K, 3.5K, 4K, 8K, 16K,32K,64K,128K,256K, 1M'       # to set the num of blocks",
      "threads": "10",
      "flcache": "true",
      "align": "y",
      "offset": "4  # start offset to be 4M aligned",
      "area_sz": "4",
      "creatdev_file": "tsagddef"
    },
    "description": "tsagflcachepopiov.tsc - This iov test is to check for FLASHCACHE POP tag.\n\nThis is iov test for flashcache pop and it will check and run iov test for FC POP values.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflcell.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflcell.tsc - create cell changes for flashcache",
    "platform": null
  },
  {
    "test_name": "tsagflcli1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflcli1.tsc - exadata smart flashlogs",
    "platform": null
  },
  {
    "test_name": "tsagflcli2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflcli2.tsc - exadata smart flashlogs",
    "platform": null
  },
  {
    "test_name": "tsagflcli3.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflcli3.tsc - Exadata Smart Flash Log tests\n\nExadata Smart Flash Log tests",
    "platform": null
  },
  {
    "test_name": "tsagflcltime.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_no_asmdb": "true"
    },
    "description": "tsagflcltime.tsc - Alter cell time by 6 months and check flash devices",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflerrinj.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_no_asmdb": "true",
      "suf": "with_ms_restart",
      "testname": "tsagflerrinj",
      "test_part": "2"
    },
    "description": "tsagflerrinj.tsc - Flashcache Error Injection Scripts",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflfail.tsc",
    "setup": "tsaginit",
    "flags": {
      "disable_multims": "true",
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagflfail.tsc - Flash disk failure - Resilvering Test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflfail1.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagflfail1.tsc - DB workload with Flash poor performance\n\nFlash failure - Poor performance with DB workload running",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflfailall.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_no_asmdb": "true"
    },
    "description": "tsagflfailall.tsc - Fails all flashdisks using SLOW_DISK_INJ event\n\nspecify evargs3 to be 7, the simulation event will always delete the underlying device.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfliov1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagfliov1.tsc - FlashLog IOV test 1\n\nFlashlog IOV test 1. The setup for this test is present in tsagiov.tsc",
    "platform": null
  },
  {
    "test_name": "tsagfliov2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagfliov2.tsc - Flashlog IOV test 2\n\nFlashlog IOV test 2\n\nThis is called from tsagiov.tsc",
    "platform": null
  },
  {
    "test_name": "tsagflmcell.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflmcell.tsc - Flashcache with multiple flash devices",
    "platform": null
  },
  {
    "test_name": "tsagflmis.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagflmis.tsc - Flash disk push/pull",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflog2.tsc",
    "setup": "tsagnini",
    "flags": {
      "celltrcdir": "^ADR_BASE^^s^diag^s^asm^s^cell^s^^HOSTNAME^^s^trace",
      "asm_allow_sysdba": "true",
      "asm_ausize": "1048576",
      "condba": "'sys/knl_test7 as sysdba'",
      "conasm": "'sys/knl_test7 as sysasm'",
      "processes": "250"
    },
    "description": "tsagflog2.tsc - Exadata flashlog tests\n\nExadata smart flashlog tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflog3.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagfl3def",
      "asm_ausize": "1048576"
    },
    "description": "tsagflog3.tsc - exadata smart flashlog tests\n\nexadata smart flashlog tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflog4.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflog4.tsc - Exadata Smart Flash log tests\n\nExadata Smart Flash Log tests",
    "platform": null
  },
  {
    "test_name": "tsagflog_cc.tsc",
    "setup": "tsagnini",
    "flags": {
      "asm_allow_sysdba": "true",
      "conasm": "'sys/knl_test7 as sysasm'",
      "nflint": "1",
      "condba": "'sys/knl_test7 as sysdba'",
      "compatible": "^max_compatibility^",
      "processes": "250"
    },
    "description": "tsagflog_cc.tsc - Flash Log Code Coverage tests\n\nFlash Log Code Coverage tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflogcli1.tsc",
    "setup": "tsagnini",
    "flags": {
      "asm_ausize": "1048576",
      "creatdev_file": "tsagr3def"
    },
    "description": "tsagflogcli1.tsc - CellCLI commands for Exadata Smart Flash Logs\n\nCellCLI commands for Exadata Smart Flash Logs",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflogsr1.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagr3def",
      "asm_ausize": "1048576",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagflogsr1.tsc - Exadata Smart Flash log saved redo tests\n\nExadata Smart Flash log saved redo tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflogsr2.tsc",
    "setup": null,
    "flags": {
      "genlogfile": "tsagflogflsh_excld"
    },
    "description": "tsagflogsr2.tsc - Exadata Smart flash logging Saved redo tests\n\nExadata Smart flash logging Saved redo tests",
    "platform": null
  },
  {
    "test_name": "tsagflppf.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_ausize": "1048576",
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagflppf.tsc - Flash disk poor performance",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflsg_inq.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagflsg_inq.tsc - SImulate sg_inq hang",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflsim1.tsc",
    "setup": "tsagnini",
    "flags": {
      "skip_chkms": "1",
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagflsim1.tsc - SAGE Fault Simulation\n\nSimulates fault at various levels while doing predicate push and incremental backup\n\nThe test cases follow the foll steps:\n     - set event\n     - either do a predicte query or incremental backup\n     - unset event",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflsim2.tsc",
    "setup": "tsaginit",
    "flags": {
      "celldsk1": "'c9datafile0'",
      "celldsk2": "'c9datafile1'"
    },
    "description": "tsagflsim2.tsc - SAGE Fault Simulation\n\nSimulates corruption of metadata and crash of OSS\n\nPART A: Corruption of metadata\n     PART B: Crash of OSS\n     Does not need ASM",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflsim3.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagflsim3.tsc - SAGE Fault Simulation test for bug 7639121\n\na)  create a diskgroup in mirrored mode\n     b)  enable PRED_CRASH_IN_PREDCACHEGET simulation event\n     c) Run a predicate push query on a somewhat of a large table\n        Say about 2000 rows where each row is about 8k blocks.\n     d) The cellsrv will crash without any incidents. RS will\n       restart the cellsrv\n     e) Ensure query execution completes",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflsim4.tsc",
    "setup": "tsagnini",
    "flags": {
      "skip_chkms": "1",
      "creatdev_file": "tsagccdef",
      "compatible": "11.2.0.0",
      "argone": "'&1'"
    },
    "description": "tsagflsim4.tsc - SAGE Fault Simulation\n\nSimulates fault at various levels while doing predicate push\n     Has two test cases:\n     Test case # 1: Simulates fault using cell side event \"PRED_DISK_GROW_SLOTS\"\n     Test case # 2 Simulate large payload in kcfis\n     Test case # 3: Simulates fault using cell side event \"PRED_BUFFER_EVICTION\"\n     for HCC and non-HCC tables",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflsim5.tsc",
    "setup": "tsagnini",
    "flags": {
      "skip_chkms": "1",
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagflsim5.tsc - SAGE Fault Simulation\n\nCorrupts mirrors (primary and secondary) and does incremental backup/recover\n\nThe test cases follow the foll steps:\n     - set event 15207 for corrupting primary and secondary mirrors (set on ASM instance)\n     - incremental backup\n     - open db and check for results",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflsim6.tsc",
    "setup": "tsagnini",
    "flags": {
      "compatible": "^def_compatible^",
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagflsim6.tsc - SAGE Fault Simulation test for bug 11701685\n\na)  create a diskgroup in mirrored mode\n     b)  enable PRED_FP_CORRUPT_FLASHIO simulation event\n     c) Run a predicate push query\n     d) Check for \"num_corrupt_block_requeues\" in v$cell_state",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfltmout.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_no_asmdb": "true",
      "suf": "with_ms_restart",
      "testname": "tsagfltmout",
      "test_part": "2"
    },
    "description": "tsagfltmout.tsc - Flash Disks  Timeout scripts",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagflushcancel.tsc",
    "setup": null,
    "flags": {
      "errbasename": "tsagflushcanceltmp.log"
    },
    "description": "tsagflushcancel.tsc - loop to flush and cancel flush flashcache\n\nCalled by tsagdropflash.tsc to try multiple \"flush nowait\" and\n     \"cancel flush\" while a workload is going on.",
    "platform": null
  },
  {
    "test_name": "tsagflwrkld.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflwrkld.tsc - DB workload for Flash failure test cases\n\nGeneric File - does DB workload on a Normal redundancy diskgroup",
    "platform": null
  },
  {
    "test_name": "tsagflwrkld2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagflwrkld.tsc - DB workload for Flash failure test cases\n\nGeneric File - does DB workload on a Normal redundancy diskgroup",
    "platform": null
  },
  {
    "test_name": "tsagfnddtraces.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagfnddtraces.tsc - fndd traces test\n\npls see below\n\nto be added in lrgsafndd1",
    "platform": null
  },
  {
    "test_name": "tsagfolwregsclsrv.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true",
      "egstrcdir": "^ADR_BASE^^s^diag^s^EXC^s^exc^s^^HOSTNAME^^s^trace^s^",
      "saved_ossconf": "^OSSCONF^"
    },
    "description": "tsagfolwregsclsrv.tsc - EGS follower cellsrv monitor test",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagforcevoldetach.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagforcevoldetach.tsc - Test rmvolumeattachment --force\n\nUsing force option, volume attachment can be removed even if its logged in.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagformatsp.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagformatsp.tsc - Test for format/free Simulation Events",
    "platform": null
  },
  {
    "test_name": "tsagformatsp_n.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagformatsp_n.tsc - Format/Free simulation event Test",
    "platform": null
  },
  {
    "test_name": "tsagfortify.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagfortify.tsc - Automate fortify run and producing result for comparison",
    "platform": null
  },
  {
    "test_name": "tsagfplib_heartbeat.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagfplib_heartbeat.tsc - Functional Test for Enhanced FPLIB Heartbeat\n\nFunctional Test for Enhanced FPLIB Heartbeat",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagfpts1.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "imc_size": "500M",
      "user": "tpch",
      "pwd": "^user^",
      "msg0": "'sageDataFilterBlock.* scan timeout'",
      "msg1": "'sageDataProcessBlock.* sent block on error'",
      "msg2": "'sageDataProcessBlocks.* timeout'",
      "msg3": "'sageDataProcessBlock Exception'",
      "msg4": "'sageDataProcessCU.* scan timeout'",
      "msg5": "'sageDataProcessHCCBlocks.* send block on error'",
      "msg6": "'sageDataProcessBlocks.* simulated timeout'"
    },
    "description": "tsagfpts1.tsc - this is test for fplib timeout feature\n\nthis test use simulation event invoke fplib timeout and check corresponding stat\n\ninclude 3 test for non-ehcc,regexp query, ehcc",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagfullvolbkptest.tsc",
    "setup": null,
    "flags": {
      "setup_blockstore": "true",
      "iscsi": "true",
      "egs_deplmode": "sharedCloud"
    },
    "description": "tsagfullvolbkptest.tsc - tests full backup tree with bsw+bsm restart\n\nThis test exercises full volume backup functionality with bsm+bsw\n     restart in parallel.",
    "platform": null
  },
  {
    "test_name": "tsaggdbclone_test.tsc",
    "setup": null,
    "flags": {
      "TNSPROTOCOL": "v2_tcp",
      "TCPPORT": "1521",
      "sqlnet_string": "'BEQUEATH_DETACH=yes'",
      "conn_str": "sys/knl_test7@inst1"
    },
    "description": "tsaggdbclone_test.tsc - fake hw test for gdbclone\n\nFunctional test for gdbclone on fake hw\n\nThis test runs gdbclone tests on exascale fake hw in different configurations\n     More information about gdbclone can be found at:\n     https://confluence.oraclecorp.com/confluence/display/~rajeev.k.jain@oracle.com/gDBClone+Tests",
    "platform": null
  },
  {
    "test_name": "tsaggddeact.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsaggddeact.tsc - Functional test for griddisk deactivation during patching\n\nFunctional test for griddisk deactivation during patching\n\nTest 1 steps:\n       1.Set ASM power limit to 0\n       2.Fail one disk on cell1 and check asmdeactivationoutcome\n       3.To start deactivation on cell2\n     Test 2 steps:\n       1. Bring back the failed disk on cell 1\n       2. Deactivate griddisk on 2\n       3. Fail one disk on 2\n       4. Restart all cell services\n       5. Reactivate griddisk -\n          griddisk should be all ONLINE except the dropped one",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaggenscripts.tsc",
    "setup": null,
    "flags": null,
    "description": "tsaggenscripts.tsc - generate helper scripts\n\ngenerate helper scripts for volumebackup tests",
    "platform": null
  },
  {
    "test_name": "tsaggenvolbkpscripts.tsc",
    "setup": null,
    "flags": null,
    "description": "tsaggenvolbkpscripts.tsc - generates tests shell scripts\n\nGenerates required shell scripts for volume backup feature testing\n\nGenerates required shell scripts for volume backup feature testing",
    "platform": null
  },
  {
    "test_name": "tsaggrddsk_hw.tsc",
    "setup": null,
    "flags": {
      "celdsk_size": "^disk_size^",
      "free_spce": "^disk_size^   ## safely assuming that size of disk datafile0 and datafile2 are same in tsagddef.",
      "dsk38_size": "^disk_size^"
    },
    "description": "tsaggriddsk.tsc -\n\nIt tests the various operations that can be performed on Griddisk through the SAGE command line interface.",
    "platform": null
  },
  {
    "test_name": "tsaggriddsk.tsc",
    "setup": null,
    "flags": {
      "celdsk_size": "1536 ## disk size sepcified in tsagrddef",
      "free_spce": "^disk_size^   ## safely assuming that size of disk datafile0 and datafile2 are same in tsagddef.",
      "dsk38_size": "^disk_size^"
    },
    "description": "tsaggriddsk.tsc - Griddisk unit test\n\nIt tests the various operations that can be performed on Griddisk through\n     the SAGE command line interface.\n     Note: testing of dropping a griddisk which is in use (is opened by asm/db)\n     is done in tsaggridre.tsc",
    "platform": null
  },
  {
    "test_name": "tsaggriddsk_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsaggriddsk_nls.tsc -",
    "platform": null
  },
  {
    "test_name": "tsaggriddskdrop.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef"
    },
    "description": "tsaggriddskdrop.tsc - test case for rti 28290738\n\ntest case for rti 28290738\n\nthis test has below steps:\n     Step 1: set the simulation event\n     Step 2: create 2 griddisks on c9datafile2\n     Step 3: simulate write IO errors\n     Step 4: drop datafile21 and expect failure\n     Step 5: recreate griddisk datafile21",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaggridre.tsc",
    "setup": "tsagnini",
    "flags": {
      "numdisks": "500",
      "siz": "^TST_EXE_RESULT^",
      "creatdev_file": "tsagrddef",
      "asm_ausize": "1048576",
      "gd_last": "^gd_count^",
      "celltrcdir": "^t_diag^^s^diag^s^asm^s^'cell'^s^^hostname^^s^trace^s^",
      "bfullmsg": "'ossmmap_record_mmap_call_1: alloc map buffer full '"
    },
    "description": "tsaggridre.tsc - Griddisk Reuse Test\n\nCreate 1100 griddisks on a celldisk, create a large table\n     on the griddisks and perform insert, delete.\n     Thereafter the griddisks are dropped and recreated\n     and the before mentioned steps are repeated.\n     Also includes testing of cellcli 'drop/rename griddisk' for a disk\n     which is in use (opened by asm)",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaghangman2.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsaghangman2.tsc - Hangman utility test # 2\n\nRuns hangman utility after simulating a cellsrv hang",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaghard1.tsc",
    "setup": "tsaginit",
    "flags": {
      "tsagcln": "3",
      "file_dest": "'+DATAFILE'",
      "file_dest1": "'+DATAFILE/'",
      "file_dest2": "'+DATAFILE/dbs/mydrconfigfile1.dat'",
      "file_dest3": "'+DATAFILE/dbs/mydrconfigfile2.dat'",
      "db_compatibile": "11.2.0.2.0",
      "scompatible.rdbms": "11.2.0.2.0",
      "reffile": "tsagrman.log"
    },
    "description": "tsaghard1.tsc - Create files needed for HARD kit\n\nExecute HARD kit and creates files needed by kit.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaghdcellini.tsc",
    "setup": null,
    "flags": {
      "cell_with_pmem_cache": "true",
      "tsnum": "0",
      "bds_event_i": "bds_event_^tsnum^",
      "event": "^rtest^",
      "comma1_begin": "'\"'",
      "comma1_end": "'\",'",
      "bds_event_val_i": "bds_event_val_^tsnum^",
      "event_value": "^vtest^",
      "unit_event": "bds_event_^j^",
      "unit_event_val": "bds_event_val_^j^",
      "tmp_port": "^free_port_number^",
      "tag": "'cellsrv[2-'^oss_testing^'] tsaghdcellini'"
    },
    "description": "tsaghdcellini.tsc - Hadoop Cell initilization\n\nStarts multiple exadooprs and exadoopsrv services\n\nTo start multiple exadoopsrv (example 4):\n      oratst tsaghdcellini.tsc oss_testing=4\n      Default is 2\n\n     To set an event in cellinit.ora and celloffloadinit.ora, use oratst parameters\n     event and event_value",
    "platform": null
  },
  {
    "test_name": "tsaghealthfac.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sage_mirror_mode": "normal",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "oss_failgroup": "failalldbdg"
    },
    "description": "tsaghealthfac.tsc - Tests healthfactor of disks\n\nTests health factor for GDs whose caching flash disk is failed and replaced",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagheap1.tsc",
    "setup": "tsaginit",
    "flags": {
      "maxinstances": "1",
      "cluster_database": "true",
      "nflint": "1",
      "processes": "200",
      "tmp_nowarn": "^tst_nowarn^"
    },
    "description": "tsagheap1.tsc - RemoteSendPort Leak Test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagheap1_wkld.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagheap1_wkld.tsc - db workload in tsagheap1.tsc",
    "platform": null
  },
  {
    "test_name": "tsaghighsparsedg.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "SAGE_MIRROR_MODE": "high",
      "sparsedg_redund": "high"
    },
    "description": "tsaghighsparsedg.tsc - sparsedb test - high redundancy\n\nCheck for dbverification and checkfile after doing various opeartion on GD.\n\n   no IO\n   a workload of reads\n   a workload of writes\n   a loop of create/delete a large file\n   a loop of create/delete a hundred small files.",
    "platform": null
  },
  {
    "test_name": "tsaghlthfact_attr.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsaghlthfact_attr.tsc - Test for grid disk's new attribute healthfactor\n                             in cellcli\n\nThe test procedures are documented on the Confluence page -\n     https://confluence.oraclecorp.com/confluence/x/EJ88zwE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaghlthfactor.tsc",
    "setup": "tsaginit",
    "flags": {
      "locally_managed": "true",
      "locally_managed_db": "true",
      "auto_undo_management": "true",
      "noiovrun": "true",
      "resilver_test": "true",
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrh2def"
    },
    "description": "tsaghlthfactor.tsc - Test for Health Factor\n\nTests that health factor of disk changes as per resilvering and flash replacement",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaghostacsctrl.tsc",
    "setup": null,
    "flags": {
      "machconnstr": "root@^MACH_NAME^"
    },
    "description": "tsaghostacsctrl.tsc\n\nTests for the host_access_control utility",
    "platform": null
  },
  {
    "test_name": "tsaghostacsctrl_db.tsc",
    "setup": null,
    "flags": {
      "machconnstr": "root@^MACH_NAME^"
    },
    "description": "tsaghostacsctrl_db.tsc - Tests for the host_access_control utility on compute nodes",
    "platform": null
  },
  {
    "test_name": "tsaghsuit.tsc",
    "setup": "tsagnini",
    "flags": {
      "asm_ausize": "1048576"
    },
    "description": "tsaghsuit.tsc - Specific block corruption test required for HARD code\n                     coverage\n\nDriver for HARD test cases.\n\nDriver for HARD test cases.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaghwnfwprofile.tsc",
    "setup": null,
    "flags": {
      "cellconnstr": "root@^MACH_NAME^",
      "MACH_PASSWD": "welcome1"
    },
    "description": "tsaghwnfwprofile.tsc - Tests for CheckHWnFWProfile\n\nTests for CheckHWnFWProfile",
    "platform": null
  },
  {
    "test_name": "tsaghwnfwprofile_db.tsc",
    "setup": null,
    "flags": {
      "dbconnstr": "root@^MACH_NAME^",
      "MACH_PASSWD": "^MACH_PASSWD^"
    },
    "description": "tsaghwnfwprofile_db.tsc - Exadata FW/HW checker tests on db node",
    "platform": null
  },
  {
    "test_name": "tsaghybrid_addcell.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "vault_db": "DATA",
      "disable_multims": "true",
      "oss_failgroup": "failalldbdg",
      "nflint": "1",
      "oss_auto_manage_disks": "true",
      "oss_exascale_asm_testing": "true",
      "sage_mirror_mode": "high",
      "maxtimeout": "20000"
    },
    "description": "tsaghybrid_addcell.tsc - test for Bug 36394496 - HYBRID: SOME CELLS IN\n     ASM DISKGROUP BELONG TO EXASCALE CLUSTER AND SOME DO NOT -\n#           [REMOTECELLMGR::GETCELLHANDLEFORDISK:BADRES]\n\nstart with 3 cell ASM + EXASCALE hybrid env\n#    start workload on both asm and exascale\n#    add a cell only to ASM and do rebalance\n\nadded in hybrid7 lrg",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsaghybridcellsrvrestart.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "sage_mirror_mode": "high",
      "oss_multims_testing": "true",
      "oss_exascale_asm_testing": "true"
    },
    "description": "tsaghybridcellsrvrestart.tsc - exadata fencing test\n\nHybrid (ASM + Exascale with 3 cell setup) - Concurrent cellsrv restarts\n\nConcurrent cellsrv restarts:\n     kill -9 cellsrv 1 and kill -9 cellsrv 3 (with RS running so that cellsrv can be restarted)",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsaghybriddbcellrestart.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "vault_db": "DATA",
      "oss_auto_manage_disks": "true",
      "oss_exascale_asm_testing": "true",
      "num_cells": "4",
      "maxtimeout": "20000"
    },
    "description": "tsaghybriddbcellrestart.tsc - exadata fencing test\n\nHybrid (ASM + Exascale with 4 cell setup) - Concurrent multiple cell deaths\n\nHybrid (ASM + Exascale with 4 cell setup) - Concurrent multiple cell deaths\n     Shutdown RS on cell 1 and cell 3\n     kill -9 cellsrv on cell 1 and cell 3",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsaghybridrebal_egsrestart.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "oss_auto_manage_disks": "true",
      "oss_exascale_asm_testing": "true",
      "sage_mirror_mode": "high",
      "disable_multims": "true"
    },
    "description": "tsaghybridrebal_egsrestart.tsc -<Double failure test for hybrid setup>\n\nFail/reenable 2 disks in a loop and restart egs to check rebalance.\n\n=== Flow of Test Case for Rebal  ===\n    1. In a loop count of 3\n    2. Get ASM alert log size\n    3. Fail 2 disk and then reboot EGS leader 3 times in interval of 10sec.\n    4. Check EGS leader started successfully and wait for disks to be dropped.\n    5. Make sure rebalance completed for Exascale and ASM.\n    6. Again, get ASM alert log size and reenable the disk.\n    7. Reboot EGS leader 3 times in interval of 10 sec and then check EGS leader\n    started successfully.\n    8. Wait for disk to be added\n    9. Make sure rebalance completed for Exascale and ASM.\n\n\n    === Flow of Test Case for Resilver ===\n    1. In a loop count of 3\n    2. Fail 2 Flash disk and then reboot EGS leader 2 times in interval of 10sec.\n    3. Check EGS leader started successfully and wait for disks to be dropped.\n    4. Restart cellsrv\n    5. Make sure resilver completed for cellsrv\n    6. Reenable the 2 failed flash disk.\n    7. Reboot EGS leader 2 times in interval of 10 sec and then check EGS leader\n    started successfully.\n    8. Wait for disks to be added",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsaghybridspreconfig.tsc",
    "setup": null,
    "flags": {
      "oss_failgroup": "failalldbdg",
      "oss_auto_manage_disks": "true",
      "oss_exascale_asm_testing": "true",
      "sage_mirror_mode": "high"
    },
    "description": "tsaghybridspreconfig.tsc - Hybrid setup disk failures\n\nTest to check data rebalance in hybrid setup with workload running\n\n1. Setup xrdbmsini with oss_exascale_asm_testing true,\n         sage_mirror_mode high and oss_auto_manage_disks true.\n     2. Start DB workload using tsaghybridworkload_util.tsc\n     3. Run normal and predictivefailure and check for\n        data rebalance to happen",
    "platform": null
  },
  {
    "test_name": "tsaghybridworkload_util.tsc",
    "setup": null,
    "flags": null,
    "description": "tsaghybridworkload_util.tsc - Utility file for running background workload\n\nThe file have two modes : 1 and 2\n        1 : Run a read workload on tbl1 until an external file exists\n        2 : Repeatedly add then delete rows from tbl1 until tbl2 has a row with particular value\n     For each mode, there are four types, i.e to start or stop database 1 or 2.\n\nTo run the file, pass the parameters : mode (1 or 2),\n     and type(start_db_1, start_db_2, stop_db_1 or stop_db_2)",
    "platform": null
  },
  {
    "test_name": "tsagibport.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagibport.tsc - Tests Cellcli IBPort object",
    "platform": null
  },
  {
    "test_name": "tsagibvipcdatstress.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagibvipcdatstress.tsc - ibv_stress and ipcdat_stress test\n\nValidates IB verbs based library for IPC, which is currently tested\n     by using a fake implementation of IB verbs. Using fakeibv library,\n     it runs ipcdat_stress (ipcdat tester) and ibv_stress.",
    "platform": null
  },
  {
    "test_name": "tsagibvipcdatstressmsg.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagibvipcdatstressmsg.tsc - ipcdat_stress_messaging test\n\nipcdat_stress_messaging test",
    "platform": null
  },
  {
    "test_name": "tsagifc.tsc",
    "setup": null,
    "flags": {
      "tmp_file": "tktgifco.log",
      "sep_var1": "_",
      "sep_var2": ".",
      "enctbs": "''''",
      "imcfclvwd": "'cellmemory'",
      "message": "'KCFIS reporting corrupt block:'",
      "lvl": "1",
      "imaoflgrp": "''^sysoflgrp^''",
      "skip_trim_payload": "true"
    },
    "description": "tsagifc.tsc - split from tktgifc.tsc, avoid mix vob\n\nrun columnar cache test on tpch table, lrgsaa1cccfcX\n\nAdded bitwvec test case\n     Open issue:\n     Bug 30601245: cause dif in basic lrgdbconsaa1cccfcX, will not run new pay load query before bug fix\n     Bug 30627562: issue in lrgcmainr18100saa1cccfc3oltp, won't enable before bug fix\n     Bug 30663964: highly intermittent wrong result in lrgdbconsaa1cccfc3enc\n     Bug 30105215: wrong result in 12.2.0.1 db interop, not fix yet",
    "platform": null
  },
  {
    "test_name": "tsagiffs.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cell_offload_plan_display": "auto",
      "creatdev_file": "tsagccdef"
    },
    "description": "tsagiffs.tsc - SAGE Index Fast File Scan\n\nTests Index Fast File scan",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagilom_fail_simln.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagilom_fail_simln.tsc - <ILOM FAILURE SIMULATION TEST>\n\nThe test uses the ilomfailure_simulation.sh to simulate timeouts and check for alerts using COMMAND FAILURE SIMLUATION property.\n\nThe test is added to lrgrh2sams and uses only 1 cell and no db node.",
    "platform": null
  },
  {
    "test_name": "tsagilom_reset.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagilom_reset.tsc - Checking for ilomNextResetTime\n\nTest for getting reset value of ilomNextResetTime",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagilomfaultsnolan.tsc",
    "setup": null,
    "flags": {
      "cell": "scas15celadm07"
    },
    "description": "tsagilomfaultsnolan.tsc - real hardware functional test for mining ilom faults without LAN\n\nSteps:\n          1) Login to sunservice account with root access on ilom on X5-2 cell.\n          2) run ilomfaulttest.sh in background\n          3) Logout from sunservice account and login to cell as root user\n          4) disable ilom LAN on cell side\n          5) wait and check MS alerthistory to catch the new alerts mined from ilom\n          6) reenable ilom network",
    "platform": null
  },
  {
    "test_name": "tsagilominterval.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_auto_manage_disks": "true",
      "mstrc": "^celltrcdir^ms-odl.trc"
    },
    "description": "tsagilominterval.tsc - ILOM interval reset test\n\nILOM reset interval test for fake hw",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagilommultifault.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagilommultifault.tsc - Multi failure test for ilom",
    "platform": null
  },
  {
    "test_name": "tsagilompmemfault.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true"
    },
    "description": "tsagilompmemfault.tsc - ilom pmem fault testcases\n\nTestcase 1 - ilom PMEM fault handling test\n    test2- To test dirty shutdown handling on PMEM\n    test3- To test cellutil metadata dump on PMEM\n\nto be added in lrgrhx7sadaxorion",
    "platform": null
  },
  {
    "test_name": "tsagim_exasplice_unit.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagim_exasplice_unit.tsc - oratst description file for testing Exasplice Driver.\n\n- Create T_WORK/exasplice_unit_test directory\n     - Copy over dbserver.patch.zip and exasplice*zip to T_WORK/exasplice_unit_test",
    "platform": null
  },
  {
    "test_name": "tsagim_solaris_setup.tsc",
    "setup": null,
    "flags": {
      "solaris_file_path": "pxeqa_^mach_name^",
      "sk_ip": "152.68.218.67",
      "ai_service_setup_server": "sgimgc01",
      "ai_server_user": "dpant",
      "ai_server_passwd": "welcome1",
      "ai_service": "'ai_exadata_'^solaris_file_path^",
      "tftp_tbz_unzip": "^ai_service^.tftp.tbz"
    },
    "description": "tsagim_solaris_setup.tsc - Exadata Solaris Reimaging Script",
    "platform": null
  },
  {
    "test_name": "tsagimag.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimag.tsc- Exadata Imaging Test I\n\nThe test uses the following environment variable which should be set\n     before running the test standalone.\n     When run as part of lrgsaimage variables are set in the driver.\n     MACH_NAME-The machine name where test has to run\n     MACH_PASSWD-The machine password\n     USER_PASSWD-The password of the user running the test\n\n     Already existing variable which are used are:\n\nFor details on the scripts used in the test see their description.",
    "platform": null
  },
  {
    "test_name": "tsagimageinfowpr.tsc",
    "setup": null,
    "flags": {
      "temp_nowarn": "^tst_nowarn^",
      "USER_TXN_NAME": "na",
      "OSSMAINLABEL": "OSS_MAIN_LINUX.X64",
      "main_label": "main"
    },
    "description": "tsagimageinfowpr.tsc - Wrapper file for tsagrealhwimageinfo.tsc\n\nThe test is called by the lrg to execute tsagrealhwimageinfo.tsc on Friday\nwith Integration job for lrgsacmdata2",
    "platform": null
  },
  {
    "test_name": "tsagimageintegration.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimageintegration.tsc - Exadata script to run reimage/oeda during label build",
    "platform": null
  },
  {
    "test_name": "tsagimagelogger.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimageLogger.tsc - oratst description file for testing imageLogger\n\nCopied imageloggertest.sh from oss/test/tsage/sosd directory to the\n     test system at /opt/oracle.cellos/\n     Then runs the test and captures the output from all log and trace\n     files generated into imageloggertest.log and compares it to the\n     reference log at oss/test/tsage/log/tsagimagelogger.log\n\nMake sure imageLogger is copied onto the test system at /opt/oracle.cellos",
    "platform": null
  },
  {
    "test_name": "tsagimagf.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimagf.tsc - Exadata Imaging Tests",
    "platform": null
  },
  {
    "test_name": "tsagimagp.tsc",
    "setup": null,
    "flags": {
      "sep": "'/'",
      "secureeraser-options": "'--hdd --dryrun quick -t ''''John Smith'''' -w ''''Jane Doe''''",
      "secureeraser-reboot": "false",
      "ilom_name": "^MACH_NAME^'-c'",
      "ilom_user_name": "root",
      "ilom_user_passwd": "welcome1",
      "mach_prereq_status": "^TST_STATUS^",
      "preconf": "not_required",
      "domu_xml_tar": "^domu_xml_tar_name^",
      "ovs": "yes"
    },
    "description": "tsagimagp.tsc - Exadata PXE Imaging",
    "platform": null
  },
  {
    "test_name": "tsagimagp1.tsc",
    "setup": null,
    "flags": {
      "kernelfile": "^kernel_string^",
      "initrdfile": "^initrd_string^",
      "sk_dir": "'pxeqa_'^mach_name_short^",
      "timeout": "3000"
    },
    "description": "tsagimagp1.tsc - PXE Imaging Test",
    "platform": null
  },
  {
    "test_name": "tsagimagp_sparc.tsc",
    "setup": null,
    "flags": {
      "sep": "'-'",
      "sparc_dir": "'pxeqa_'^MACH_NAME^",
      "ilom_name_tmp": "^MACH_NAME^'-c'"
    },
    "description": "tsagimagp_sparc.tsc - Exadata Test script for reimaging LoS nodes",
    "platform": null
  },
  {
    "test_name": "tsagimags.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimags.tsc - Sets up Imaging Environment\n\nRun this script to run imaging on a cell other than dadzaa11s\n     Set the variables MACH_NAME, MACH_PASSWD and USER_PASSWD before\n     running the test.",
    "platform": null
  },
  {
    "test_name": "tsagimagxml.tsc",
    "setup": null,
    "flags": {
      "image_label": "^TST_EXE_RESULT^",
      "reclaimdisk_post_reimage": "false",
      "ovs": "no"
    },
    "description": "tsagimagxml.tsc - Exadata reimaging using OEDA XML",
    "platform": null
  },
  {
    "test_name": "tsagimapplydbpack.tsc",
    "setup": null,
    "flags": {
      "patch_base_release": "true",
      "pxe_mach_name": "dscbbg02",
      "pxe_mach_passwd": "welcome1",
      "retain_minimal_pack_setup": "false"
    },
    "description": "tsagimapplydbpack.tsc - Runs various options for Exadata Db Minimal pack",
    "platform": null
  },
  {
    "test_name": "tsagimcelld.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimcelld.tsc - Service celld start,stop and restart test case for image and upgrade",
    "platform": null
  },
  {
    "test_name": "tsagimcellmonpriv.tsc",
    "setup": null,
    "flags": {
      "product_name": "^TST_EXE_RESULT^",
      "interconnect": "'interconnect1=bondib0'"
    },
    "description": "tsagimcellmonpriv.tsc - Test to Verify cellmonitor user privileges",
    "platform": null
  },
  {
    "test_name": "tsagimcellsec.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimcellsec.tsc - Exadata cell security test\n\nAdded with the introduction of new user cellofl",
    "platform": null
  },
  {
    "test_name": "tsagimcheckdev.tsc",
    "setup": null,
    "flags": {
      "log_suffix": "'validate_config_'"
    },
    "description": "tsagimcheckdev.tsc - Exadata cell checkdev thread tests\n\nThis test can be run on any cell and can be used to run one of the following scenarios\n   a) Failure tests on system disks [ failure_option -sysdisk ]\n   b) Failure tests on usb [ failure_option -usb ]\n   c) Failure tests on md devices [ failure_option -md ]\n   d) A combination of failures on system disk,usb and md devices. [ failure_option -all ]\n   e) Failure scenarios [ failure_option -fail ]\n\n   Syntax:\n   oratst -d tsagimcheckdev.tsc mach_name=<cell name> mach_passwd=<cell root password>\n   failure_option=<-usb|-sysdisk|-md|-all|-fail> [install_rpm=true]\n\n   Using install_rpm=true would install the rpm from the view on the cell before\n   running any tests.",
    "platform": null
  },
  {
    "test_name": "tsagimcheckdevr.tsc",
    "setup": null,
    "flags": {
      "sep": "','"
    },
    "description": "tsagimcheckdevr.tsc - Exadata cell checkdev thread test calling script",
    "platform": null
  },
  {
    "test_name": "tsagimcrsh.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimcrsh.tsc - Test case for BUG 12352528\n\nBUG 12352528 - KERNEL CRASHDUMP FILLS UP /VAR/LOG/ORACLE, THEN ROOT FILESYSTEM",
    "platform": null
  },
  {
    "test_name": "tsagimdbminpack.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimdbminpack.tsc - Test to apply Exadata DB Minimal Pack",
    "platform": null
  },
  {
    "test_name": "tsagimdbmonpriv.tsc",
    "setup": null,
    "flags": {
      "product_name": "^TST_EXE_RESULT^",
      "interconnect": "'interconnect1=bondib0'"
    },
    "description": "tsagimdbmonpriv.tsc - Dbmmonitor tests on Exadata Db node",
    "platform": null
  },
  {
    "test_name": "tsagimdo.tsc",
    "setup": null,
    "flags": {
      "tmp_nowarn": "^tst_nowarn^"
    },
    "description": "tsagimdo.tsc - Exadata Imaging Script\n\nEvaluates DO location for farm submissions and integration runs",
    "platform": null
  },
  {
    "test_name": "tsagimdoupgrade.tsc",
    "setup": null,
    "flags": {
      "cell_generic_root_password": "welcome1",
      "reah_hw_schedule_machine": "slcb02db05",
      "retain_minimal_pack_setup": "false",
      "real_hw_compute_node_ade": "true",
      "retain_patchmgr_setup": "false"
    },
    "description": "tsagimdoupgrade.tsc - Helper Script for upgrading db nodes and cells for real hw lrgs",
    "platform": null
  },
  {
    "test_name": "tsagimelasticconfig.tsc",
    "setup": null,
    "flags": {
      "sep": "','"
    },
    "description": "tsagimelasticconfig.tsc - Elastic Config on Exadata",
    "platform": null
  },
  {
    "test_name": "tsagimelasticconfig_run.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimelasticconfig_run.tsc - Helper Script for Elastic Config",
    "platform": null
  },
  {
    "test_name": "tsagimemrd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimemrd.tsc - Verify EM readable files after onecommand",
    "platform": null
  },
  {
    "test_name": "tsagimfork.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimfork.tsc - Test to run Imaging tests on multiple cells\n\ntsagimfork.tsc cells=<comma separated list of machines>\n     dbnodes=<comma separated list of machines>\n     passwd_cell=<comma separated list of passwords for cells>\n     or\n     passwd_db=<comma separated list of passwords for db nodes>\n     test=<test name>\n     The other arguments are similar to the ones passed to the test name passed.\n\n      For using ILOM\n      cells_ilom=<comma separated list of cells> cells_ilom_users=<comma separated list of users for ilom>\n      cells_ilom_passwd=<comma separated list of passwords for ilom>\n\n      dbnodes_ilom=<comma separated list of db nodes> dbnodes_ilom_users=<comma separated list of users for ilom>\n      dbnodes_ilom_passwd=<comma separated list of passwords for ilom>\n\nCurrently this test runs tsagimfwhw.tsc and tsagimswchk.tsc",
    "platform": null
  },
  {
    "test_name": "tsagimfwhw.tsc",
    "setup": null,
    "flags": {
      "cell": "dadzaa09s",
      "torture_testing_iteration": "1",
      "downgrade_flag": "^ihca^^diskc^^biosv^^diskf^",
      "iteration": "7"
    },
    "description": "tsagimfwhw.tsc - CheckHWnFWProfile Utility test\n\nTests CheckHWnFWProfile Utility on Exadata Cells\n\nbiosv-BIOS\n     diskc-Disk Controller\n     ihca-Infiniband HCA\n     flagn-As of now we test three fw downgrade/upgrade- biosv, diskc and ihca\n           We have 7 combinations now and the value of flagn will be the no of\n           random combinations of the above parameters which will be tested\n           If no value is specified then all combinations are tested.\n     This will be extrapolated for further fw options added.\n     torture_testing-When set will repeat the code flow for 'flagn' the specified\n           number of times.It is recommended to run this option on a cell which\n           is exclusive and does not have any important data as it can break the\n           cell.\n     When running standalone\n     tsagimfwhw.tsc mach_name=[cell] mach_passwd=[password] ........",
    "platform": null
  },
  {
    "test_name": "tsagimginfohist.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimginfohist.tsc - imageinfo and imagehistory test\n\nTest the output of imageinfo and imagehistory on cell",
    "platform": null
  },
  {
    "test_name": "tsagimlabelp.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimlabelp.tsc - Exadata Label to Label Upgrade test",
    "platform": null
  },
  {
    "test_name": "tsagimlog.tsc",
    "setup": null,
    "flags": {
      "mode": "pxe_^partition^",
      "mach_model": "^TST_EXE_RESULT^"
    },
    "description": "tsagimlog.tsc - Script to copy logs and compare them for Exadata Imaging Test",
    "platform": null
  },
  {
    "test_name": "tsagimonec.tsc",
    "setup": null,
    "flags": {
      "exit_onecommand": "false",
      "db_user_passwd_string": "^db_user^_passwd_string_db",
      "sep": "','"
    },
    "description": "tsagimonec.tsc - General test script for Exadata Imaging and Onecommand Setup",
    "platform": null
  },
  {
    "test_name": "tsagimonec_common.tsc",
    "setup": null,
    "flags": {
      "tmp_nowarn": "^tst_nowarn^"
    },
    "description": "tsagimonec_common.tsc - Helper Script for onecommand lrgs",
    "platform": null
  },
  {
    "test_name": "tsagimp.tsc",
    "setup": null,
    "flags": {
      "asm_raw_dir": "raw1",
      "test_no": "^arg1^",
      "nflint": "true",
      "creatdev_file": "dsk_def"
    },
    "description": "tsagimp.tsc - Part of Import Export tests\n\nThis test imports celldisks exported by tsagimp and export its own.",
    "platform": null
  },
  {
    "test_name": "tsagimpasswdilom.tsc",
    "setup": null,
    "flags": {
      "ilom_combo": "'root:Ed1s_P0#&G root:welcome1'"
    },
    "description": "tsagimpasswdilom.tsc - Exadata Imaging Helper Script to Extract ILOM Passwords",
    "platform": null
  },
  {
    "test_name": "tsagimpatch_asm_db_setup.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high",
      "creatdev_file": "tsagimdef"
    },
    "description": "tsagimpatch_asm_db_setup.tsc - Asm and DB setup script for Exadata Upgrade Tests",
    "platform": null
  },
  {
    "test_name": "tsagimpatch_common.tsc",
    "setup": null,
    "flags": {
      "tmp_nowarn": "^tst_nowarn^"
    },
    "description": "tsagimpatch_common.tsc - common script for Exadata upgrade",
    "platform": null
  },
  {
    "test_name": "tsagimpatch_setup_ade.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimpatch_setup_ade.tsc - Script to install ADE and run remote workload",
    "platform": null
  },
  {
    "test_name": "tsagimpatchbr.tsc",
    "setup": null,
    "flags": {
      "patch_base_release": "true",
      "pxe_mach_name": "dscbbg02",
      "pxe_mach_passwd": "welcome1",
      "retain_patchmgr_setup": "false",
      "mach_info": "'patch_base_release_flow_info.log'"
    },
    "description": "tsagimpatchbr.tsc - Exadata Patch Tests",
    "platform": null
  },
  {
    "test_name": "tsagimpatchbr_db.tsc",
    "setup": null,
    "flags": {
      "testfile": "tsagimpatchmgr_db.tsc",
      "log_suffix": "rolling_^count^",
      "retain_patchmgr_setup": "true"
    },
    "description": "tsagimpatchbr_db.tsc - Exadata db node upgrade helper script",
    "platform": null
  },
  {
    "test_name": "tsagimpatchbr_db_scale.tsc",
    "setup": null,
    "flags": {
      "log_suffix": "rolling_^count^",
      "retain_patchmgr_setup": "false"
    },
    "description": "tsagimpatchbr_db_scale.tsc - Exadata db node upgrade using update at scale helper script",
    "platform": null
  },
  {
    "test_name": "tsagimpatchbr_ovs.tsc",
    "setup": null,
    "flags": {
      "testfile": "tsagimpatchmgr_ovs.tsc",
      "retain_patchmgr_setup": "true"
    },
    "description": "tsagimpatchbr_ovs.tsc - Helper script for upgrading dom0/domU",
    "platform": null
  },
  {
    "test_name": "tsagimpatchbr_scale.tsc",
    "setup": null,
    "flags": {
      "log_suffix": "rolling_^count^",
      "retain_patchmgr_setup": "false"
    },
    "description": "tsagimpatchbr_scale.tsc - Helper script for Exadata update at scale",
    "platform": null
  },
  {
    "test_name": "tsagimpatchbr_v2.tsc",
    "setup": null,
    "flags": {
      "timer": "0",
      "log_suffix": "rolling_^count^",
      "retain_patchmgr_setup": "false",
      "sep": "','"
    },
    "description": "tsagimpatchbr_v2.tsc - Clone of tsagimpatchbr.tsc for X2-2 cells",
    "platform": null
  },
  {
    "test_name": "tsagimpatchbr_xml.tsc",
    "setup": null,
    "flags": {
      "log_suffix": "rolling_^count^",
      "retain_patchmgr_setup": "true"
    },
    "description": "tsagimpatchbr_xml.tsc - Exadata upgrade script using OEDA XML",
    "platform": null
  },
  {
    "test_name": "tsagimpatchmgr.tsc",
    "setup": null,
    "flags": {
      "iteration": "'non_rolling_'^iteration^",
      "is_x22_cell_present": "true",
      "patchzipfile_dir": "^T_WORK^",
      "product_name": "^TST_EXE_RESULT^",
      "mach_model": "^TST_EXE_RESULT^"
    },
    "description": "tsagimpatchmgr.tsc - Patchmgr Test for Exadata",
    "platform": null
  },
  {
    "test_name": "tsagimpatchmgr_db.tsc",
    "setup": null,
    "flags": {
      "iteration": "'non_rolling_'^iteration^",
      "STAGING_MACH_NAME": "^HOST^",
      "product_name": "^TST_EXE_RESULT^",
      "mach_model": "^TST_EXE_RESULT^"
    },
    "description": "tsagimpatchmgr_db.tsc - Exadata db node upgrade helper script",
    "platform": null
  },
  {
    "test_name": "tsagimpatchmgr_db_xml.tsc",
    "setup": null,
    "flags": {
      "iteration": "'non_rolling_'^iteration^",
      "STAGING_MACH_NAME": "^HOST^",
      "patchmgr_log_dir": "^TST_EXE_RESULT^",
      "mach_model": "^TST_EXE_RESULT^"
    },
    "description": "tsagimpatchmgr_db_xml.tsc - Exadata compute upgrade/downgrade with OEDA xml",
    "platform": null
  },
  {
    "test_name": "tsagimpatchmgr_ovs.tsc",
    "setup": null,
    "flags": {
      "log_suffix": "^mach_types^",
      "iteration": "'non_rolling_'^iteration^",
      "STAGING_MACH_NAME": "^HOST^",
      "product_name": "^TST_EXE_RESULT^",
      "mach_model": "^TST_EXE_RESULT^"
    },
    "description": "tsagimpatchmgr_ovs.tsc - Helper script for upgrade on dom0/domU",
    "platform": null
  },
  {
    "test_name": "tsagimpatchmgr_ovs_xml.tsc",
    "setup": null,
    "flags": {
      "log_suffix": "^mach_types^",
      "iteration": "'non_rolling_'^iteration^",
      "STAGING_MACH_NAME": "^HOST^",
      "patchmgr_log_dir": "^TST_EXE_RESULT^",
      "mach_model": "^TST_EXE_RESULT^"
    },
    "description": "tsagimpatchmgr_ovs_xml.tsc - Exadata DOM0/DOMU upgrade/downgrade with OEDA xml",
    "platform": null
  },
  {
    "test_name": "tsagimpatchmgr_xml.tsc",
    "setup": null,
    "flags": {
      "iteration": "'non_rolling_'^iteration^",
      "STAGING_MACH_NAME": "^HOST^",
      "patchmgr_log_dir": "^TST_EXE_RESULT^",
      "mach_model": "^TST_EXE_RESULT^"
    },
    "description": "tsagimpatchmgr_xml.tsc - Exadata cells upgrade/downgrade with OEDA xml",
    "platform": null
  },
  {
    "test_name": "tsagimpatchtests.tsc",
    "setup": null,
    "flags": {
      "mach_model": "^TST_EXE_RESULT^",
      "mach_arch": "'SUN'",
      "temp_state": "change_fc_state_before_rollback_^iteration^"
    },
    "description": "tsagimpatchtests.tsc - Script to run verifications before and after patch",
    "platform": null
  },
  {
    "test_name": "tsagimpatchtests_db.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimpatchtests_db.tsc - Helper script to add post upgrade tests for Exadata db node",
    "platform": null
  },
  {
    "test_name": "tsagimpflash.tsc",
    "setup": null,
    "flags": {
      "errbasename": "^tst_tscname^tmp.log"
    },
    "description": "tsagimpflash.tsc - export/import with flashcache\n\nMove a FLASH disk from one slot to another and import.\n     Move a FLASH disk from one cell to another and import.\n     Move the FLASH disk back to original cell and import.",
    "platform": null
  },
  {
    "test_name": "tsagimreclaimdisk.tsc",
    "setup": null,
    "flags": {
      "product_name": "^TST_EXE_RESULT^"
    },
    "description": "tsagimreclaimdisk.tsc - Exadata nodes reclaimdisk test",
    "platform": null
  },
  {
    "test_name": "tsagimrel.tsc",
    "setup": null,
    "flags": {
      "sk_ip": "10.133.152.95",
      "pxe_server": "bur",
      "image_release": "true",
      "pxe_mode": "nfs"
    },
    "description": "tsagimrel.tsc - Exadata Imaging to a release\n\nThis test images a cell/compute node to a particular Exadata release\n     version.Allowed versions are\n     11.2.1.2.3, 11.2.1.2.4, 11.2.1.2.6, 11.2.1.3.1, 11.2.2.1.0, 11.2.2.1.1,11.2.2.2.0\n     Also the preconf file(location on dscbbg02)  for pxe imaging on dscbbgg2 needs to be specified\n     Preconf file for dadzaa machines is default and need not be provided.\n\n     Usage:\n     tsagimrel.tsc mach_name=[mach name] mach_passwd=[mach passwd] mach_type=[cell|compute]\n     image_to_release=[release version] preconf=[location]\n\nPXE redirect should be enabled for the concerned machine\n     All PXE setup is done on dscbbg02\n     ADE setup is only for machines in ADC",
    "platform": null
  },
  {
    "test_name": "tsagimrelflow.tsc",
    "setup": null,
    "flags": {
      "ade_release_dir": "'/ade_autofs/ade_release_unix,/ade_autofs/ade_base2'",
      "initial_image": "initial_image_iteration_^i^",
      "final_image": "final_image_iteration_^i^",
      "retain_minimal_pack_setup": "false"
    },
    "description": "tsagimrelflow.tsc - Exadata Script to image a cell/db node to a desired release\n\nThe script is used to bring a cell/db node to a particular release\n     by upgrade.For example a db node on 11.2.1.3.1 release can be brought\n     to 11.2.2.3.2 release.The script also can upgrade in order.For example\n     we need a db node which was fresh imaged to 11.2.1.3.1 and upgraded\n     to 11.2.2.2.0 and finally upgraded to 11.2.2.3.2.",
    "platform": null
  },
  {
    "test_name": "tsagimrelm.tsc",
    "setup": null,
    "flags": {
      "sep": "'/'",
      "common_cell": "'true'",
      "common_compute": "'true'",
      "preconf_check_done": "true"
    },
    "description": "tsagimrelm.tsc - Imaging multiple machines to an Exadata Release\n\nThis test takes as input a config file of the format\n     machine name,password,release version,cell or compute\n     dadzaa05 welcome1 11.2.2.1.1 compute setup_ade\n     dadzaa07s welcome1 11.2.2.1.0 cell\n\n     For using ilom\n     machine name,password,release version,cell or compute,ilom name,ilom user,ilom user password\n     dadzaa05 welcome1 11.2.2.1.1 compute dadzaa05-c sage welcome1 setup_ade\n     dadzaa07s welcome1 11.2.2.1.0 cell dadzaa07s-c admin admin\n\n     The config file needs to be in the work directory\n     before the test is run.\n\n     ADE setup in for compute nodes only\n\n     Usage:\n     tsagimrelm.tsc mach_info=<config file>",
    "platform": null
  },
  {
    "test_name": "tsagimrescue.tsc",
    "setup": null,
    "flags": {
      "destroy_case": "false",
      "recon_case": "false",
      "product_name": "^TST_EXE_RESULT^",
      "flashcache": "false",
      "fc_size": "'596.125G'"
    },
    "description": "tsagimrescue.tsc- Exadata Cell Rescue Test\n\nThis tests rescue and destruction cases during imaging\n     1) Nulling of partition tables(disabled for now)\n     2) Destroying the partition having root filesystem(disabled for now)\n     3) Testing data preservation and hands free first boot in rescue\n     For details on the scripts used in the test see their description.",
    "platform": null
  },
  {
    "test_name": "tsagimrescuer.tsc",
    "setup": null,
    "flags": {
      "sep": "','"
    },
    "description": "tsagimrescuer.tsc - Helper Script for running rescue parallely on 3 cells",
    "platform": null
  },
  {
    "test_name": "tsagimrunexachk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimrunexachk.tsc - Verifying exachk utility on compute nodes",
    "platform": null
  },
  {
    "test_name": "tsagimsclcypasswd.tsc",
    "setup": null,
    "flags": {
      "passwords": "'welcome1,hard;Root;Passw0rd,We1come$'"
    },
    "description": "tsagimsclcypasswd.tsc - Helper script for extracting passwords\n\nUsed to determine passwords on sclcy quarter rack",
    "platform": null
  },
  {
    "test_name": "tsagimswchk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimswchk.tsc - Exadata Sw checker test\n\ntsagimswchk.tsc switch_names=<comma separated switch names>\n     switch_ip=<comma separated switch ips> switch_passwd=\n     <comma separated switch passwords>\n     Please note that passwords should be equal the number of\n     switches even if all switches have same password.\n\nWhen running standalone\n     tsagimswchk.tsc mach_name=[cell] mach_passwd=[password]....",
    "platform": null
  },
  {
    "test_name": "tsagimupdatescale.tsc",
    "setup": null,
    "flags": {
      "iteration": "'non_rolling_'^iteration^",
      "STAGING_MACH_NAME": "^HOST^",
      "product_name": "^TST_EXE_RESULT^",
      "mach_model": "^TST_EXE_RESULT^"
    },
    "description": "tsagimupdatescale.tsc - Helper script for Exadata update at scale",
    "platform": null
  },
  {
    "test_name": "tsagimupdatescale_db.tsc",
    "setup": null,
    "flags": {
      "iteration": "'non_rolling_'^iteration^",
      "STAGING_MACH_NAME": "^HOST^",
      "product_name": "^TST_EXE_RESULT^",
      "mach_model": "^TST_EXE_RESULT^"
    },
    "description": "tsagimupdatescale_db.tsc -  Exadata db node upgrade using update at scale helper script",
    "platform": null
  },
  {
    "test_name": "tsagimupdatescale_db_func.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimupdatescale_db_func.tsc - Helper script for Exadata update at scale on db node",
    "platform": null
  },
  {
    "test_name": "tsagimupdatescale_func.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagimupdatescale_func.tsc - Helper script for Exadata update at scale",
    "platform": null
  },
  {
    "test_name": "tsagimvmdeploy.tsc",
    "setup": null,
    "flags": {
      "active_partition": "^TST_EXE_RESULT^",
      "sep": "','"
    },
    "description": "tsagimvmdeploy.tsc - Exadata Test for deploying VMs on compute nodes",
    "platform": null
  },
  {
    "test_name": "tsagimworkload.tsc",
    "setup": null,
    "flags": {
      "db_user_passwd_string": "^db_user^_passwd_string_db"
    },
    "description": "tsagimworkload.tsc - Helper script for running db workload",
    "platform": null
  },
  {
    "test_name": "tsagindexrange.tsc",
    "setup": null,
    "flags": {
      "vault_db": "DATA",
      "cdb": "true"
    },
    "description": "tsagindexrange.tsc - Test for code coverage\n\nThis tests adds miscellaneous code coverage tests\n\ntest added in lrgdbconsaexacldcov1",
    "platform": null
  },
  {
    "test_name": "tsaginfinichk.tsc",
    "setup": null,
    "flags": {
      "MACH_PASSWD": "^passwd^",
      "isactive": "true",
      "dbnode_ip": "'192.168.68.76,192.168.68.77,192.168.68.98,192.168.68.99'",
      "cell_ip": "'192.168.68.89,192.168.68.90,192.168.68.111,192.168.68.112'",
      "rack": "x2_2_active",
      "dbnodestr": "root@^hostname^"
    },
    "description": "tsaginfinichk.tsc - Test for infinicheck commands.\n\nTest for infinichck commands. The ibdiagtools directory is used to run the comamnds.",
    "platform": null
  },
  {
    "test_name": "tsaginiiormpmem.tsc",
    "setup": null,
    "flags": null,
    "description": "tsaginiiormpmem.tsc - Prepare testing env. for IORM PMEM test cases\n\ncreate testing tbs, users, consume groups, categories",
    "platform": null
  },
  {
    "test_name": "tsaginiiormqm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsaginiiormqm.tsc - Prepare testing env. for IORM QM test cases\n\ncreate testing tbs, users, consume groups, categories",
    "platform": null
  },
  {
    "test_name": "tsaginimcell.tsc",
    "setup": null,
    "flags": {
      "dot": "'.'",
      "creatdev_file": "tsagr2def",
      "sep_var": "','",
      "sep_var1": "'='",
      "vault_log": "^vault_db^",
      "vault_db": "DB^ORA_SID_UPPER^",
      "cid": "1"
    },
    "description": "tsaginimcell.tsc - Setup Exadata&Exascale Multiple cell versions with one database\n\nThis script will setup Exadata&Exascale Multiple cell versions in multi-views\n\n     For Exadata:\n       1). Setup three views: two oss views with OSS_MAIN, OSS_XXX, one RDBMS VIEW\n       2) In OSS views, just setup cell services and setup GI/ASM/DB instances in RDBMS VIEW\n       3) In rdbms view, we should setup ASM DG with normal redu that using griddisks\n          from different cell versions\n\n     For Exascale:\n       1). Setup three views: two oss views with OSS_MAIN latest label, OSS_MAIN old base label, one RDBMS VIEW\n       2) In OSS views, just setup full Exascale cell services and setup GI/DB instances in RDBMS VIEW\n       3) In rdbms view, we should use Exascale vault to setup databases\n\n     Once set oss_exascale_testing, it will setup multiple exascale cell versions, by default\n     we setup multiple exadata cell versions, below items are different with Exadata:\n       1). setup one cell in high cell view and two cells in low cell view\n       2). setup egs cluster with 3 instances, one comes from high cell view and two comes from low cell view\n       3). setup two ers instances, each of them come from different cell view\n\nHow to use\n\n     A. If you don't have any views, just run: oratst -d tsaginimcell to setup this env.\n     B. If you have two views or one view, please run\n        oratst -d tsaginimcell oss_view2=juwawang_phoenix92470_ossv2 rdbms_view=juwawang_phoenix92470_rdbmsv1\n        oratst -d tsaginimcell rdbms_view=juwawang_phoenix92470_rdbmsv1\n\n     if you want to set some cell setup parameters, please use flag - cell_setup_parameters\n        let cell_setup_parameters 'cell_with_pmemcache=1,cell_with_xrmem_cache=0'\n\n     if you want to set some cell parameters, please use flag - cell_parameters\n        let cell_parameters '_cell_storage_index_diag_mode=1,_cell_fc_columnar_force_cc1=FALSE'\n\n     if you want to set some db parameters, please use flag - db_parameters\n        let db_parameters 'oss_auto_manage_disks=true,oss_auto_manage_disks1=true'\n\n\n      A full demo to run your special test:\n\n         # setup multiple cell views env.\n         include tsaginimcell\n\n         # run your test\n         runtest your_tsc_file\n\n         # clean up multiple cell views env.\n         include tsagclnmcell\n\n      All user guide can be found from page: https://confluence.oraclecorp.com/confluence/pages/viewpage.action?pageId=4907585070",
    "platform": null
  },
  {
    "test_name": "tsaginimtc.tsc",
    "setup": null,
    "flags": {
      "no_gd_with_prefix": "true",
      "adg_dbname": "exadb",
      "ora_sid": "exadb",
      "db_unq": "^db_unq^",
      "cdb_dd": "pdbs_across_cdbs",
      "sep_var": "','",
      "sep_var1": "'='",
      "file_dest": "@^vault_db^",
      "db_count": "^num_dbs^"
    },
    "description": "tsaginimtc.tsc  - limited asm disk discovery using 2 asm clusters\n\nBasic test case for limited asm disk discovery\n     this tsc will setup an oss cell have 2 set of griddisk, diska*,datafile*\n     when cluster 1 create a diskgroup using diska0, cluster2 should not see the used asm disk\n     Two cells with normal redundancy.\n\nThis test will setup two more RDBMS views to run 2 asm clusters share the same storage\n     from current view which setup an oss cell\n\n\n     1. Setup n-CDBs with n-PDBs in each clusters:\n\n  oratst -d tsaginimtc cdb=true num_dbs=n pdbs_per_cdb=n cdb_dd=pdbs_across_cdbs duplicate_dbnm=true oss_asm_sec=true\n\n     pdbs_per_cdb : the default value is 1\n     num_dbs      : the default value is 1, , it is the number of databases in each cluster\n                    if cdb_dd is pdbs_across_cdbs, then it's the number of CDB, in this mode,\n                    you can use pdbs_per_cdb to define the number of PDB in each CDB\n                    or else, it's the number of PDBs when cdb_dd is pdbs_in_cdbs\n     cdb_dd       : PDB mode, the default is pdbs_across_cdbs, another value is pdbs_in_cdbs\n     duplicate_dbnm: when it is true, we'll create the same db unique name in different clusters without ASM scoped security\n                     db1 name: exadb, db2 name: exadbb, db3 name: exadbc, db4 name: exadbd\n     duplicate_dbnm_sec: when it is true, we'll create the same db unique name in different clusters with ASM scoped security\n                     db1 name: exadb, db2 name: exadbb, db3 name: exadbc, db4 name: exadbd\n     cluster2_db_nums: this is used to create the number of database in cluster2 if it is set\n                      or else create num_dbs databases in it, but we always create num_dbs databases in cluster1\n     oss_asm_sec: enable ASM-scroped security\n     disable_cluster_sec: 1 or 2 is the cluster where ASM-scoped security should be disabled\n     cluster1_db_sec: the name of the db in cluster 1, we setup the database-scoped security for it\n     cluster2_db_sec: the name of the db in cluster 2, we setup the database-scoped security for it\n     cluster2_cdb:   when it is true, will enable cdb mode in cluster 2, but still create normal databases in cluster 1\n\n     2. Setup n-DBs(non-CDB) in each clusters:\n      oratst -d tsaginimtc num_dbs=n\n\n     num_dbs      : the default value is 1, it is the number of databases in each cluster\n     duplicate_dbnm: when it is true, we'll create the same db unique name in different clusters\n                     db1 name: exadb, db2 name: exadbb, db3 name: exadbc, db4 name: exadbd\n     duplicate_dbnm_sec: when it is true, we'll create the same db unique name in different clusters with ASM scoped security\n                     db1 name: exadb, db2 name: exadbb, db3 name: exadbc, db4 name: exadbd\n     cluster2_db_nums: this is used to create the number of database in cluster2, we always create num_dbs databases in cluster1\n     oss_asm_sec: enable ASM-scroped security\n     disable_cluster_sec: 1 or 2 is the cluster where ASM-scoped security should be disabled\n     cluster1_db_sec: the name of the db in cluster 1, we setup the database-scoped security for it\n     cluster2_db_sec: the name of the db in cluster 2, we setup the database-scoped security for it\n     cluster2_cdb:   when it is true, will enable cdb mode in cluster 2, but still create normal databases in cluster 1\n\n     3. Setup Exascale ADG across clusters(Primary and standby are in different clusters):\n       oratst -d tsaginimtc oss_mtc_adg=xx [adg_standby_file_mode=xx] [adg_standby_creation_mode=xx]\n\n       oss_mtc_adg : Enable to setup Exascale ADG in MTC test fw\n\n          1). oss_mtc_adg=adg_indifferent_vault : Primary/Standby DB are created in different vaults\n              oratst -d tsaginimtc oss_mtc_adg=adg_indifferent_vault\n\n          2). oss_mtc_adg=adg_insame_vault : Primary/Standby DB are created in same vault\n              oratst -d tsaginimtc oss_mtc_adg=adg_insame_vault\n\n          3). oss_mtc_adg=adg_hybrid_asm_vault : Primary is on ASM in cluster 1 and Standby DB is created on vault in cluster 2\n              ASM griddisks and vault pool disks come from same cells\n\n              oratst -d tsaginimtc oss_mtc_adg=adg_hybrid_asm_vault\n\n          4). oss_mtc_adg=adg_hybrid_cross_cell : Primary is on ASM in cluster 1 and Standby DB is created on vault in cluster 2\n              ASM griddisks and vault pool disks come from separated cells, in this setup, it creates 2 cell views and 2 cluster views\n\n                1). Cell    view 1(root view): setup Exascale full serivces, it servers for the standby db in cluster view 2\n                2). Cluster view 1(cluster 1): setup GI/ASM/Primary Database, it uses the ASM diskgroups in cell view 2\n                3). Cluster view 2(cluster 2): setup GI/Standby Database, it uses the vault in cell view 1\n                4). Cell    view 2(cluster 3): setup Exadata full serivces, it servers for the primary db in cluster view 1\n\n            In summary, we setup ADG on hbrid (ASM -> Exascale) like this:\n\n            1). Setup Exadata ASM in cell view 2 and cluser view 1 and create primary database on it\n\n            2). Setup Exadata Exascale in cell view 1 and cluser view 2 and use rman duplicate/restore database from primary services\n                to create standby database on it\n\n            3). Config ADG between cluster view 1 and cluster view 2 to ensure MRP works well\n\n        How to setup locally:\n\n              oratst -d tsaginimtc oss_mtc_adg=adg_hybrid_cross_cell\n\n\n       adg_standby_file_mode : config file creation mode for standby database in cluster 2\n\n          1). adg_standby_file_mode=omf : just enable OMF in standby database, default value\n\n          2). adg_standby_file_mode=omf_all_convert :  enable OMF and db/log file_name_convert in standby database\n\n          3). adg_standby_file_mode=omf_db_convert :   enable OMF and db file_name_convert in standby database\n\n          4). adg_standby_file_mode=omf_log_convert : enable OMF and log file_name_convert in standby database\n\n          5). adg_standby_file_mode=off : just enable db/log file_name_convert in standby database\n\n              oratst -d tsaginimtc oss_mtc_adg=adg_hybrid_asm_vault adg_standby_file_mode=omf\n\n       adg_standby_creation_mode : config standby database creation mode in cluster 2\n\n          1). adg_standby_creation_mode=duplicate :  use rman duplicate command to create standby database, default value\n\n          2). adg_standby_creation_mode=restore_service : use rman restore services command to create standby database\n\n              oratst -d tsaginimtc oss_mtc_adg=adg_hybrid_asm_vault adg_standby_creation_mode=restore_service",
    "platform": null
  },
  {
    "test_name": "tsaginimtciormqm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsaginimtciormqm.tsc - Prepare testing env. for IORM QM test cases\n\ncreate testing tbs, users, consume groups, categories",
    "platform": null
  },
  {
    "test_name": "tsaginit.tsc",
    "setup": null,
    "flags": {
      "cur_event": "event^i^",
      "dbgprfx": "\"tsaginit::debugg:\"",
      "cell_with_pmem_disk": "0",
      "cell_with_xrmem_cache": "0",
      "NUM_QLC_PER_CELL": "8",
      "QLC_DISK_SIZE_MB": "640 # in MB",
      "lrg_name": "'invalid_lrg'",
      "cell_xrmem_cache_size": "3000",
      "CELL_WITH_PMEM_CACHE": "1",
      "cell_with_pmem_cache": "0",
      "BIGSCN": "1",
      "oss_dbversion": "99999",
      "temp_nowarn": "^tst_nowarn^",
      "JAVA_HOME": "^ADE_VIEW_ROOT_CV^/jdk7",
      "CELL_WITH_SPARSE_GD": "^tmpvar1^",
      "runfakeibsrv": "true",
      "cell_with_ram_cache": "0",
      "cell_with_write_cache": "false",
      "write_cache_size": "0",
      "cell_with_wtflash_cache": "false",
      "tmp_nowarn": "^tst_nowarn^",
      "DEBUGCLI": "^OSS_HOME^/src/tools/DebugCli/DebugCli",
      "egs_test": "true  ## Reqd to set skgxp_dynamic_protocol=3 in Exascale lrgs",
      "asm_ausize": "4194304",
      "RECON_CELL_FREQ_IN_SEC": "4",
      "RECON_CELL_ATTEMPT_COUNT": "4",
      "MY_SKGXP_DYNAMIC_PROTOCOL": "3",
      "CELL_REF_CNT_LOG_ENABLE": "true",
      "CELL_CLIENT_DUMP_INTERVAL": "0",
      "pcw_auto_start": "true",
      "node_name": "multi_node^i^",
      "rmt_tst_exec": "^TST_STATUS^",
      "CLI_TESTING": "true",
      "num_failgroup": "3",
      "oss_testing": "^num_failgroup^",
      "server_event": "'\"trace[CELLSRV_EVENTS_Layer.*] disk=highest,memory=highest\"'",
      "libcell_event": "'\"trace[libcell.connect_layer.*] disk=medium,memory=medium\"'",
      "fc_trace": "'\"trace[CELLSRV_FFI_Layer.*] '^ffi_trace^\"",
      "mdmgr_layer_trace": "'\"trace[CELLSRV_MetadataManager_Layer.*] '^mdmgr_trace^\""
    },
    "description": "tsaginit.tsc - Initialization for SAGE tests\n\nSet up variables to use SAGE/OSS disks.\n\nThis script should be invoked with include at the beginning of\n     SAGE tests.",
    "platform": null
  },
  {
    "test_name": "tsaginitcc2.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagcfcdef",
      "nflint": "1",
      "PWFILE_ON_EXC": "false",
      "no_gd_with_prefix": "true",
      "file_dest": "'+DATAFILE'"
    },
    "description": "tsaginitcc2.tsc - A test file to setup an rdbms instance with asm, oss and enabled CC2 (default is 65 and uncompressed table in non-encrypted tbs)\n\nset imcfclv = 33/65/97\n     set imcfcoltp = true - compresse as oltp\n     set encalgo = AES256 - encrypted/non-encrypted tbs\n     _enable_columnar_cache=33 (CC1 -> create ehcc table no cellmemory)\n     _enable_columnar_cache=65 (CC2 -> cellmemory memcompress for query)\n     _enable_columnar_cache=97 (CC2 -> cellmemory memcompress for capacity)\n\nThis test is mainly for setup CC2 with lineitem schema\n     ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n     To run this script locally on Exascale setup:\n          ade copydb -noscript -dir $CDB_NLS_LMT_SEED\n          oratst tsaginitcc2.tsc oss_exascale_testing=true",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaginitcc2si.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagrddef",
      "columnar_cache": "1",
      "rdbms_internal_fplib": "false",
      "oltp_compress": "false",
      "user": "tpch",
      "pwd": "^user^"
    },
    "description": "tsaginitcc2si.tsc - Setup Exadata for SI Dense Bloom Test\n\nSetup Exadata for SI Dense Bloom Test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaginitchk.tsc",
    "setup": null,
    "flags": {
      "FORCE_TEMP": "^FORCE_REIMAGE^",
      "FORCE_REIMAGE": "^FORCE_TEMP^"
    },
    "description": "tsaginitchk.tsc - Health check macro at the beginning of LRG\n\nTo conduct health check at the beginning of LRG\n\nTo conduct health check at the beginning of LRG",
    "platform": null
  },
  {
    "test_name": "tsaginitchk_db.tsc",
    "setup": null,
    "flags": {
      "FORCE_TEMP": "^FORCE_REIMAGE^",
      "FORCE_REIMAGE": "^FORCE_TEMP^"
    },
    "description": "tsaginitchk_db.tsc - Health check macro at the beginning of LRG on DBnode\n\nTo conduct health check at the beginning of LRG on DBnode\n\nTo conduct health check at the beginning of LRG on DBnode",
    "platform": null
  },
  {
    "test_name": "tsaginitchk_db_cell.tsc",
    "setup": null,
    "flags": {
      "TZ": "^db_tz^",
      "cellconnstr": "^cell2connstr^"
    },
    "description": "tsaginitchk_db_cell.tsc - Health check macro at the beginning of LRG\n\nTo conduct health check at the beginning of LRG on cell and db\n\nTo conduct health check at the beginning of LRG",
    "platform": null
  },
  {
    "test_name": "tsaginitoflworkload.tsc",
    "setup": null,
    "flags": {
      "hidestr": "'##'"
    },
    "description": "tsaginitoflworkload.tsc - Common code for offloadgroup workload cases\n\ntsagcrttbs.sql: create table for workload\n     crtpkgtsagoffload.sql: define workload procedure tsagoffload.work_till_time\n     tsaginitchk.sql: init sql running(pkg create, etc)\n     tsagoffloadchk.sql: call tsagmdbchk to check SmartScan state\n     crashcellsrv.tsc: use simulation event to crash cellsrv\n     crashcelloflsrv.tsc: use simulation event to crash celloflsrv\n     crtstartoflgrp4hook.tsc: create/start offloadgroup\n     shutdropoflgrp4hook.tsc: shutdown/drop offloadgroup\n     chkdurationstate4hook.tsc: check SmartScan state between certain duration\n     chk0/1/2state4hook.tsc: same above, but seperate into two sub script\n     chkdbstate4hook.tsc: check smartscan state from db perspective\n     resetdbcelloflgrp4hook.tsc: call tsagresetdbcelloflgrp.sql to reset v$cell_state\n     tsagresetdbcelloflgrp.sql: reset cell_offloadgroup_name if the case set this param at system level\n     restartdb4hook.tsc: call tsagrestartdb.sql to restart db\n     tsagrestartdb.sql: restart db\n     tsagmdbworkload.sql: run workload for (specify) offloadgroup at session/system level\n     defaultHook.tsc: reset *hooks.tsc for workload\n     tsagoflworkloadtemplate.tsc: MAIN ENTRANCE, basic template for workload tests\n     \t\t\t    call before/mid/after hooks for each test\n\nFollowings need to initialize in the parent tsc\n     tkrmlog\n     tsagofllog\n     sysdba\n     chkduration\n     midhookstart\n     workloadtime\n     pfile_path(for create spfile,in n-view, it's at ^T_WORK^/worklv^db_num^/t_init1.ora\n     and sesql tsaginitchk.sql to initialize the sql",
    "platform": null
  },
  {
    "test_name": "tsaginrepofilter1.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsaginrepofilter1.tsc - Test case for in-repo filtering support of volume objects\n\nFollowing test cases are covered:\nTest 1 : Filter volume snapshots by volume ids\nTest 2 : Filter volume backups by volume ids\nTest 3 : Filter volume backups by volume snapshot ids\nTest 4 : Filter using volume id and volumesnapshot id of another user\nTest 5 : Filtering for datacopy backup by volume id and volume snapshot id",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagins.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1"
    },
    "description": "tsagins.tsc - Sets up Install lrg Enviroment\n\noratst tsagins.tsc mach_name=<cell name> mach_passwd=<cell passwd>",
    "platform": null
  },
  {
    "test_name": "tsaginstfence.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "VF_ON_EXC": "true",
      "setup_blockstore": "true"
    },
    "description": "tsaginstfence.tsc - Instance Level Fencing Test\n\nTest checks ESNP fencing messages for DB , Edstool , IOV and CSS.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsaginstl1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsaginstl1.tsc - Exadata Phase-wise Upgrade Test\n\nTests rpm upgrade in phases\n\nBefore running the test set the following environment variables\n     MACH_NAME-Machine on which to run the test\n     MACH_PASSWD-Password of the machine on which to run test\n     or\n     oratst tsaginstl1.tsc mach_name=<cell name> mach_passwd=<cell_passwd>\n     The test sets up hands free ssh connection between the host\n     and the cell.",
    "platform": null
  },
  {
    "test_name": "tsaginstl2.tsc",
    "setup": null,
    "flags": {
      "log_suffix": "up"
    },
    "description": "tsaginstl2.tsc - Exadata Install and Upgrade Test\n\nThis test tests the cell rpm.The test has 2 modes\n     1) Install:When cell rpm version on the target machine is same as\n                the one being installed.Downgrade is not tested\n                in this case\n     2) Upgrade:When cell rpm version on the target machine is lower\n                than the one being installed.Downgrade is tested.\n\nBefore running the test set the following environment variables\n     MACH_NAME-Machine on which to run the test\n     MACH_PASSWD-Password of the machine on which to run test\n     or\n     oratst tsaginstl2.tsc mach_name=<cell name> mach_passwd=<cell_passwd>\n     The test sets up hands free ssh connection between the host\n     and the cell.",
    "platform": null
  },
  {
    "test_name": "tsagintdb.tsc",
    "setup": null,
    "flags": {
      "iormpart1": "'(name=d050fs,share=24),(name=d060rc,share=4),(name=d1675o,share=4),(name=d1677o,share=4),(name=d1680o,share=4),(name=d1769t,share=4),(name=d1770t,share=4),(name=d1771t,share=4),(name=d1773t,share=4),(name=d1784t,share=4),(name=d1791a,share=8),(name=d1792a,share=8),(name=d1793o,share=4),(name=d1794o,share=4),(name=d1798o,share=4),(name=d1799o,share=4),(name=d1800o,share=4),(name=d1803o,share=4),(name=d1805o,share=4),(name=d1809t_stb1,share=4),(name=d1813t_stb1,share=4),(name=d1817t_stb1,share=4),(name=d1818t_stb1,share=4),(name=d1820t,share=4),(name=d1821t,share=4),(name=d1822t,share=4),(name=d1824t,share=4),(name=d1825t,share=4),(name=d1827t,share=4),(name=d1828t,share=4),'",
      "iormpart2": "'(name=d1829t,share=4),(name=d1834a_stb1,share=8),(name=d1835a,share=8),(name=d1837a,share=8),(name=d1837a_stb1,share=8),(name=d1838a,share=8),(name=d1875a_stb1,share=8),(name=d1876a_stb1,share=8),(name=d1878t,share=4),(name=d1879t,share=4),(name=d1887a,share=8),(name=d1897a,share=8),(name=d1928t,share=4),(name=d1931a_stb1,share=8),(name=d1932a_stb1,share=8),(name=d1944a,share=8),(name=d1952a_stb1,share=8),(name=d1980o,share=4),(name=d1982o,share=4),(name=d1983o,share=4),(name=d1988t,share=4),(name=d1989t,share=4),(name=d1990t,share=4),(name=d1995a,share=8),(name=d1998o,share=4),(name=d1999a_stb1,share=8),(name=d2001t,share=4),(name=d2007t,share=4),(name=d2008o,share=4),(name=d2009o,share=4),'",
      "iormpart3": "'(name=d2014a_stb1,share=8),(name=d2015a_stb1,share=8),(name=d2016a,share=8),(name=d2017a,share=8),(name=d2021a_stb1,share=8),(name=d2024t,share=4),(name=d2031t,share=4),(name=d2034a,share=8),(name=d2035a,share=8),(name=d2041a,share=8),(name=d2042a,share=8),(name=d2043a,share=8),(name=d2044a,share=8),(name=d2069a,share=8),(name=d2090t,share=4),(name=d2100t,share=4),(name=d2101t,share=4),(name=d2129a_stb1,share=8),(name=d2147o,share=4),(name=d2151o,share=4),(name=d2153o,share=4),(name=d2154t,share=4),(name=d2157t,share=4),(name=d2158t,share=4),(name=d2160t,share=4),(name=d2161t,share=4),(name=d2163t,share=4),(name=d2164a_stb1,share=8),(name=d2166a,share=8),(name=d2167a,share=8),'",
      "iormpart4": "'(name=d2168a,share=8),(name=d2169a,share=8),(name=d2170a,share=8),(name=d2171a,share=8),(name=d2172a,share=8),(name=d2173a,share=8),(name=d2174a,share=8),(name=d2206t,share=4),(name=d2207t,share=4),(name=d2208t,share=4),(name=d2209t,share=4),(name=d2212t,share=4),(name=d2213t,share=4),(name=d2214t,share=4),(name=d2215t,share=4),(name=d2254o,share=4),(name=d2255o,share=4),(name=d2257o,share=4),(name=d2258o,share=4),(name=d2259o,share=4),(name=d2260o,share=4),(name=d2261o,share=4),(name=d2262o,share=4),(name=d2263o,share=4),(name=d2265t,share=4),(name=d2268t,share=4),(name=d2269t,share=4),(name=d2270t,share=4),(name=d2271t,share=4),(name=d2273t,share=4),'",
      "iormpart5": "'(name=d2274o,share=4),(name=d2275o,share=4),(name=d2276o,share=4),(name=d2277o,share=4),(name=d2278o,share=4),(name=d2283t,share=4),(name=d2284t,share=4),(name=d2308o,share=4),(name=d2309o,share=4),(name=d2310o,share=4),(name=d2311o,share=4),(name=d2312o,share=4),(name=d2314o,share=4),(name=d2324t,share=4),(name=d2384a,share=8),(name=d2385a,share=8),(name=d2386a,share=8),(name=d2389a,share=8),(name=d2390a,share=8),(name=d2391a,share=8),(name=d2392a,share=8),(name=d2393a,share=8),(name=d2396t,share=4),(name=d2398t,share=4),(name=d2399t,share=4),(name=d2401t,share=4),(name=d2402t,share=4),(name=d2403t,share=4),(name=d2414a,share=8),(name=d2415a,share=8),'",
      "iormpart6": "'(name=d2417a,share=8),(name=d2418a_stb1,share=8),(name=d2419a,share=8),(name=d2420a,share=8),(name=d2421a,share=8),(name=d2422a,share=8),(name=d2423a,share=8),(name=d2424o,share=4),(name=d2425o,share=4),(name=d2426o,share=4),(name=d2427o,share=4),(name=d2428o,share=4),(name=d2429o,share=4),(name=d2430o,share=4),(name=d2431o,share=4),(name=d2432o,share=4),(name=d2433o,share=4),(name=d2454o,share=4),(name=d2458t,share=4),(name=d2459t,share=4),(name=d2460t,share=4),(name=d2492a,share=8),(name=dbm,share=4),(name=d1796o,share=4),(name=d1823t,share=4),(name=d1826t,share=4),(name=d1830t,share=4),(name=d1836a,share=8),(name=d1943a,share=8),(name=d1991t,share=4),(name=d2091a,share=8),(name=d2162t,share=4),(name=d2266t,share=4),(name=d2267t,share=4),(name=d2272t,share=4),(name=d2494a,share=8),(name=d2495a,share=8),(name=d2496a,share=8),(name=d2515t,share=4),(name=d2520t,share=4),(name=default,share=1)'"
    },
    "description": "tsagintdb.tsc - InterDatabasePlan tests\n\nIt tests the various operations that can be performed on InterDatabasePlan through SAGE command line interface.",
    "platform": null
  },
  {
    "test_name": "tsagintdb_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagintdb_nls.tsc - export import CellDisks\n\nTests InterDatabasePlan commands of cellcli",
    "platform": null
  },
  {
    "test_name": "tsagintegration_unit_tests.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagintegration_unit_tests.tsc - Test to run imaging unit tests",
    "platform": null
  },
  {
    "test_name": "tsaginterdbconv.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "job_queue_processes": "0"
    },
    "description": "tsaginterdbconv.tsc - InterDatabbasePlan Conversion tests\n\nThis tests the plan type conversion from an allocation plan to a share\n     plan. This conversion is for supporting the hybrid setting in Exascale.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaginteropossexc.tsc",
    "setup": null,
    "flags": {
      "temp_nowarn": "^tst_nowarn^"
    },
    "description": "tsaginteropossexc.tsc - Interop driver to execute tests on remote RDBMS view\n\nThis script sets the necessary environment and flags to run a functional\n     test on remote RDBMS view for exasclae - RDBMS remote interop",
    "platform": null
  },
  {
    "test_name": "tsaginteroprmtfrexc.tsc",
    "setup": null,
    "flags": {
      "tmp_nowarn": "^tst_nowarn^"
    },
    "description": "tsaginteroprmtfrexc.tsc - Exascale interop driver\n\nThis script sets the environment for exascale on the local node\n\nRDBMS instance that consumes the exascale storage is brought up on\n     the remote node",
    "platform": null
  },
  {
    "test_name": "tsagintgrid.tsc",
    "setup": "tsaginit",
    "flags": {
      "dangerousfg": "1",
      "differentszfg": "1",
      "creatdev_file": "tsagr3def",
      "raw_path": "^T_WORK^/raw/"
    },
    "description": "tsagintgrid.tsc - interleaved griddisk tests\n\nTests for Interleaved Griddisk project",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagintgrid2.tsc",
    "setup": "tsaginit",
    "flags": {
      "dangerousfg": "1",
      "differentszfg": "1",
      "creatdev_file": "tsagr2def",
      "raw_path": "^T_WORK^/raw/"
    },
    "description": "tsagintgrid2.tsc - interleaved celldisks tests\n\ntests for Interleaved celldisks on exadata",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagintgrid3.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagr2def",
      "dgattr": "'attribute '''compatible.asm'''='''^scompatible.asm^''','''compatible.rdbms'''='''^scompatible.rdbms^''','''cell.smart_scan_capable'''='''true''",
      "raw_path": "^T_WORK^/raw/"
    },
    "description": "tsagintgrid3.tsc - interleaved griddisk test\n\nresize griddisk on a interleaved celldisk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagintgrid4.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagr3def",
      "dgattr": "'attribute '''compatible.asm'''='''^scompatible.asm^''','''compatible.rdbms'''='''^scompatible.rdbms^''','''cell.smart_scan_capable'''='''true''",
      "raw_path": "^T_WORK^/raw/"
    },
    "description": "tsagintgrid4.tsc - interleaved griddisk tests\n\ninterleaved griddisk tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagintgridh2.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagr3def",
      "dgattr": "'attribute '''compatible.asm'''='''^scompatible.asm^''','''compatible.rdbms'''='''^scompatible.rdbms^''','''cell.smart_scan_capable'''='''true''",
      "raw_path": "^T_WORK^/raw/"
    },
    "description": "tsagintgridh2.tsc - offset tests for interleaved griddisk\n\nInterleaved Griddisks Tests for HIGH_REDUNDANCY Disks. It tests the\n      freeSpaceMap of a high_redundancy celldisk after creating griddisks\n      of various sizes at various offset.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagintgridhigh.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagr3def",
      "dangerousfg": "1",
      "differentszfg": "1",
      "raw_path": "^T_WORK^/raw/"
    },
    "description": "tsagintgridhigh.tsc - interleaved griddisk tests\n\nInterleaved Griddisks Tests for HIGH_REDUNDANCY Disks",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagintshort.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagintshort.tsc - tests for exadata features on interop\n\nTests for the following exadata features running with interop\n        1.  Storage Index\n        1b. Smart Scan\n        2.  EHCC\n        3.  Query offload in an encrypted tablespace",
    "platform": null
  },
  {
    "test_name": "tsagintsuit.tsc",
    "setup": "srdbmsini",
    "flags": {
      "noiovrun": "true",
      "resilver_test": "true"
    },
    "description": "tsagintsuit.tsc - exadata interop suite\n\nexadata interop suite",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaginvalidip.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagaudef",
      "disk": "datafile0"
    },
    "description": "tsaginvalidip.tsc - Testing response to invalid IP addresses in cellinit.ora\n\nThe Test is outlined as below:\n1) Bring up the stack\n2) shutdown DB instance\n3) Modify cellinit.ora to have invalid IP addresses\n4) Startup DB instance\n5) Look for ORA-56865 in error logs",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaginvalidip1.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagaudef",
      "disk": "datafile0"
    },
    "description": "tsaginvalidip1.tsc - Testing cellcli response to invalid IP addresses in cellinit.ora (bug-11677873)\n\nThe Test is outlined as below:\n      1) Bring up the stack\n      2) shutdown RS,MS,cellsrv\n      3) Modify cellinit.ora to have invalid IP addresses\n      4) Startup RS\n      5) Startup cellsrv\n      6) CELL-01533 error should be thrown.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagioacrossbswf.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagioacrossbswf.tsc - The test restarts bsw while IO is in progress on ISCSI devices.\n\nThe test case restarts bsw during I/O on iscsi devices.\n     I/O is done on iscsi devices of volume, volume clone, clone of clone ,\n     restored volume and a clone from this restored volume.\n     Also on a volume created by a second user .",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagiocalib.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagiocalib.tsc - Test SAGe IO CALIBration\n\nExecutes I/O Calibration tests for fake hardware",
    "platform": null
  },
  {
    "test_name": "tsagiocancel.tsc",
    "setup": "srdbmsini",
    "flags": {
      "noecho_testname": "true",
      "creatdev_file": "mydiskdef",
      "SAGE_MIRROR_MODE": "normal",
      "oss_devdir1": "^T_WORK^/raw^oss_port^",
      "oss_devdir2": "^T_WORK^/raw^oss_port2^"
    },
    "description": "tsagiocancel.tsc - Slow I/O handling\n\nSlow I/O handling - Cancel all pending I/O req once disk is 'confined'.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiocancel_ht.tsc",
    "setup": "tsaginit",
    "flags": {
      "noecho_testname": "true",
      "num_gd": "5    # number of griddisks to create and use",
      "duration": "600 # run IOV for 10 minutes",
      "flash_size": "96",
      "threads": "3   # number of threads per each IOV process",
      "area_sz": "40   # size of each area on gd for an IOV process, in MB",
      "sleep_btw_test": "120",
      "gdsz": "368     # gd size in MB",
      "max_areas": "5  # max number of areas on each device",
      "creatdev_file": "tkfgiovdef",
      "cfgdir": "tsagfciov.sav"
    },
    "description": "tsagiocancel_ht.tsc - Cell I/O cancel (hair triggers)\n\nThis script has scenarios to test new trigger (hair trigger) to cancel IOs when all IOs to a disk are hung for a certain amount of time",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiof1.tsc",
    "setup": "tsagnini",
    "flags": {
      "auto_undo_management": "true",
      "asm_ausize": "65536",
      "creatdev_file": "tkfgrddef",
      "oss_testing": "2",
      "num_failgroup": "2",
      "cluster_database": "true",
      "maxinstances": "4"
    },
    "description": "tsagiof1.tsc - SAGE process fencing test\n\nKill shadow process while doing direct path IO's\n    Runs in single instance (non-RAC) mode\n\n- set event to HOLD IO\n     - Fork sql which does an IO\n     - kill shadow process\n     - check if IO fencing happened",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiof2.tsc",
    "setup": "tsagnini",
    "flags": {
      "tmp_cellconnstr": "^cellconnstr^",
      "cluster_database": "true",
      "maxinstances": "4",
      "auto_undo_management": "true",
      "asm_ausize": "65536",
      "creatdev_file": "tkfgrddef"
    },
    "description": "tsagiof2.tsc - SAGE RDBMS instance crash fencing test\n\nCrashes RDBMS instance while doing direct path IO's\n\n- set event to HOLD IO\n     - Fork sql which does an IO\n     - shutdown abort RDBMS instance\n     - check if IO fencing happened\n\n     ***Note***\n        This is how the test is now:\n        ----------------------------\n        inst 1 -> hold IO -> shutdown abort -> make sure table didn't get updated\n        inst 2 -> donot hold IO -> donot shutdown abort -> do not check for table updates\n\n        This is how the test should be:\n        -------------------------------\n        inst 1 -> hold IO -> shutdown abort -> make sure table didn't get updated\n        inst 2 -> hold IO -> -> make sure table didn't get updated\n\n        Being addressed:\n        ----------------\n        **The shutdown abort of inst 2 hangs **\n        **If anything (inserts with HOLD event set) is done in inst 2, **\n        **the shutdown abort of inst 1 hangs**",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiof3.tsc",
    "setup": "tsagnini",
    "flags": {
      "tmp_cellconnstr": "^cellconnstr^",
      "creatdev_file": "tkfgrddef",
      "asmdisks_created": "2",
      "num_failgroup": "2",
      "ainst": "asm_instance^i^",
      "oss_testing": "1"
    },
    "description": "tsagiof3.tsc - SAGE ASM instance crash fencing test\n\nCrashes ASM instance while doing direct path IO's\n     Needs RAC since second instance of ASM is used to release IO\n\n- set event to HOLD IO\n     - Fork sql which does an IO\n     - shutdown abort ASM instance\n     - set event to RELEASE IO from another ASM instance\n     - check if IO fencing happened",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiof4.tsc",
    "setup": "tsaginit",
    "flags": {
      "maxinstances": "1",
      "auto_undo_management": "true",
      "asm_ausize": "65536",
      "creatdev_file": "tkfgrddef",
      "compatible": "^def_compatibility^"
    },
    "description": "tsagiof4.tsc - SAGE cluster fencing test\n\nKill css while doing direct path IO's\n\nThie test is run in the emulated multi-node test env\n     The test is invoked from tsagiofncd.tsc",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiof5.tsc",
    "setup": "tsagnini",
    "flags": {
      "cluster_database": "true",
      "maxinstances": "4",
      "creatdev_file": "tkfgrddef"
    },
    "description": "tsagiof5.tsc - SAGE IMPLICIT fence test\n\nKills CSS on a single node env",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiof7.tsc",
    "setup": "tsagnini",
    "flags": {
      "cluster_database": "true",
      "maxinstances": "4",
      "auto_undo_management": "true",
      "asm_ausize": "65536",
      "creatdev_file": "tkfgrddef",
      "compatible": "^def_compatibility^"
    },
    "description": "tsagiof3.tsc - SAGE ASM instance crash fencing test\n\nCrashes ASM instance while doing direct path IO's\n     Needs RAC since second instance of ASM is used to release IO\n\n- set event to HOLD IO\n     - Fork sql which does an IO\n     - alter diskgroup dismount force\n     - set event to RELEASE IO from another ASM instance\n     - check if IO fencing happened\n\n   ***Note***\n     This test is not working now due to the HOLD/RELEASE mechanism. Probably needs more\n     code change to get it to work.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiof8.tsc",
    "setup": "tsagnini",
    "flags": {
      "cluster_database": "true",
      "maxinstances": "4",
      "creatdev_file": "tkfgrddef"
    },
    "description": "tsagiof8.tsc - EXADATA single node diskmon kill test\n\nKills diskmon on a single node env\n     - Have a workload doing I/O's\n     - Kill diskmon-ohasd restarts diskmon\n     - ASM and RDBMS instance should reconnect to new diskmon instance.\n     - I/O's should not fail",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiof9.tsc",
    "setup": "tsagnini",
    "flags": {
      "cluster_database": "true",
      "maxinstances": "4",
      "creatdev_file": "tkfgrddef"
    },
    "description": "tsagiof9.tsc - EXADATA single node diskmon kill test\n\nKills diskmon on a single node env\n     - Have a workload doing I/O's. Complete the workload.\n     - Kill diskmon-ohasd restarts diskmon\n     - ASM and RDBMS instance are brought down.\n     - Create new table and make inserts. New ASM/RDBMS instance will be communcating with new diskmon.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiofensuit.tsc",
    "setup": "tsaginit",
    "flags": {
      "cluster_database": "true"
    },
    "description": "tsagiofensuit.tsc - SAGE IO Fencing tests\n\nDriver test for SAGE IO Fencing tests\n\nRun by tkmain_saiofe.tsc (lrgsaiofe)",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiofncd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagiofncd.tsc - Driver test for SAGE cluster fencing test\n\nThis test setups the emulated multi-node env\n\n- setsup emulated multi-node env which has mutiple css\n     - start diskmon on node 1\n     - run RDBMS & ASM instance on node 0\n     - kill css on node 0 (brings down ASM & RDBMS)\n     - check for fencing",
    "platform": null
  },
  {
    "test_name": "tsagiohangreaderr.tsc",
    "setup": "tsagnini",
    "flags": {
      "uniq_dsknames": "all",
      "sage_mirror_mode": "normal"
    },
    "description": "tsagiohangreaderr.tsc - io error + hang + orion test\n\nThe test is to verify the fix for\n     bug 30411293 - confine IO error disk in disk controller hang detection\n\ndescription below, test added in dbconsahangman",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiohng.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagr2def",
      "asm_disable_scrub_strict": "TRUE",
      "dbusr": "'scott/tiger'"
    },
    "description": "tsagiohng.tsc - Simulate Celldisk I/O HANG and ERROR",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiohng2.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagrddef",
      "asm_disable_scrub_strict": "TRUE"
    },
    "description": "tsagiohng2.tsc - Simulate Celldisk I/O HANG and CELLSRV sleep",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagioplv.tsc",
    "setup": "tsaginit",
    "flags": {
      "tmp_port": "^oss_port^",
      "devdef": "datadev",
      "flash2add": "1"
    },
    "description": "tsagioplv.tsc - RDBMS<--> oss compatibility\n\nThis sets up the OSS view and creates griddisks",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiopset.tsc",
    "setup": null,
    "flags": {
      "temp_nowarn": "^tst_nowarn^",
      "sep": "'/'",
      "col": "':'"
    },
    "description": "tsagiopset.tsc - Startup file for exadata interop lrg\n\nSets up the connect strings and other required ORA*TST variables\n     so that a current version exadata can talk to higher version ASM and DB",
    "platform": null
  },
  {
    "test_name": "tsagiopuv.tsc",
    "setup": "tsaginit",
    "flags": {
      "tkcompat_nowarn": "^TST_NOWARN^",
      "oss_port1": "^oss_port^",
      "oss_port2": "^oss_port1^",
      "raw_path1": "^asm_diskstring_dir^",
      "oss_host_port2": "^cellip^:^oss_port2^",
      "asmdisks_created": "2",
      "compatible": "11.2.0.0",
      "cluster_database": "false"
    },
    "description": "tsagiopuv.tsc - RDBMS and exadata compatibility tests\n\nTests 11.2.0.2 rdbms patchset compatibility with 11.2.0.1 cellsrv",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagioreaderr2.tsc",
    "setup": "tsagnini",
    "flags": {
      "uniq_dsknames": "all",
      "oss_auto_manage_disks": "true",
      "sage_mirror_mode": "normal"
    },
    "description": "tsagioreaderr2.tsc - Confine fd read error test\n\nConfinement should tolerate repeated read IO errors on FlashDisk\n     see bug 33369819.\n\ntest to be added in lrgdbconsahangioerror2",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagioreason.tsc",
    "setup": "tsagnini",
    "flags": {
      "SKGXP_UDP_USE_TCB": "true",
      "timestamp": "^tst_tscname^ts.log"
    },
    "description": "tsagioreason.tsc - Test for io reasons.\n\ndump io reasons before load, perform load, dump io reasons after load.\n     confirm certain reasons have advanced.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiorm1.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "cell_with_pmem_cache": "true       ## create pmem cache",
      "maxtime": "3600",
      "nowarn": "^TST_NOWARN^"
    },
    "description": "tsagiorm1.tsc - OSS IORM test # 1\n\nSAGE cellcli IORM tests\n\nUse tmvfile/trmfile/tcpfile/tcellcli/tperl while adding any new test\n   Bug 23168469 has been entered to track the tests commented out for interop\n   veresion (lrgc9saiorm_los).",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiorm2.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagiorm2.tsc - OSS IORM test # 2\n\nSAGE cellcli IORM tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiorm3.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "cell_with_pmem_cache": "true       ## create pmem cache",
      "asm_ausize": "1048576",
      "extent_mgmt": "local",
      "dbusr2": "^dbusr^",
      "workloadNum": "3",
      "workloadRun": "1"
    },
    "description": "tsagiorm3.tsc - OSS IORM test # 3\n\nExadata cellcli IORM tests\n\nUse tmvfile/trmfile/tcpfile/tcellcli/tperl while adding any new test",
    "platform": null
  },
  {
    "test_name": "tsagiormarbtr.tsc",
    "setup": null,
    "flags": {
      "dbid": "0"
    },
    "description": "tsagiormarbtr.tsc - Arbiter unit test to simulate IORM plans",
    "platform": null
  },
  {
    "test_name": "tsagiormcl1.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "extent_mgmt": "local"
    },
    "description": "tsagiormcl1.tsc - SAGE IORM test for Exadata Cloud multi cluster support\n\nIORM test for supporting DB Unique names on Exadata Cloud containing\n     multiple clusters.\n\nUse tmvfile/trmfile/tcpfile/tcellcli/tperl while adding any new test",
    "platform": null
  },
  {
    "test_name": "tsagiormcl2.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "extent_mgmt": "local"
    },
    "description": "tsagiormcl2.tsc - OSS IORM asmcluster test # 2\n\nCellcli tests for asmcluster attribute in IORM dbplan.\n\nUse tmvfile/trmfile/tcpfile/tcellcli/tperl while adding any new test",
    "platform": null
  },
  {
    "test_name": "tsagiormfc.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagiormfc.tsc - OSS IORM FC tests using Orion\n\nExadata IORM flashcache setting tests with DB role using Orion",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiormfcdg.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagiormfcdg.tsc - OSS IORM FC tests using DataGuard\n\nExadata IORM flashcache setting tests with DB role using DataGuard",
    "platform": null
  },
  {
    "test_name": "tsagiormini.tsc",
    "setup": "tsaginit",
    "flags": {
      "format_long_identifier": "true",
      "processes": "200",
      "sessions": "220",
      "creatdev_file": "tkfgrcddef"
    },
    "description": "tsagiormini.tsc - initialization for tsagiorm*",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiormmapping.tsc",
    "setup": null,
    "flags": {
      "check_mode": "excld_vol",
      "dbobj": "DB^dbname^.^dbname^",
      "pdbobj": "DB^dbname^.^dbname^.CDB1_PDB1",
      "log_name": "^tst_tscname^_^check_mode^.log"
    },
    "description": "tsagiormmapping.tsc - check the IORM mapping\n\nThis test has below steps:\n     Step 1: Check plan entry\n     Step 2: Issue workload to CDB1_PDB1\n     Step 3: Check the metrics\n     Step 4: Dump FCGroup and make sure type is non-zero\n     Step 5: Check IORM State Dump\n\nThis is common test script that will run in below setup\n    1- ASM Standalone DBs (CDB and non-CDB)  : lrgsarslv1_nvc_cmainr19500/lrgdbconsaiorm\n    2- Exascale Standalone DBs & Exascale Standalone PDBs\n       (cluster guid is generated for them) without GI - lrgdbcsaiormvault\n    3- ASM DBs (CDB and non-CDB) under GI (ASM scoped security configured)\n       - lrgdbconsaiorm_asmsc, lrgdbconsamtciorm\n    4- Exascale DBs under vault & Exascale PDBs under vault\n       (Cluster guid is set from GI) - lrgdbconsamtciorm_excld?lrgdbcsaiorm1_excld\n    5- Volumes in Exascale - lrgdbcsaexciormvol\n\n   Original Request from Zak:\n    - Common IORM mapping check\n     Confirm the plan entries. Run the following based on the case type\n     above: List vault/List cluster/List database/list volume/List\n     pluggabledatabase\n    - Issue read and write workloads to the DBs/PDBS/Volumes. Use tsagworkload.sql\n      to run some read/write IOs-\n      Dump FC cache groups - make sure FC group is\n      created for the PDBs/ Volumes / non-CDB You can check the entry name, DB\n      ID/Volume ID, and CDB ID from FC group dump. If group is created for them in FC\n      dump and the type is not equal to zero, then we can consider that as successful\n      mapping. If it is possible, we would like to confirm that there is only one\n      group for each PDBs, non-CDB and volume. Note: FC group is only created\n      for PDBs / Volumes / non-CDBs. We cant confirm CDB mapping from FC group dump\n      (PDB mapping should be enough).\n     - Check IORM metric:\n     For DBs, DB_FD_IO_RQ_SM_SEC, DB_IO_RQ_SM_SEC,DB_FD_IO_RQ_LG_SEC,",
    "platform": null
  },
  {
    "test_name": "tsagiormpdblimit.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line"
    },
    "description": "tsagiormpdblimit.tsc - OSS IORM PDB Limit test cases\n\nOSS PDB limit calculation tests",
    "platform": null
  },
  {
    "test_name": "tsagiormplan_vol.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagiormplan_vol.tsc - Test cases for Iorm plan on volume\n\nAdditional: IORM plan tests for blockstore volumes",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagiormpush.tsc",
    "setup": null,
    "flags": {
      "sga_target": "1024M",
      "use_self_tune_sga": "true"
    },
    "description": "tsagiormpush.tsc - validates if intra-db plan is pushed to cell side\n\nDefines an intra db plan for a cdb for all its pdbs and make sure\n     the plan is pushed to cell side.\n\nDefines an intra db plan for a cdb for all its pdbs and make sure\n     the plan is pushed to cell side.",
    "platform": null
  },
  {
    "test_name": "tsagiormspilltest.tsc",
    "setup": null,
    "flags": {
      "sort_retain": "100000",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagiormspilltest.tsc - Verify that TEMP segments spill over to FC",
    "platform": null
  },
  {
    "test_name": "tsagiotimeout.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "mydiskdef",
      "asm_read_cancel": "10000",
      "asm_write_cancel": "20000",
      "asm_cancel_alert_time": "10",
      "log_file": "tsagiotimeout.log"
    },
    "description": "tsagiotimeout.tsc - Testcase for Project#55885 Client-Side Exadata I/O timeout\n\nProject#55885 Client-Side Exadata I/O timeout tests, it includes:\n\n       a)Testcase 1 : test for _asm_read_cancel\n         1). Hang all read IOs in cell to trigger read/write IO cancel on the client side\n         2). Run regular (non-Smart) read/write which takes longer than this value is redirected to\n              a different mirror.\n\n       b)Testcase 2 : test for _asm_write_cancel\n         1). Hang all read and write IOs in cell to trigger read/write IO cancel on the client side\n         2). Run regular (non-Smart) read which takes longer than this value is redirected to a different\n             mirror.\n         3). Run regular (non-Smart) write which takes longer than this value is cancelled, and its disk\n             is pending. Since write cancel still requires some code changes, so only reads will result\n             in a time-out. Writes will simply be held, this is because ASM metadata writes will pause\n             at this time, as well, until the reconnect succeeds, and that should be the expected behavior\n             without the write cancel offline support.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiov.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1",
      "creatdev_file": "tsagddef",
      "OSS_BIN": "^ADE_VIEW_ROOT^/oss/bin"
    },
    "description": "tsagiov.tsc - Tests for IOV\n\nIOV tests for flashlogs",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiovactvdskfail.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "debugcli": "1",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagiovactvdskfail.tsc - Confinement test for activedisk running IOV\n\nConfinement test for activedisk running IOV workload\n\nSteps executed:\n     1. Setup a 3 cell stack\n     2. Run IOV - eds_snap_l10_rw.xml workload in background\n       2.a. Make sure one of the phase is longer - phase 5 in this test\n     3. Run various confinements on active disk.\n       3.a. confinedOnline->confinedOffline->GOOD transition ( evarg2=430 )\n       3.b. confinedOnline->confinedOffline->FAIL transition ( evarg2=438 )\n            - needs reenable\n       3.c. confinedOffline->GOOD transition ( evarg2=433 )\n       3.d. confinedOffline->FAIL transition ( evarg2=431 ) - needs reenable\n     4. Check cell and EGS alerts for health change traces",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagiovactvdskfail2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagiovactvdskfail2.tsc - ExaScale VES disk write IO hang test\n\nSimulate temporary and permanent disk hang on VES griddisk\n     EGS will take disk offline after write is cancelled.\n     If it is temporary hang, EGS will online the disk later.\n     If it is permanent hang, Cellsrv will confine the disk.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagiovdxdfk.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef",
      "dxd": "true",
      "clusterid": "'ASMClusterName'"
    },
    "description": "tsagiovdxdfk.tsc - test for dxd devices to be added in lrgsapmem3\n\nWhile fakeib is running , run iov on dxd devices and check respective ipcdat stats\n\nlrg sets dxd=true for this test. in case of standalone run use\n   oratst -d tsagiovdxdfk.tsc dxd=true",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagiovextstor.tsc",
    "setup": "tsaginit",
    "flags": {
      "max_areas": "5  # max number of areas on each device",
      "gd": "'ext_store_gd0,ext_store_gd1' # extent_store griddisks",
      "duration": "1800  # run IOV for 0.5 hours",
      "threads": "3",
      "offset": "4  # start offset to be 4M aligned",
      "area_sz": "10",
      "creatdev_file": "tsagddef"
    },
    "description": "tsagiovextstor.tsc - IOV test for Extent Store Griddisk\n\nDoes IOV tests for 30 mins on 2 Extent Store GDs",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagipcdatcancel.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagipcdatcancel.tsc - two new fakeIB simulations that trigger the new IO\n                              cancellation code path\n\nPlease see below.\n\nto be added in lrgsafciov_rdma",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagipcdatlib.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagipcdatlib.tsc - test for skgzepclib and skgrlib library\n\nskgzepclib is the data exchange library and its unit\n   test makes sure that data is being sent and received properly.\n   skgrlib is the rdma library and its unit test will make\n   sure that rdma are working as expected.\n\nThis is a unit test for FNDD using skgzepclib and skgrlib",
    "platform": null
  },
  {
    "test_name": "tsagipcdatstre.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrddef"
    },
    "description": "tsagipcdatstre.tsc - IPCDAT stress test\n\nIPCDAT stress test\n\nIPCDAT stress test:\n      ipcdat_stress is designed to measure the performance of RoCE and\n    Persistent Memory harware ? but also runs in an ordinary ADE view. This\n    test will start two processes: a server and a client.\n      server:       ipcdat_stress -A 127.0.0.1 -i 15\n      client:       ipcdat_stress -A 127.0.0.1 127.0.0.1 -n 2\n    When everything works, it will create 2 TCP connections with 2 ipcdat\n    contexts at each end, perform PMemLog-style IO across all of them for 5\n   seconds, and display a set of statistics at the end including a histogram.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagkfod.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagkfod.tsc - Test for kfod\n\nTests kfod \"op=cellconfig\" command",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaglarge_reconf_buffer.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsaglarge_reconf_buffer.tsc - Test cases for handling large reconfig buffer",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsaglatsimu.tsc",
    "setup": "tsaginit",
    "flags": {
      "cell_with_xrmem_cache": "1",
      "cell_with_pmem_cache": "0",
      "asm_disk_def": "tsagrdmadef",
      "trc_inmem_bucket_val": "'false,true'",
      "client_event": "'cellclnt_read_outlier_limit'"
    },
    "description": "tsaglatsimu.tsc - Testing the new parameters for tracing latency outliers\n\nTest covers different combinations of new parameters for outlier tracing event\n     All combinations listed below:\n       1. Write outlier tracing event: libcell.cellclnt_write_outlier_limit\n                transport=skgxp, iotarget=harddisk|flash, trc_inmem_bucket=true|false\n          eg: _cell_client_event=\"libcell.cellclnt_write_outlier_limit latency_threshold=100, transport=skgxp,iotarget=flash, trc_inmem_bucket=false\"\n       2. Log write outlier tracing event: cellclnt_lgwrite_outlier_limit\n                transport=ipcdat, iotarget=xrmem  trc_inmem_bucket=true|false\n                transport=skgxp, iotarget=xrmem|flash|harddisk   trc_inmem_bucket=true|false\n          eg: _cell_client_event=\"libcell.cellclnt_lgwrite_outlier_limit latency_threshold=100, transport=skgxp,iotarget=xrmem, trc_inmem_bucket=false\"\n       3. Read outlier tracing event: libcell.cellclnt_read_outlier_limit\n                transport=ipcdat, iotarget=xrmem  trc_inmem_bucket=true|false\n                transport=skgxp, iotarget=xrmem|flash|harddisk   trc_inmem_bucket=true|false\n          eg: _cell_client_event=\"libcell.cellclnt_read_outlier_limit latency_threshold=100, transport=skgxp,iotarget=harddisk, trc_inmem_bucket=false\"\n     Orareview: https://orareview.us.oracle.com/131846239 (Transaction is being grabbed using this transaction)\n     Owner email: shreyas.udgaonkar@oracle.com\n\ntitle",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaglatsimuunit.tsc",
    "setup": null,
    "flags": null,
    "description": "tsaglatsimuunit.tsc - title\n\nUnit test for new parameters testing in latency outlier tracing event.\n     Parameters:\n     1. client_event    2. transport\n     3. iotarget        4. trc_inmem_bucket\n\ncall this unit test with required parameters or run parent test tsaglatsimu",
    "platform": null
  },
  {
    "test_name": "tsagledinit.tsc",
    "setup": "srdbmsini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true",
      "SAGE_MIRROR_MODE": "normal",
      "DEBUGCLI": "1",
      "connstr": "'sys/knl_test7@inst11 as sysasm'"
    },
    "description": "tsagledinit.tsc - Includes scripts required for DNS LED tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaglogoutndetach.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsaglogoutndetach.tsc - Fails detach attempt if iscsiloggedin.\n\nFails detach attempt if client[s] have active login sessions.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsaglongdbun.tsc",
    "setup": "tsagnini",
    "flags": {
      "db_unique_name": "a123456789012345678901234567890"
    },
    "description": "tsaglongdbun.tsc - test with db_unique_name too long\n\nTest with db_unique_name having 31 chars.\n     cellsrv shouldn't crash in this case.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaglongsql.tsc",
    "setup": null,
    "flags": {
      "run_srdbmsini": "true",
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'"
    },
    "description": "tsaglongsql.tsc - testcase for long-running SQL of cellsrvstat\n\ntestcase for long-running SQL of cellsrvstat\n\ntestcase for long-running SQL of cellsrvstat",
    "platform": null
  },
  {
    "test_name": "tsagloopcrash.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagloopcrash.tsc - recovry test for a long running workload\n\nKeep crash cellsrv/celloflsrv in 2 hours workload",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaglowpowermode.tsc",
    "setup": null,
    "flags": null,
    "description": "tsaglowpowermode.tsc - Tests for Low Power mode features\n\nFunctional tests for Low Power mode feature introduced with\n     Exadata 11M Release.\n     https://confluence.oraclecorp.com/confluence/display/~XIANMING.JIMMY.LUO@ORACLE.COM/MS+Low+Power+Mode+Tests\n\nAdded in lrgrhx11sapwrsave",
    "platform": null
  },
  {
    "test_name": "tsaglowpowerwrkld.tsc",
    "setup": null,
    "flags": null,
    "description": "tsaglowpowerwrkld.tsc - Workload monitoring for Low Power mode\n\nThis file simulates various types of stress workload on a node to\n     verify that low power mode automatically turns off in cases of high workload\n   Workload simulation confluence - https://confluence.oraclecorp.com/confluence/display/~XIANMING.JIMMY.LUO@ORACLE.COM/Workload+Simulation\n     Tests - https://confluence.oraclecorp.com/confluence/pages/viewpage.action?pageId=12157540636\n\nRuns in lrgrhx11sapwrsave",
    "platform": null
  },
  {
    "test_name": "tsaglsscsi.tsc",
    "setup": "tsaginit",
    "flags": {
      "dg1": "8",
      "dg2": "6",
      "dg3": "4",
      "temp": "^sn2^",
      "sn2": "^sn3^",
      "sn1": "^temp^",
      "sn3": "^temp^"
    },
    "description": "tsaglsscsi.tsc - Test drops and recreate lun using storcli\n\ndrops 3 logical devices, recreate them in non increasing order\n      of slot numbers",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaglsvoltest.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsaglsvoltest.tsc - This is an additional test script after bug fix\n     37946606 where we check the latency for lsvolume getting reduced after the\n     previous latency is cached for the same.\n\nThis is an additional test script after bug fix 37946606 where we check the\n     latency for lsvolume reduced after the previous latency is cached for the same.\n     Here are the test steps:\n       1. Create vault and 200 volumes in it.\n       2. Restart bsm service\n       3. Run escli lsvolume --attributes numSnapshots and measure the latency.\n       4. Run the step 3 again.\n       5. Check the latency for second time has been reduced as expected",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsaglw_importance_caching.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0",
      "cell_with_xrmem_cache": "0"
    },
    "description": "tsaglw_importance_caching.tsc - orion test case for LW cachig by importance\n\nTest case for LW caching by importance\n\nTest steps:\n     1. Run the test with LW flush, LW imbalance check, LW hd busy check\n        disabled, fc_free_space set to 0.\n     (1) Run unimportant LW with 64MB region of griddisk, nearly all should be\n         absorbed.\n     (2) Run important LW with 64MB region of griddisk, nearly all should be\n         absorbed.\n     2. Run the test with LW flush, LW imbalance check, LW hd busy check\n        disabled, fc_free_space set to 100.\n     (1) Run unimportant LW with 64MB region of griddisk, all should be\n         rejected.\n     (2) Run important LW with 64MB region of griddisk, nearly all should be\n         absorbed.\n     3. Run the test with LW flush, LW imbalance check, LW hd busy check\n        disabled, fc_free_space set to 50.\n     (1) Run OLTP write workload with 128MB region of griddisk, all should be\n         absorbed by flashcache and takes 40% of flash cache space.\n     (1) Run unimportant LW with 64MB region of griddisk, part of it should be\n         rejected.\n     (2) Run important LW with 64MB region of griddisk, nearly all should be\n         absorbed.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmacro.tsc",
    "setup": null,
    "flags": {
      "tsagmacros_defined": "1"
    },
    "description": "tsagmacro.tsc - Macro definitions for SAGE tests\n\nSet up all macro definitions for SAGE/OSS/EXASCALE\n\nThis script is invoked as part of tkstart and need not be included by tests",
    "platform": null
  },
  {
    "test_name": "tsagmacrosusg.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmacrosusg.tsc - Usage for new macros\n\nThis test has examples to show the macros usages\n\nExamples of macros like\n       1. tcellcli\n       2. tsh",
    "platform": null
  },
  {
    "test_name": "tsagmcellinst.tsc",
    "setup": null,
    "flags": {
      "oldinst": "1",
      "ossinstlog": "ossinst^ossinstnum^_^ossinst^.lst",
      "ossp": "^ossinst^",
      "oss_port": "^^ossp^",
      "oss_devdir": "^t_work_raw^^s^^^ossp^",
      "ossconf": "^CELL_TWORK2^",
      "rs_port": "^^ossp^",
      "OSS_BIN": "^OSS_HOME^/bin"
    },
    "description": "tsagmcellinst.tsc - switch OSS context in multiple cell views\n\nswitch OSS context in multiple cell views\n\nswitch OSS context in multiple cell views",
    "platform": null
  },
  {
    "test_name": "tsagmcellswtview.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmcellswtview.tsc - Switch view context between OSS views and rdbms view\n\nSwitches context to OSS/RDBMS view during 3-view with 2 different cell verions\n     by changing some ENV to point to the OSS/RDBMS view.\n     This helps to switch context to OSS view for performing cell side\n     operations when current context is RDBMS.\n\nFor changing or restoring context to / from OSS view\n     include tsagmcellswtview switch/restore\n     cell=1/2, means switch to cell 1 or cell2\n     dbnode=1/2, means switch to rdbms view 1/2 from oss view",
    "platform": null
  },
  {
    "test_name": "tsagmcshort.tsc",
    "setup": null,
    "flags": {
      "dbfile_dest": "'+datafile'"
    },
    "description": "tsagmcshort.tsc - Short Rregress test to run in multiple cell version\n\nShort Rregress test to run in multiple cell version\n\nShort Rregress test to run in multiple cell version",
    "platform": null
  },
  {
    "test_name": "tsagmdbalert.tsc",
    "setup": null,
    "flags": {
      "sysdba": "'sys/knl_test7 as sysdba'",
      "tablename": "tb_mdb_msalert",
      "log_file": "tsagmdbmsalert.log"
    },
    "description": "tsagmdbalert.tsc - list alerthistory for celloflsrv\n\nWe are expecting a ORA-7445 (or ORA-600) for an oflloadsrv, do a 'list alert history'\n         in cellcli to see if the expected ORA-7445|600",
    "platform": null
  },
  {
    "test_name": "tsagmdbbufevic.tsc",
    "setup": null,
    "flags": {
      "simu_count": "500000",
      "loop_num": "1",
      "db_offloadgrp": "^sys_group_name_11233^",
      "cell_tb_tmp": "cell_stat2"
    },
    "description": "tsagmdbbufevic.tsc - buffer eviction tests for offload server\n\nAdd buffer eviction tests for offload server into samdbshort",
    "platform": null
  },
  {
    "test_name": "tsagmdbbugsuite.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmdbbugsuite.tsc - run the mdb bug suite",
    "platform": null
  },
  {
    "test_name": "tsagmdbcap.tsc",
    "setup": null,
    "flags": {
      "loop_num": "2"
    },
    "description": "tsagmdbcap.tsc - new cellsrv capabilities on MDB project\n\nThe approximate capability resolution test, one can do the following:\n     1. CellCli alter cell events=\"cellsrv_simevent[STALE_CAPVERSION] frequency=1\";\n     2. Start a database and check X$KCFISTCAP, and the values of MINMAX_INDICATOR\n        should be 2 and 3\n     3. CellCli alter cell events=\"cellsrv_simevent[STALE_CAPVERSION] off\";\n     4. Wait for 5 minutes\n     5. Check X$KCFISTCAP again and the values of  MINMAX_INDICATOR should be now\n        0 and 1",
    "platform": null
  },
  {
    "test_name": "tsagmdbcc.tsc",
    "setup": null,
    "flags": {
      "cdb_lv1": "^cdb^",
      "cdb_lv2": "^cdb^",
      "loop_num": "1",
      "cur_cell": "1",
      "cur_cdblv": "cdb_lv^i^",
      "cur_version": "version^i^",
      "ref_log": "'tsagmdbcc_db2.log'",
      "db_offloadgrp": "^sys_group_name_11233^",
      "log_file": "tsagmdbcc_db^i^.log"
    },
    "description": "tsagmdbcc.tsc - Basic CC1/CC2 tests on MDB lrgs\n\nBasic CC1/CC2 tests on MDB lrgs\n       1). _enable_columnar_cache=33 (CC1 - EHCC)\n       2). _enable_columnar_cache=65 (CC2 -> compress for query)\n       3). _enable_columnar_cache=97 (CC2 -> compress for capacity)\n\nFor 12.2, we use ddl to define the columar cache level,\n      the oltp and normal tabel are using cc2(compress for capacity) by default\n      for CC1, we should use \"no cellmemory\"\n      for CC2: we use below to create the CC2 tables\n        cellmemory memcompress for capacity\n        cellmemory memcompress for query\n\n    For 12.1, we have to use _enable_columnar_cache to change columar cache level\n      the oltp and normal tabel are not supported\n      for CC1, should set _enable_columnar_cache=33\n      for CC2: should set _enable_columnar_cache=65/97",
    "platform": null
  },
  {
    "test_name": "tsagmdbcln.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmdbcln.tsc - cleanup during mdb tests",
    "platform": null
  },
  {
    "test_name": "tsagmdbhybridccpersist.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmdbhybridccpersist.tsc - CC persistence test for MDB Hybrid mode\n\nTests cc persistence feature with active db workload in MDB Hybrid mode",
    "platform": null
  },
  {
    "test_name": "tsagmdbhybridresync.tsc",
    "setup": null,
    "flags": {
      "devdir1": "^T_WORK^/raw^oss_port^",
      "devdir2": "^T_WORK^/raw^oss_port2^",
      "devdir3": "^T_WORK^/raw^oss_port3^"
    },
    "description": "tsagmdbhybridresync.tsc - Resync test for Hybrid MDB LRGs\n\nResync test for Hybrid MDB LRGs\n\nResync test for Hybrid MDB LRGs\n     DB workload is running in parallel on both the DBs - on Exascale\n     and ASM/Exadata during the whole test.\n\n     Has three sub tests:\n     (1) tsagmdbhybridresync_1 :\n         In a loop of 5,\n         alter cell shutdown services all + wait for alerts\n               Let DB Workload run for 5 mins\n         alert cell startup services all + wait for alerts\n\n     (2) tsagmdbhybridresync_2 :\n         kill -9 cellsrv1 + wait for alerts + wait for 5 mins\n         kill -9 cellsrv2 + wait for alerts + wait for 5 mins\n         kill -9 cellsrv3 + wait for alerts + wait for 5 mins\n\n     (3) tsagmdbhybridresync_3 :\n         In a loop of 5,\n         alter cell restart services cellsrv + wait for alerts\n                          + wait for 5 mins",
    "platform": null
  },
  {
    "test_name": "tsagmdbhybridworkload.tsc",
    "setup": null,
    "flags": {
      "maxtimeout": "9000"
    },
    "description": "tsagmdbhybridworkload.tsc - DB Workload for MDB Hybrid LRGs\n\nDB Workload for MDB Hybrid LRGs\n\nDB Workload for MDB Hybrid LRGs\n     Sets up 2 different DB Workloads for\n     ExaScale and ASM respectively",
    "platform": null
  },
  {
    "test_name": "tsagmdbiorm.tsc",
    "setup": null,
    "flags": {
      "log_file": "tsagmdbiorm^db_num^.log",
      "my_plan1": "mdbiormplan1",
      "my_plan2": "mdbiormplan2",
      "ms_log_home": "^T_WORK^^s^mslogdir.sav"
    },
    "description": "tsagmdbiorm.tsc - IORM tests for Multiple database version project\n\n< DESCRIPTION >",
    "platform": null
  },
  {
    "test_name": "tsagmdbiormqmini.tsc",
    "setup": null,
    "flags": {
      "iormreflog": "tsagmdbiormini_mcell.log"
    },
    "description": "tsagmdbiormqmini.tsc - Init testing env. to run IORM and QM for MDB short regress\n\n1. Create iormplan on cellsrv side",
    "platform": null
  },
  {
    "test_name": "tsagmdbmm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmdbmm.tsc - Run the mdb test cases under mirror mode\n\n< DESCRIPTION >",
    "platform": null
  },
  {
    "test_name": "tsagmdbmutex.tsc",
    "setup": "tsagnini",
    "flags": {
      "format_long_identifier": "true"
    },
    "description": "tsagmdbmutex.tsc - To test event command\n\nTest the following command:\n      alter cell events = \"immediate cellsrv.cellofl_test('kill_ipc',<groupname>,param)\";\n      When parameter is 0/3//6 which means critial crash, then cellsrv would be restarted;\n      else only cell offload server is restarted.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmdbpdbcc2.tsc",
    "setup": null,
    "flags": {
      "disabled_pdb": "1"
    },
    "description": "tsagmdbpdbcc2.tsc - Check Per-PDB enablement of columnar cache",
    "platform": null
  },
  {
    "test_name": "tsagmdbqm.tsc",
    "setup": null,
    "flags": {
      "cdb_lv1": "^cdb^",
      "cdb_lv2": "^cdb^",
      "loop_num": "2",
      "log_file": "tsagmdbqmini.log"
    },
    "description": "tsagmdbqm.tsc - MDB QM tests\n\nThis tsc will cover the below kinds of quaratnines on multiple views for MDB project:\n       1. Database\n       2. SQL PLAN\n       3. Disk Region",
    "platform": null
  },
  {
    "test_name": "tsagmdbrergstr.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagmdbrergstr.tsc - Exadata test reregistration pacakage\n\nReregistration of celloflsrv package\n\nTest celloflsrv reregistration",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmdbrsdeallo.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmdbrsdeallo.tsc - Resource deallocation test\n\nCheck memory consumption when offloadgroup is/not in use\n     1.  Create one offload group in cell 1\n     2.  Run two workloads in two databases\n     3.  Set workload is run by created offload group\n     4.  Check memory consumption for two cells\n     5.  When offloadgroup are shutdown the value should be 0\n\nThere is no paticular parameter to indicate the memory consumption of an offloadgroup.\n     So we caculate the memory stat of all groups in v$cell_stat.\n      1. total_osmem_all_groups in cellsrv memory node;\n      2. total_osmem_all_groups in sga memory type node;\n      3. total_osmem_all_groups in oflgroup memory type node.\n      4.  when all groups are shutdown, the node oflgroup would be removed in v$cell_stat.",
    "platform": null
  },
  {
    "test_name": "tsagmdbshort.tsc",
    "setup": null,
    "flags": {
      "format_long_identifier": "true",
      "tmp_datafile": "datafile_dest^db_num^",
      "db_vault": "'@DBRDBMSV1'",
      "cell_tb_tmp": "cell_stat2",
      "db_vault2": "'@DBRDBMSV2'",
      "sys_group_name": "^sys_group_name_11233^",
      "cdb_lv1": "^cdb^",
      "cdb_lv2": "^cdb^",
      "cur_cdblv": "cdb_lv^db_num^",
      "cur_version": "version^db_num^",
      "log_file": "tsagmdbshort^db_num^.log",
      "level1": "'compress for query low'",
      "level2": "'compress for query high'",
      "level3": "'compress for archive low'",
      "level4": "'compress for archive high'",
      "tmp_lvl": "level^lvl^",
      "level": "^^tmp_lvl^",
      "loop_num": "10",
      "tablespace_name": "tbs_test_^db_num^",
      "data_file": "^^tmp_datafile^'/mdb'^db_num^'_df'^i^'.f'",
      "backup_dest": "^^tmp_datafile^",
      "pfile_path": "^T_WORK^^s^t_init1.ora",
      "temprdbmswork": "t_work_rdbms^db_num^",
      "ofl1": "TRUE",
      "ofl2": "FALSE",
      "fp1": "TRUE",
      "fp2": "FALSE",
      "oflflag": "ofl^i^",
      "fpflag": "fp^j^"
    },
    "description": "tsagmdbshort.tsc - short regress tests for Multiple database version project\n\nshort regress includes the following basic tests on Exadata\n\n      Test 1  Storage Index\n      Test 2  Smart Scan:\n           2a -- Bloom Filter\n           2b -- NON-EHCC\n           2c -- EHCC\n           2d -- Data Mining\n      Test 3  Fast file creation\n      Test 4  IORM\n      Test 5  Commit Cache\n      Test 6  Push/list Event\n      Test 7  Incremental backup test\n      Test 8. TopCpu Test\nTest 9. Basic query cover bug 18339424\n\nFor cdb mode, we create two database versions, the name of rdbms_version1 is\n       rdbmsv1.cdb1_pdb1, and name of rdbms_version2 is rdbmsv2.cdb1_pdb1\n     For connect service, we should use cdb1_pdb1 and cdb2_pdb1 in oss_view\n     But when you run sql from db view, all of them should use cdb1_pdb1",
    "platform": null
  },
  {
    "test_name": "tsagmdbsi.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagmdbsi.tsc - A basic SI for multidb\n\nIt is a part of SAGE_STORAGE_INDEX and should be run on the OSS multidb branch.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmdbsimevent.tsc",
    "setup": null,
    "flags": {
      "file_name": "alert_^oss_port^.log",
      "log_file": "tsagmdbsimuevent.log"
    },
    "description": "tsagmdbsimevent.tsc - Simulation events for MDB\n\nTest Case 1. Simulation Event OCL_IPC_GET_FAIL\n      Test Case 2. Simulation Event OCL_IPC_SEND_FAIL\n      Test Case 3. Simulation Event OFLSRV_OAL_FAIL\n      Test Case 4. Simulation Event OCL_IPC_MSG_DIE_INQ\n      Test Case 5. Simulation Event OCL_IPC_MSG_DIE_DQD\n      Test Case 6. Simulation Event OCL_OFLGRP_OPEN_FAIL\n\n     suite = mdb3 : run Test Case 1-2\n     suite = mdb4 : run Test Case 3-4\n     suite = mdb5 : run Test Case 5-6\n\n   For Test Case 1-5, we run different kinds of workloads to exercise different tags under each simulation\n   event, but there is not SYSRPC test for Test Case 3-5.\n\n       section = SI :  Run smart scan workload to exercise tags sicheck,si_write_population,\n                       fpblock_process,ofl_group_stats,ofldisk_close\n           The output file is tsagmdbsimuevent[1-5]_1.log\n\n       section = FAST_FILE_CREATION :  Run Smart File Creation workload to exercise tag fcre_kuty\n           The output file is tsagmdbsimuevent[1-5]_2.log\n\n       section = IORM :  Run the IORM workoad to exercise tags iorm_kuty, free_memory\n           The output file is tsagmdbsimuevent[1-5]_3.log\n\n       section = CACHE_COMMIT :  Run Cache Commit workoad to exercise tag push_commit_cache\n           The output file is tsagmdbsimuevent[1-5]_4.log\n\n       section = PUSH_EVENT :  Run Push and List event workoads to exercise\n                                tags push_diagevent, list_diagevent\n           The output file is tsagmdbsimuevent[1-5]_5.log\n\n       section = INCREMENTAL_BACKUP :  Run incremental backup workoad to exercise tag bkflib_kuty\n           The output file is tsagmdbsimuevent[1-5]_6.log\n\n       section = PRED_BUFFER_EVICTION :  Run Buffer eviction workoad to exercise tag pred_buf_eviction\n           The output file is tsagmdbsimuevent[1-5]_7.log\n\n       section = OFL_SNAPSHOT :  Run offload group snapshot workoad to exercise tag ofl_group_snapshot\n           The output file is tsagmdbsimuevent[1-5]_8.log\n\n       section = EVENT_REG_LOG :  Run Event registry and Event Log workoad to exercise\n                                  tags ofl_group_event_reg, ofl_group_event_log\n           The output file is tsagmdbsimuevent[1-5]_9.log\n\n       section = OFL_TOPCPUSQL :  Run offload group top cpusql workoad to exercise tag ofl_group_topcpusql\n           The output file is tsagmdbsimuevent[1-5]_10.log\n\n       section = SYSRPC :  Run the IORM workoad to exercise tag sysrpc, we just cover this test under event\n                           OCL_IPC_GET_FAIL and OCL_IPC_SEND_FAIL\n           The output file is tsagmdbsimuevent[1-2]_11.log\n\n   For Test Case 6, we run different kinds of workloads to exercise different tags under each\n  simulation event\n\n       section = SI1 :  Run smart scan workload to exercise tags startup,process_dbinfo\n                        preddisk_init and ofl_group_stats\n           The output file is tsagmdbsimuevent6_1.log\n#\n       section = IORM :  Run the IORM workoad to exercise tags iorm_kuty, free_memory\n           The output file is tsagmdbsimuevent6_2.log\n\n       section = CACHE_COMMIT :  Run Cache Commit workoad to exercise tag push_commit_cache\n           The output file is tsagmdbsimuevent6_3.log\n\n       section = PUSH_EVENT :  Run Push and List event workoads to exercise\n                                tags push_diagevent, list_diagevent\n           The output file is tsagmdbsimuevent6_4.log\n\n       section = OFL_SNAPSHOT :  Run offload group snapshot workoad to exercise tag ofl_group_snapshot\n           The output file is tsagmdbsimuevent6_5.log\n\n       section = EVENT_REG_LOG :  Run Event registry and Event Log workoad to exercise\n                                  tags ofl_group_event_reg, ofl_group_event_log\n           The output file is tsagmdbsimuevent6_6.log\n\n       section = OFL_TOPCPUSQL :  Run offload group top cpusql workoad to exercise tag ofl_group_topcpusql\n           The output file is tsagmdbsimuevent6_7.log\n\n       section = SYSRPC :  Run the IORM workoad to exercise tag sysrpc\n           The output file is tsagmdbsimuevent6_8.log",
    "platform": null
  },
  {
    "test_name": "tsagmdbsipersist.tsc",
    "setup": null,
    "flags": {
      "cur_cdblv": "cdb_lv^db_num^"
    },
    "description": "tsagmdbsipersist.tsc - SI persistence tests for multiple database versions\n\nSI persistence tests for multiple database versions\n\nSI persistence tests for multiple database versions",
    "platform": null
  },
  {
    "test_name": "tsagmdbsipur.tsc",
    "setup": null,
    "flags": {
      "log_file": "tsagmdbsipurge.log"
    },
    "description": "tsagsipurge.tsc - Run SI purge on n-view for mdb\n\n< DESCRIPTION >",
    "platform": null
  },
  {
    "test_name": "tsagmdbsuite.tsc",
    "setup": null,
    "flags": {
      "temp_nowarn": "^tst_nowarn^",
      "dot": "'.'",
      "tsagmdbsuite": "^rtest^",
      "mdb_init_script": "'tsagmdbiormqmini.tsc'",
      "mdb_post_script": "'tsagmdbqm.tsc'",
      "tsnum": "0",
      "mdbtesti": "mdbtest^tsnum^",
      "unit_test": "^^mdbtesti^'.tsc'"
    },
    "description": "tsagmdbsuite.tsc - The simple test framework to run multiple database version tests\n\nThis script is used to run mdb unit tests, the usage is :\n\n        echo > cell_script.tsc\n             > ....\n        endecho\n        let mdb_init_script 't_work:cell_script.tsc'\n        let tsagmdbsuite 'tsagmdbsi,tsagmdbqm'\n        let mdbclean 1\n        let rdbms_version2 RDBMS_MAIN_RELEASE\n        runtest tsagmdbsuite\n\n     Please note: tsagmdbsi.tsc and tsagmdbqm.tsc should be created in t_source area by you,\n     and the framework will fork to run them on database instances with different version\n\n     mdb_init_script - This script that you defined will be run before running test cases concurrently\n                        in tsagmdbsuite\n     mdb_post_script - This script that you defined will be run at the end of tsagmdbsuite\n     tsagmdbsuite   - define the user testing file names, such as 'tsagmdbsi,tsagmdbqm,..'\n     rdbms_version1 - the default value is : RDBMS_MAIN_RELEASE\n     rdbms_version2 - the default value is :  RDBMS_11.2.0.4.0_RELEASE\n     mdbclean       - if mdbclean=1, it will clean all test env",
    "platform": null
  },
  {
    "test_name": "tsagmdbupgshort.tsc",
    "setup": null,
    "flags": {
      "format_long_identifier": "true",
      "tmp_datafile": "datafile_dest^db_num^",
      "cell_tb_tmp": "cell_stat2",
      "sys_group_name": "^sys_group_name_11233^",
      "log_file": "tsagmdbcc^upgrade_prefix^.log",
      "level1": "'compress for query low'",
      "level2": "'compress for query high'",
      "level3": "'compress for archive low'",
      "level4": "'compress for archive high'",
      "tmp_lvl": "level^lvl^",
      "level": "^^tmp_lvl^",
      "loop_num": "10",
      "tablespace_name": "tbs_test_^db_num^",
      "data_file": "^^tmp_datafile^'/mdb'^db_num^'_df'^i^'.f'",
      "ref_log": "'tsagmdbcc_12102.log'",
      "db_offloadgrp": "^sys_group_name_11233^"
    },
    "description": "tsagmdbupgshort.tsc - short regress tests for Multiple database upgrade\n\nshort regress includes the following basic tests on Exadata\n\n      Test 1  Storage Index\n      Test 2  Smart Scan:\n           2a -- Bloom Filter\n           2b -- NON-EHCC\n           2c -- EHCC\n           2d -- Data Mining\n      Test 3  Fast file creation\n      Test 4  IORM\n      Test 5  Commit Cache\n      Test 6  Columnar cache",
    "platform": null
  },
  {
    "test_name": "tsagmdbwkld.tsc",
    "setup": null,
    "flags": {
      "loop_num": "3",
      "tablespace_name": "tbs_test",
      "data_file": "^^tmp_datafile^'+datafile/mdb_df'^i^'.f'",
      "dbvar": "dbnm^i^",
      "my_plan1": "mdbiormplan1",
      "my_plan2": "mdbiormplan2"
    },
    "description": "tsagmdbwkld.tsc - Run different workloads to exercise different features\n     such as SI, IORM,..",
    "platform": null
  },
  {
    "test_name": "tsagmddump.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagmddump.tsc - cellutil utility\n\ntests for cellutil utility",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmdsubver.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmdsubver.tsc - Test for Metadata Subversion\n\nMetadata Subversion Test",
    "platform": null
  },
  {
    "test_name": "tsagmemdumpcelltest.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmemdumpcelltest.tsc - Runs celltest\n\nRuns celltest on trace file with and without mem dump",
    "platform": null
  },
  {
    "test_name": "tsagmemleak_dumpmem.tsc",
    "setup": null,
    "flags": {
      "dump_sufix": "start"
    },
    "description": "tsagmemleak_dumpmem.tsc - dump SGA/PGA/FSA for cellsrv/cellofsrv/exascale stacks\n\ndump SGA/PGA/FSA for cellsrv/cellofsrv/exascale stacks\n\ndump SGA/PGA/FSA for cellsrv/cellofsrv/exascale stacks",
    "platform": null
  },
  {
    "test_name": "tsagmemleakloop.tsc",
    "setup": null,
    "flags": {
      "loop_cnt": "10"
    },
    "description": "tsagmemleakloop.tsc - memleak loop smart scan tests including tpc-h, tpc-ds, ssb\n\nmemleak loop smart scan tests including tpc-h, tpc-ds, ssb\n\nmemleak loop smart scan tests including tpc-h, tpc-ds, ssb",
    "platform": null
  },
  {
    "test_name": "tsagmemleakstart.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmemleakstart.tsc - start script of memory leak\n\nstart script of memory leak",
    "platform": null
  },
  {
    "test_name": "tsagmemleakstop.tsc",
    "setup": null,
    "flags": {
      "hpcntr": "0"
    },
    "description": "tsagmemleakstop.tsc - memory leak end  script\n\nmemory leak end script",
    "platform": null
  },
  {
    "test_name": "tsagmemstats.tsc",
    "setup": null,
    "flags": {
      "loop_num": "2",
      "tbs_name": "tbs_6",
      "tbs_datafile": "'+datafile1'",
      "db_offloadgrp": "^sys_group_name_11233^",
      "db_offloadgrp_param": "'compatible='^tk_parsearg_force_compatible^"
    },
    "description": "tsagmemstats.tsc - memory thresholds testcases for mdb project\n\nAdd below tests about  memory thresholds:\n        Testcase 1:  Simulate low-mem threshold failures in celloflsrv\n        Testcase 2:  Simulate no-mem threshold failures in celloflsrv\n        Testcase 3:  Test celloflsrv top heap garbage collection",
    "platform": null
  },
  {
    "test_name": "tsagmemthres.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "creatdev_file": "tsagrddef",
      "nflint": "1",
      "redund": "external",
      "oss_testing": "1"
    },
    "description": "tsagmemthres.tsc - test cases for Simulate low-mem threshold failures in# cellsrv",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmetadatacrash.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagmetadatacrash.tsc - test for celldisk metadata persistence\n\nin the test below.\n\ntest added in lrgdbcsafastermetadata",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmetdata.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagmetdata.tsc - Celldisk metadata test\n\nCorrupt primary and secondary metadata of celldisk and check the behavior\n\n     _cell_primmd_badsec_dev=/dev/sdd       ---> primary metadata\n     _cell_secmd_badsec_dev=/dev/sde        ---> secondary metadata\n     _cell_undomd_badsec_dev=/dev/sdd       ---> undo metadata",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmetdata2.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagmetdata2.tsc - metadata corruption tests\n\nThe test corrupts primary and secondary metadata regions and try to recover it",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmetriccurrent.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagmetriccurrent.tsc - Test for LIST METRICCURRENT\n\nThis tests the list metriccurrent using cellcli",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmetrichistory1.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagmtrc2.tsc - Test for LIST METRICHISTORY\n\nThis tests the list metrichistory using cellcli",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmetricinterval.tsc",
    "setup": "srdbmsini",
    "flags": {
      "file_dest": "'+'datafile"
    },
    "description": "tsagmetricinterval.tsc - cell metric poll test\n\nThis test verifies the new feature for changing the cell metric poll.\n     Test checks the metric collection by setting the following values:\n     Negative test: -10, 0\n     Positive test: 10, 30, 59, 100\n     Test confirms there are no error comming while collecting the metrics and number of metrics is as expected.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmetricstream.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1"
    },
    "description": "tsagmetricstream.tsc - Test for the metric streaming functionality\n                            in cells\n\nTest for the metric streaming functionality in cells. Steps:\n     (Initially restart services on the cell)\n\n      1) Metric attributes initialization\n      2) Modifying \"Enabled\" metricdefiniton\n      3) Modifying \"Disabled\" metricdefiniton\n      4) Attempt to modify \"Disallowed\" metricdefiniton\n      5) Altering Stream and Collector Intervals\n      6) Adding Metric Stream tags\n      7) Get metrics from REST Endpoint of cell with metric_collector user\n      8) Get metrics from list metricstream command on cell\n      9) Verify the number of metrics on REST Endpoint and on cell\n     10) Verify that timestamps of metrics are unique\n     11) Drop Existing Users and Roles\n     12) Add CellDiag role and verify allocation of metric stream privileges\n     13) Verify upload to metric streaming database: Success msg in ms-odl.trc\n     14) Verify data packet exchange to http server using tcpdump\n     15) Altering metric stream endpoints\n     16) Verify file persistence in pending area for fake endpoint\n     17) Verify support for multiple endpoints",
    "platform": null
  },
  {
    "test_name": "tsagmetricstream_db.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1"
    },
    "description": "tsagmetricstream_db.tsc - Test for the metric streaming functionality\n                               in DBNodes\n\nTest for the metric streaming functionality in DBNodes. Steps:\n     Initially, restart all services on DBNode\n\n      1) Metric attributes initialization\n      2) Modifying \"Enabled\" metricdefiniton\n      3) Modifying \"Disabled\" metricdefiniton\n      4) Altering Stream and Collector Intervals\n      5) Adding Metric Stream tags\n      6) Get metrics from REST Endpoint of DBNode with metric_collector user\n      7) Get metrics from list metricstream command on DBNode\n      8) Verify the number of metrics on REST Endpoint and on DBNode\n      9) Verify that timestamps of metrics are unique\n     10) Drop Existing Users and Roles\n     11) Add CellDiag role and verify allocation of metric stream privileges\n     12) Verify upload to metric streaming database: Success msg in ms-odl.trc\n     13) Altering metric stream endpoints\n     14) Verify data packet exchange to http server using tcpdump\n     15) Verify file persistence in pending area for fake endpoint\n     16) Verify support for multiple endpoints",
    "platform": null
  },
  {
    "test_name": "tsagmigrateids.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmigrateids.tsc - Test for migrate_ids.sh\n\nFunctional Test for migrate_ids.sh script",
    "platform": null
  },
  {
    "test_name": "tsagmiptst.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagrddef",
      "db_block_size": "8192",
      "disk": "datafile0"
    },
    "description": "tsagmiptst.tsc - alters the management IP address of the cell and checks whether the change is detectable with kfod",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmixeddg.tsc",
    "setup": "tsagnini",
    "flags": {
      "nflint": "0",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagmixeddg.tsc - table on diff DGs with smart_scan_capable=true/false\n\nCreate 2 diskgroups one with smart_scan_capable set ,other without it.\n     Have a tablespace on each of these diskgroups.\n     Create a table with one partition each on such tablespaces.\n     Querying the table should succeed.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmnset.tsc",
    "setup": "tsaginit",
    "flags": {
      "nowarn": "^TST_NOWARN^",
      "orcl_sid_temp": "^ORACLE_SID^",
      "vault_log": "^vault_db^",
      "db_dest": "EXASCALE",
      "PWFILE_ON_EXC": "false",
      "creatdev_file": "min_ex_dbsz",
      "MAX_INSTANCE": "2",
      "asm_ausize": "65536",
      "maxinstances": "^MAX_INSTANCE^",
      "test_instances": "^MAX_INSTANCE^",
      "dbs_dir": "^asmprefix^/^asm_dbs_dir^",
      "atmp_dir": "^asmprefix^/^asm_tmp_dir^",
      "tkmp_dir": "^asmprefix^/^asm_tkmp_dir^",
      "otemp": "asm_instance^m^",
      "oinst": "^^otemp^",
      "ainst": "asm_instance^j^",
      "nd": "0"
    },
    "description": "tsagmnset.tsc - Exadata emulated multi-node setup file\n\nSets up ASM and RDBMS on emulated multi-node environment\n\nClone of tclsa_asm_env.tsc",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmonitor_error.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmonitor_error.tsc - driver script to start error monitor\n                             on lrgs using tsagexastackup\n\nThis is a driver script which checks for current lrg in\n     specified lrg list. If found, it runs tsagerrmonitor.sh\n     which searches for specific errors in specified trace\n     file. If error is found, a state dump is taken of the\n     given services.",
    "platform": null
  },
  {
    "test_name": "tsagmount.tsc",
    "setup": "srdbmsini",
    "flags": {
      "OSS_TESTING": "1"
    },
    "description": "tsagmount.tsc - test for bug 7460216\n\ntest for bug 7460216",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmpkqm.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagmpkqm.tsc - New quarantine feature for memory protection keys\n\nNew quarantine feature for memory protection keys\n\nAdding a new test about quarantine feature for memory protection keys (mpk) to\n     prevent the cellsrv from being kept down. The feature works\n     by keeping track of how many mpk crashes there have been and turning off mpk once there\n     have been too many. A new simulation event will cause an mpk crash:\n\n     alter cell events=\"cellsrv_simevent[PKEY_RANDOM_CRASH] err_type=internal,frequency=1,count=5¡±\n\n     It includes:\n\n     1. Set _cell_mpk_quarantine_threshold to default value 3, after cellsrv crashed for three times,\n        the event can't crash cellsrv in the fourth time\n\n     2. Set _cell_mpk_quarantine_threshold to default value 2, after cellsrv crashed for two times,\n        the event can't crash cellsrv in the third time\n\n     3. Add corruption test by corrupting $OSSCONF/cellsrv_mpk_quarantine.state, then MPK will be disabled on\n        the next cellsrv startup. Then the simulation event should have no effect.",
    "platform": null
  },
  {
    "test_name": "tsagmsbugsuit.tsc",
    "setup": null,
    "flags": {
      "TZ": "PST8",
      "asm_ausize": "1048576"
    },
    "description": "tsagmsbugsuit.tsc - MS specific bug test suite\n\nPlease DO NOT add any Smart Scan related bug test here.",
    "platform": null
  },
  {
    "test_name": "tsagmscon1.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1",
      "timestamp": "^tst_tscname^ts.log"
    },
    "description": "tsagcon1.tsc - MS concurrency test #1\n\nConfirm various commands to MS can be run concurrently.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmscon1_a.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcon1_a.tsc - MS concurrency test #1\n\nConfirm various commands to MS can be run concurrently.",
    "platform": null
  },
  {
    "test_name": "tsagmscon1_b.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcon1_b.tsc - MS concurrency test #1\n\nConfirm various commands to MS can be run concurrently.",
    "platform": null
  },
  {
    "test_name": "tsagmscon2.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1",
      "timestamp": "^tst_tscname^ts.log"
    },
    "description": "tsagcon1.tsc - MS concurrency test #2\n\nConfirm commands started by seconds MS will not commence until\n   commands from first MS have completed.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmscon2_a.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcon2_a.tsc - MS concurrency test #2 - helper\n\nConfirm commands started by seconds MS will not commence until\n   commands from first MS have completed.",
    "platform": null
  },
  {
    "test_name": "tsagmscon2_b.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagcon2_b.tsc - MS concurrency test #2 - helper\n\nConfirm commands started by seconds MS will not commence until\n   commands from first MS have completed.",
    "platform": null
  },
  {
    "test_name": "tsagmserssync.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmserssync.tsc - Checks ms-ers syncing\n\nSet ip with mask like 172.18.2.100/25, restart ms for\n     ms-ers syncing and later check that mask is removed in\n     alerts",
    "platform": null
  },
  {
    "test_name": "tsagmsunittests.tsc",
    "setup": null,
    "flags": {
      "ORIG_JAVA_HOME": "^JAVA_HOME^",
      "JAVA_HOME": "^ORIG_JAVA_HOME^"
    },
    "description": "tsagmsunittests.tsc - junit tests for MS\n\njunit tests for MS",
    "platform": null
  },
  {
    "test_name": "tsagmtcacc.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^.log"
    },
    "description": "tsagmtcacc.tsc - Use Cellcli commands to add negative tests for ASMCLUSTER client",
    "platform": null
  },
  {
    "test_name": "tsagmtcadgtest.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmtcadgtest.tsc - Run tests in Exascale ADG across clusters\n\nRun tests in Exascale ADG across clusters\n\nRun tests in Exascale ADG across clusters\n     All MTC ADG tests can be added into this file",
    "platform": null
  },
  {
    "test_name": "tsagmtcfc3.tsc",
    "setup": null,
    "flags": {
      "clusterName": "^CSS_CLUSTERNAME^",
      "log_name": "^tst_tscname^_^viewid^.log"
    },
    "description": "tsagmtcfc3.tsc - Tests exadata Flash cache in multiple clusters\n\n1. Changed the FC storage clause. Alter tbl_kep from KEEP to DEFAULT in cluster1(db1)\n     2. Check the FC, cluster2 shouldn't be affected",
    "platform": null
  },
  {
    "test_name": "tsagmtcfc3ini.tsc",
    "setup": null,
    "flags": {
      "clusterName": "^CSS_CLUSTERNAME^",
      "log_name": "^tst_tscname^^viewid^.log"
    },
    "description": "tsagmtcfcini.tsc - Tests exadata Flash cache in multiple clusters\n\n1. Create different FC testing tables in cluster1(db1) and\n       cluster2(db2) with same table name.\n       2. Pop tables and check the FC accordingly",
    "platform": null
  },
  {
    "test_name": "tsagmtcfc4.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^.log"
    },
    "description": "tsagmtcfc4.tsc - Large write/OLTP workload test\n\nTEST 4: Large write/OLTP workload test, run tsagmtcfc4 from cell view\n     Concurrently running the workload in each clusters, check FC/NRW/MRW/ROW\n     1. NRW stands for No-Read Write. This type of large write only happens\n     when there's archive log write or backup write\n     2. MRW stands for Most-likely-Read Write, which happens for large write\n     on datafile.\n     3. ROW stands for Read-Once Write, which usually happens for temp sort\n     spill on datafile or tempfile.",
    "platform": null
  },
  {
    "test_name": "tsagmtchtwqm.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^_^viewid^_excld.log"
    },
    "description": "tsagmtchtwqm.tsc - check htw qm in multi cluster\n\nsimulate crash event and check htw qm\n\nset htw flag",
    "platform": null
  },
  {
    "test_name": "tsagmtciorm.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^_excld.log"
    },
    "description": "tsagmtciorm1.tsc - test inter db plan with duplicate db names\n\nsetup two clusters and create dbs on them with same unique name",
    "platform": null
  },
  {
    "test_name": "tsagmtciorm2.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^_^viewid^.log"
    },
    "description": "tsagmtciorm2.tsc - test the intra db plan with duplicate db names for multi-cluster\n\nTo test for default intra db plan and user-defined intra db plans with db restarting",
    "platform": null
  },
  {
    "test_name": "tsagmtciorm3.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmtciorm3.tsc - test iorm mappings in multicluster\n\ntest case 1: set db plan using profile\n     test case 2: set intra db plan at the same time\n\nNone.",
    "platform": null
  },
  {
    "test_name": "tsagmtciormcdb.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^_^viewid^_excld.log"
    },
    "description": "tsagmtciormcdb.tsc - intra db plans for cdb for multi-cluster run\n\nset cdb default and user-defined plans",
    "platform": null
  },
  {
    "test_name": "tsagmtciovorion.tsc",
    "setup": null,
    "flags": {
      "threads": "5",
      "duration": "36000                                          # max time to run",
      "area_sz": "64",
      "sleep_btw_test": "120",
      "cur_ossinst": "1",
      "cur_oss_devdir": "OSS_DEVDIR^cur_ossinst^",
      "OSSCONF": "^T_WORK^"
    },
    "description": "tsagmtciovorion.tsc - Run IOV and ORION tests\n\nRun IOV and ORION tests\n\nRun IOV and ORION tests",
    "platform": null
  },
  {
    "test_name": "tsagmtclcput.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagmtclcput.tsc   - test to list metrichistory cl_cput",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmtcomf.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmtcomf.tsc - OMF tests on MTC\n\nOMF tests on MTC\n\nOMF tests on MTC\n     Test Steps:\n\n     1. Verify basic DB parameters\n     2. Drop PDBs pdb_tpcds and pdb_tpch\n     3. Create PDBs pdb_tpcdss and pdb_tpch and open them\n     4. Create tablespace tbs_10\n     5. Shutdown/Startup DB - then check OMF files\n     6. Create a test table on a test tablespace\n     7. Perform PDB snapshot operation - then check OMF files\n     8. Clone a PDB - then check OMF files\n     9. Perform DB Backup using RMAN\n    10. Create a backup restore point - check OMF files\n    11. Drop the test table which was created earlier\n    12. Perform DB restore using RMAN\n    13. Verify the presence of the dropped table\n    14. List all OMF files in the DB, which are not following the\n        expected naming convention using ClusterID and ClusterName\n        (We donot expect this for SI DB without Clusterware)",
    "platform": null
  },
  {
    "test_name": "tsagmtcprimary.tsc",
    "setup": null,
    "flags": {
      "file_dest": "@^vault_db^"
    },
    "description": "tsagmtcprimary.tsc - Setup Primary database for Exascale in MTC test fw\n\nSetup Exascale Primary database in cluster 1 and the standby is created\n     in cluster 2 (tsagmtcstandby.tsc)\n     we can config DG to use same/different vault with primary, plese note: inst1\n     is bind to primary instance and inst9 is pointed to standby db in cluster 2\n\nSetup Exascale Standby database in MTC test fw\n       1) oss_mtc_adg=adg_indifferent_vault : Primary/Standby DB are created in different vaults\n       2) oss_mtc_adg=adg_insame_vault : Primary/Standby DB are created in same vault\n       3) oss_mtc_adg=adg_hybrid_asm_vault : Primary is on ASM in cluster 1 and Standby DB are created on vault in cluster 2\n       4) oss_mtc_adg=adg_hybrid_cross_cell : Primary is on ASM in cluster 1 and Standby DB are created on vault in cluster 2\n          ASM griddisks and vault pool disks come from separated cells\n                Cell    view 1(root view): setup Exascale full serivces, it servers for the standby db in cluster view 2\n                Cluster view 1(cluster 1): setup GI/ASM/Primary Database, it uses the ASM diskgroups in cell view 2\n                Cluster view 2(cluster 2): setup GI/Standby Database, it uses the vault in cell view 1\n                Cell    view 2(cluster 3): setup Exadata full serivces, it servers for the primary db in cluster view 1",
    "platform": null
  },
  {
    "test_name": "tsagmtcqm.tsc",
    "setup": null,
    "flags": {
      "log_file": "tsagmtcqm^viewid^.log"
    },
    "description": "tsagmtcqm.tsc - Storage Index + Quarantine(sqlplan+database) test on Multiple clusters\n\nStorage Index + Quarantine(sqlplan+database) test on Multiple clusters",
    "platform": null
  },
  {
    "test_name": "tsagmtcrebalqm.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^_^viewid^.log"
    },
    "description": "tsagmtcrebalqm.tsc - check asm rebalance qm\n\nsimulate crash event and then recreate asm disks",
    "platform": null
  },
  {
    "test_name": "tsagmtcstandby.tsc",
    "setup": null,
    "flags": {
      "standby_ctl": "'@'^vault_db^'/dbs/'^dbs1_controlfile_1_name^",
      "file_dest_standby": "'@'^vault_db^",
      "log_dest_standby": "'@'^vault_db^",
      "db_file_name_convert": "(\"'+'DATAFILE\",\"@^vault_db^\",\"'+'LOGFILE\",\"@^vault_db^\",\"'+'CONTROLFILE\",\"@^vault_db^\")",
      "log_file_name_convert": "(\"'+'DATAFILE\",\"@^vault_db^\",\"'+'LOGFILE\",\"@^vault_db^\",\"'+'CONTROLFILE\",\"@^vault_db^\")",
      "standby_db_opt": "db_create_file_dest=^file_dest_standby^"
    },
    "description": "tsagmtcadg.tsc - Setup Standby database for Exascale in MTC test fw\n\nSetup Exascale Standby database in cluster 2 and the primary is created\n     in cluster 1 (tsagmtcprimary.tsc)\n     It uses rman duplicate command to create physical standby db in cluster 2,\n     we can config it to use same/different vault with primary, plese note: inst1\n     is bind with standby instance and inst9 is pointed to primary db in cluster 1\n\nSetup Exascale Standby database in MTC test fw\n       1) oss_mtc_adg=adg_indifferent_vault : Primary/Standby DB are created in different vaults\n       2) oss_mtc_adg=adg_insame_vault : Primary/Standby DB are created in same vault\n       3) oss_mtc_adg=adg_hybrid_asm_vault : Primary is on ASM in cluster 1 and Standby DB are created on vault in cluster 2\n       4) oss_mtc_adg=adg_hybrid_cross_cell : Primary is on ASM in cluster 1 and Standby DB are created on vault in cluster 2\n                Cell    view 1(root view): setup Exascale full serivces, it servers for the standby db in cluster view 2\n                Cluster view 1(cluster 1): setup GI/ASM/Primary Database, it uses the ASM diskgroups in cell view 2\n                Cluster view 2(cluster 2): setup GI/Standby Database, it uses the vault in cell view 1\n                Cell    view 2(cluster 3): setup Exadata full serivces, it servers for the primary db in cluster view 1\n\n     Also introduce the parameter adg_standby_file_mode and adg_standby_creation_mode\n          adg_standby_file_mode=omf : just enable OMF in standby database, default value\n          adg_standby_file_mode=omf_all_convert :  enable OMF and db/log file_name_convert in standby database\n          adg_standby_file_mode=omf_db_convert :   enable OMF and db file_name_convert in standby database\n          adg_standby_file_mode=omf_log_convert : enable OMF and log file_name_convert in standby database\n          adg_standby_file_mode=off : just enable db/log file_name_convert in standby database\n\n          adg_standby_creation_mode=duplicate :  use rman duplicate command to create standby database, default value\n          adg_standby_creation_mode=restore_service : use rman restore services command to create standby database",
    "platform": null
  },
  {
    "test_name": "tsagmtctcellcli.tsc",
    "setup": null,
    "flags": {
      "tscname": "tcellcli^rndm^^tcellclisuffix^",
      "cur_ossinst": "1",
      "cur_oss_devdir": "OSS_DEVDIR^cur_ossinst^",
      "OSSCONF": "^T_WORK^"
    },
    "description": "mtctcellcli.tsc - Macro for handling OSS side local and remote cellcli execution\n\nThis macro essentially provides a place for separately handling cellcli\n     execution for OSS functional tests run in base form or in remote interop mode\n\nMacro is defmacroed in tsaginit and is invoked as :\n     tcellcli <arg1> <arg2> <argn>",
    "platform": null
  },
  {
    "test_name": "tsagmtctopcpu.tsc",
    "setup": null,
    "flags": {
      "clusterName": "^CSS_CLUSTERNAME^",
      "log_name": "^tst_tscname^_^viewid^.log"
    },
    "description": "tsagmtctopcpu.tsc - Exadata Multi cluster test case\n\nto check cellcli command and tables\n\ncheck topcpu active requests etc",
    "platform": null
  },
  {
    "test_name": "tsagmtcvictim.tsc",
    "setup": null,
    "flags": {
      "fc_lw_size": "50",
      "dbid": "^dbid_rel^'-'^dbid_rel^",
      "cdbid": "^cdbid_rel^'-'^cdbid_rel^",
      "oltp_gd": "sddlun3",
      "lw_gd": "sddlun4",
      "cur_ossinst": "1",
      "cur_oss_devdir": "OSS_DEVDIR^cur_ossinst^",
      "OSSCONF": "^T_WORK^"
    },
    "description": "tsagmtcvictim.tsc - Victim cache test on DBUNIQNAME\n\nVictim cache test on DBUNIQNAME\n\n1. We have two cells, in each cell, create two separated griddisks: sddlun1 and sddlun2\n  2. There are two clusters, in each cluster, create one database with the same dbUniqueName\n  3. enable ASM scoped security, sddlun1 is availableTo C1 and sddlun1 is availableTo C2\n\n   Create 2 standalone GDs (not part of any ASM diskgroup), and make one available_to C1\n   and the other available to C2.  IOV C1 uses C1 GD and IOV C2 uses C2 GD. Use standby0\n   for C1 and standby1 for C2.\n\n   Then copy C1's cellkey.ora for IOV C1 and C2's cellkey.ora for IOV C2.  This way IOV C1\n   can impersonate C1 DB by using the same dbid, and IOV C2 can impersonate C2 DB by using\n   the same dbid as well.\n\n This way, we can continue using IOV to launch LW vs OLTP workloads. It includes 2 cases:\n\n   1) If there is no inter-DB IORM plan to divvy up the FC space between C1 DB and C2 DB,\n     we should see both LW are throttled when OLTP is thrashing as is done in Dheeraj's\n     existing test case.  This functionally tests that the victim cache continues to work\n     even with ASM scoped security.\n\n   2) Configure an inter-DB IORM plan to divvy up the FC space between C1 DB and C2 DB.\n     Trigger OLTP thrashing for C1 DB only.  Verify that only C1 LW is throttled, but not C2.\n\n   For the test about generic cache thrashing mechanism (ie. not hardmax\n   groups), we need to satisfy the following criteria:\n\n    1. OLTP workload must be larger than FC size minus LW size. eg. if FC\n       size is 200MB and LW total region size  is 50MB, then the OLTP region\n       size must be 150MB or larger.\n\n    2. OLTP workload cannot be much larger than total FC size, since\n       otherwise even stopping LW would have minimal effect. eg. if total\n       FC size is 200 MB, and OLTP workload region size is 1G, no amount\n       of LW limitation can really help the hit ratio.",
    "platform": null
  },
  {
    "test_name": "tsagmtrc.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmtrc.tsc - METRICDEFN tests\n\nThis tests the list metricdefinition of cellcli",
    "platform": null
  },
  {
    "test_name": "tsagmtrc1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmtrc1.tsc - METRICHISTORY tests\n\nThis tests the list metrichistory of cellcli",
    "platform": null
  },
  {
    "test_name": "tsagmtrc_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmtrc_nls.tsc - METRICS command of CellCLI\n\nIt tests the various alerts generated on the cell.",
    "platform": null
  },
  {
    "test_name": "tsagmtrichst.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagmtrichst.tsc - purge trace files through cellcli\n\npurge trace files through cellcli command - drop metrichistory",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmtrichst_cl.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagmtrichst_cl.tsc - Exacli Metrichistory unit test\n\npurge trace files through exacli command - drop metrichistory",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmtrload.tsc",
    "setup": "tsagnini",
    "flags": {
      "MACH_PASSWD": "welcome1"
    },
    "description": "tsagmtrload.tsc -Test to  load lots of metrics into MS.\n\nThe test simulate lots of DB's, hence lots of metrics.\n    and  installed large metric files -- hence lots of metric history.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmultclu.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmutclu.tsc - Exadata multicluster test driver\n\nruns multicluster tests\n\n     <usage> run case: runtest tsagmultclu.tsc multcase=xx.tsc,xxx.tsc\n     <local run> : set nodestroy =1 would not cleanup env.",
    "platform": null
  },
  {
    "test_name": "tsagmultibsmbsw.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "multiple_bst": "true # Inorder to setup 2 BSM + 3 BSW instances",
      "setup_blockstore": "true"
    },
    "description": "tsagmultibsmbsw.tsc - testcase for Multiple bsm/bsw if one bsw is shutdown\n     while a find is in progress, we wait till the snaps and backups are moved\n     to other bsws before returning.\n\n1. oratst -d xblockini.tsc multiple_bst=true  iscsi=true, this will setup 2 bsm + 3 bsw env\n   2. Create many volumes and snapshots for all 3 bsw's to share snapshots and backups\n   3. list snapshots and backups to verify that all are id's listed\n   4. kill bsw1 -  $runasroot \"kill <pidbsw1>\"\n   5. Wait for 90s and verify its status is offline\n   6. list snapshots and backups to verify that all are id's listed\n   7. Bring bsw1 back",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagmultibsmcrash.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "oss_testing": "3",
      "setup_blockstore": "true",
      "bug32488441": "OPEN"
    },
    "description": "tsagmultibsmcrash.tsc - Simulates bsm failure through cellcli events in 3 cell env\n\nIn this test, BSM is crashed during control operation through cellcli events. The cellcli events are:\n       1. EBS_IOCTL_ERR_BEFORE_CBK - BSM is crashed while control operation is in progress\n       2. EBS_SRV_ERR_BEFORE_IOCTL_REPLY - BSM is crashed when control operation is finshed but escli has not been informed about it.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagmulticcli.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmulticcli.tsc - exacli mutli command feature\n\nexacli running multiple commands via the command line.\n     eg. exacli -e 'list cell; list celldisk'",
    "platform": null
  },
  {
    "test_name": "tsagmultims.tsc",
    "setup": "tsagnini",
    "flags": {
      "oss_multims_testing": "true",
      "sage_mirror_mode": "normal"
    },
    "description": "tsagmultims.tsc - Unit test for Multi-MS capability\n\nSets up 2 cells with MS of their own.\n     Switches 1 of the cell to use default MS and then back to its own MS",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagmultinodefencing.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "max_instance": "2",
      "racdb_on_exc": "true"
    },
    "description": "tsagmultinodefencing.tsc - Fencing tests in an env having multi nodes",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagmultispvltprovision.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "multisp": "true",
      "sage_mirror_mode": "high",
      "setup_blockstore": "true"
    },
    "description": "tsagmultispvltprovision.tsc - Tests the properties of a vault when additional resources are assigned to it from different storage pools.\n\nThis test changes the provisioned space and iops of different media types and then observe its changes in the vault properties.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagmultivexacldsuite.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagmultivexacldsuite.tsc - Test suite for multi cell/view tests\n\nTest suite for multi cell/view tests",
    "platform": null
  },
  {
    "test_name": "tsagmund_init.tsc",
    "setup": null,
    "flags": {
      "nowarn": "^TST_NOWARN^",
      "MAX_INSTANCE": "2  # this will be the clustersize as well",
      "MAX_DB": "1",
      "srvpool_sz": "^MAX_INSTANCE^",
      "PRIORITY_BOOST_VALUE": "3",
      "TEMP_ORACLE_SID": "^ORACLE_SID^_^i^",
      "total_nodes": "^has_num_nodes^",
      "JAVA_HOME": "^ORACLE_HOME^/jdk11",
      "PCW_EXC": "true",
      "PCW_CREATE_VAULT": "true",
      "scan_lsnr_base": "slsnr",
      "lsnr_name": "LISTENER",
      "maxinstances": "^MAX_INSTANCE^  # rdbms's tkstart depend on this to",
      "create_instances": "^MAX_INSTANCE^ #for gpnp",
      "torcl_sid": "^ORACLE_SID^",
      "SAGE_MIRROR_MODE": "high",
      "cluster_library": "clss         # changed in tkstart.tsc",
      "cleanup": "false",
      "logfile_nc1": "tsaglist_0.log"
    },
    "description": "tsagmund_init.tsc - Exadata emulated multi-node inititilization file\n\n- Sets up emulated multi-node framework with multiple cssd and diskmon\n     - Creates exadata devices and ASM (RAC) diskgroups.\n     - Starts RDBMS RAC instances\n\nClone of tclsracinit.tsc with exadata",
    "platform": null
  },
  {
    "test_name": "tsagmvsmart.tsc",
    "setup": "srdbmsini",
    "flags": {
      "log_file": "tsagmvsmart.log"
    },
    "description": "tsagmvsmart.tsc - Smart Scan tests on Materialized Views\n\nSmart Scan tests on Materialized Views:\n\n       1) cc1 and cc2\n       2) SI",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagndbmssuit.tsc",
    "setup": null,
    "flags": {
      "section": "ALL"
    },
    "description": "tsagndbmssuit.tsc - NLS Test suit for DBMCLI\n\nIt calls different tests for DBMCLI.",
    "platform": null
  },
  {
    "test_name": "tsagnet_prtcl_attr.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "cell1_f": "^cell1^.us.oracle.com",
      "cellconnstr": "root@^cell1_f^"
    },
    "description": "tsagnet_prtcl_attr.tsc - Check new attribute networkprotocol",
    "platform": null
  },
  {
    "test_name": "tsagnet_prtcl_attr_db.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1"
    },
    "description": "tsagnet_prtcl_attr_db.tsc - Check new attribute networkprotocol",
    "platform": null
  },
  {
    "test_name": "tsagnetdiag.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "cell": "^cell_node^",
      "cellconnstr": "root@^cell^.us.oracle.com",
      "TMP_DIR": "/tmp"
    },
    "description": "tsagnetdiag.tsc - Tests for Network Diagnostic Tool\n\nTo test netdiag.sh and individual network debugging tools",
    "platform": null
  },
  {
    "test_name": "tsagnini.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1  ## No flint disks for appliance mode, already too many disks",
      "creatdev_file": "tsagrddef",
      "scompatible.asm": "^scompatible18.asm^",
      "reconnect_to_cell_attempts": "24",
      "reconnect_to_cell_freq_in_sec": "5",
      "ossp": "oss_port^i^",
      "OSSCONF1": "^OSSCONF^",
      "timeout_fs": "120"
    },
    "description": "tsagnini.tsc - set up database diskgroups with disks in n oss servers\n\nSet up database diskgroups with disks in n oss servers,\n     where 1 <= n <= number of db disks defined in creatdev_file\n     and n = OSS_TESTING.\n\n     If not running with failgroup\n     . use stkdefosm to create database diskgroups as normal\n     . stop asm\n     . stop oss\n     . split disks to multiple oss servers and start the servers\n     . restart asm\n     . check that the disks are all accessible\n\n     If running with failgroup\n     . create datafile disks for additional failgroups and start\n       associate servers\n     . use stkdefosm to create the database diskgroups as normal\n       This will also start the associated server.\n\nAt the end of the script, oss and asm are up.\n    To collect cellsrv stat periodically in farm runs\n    Please use the following config parameters\n      farm submit <lrgname> -config \"EXADATA_STAT_COLLECTION=true;EXADATA_STAT_NAME=CELLSRVSTAT;INTERVAL=10\"\n      Explanation of parameters :\n      EXADATA_STAT_COLLECTION=true : Turn on stats collection\n      EXADATA_STAT_NAME=CELLSRVSTAT/ECSTAT\n      INTERVAL=10 : specify the interval of stat collection in seconds, for example-\n                    INTERVAL=10 will invoke and save cellsrv output every 10 seconds.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagnini_startcell.tsc",
    "setup": null,
    "flags": {
      "cell_is_online": "0"
    },
    "description": "tsagnini_startcell.tsc - Start cell in tsagnini to avoid stkdefosm timeouts\n\nStarts up a single cell during tsagnini.tsc before running stkdefosm if\n     no cells are running.\n\n     This is a performance optimization. stkdefosm will start CRS which wastes\n     2 minutes trying to connect to the cell before timing out. By starting a\n     single cell, we avoid this and other timeouts, saving 2-4 minutes.\n\n     Steps:\n     1. Check if a cell is already running. If so, we're done.\n     2. Does cell_disk_config.xml exist? If not, run \"cellcli -e alter cell...\"\n        to regenerate it.\n     3. If cell_disk_config.xml exists, start cell.\n     4. Delete oss_iorm.txt dump to avoid difs in iorm tests.\n\n     creatdev.pl will eventually create the celldisks and restart the cell in\n     stkdefosm.\n\nTo prevent difs:\n     - Don't perform this optimization for remote interop or exadoop tests.\n     - Don't start a second cell if one is already running.\n     - Don't start a cell without cell_disk_config.xml.\n\n     To help with debugging, we write the output from all startup attempts to\n     start_cell_for_crs_out.sav.",
    "platform": null
  },
  {
    "test_name": "tsagniniupg.tsc",
    "setup": null,
    "flags": {
      "tmp_oss_port": "oss_port^i^"
    },
    "description": "tsagniniupg.tsc - Setup cell services\n\nSetup all cell services",
    "platform": null
  },
  {
    "test_name": "tsagnips.tsc",
    "setup": "tsagnini",
    "flags": {
      "cellip4": "^cellip^",
      "errbasename": "^outbasename^"
    },
    "description": "tsagnips.tsc - sage support for various numbers of IP addresses\n\nCELLSRV supports anywhere from 1 to 4 IP addresses.\n     DISKMON/LIBCELL support from 1 to 64 IP addresses.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagnodehealthchk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagnodehealthchk.tsc - Macro with health checks at beginning of LRG\n\nMacro with health checks at beginning of LRG\n\nMacro with health checks at beginning of LRG",
    "platform": null
  },
  {
    "test_name": "tsagnodehealthchk_tst.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagnodehealthchk_tst.tsc - Macro with health checks between the tests\n\nMacro with health checks between the tests\n\nMacro with health checks between the tests",
    "platform": null
  },
  {
    "test_name": "tsagnodehealthfailseq.tsc",
    "setup": null,
    "flags": {
      "sshstring": "'_ssh-issue'",
      "pingstring": "'_ping-issue'",
      "tststring": "'_testing-disabled'",
      "spacestring": "'_space-issue'",
      "oldimgstring": "'_image-old'",
      "diskstring": "'_disk-failure'",
      "qstring": "'NodeUnhealthy_:'",
      "mode": "quarantine"
    },
    "description": "tsagnodehealthfailseq.tsc - Macro to handle node health failure\n\nMacro to handle node health failure\n\nMail alert, return and quarantine the node",
    "platform": null
  },
  {
    "test_name": "tsagnodereimage.tsc",
    "setup": null,
    "flags": {
      "SKIP_REIMAGE": "true",
      "FORCE_REIMAGE": "false",
      "cell_imagv": "\"\"",
      "cell_cellv": "\"\"",
      "view_imagv": "\"\"",
      "view_cellv": "^label^",
      "image_needed": "true"
    },
    "description": "tsagnodereimage.tsc - File with code for re-imaging the node\n\nThis script reimages based on some pre-checks.\n     Checks are as follows:\n\n     The script first determines (1) Image version on node\n                                 (2) Cell version on node\n                                 (3) Image version in view\n                                 (4) Cell version in view\n     Additionally, skip_reimage & force_reimage flags are imported.\n     If force_reimage is issued, re-imaging is done in all cases.\n     Else, if skip_reimage is issued, re-image is skipped.\n     If both skip_reimage and force_reimage are not issued,\n     re-imaging occurs if (1)!=3 or (2)!=(4). Else, it is skipped.\n\n     If re-imaging fails mid-way, the node is commented out\n     and added back to the pool.",
    "platform": null
  },
  {
    "test_name": "tsagnodereturn.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagnodereturn.tsc - Node return macro\n\nNode return macro\n\nReturn the node with or without quarantine",
    "platform": null
  },
  {
    "test_name": "tsagnon4kaligncachechk_16mls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagnon4kaligncachechk_16mls.tsc - 4KUnaligned Cache Write test\n\nTest case for caching unaligned write into clean page for WBFC.\n\nTest steps:",
    "platform": null
  },
  {
    "test_name": "tsagnon4kaligncachechk_32mls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagnon4kaligncachechk_32mls.tsc - 4KUnaligned Cache Write test\n\nTest case for caching unaligned write into clean page for WBFC.\n\nTest steps:",
    "platform": null
  },
  {
    "test_name": "tsagnon4kaligncachechk_af.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagnon4kaligncachechk_af.tsc - 4KUnaligned Cache Write test\n\nwritecache should not be created in all flash env\n\nTest steps:",
    "platform": null
  },
  {
    "test_name": "tsagnon4kaligncachechk_siz.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagnon4kaligncachechk_siz.tsc - 4KUnaligned Cache Write test\n\ntests related to effectiveWriteCacheSize, cache creation, writeThrough\n\nTest steps:",
    "platform": null
  },
  {
    "test_name": "tsagnon4kaligncachechk_ugdg.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagnon4kaligncachechk_ugdg.tsc - 4KUnaligned Cache Write test\n\nwritecache should remain while upgrading from 23.1 and drop when downgrading\n\nTest steps:",
    "platform": null
  },
  {
    "test_name": "tsagnonexcvolume.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagnonexcvolume.tsc - tests for non-exascale volume types\n\nTesting RAMDEVICE and NULLDEVICE backed volumes.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagnrestor.tsc",
    "setup": null,
    "flags": {
      "tmpdir": "oss_devdir^i^",
      "devdir": "^^tmpdir^",
      "cmd": "'mv '^devdir^'/* '^oss_devdir1^'/'"
    },
    "description": "tsagnrestor.tsc - restore the disks to original oss server\n\nIn tsagnsplit.tsc, disks are split out from one oss to many.\n     This script restores them back to original disk dir.\n\nAll oss servers are shutdown by this script.",
    "platform": null
  },
  {
    "test_name": "tsagnsplit.tsc",
    "setup": null,
    "flags": {
      "tmp_port": "^oss_port^",
      "tag": "'cellsrv[2-'^oss_testing^'] tsagnsplit'",
      "nextdir": "raw^tmp_port^"
    },
    "description": "tsagnsplit.tsc - distribute disks from one oss to many\n\nDistribute disks from one oss to as many as specified by oss_testing\n     and start all the servers afterwards.\n\nThe maximum number of oss servers is the number of db disks defined\n     in creatdev_file, even if OSS_TESTING is set to a higher number.",
    "platform": null
  },
  {
    "test_name": "tsagnsuit.tsc",
    "setup": "tsaginit",
    "flags": {
      "section": "ALL"
    },
    "description": "tsagnsuit.tsc - Driver for lrgnsacli\n\nIt calls different tests for CELLCLI.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagnumparam.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagnumparam.tsc - Test for bug 12742269\n\nWHERE clause not working for some cellcli commands",
    "platform": null
  },
  {
    "test_name": "tsagnvcacheiov.tsc",
    "setup": null,
    "flags": {
      "cell_with_flash_cache": "all",
      "cell_with_pmem_cache": "true",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagnvcacheiov.tsc -orion and iov test for nvcache with fakeib\n\nWhile fakeib is running , create nvcache  and run iov,orion on disk\n     Check for rdma,ipcdat,nvcache stats",
    "platform": null
  },
  {
    "test_name": "tsagnvcdbtcioctl.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cell_with_pmem_cache": "true",
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'",
      "lunsize": "5243000",
      "tblspacesize": "4800",
      "numofrec": "4000"
    },
    "description": "tsagnvcdbtcioctl.tsc - validates increment the touch count on PMEMCache\n                            with db workload\n\nThis test runs on fake hardware and validates increment the touch count on\n     PMEMCache cache lines using a side channel (IOCTLs) with DB workload, when\n     the client is mostly doing reads of the cache via RDMA (so that the IOs\n     never reaches cellsrv and hence cellsrv cannot increment the touch count\n     normally).\n\nTest uses PMEMCache",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagnvcdbworkload.tsc",
    "setup": "srdbmsini",
    "flags": {
      "dxd": "true"
    },
    "description": "tsagnvcdbworkload.tsc - run db workload expecting reads go through RDMA,\n\nThis test runs db workload expecting reads to go through RDMA, and not\n     make it through cellsrv. We validate it by looking at 'cell RDMA IOs'\n     stats from v$sysstat.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagnvdimmdbworkload.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagnvdimmdbworkload.tsc - NVDIMM remote read functional test with db workload\n\nRuns db workload on DXD griddisks and make sure all IOs are performed\n     using RDMA. Hence CellSrv statistics (Total read block IO to be issued,\n     and Total read block IO issued) don't increase before and after running workload.\n\nTraditionally in Exadata, the storage tier consists of Flash disk and hard disk.\n     To read a block into Flash disk or hard disk, database process on the compute node\n     sends a message to CellSrv process on the storage node to submit CacheGet jobs.\n     CellSrv then  reads the device on behalf of the database process and send the result\n     back to database.\n\n     Now with the advent of Persistent memory(DXD) in the storage tier (exposed either\n     as a GridDisk or as a Cache) the read/write latency of DXD based disks is very small\n     (in the order of microseconds). So involving CellSrv (a heavy weight process) during\n     read/write operations will increase the latency. DXDs are byte addressable, so instead\n     of using message based communication, one could directly do RDMA operations on the device.\n     In order to keep the read/write latency as close to the order of microseconds,\n     database process directly read or write to the requisite region using RDMA\n     without involving CellSrv (for the most part).",
    "platform": null
  },
  {
    "test_name": "tsagnvdimmorion.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_testing": "1"
    },
    "description": "tsagnvdimmorion.tsc - NVDIMM remote read functional test with orion\n\nRuns orion workload on DXD griddisks and make sure all IOs are performed\n     using RDMA. Hence CellSrv statistics (Total read block IO to be issued,\n     and Total read block IO issued) don't increase before and after running workload.\n\nTraditionally in Exadata, the storage tier consists of Flash disk and hard disk.\n     To read a block into Flash disk or hard disk, database process on the compute node\n     sends a message to CellSrv process on the storage node to submit CacheGet jobs.\n     CellSrv then  reads the device on behalf of the database process and send the result\n     back to database.\n\n     Now with the advent of Persistent memory(DXD) in the storage tier (exposed either\n     as a GridDisk or as a Cache) the read/write latency of DXD based disks is very small\n     (in the order of microseconds). So involving CellSrv (a heavy weight process) during\n     read/write operations will increase the latency. DXDs are byte addressable, so instead\n     of using message based communication, one could directly do RDMA operations on the device.\n     In order to keep the read/write latency as close to the order of microseconds,\n     database process directly read or write to the requisite region using RDMA\n     without involving CellSrv (for the most part).",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagnvoflworkload.tsc",
    "setup": null,
    "flags": {
      "caseno": "nvfork1_db2",
      "caseoflgrp": "oflgrpnvfork1",
      "casepackage": "^cur_osscellofl_package^",
      "rdbms_version1": "^EXADATA_RDBMS_MAIN_LABEL^",
      "rdbms_version2": "^EXADATA_RDBMS_1900_LABEL^"
    },
    "description": "tsagnvoflworkload.tsc - offloadgroup workload cases that run on mutiple database\n\nUse oss_mdbn to create other 2 rdbms view to run workload on corresponding offloadgroup\n     Currently have 3 workload case:\n     \t\t NV CASE1:\n\t   1) create user define offloadgroup without package\n\t   2) use user offloadgroup run workload on rdbmsv1,rdbmsv2 sequentially\n\t   3) both should pick the corresponding package do smart scan\n\t NV CASE2:\n\t   1) create user define offloadgroup with package specified(say version match rdbmsv1)\n\t   2) use user offloadgroup run workload on rdbmsv1,rdbmsv2 sequentially\n\t   3) rdbmsv1 should use smart scan and rdbmsv2 should use passthru mode\n     \t\t NV FORK CASE1:\n\t   1) create user define offloadgroup without package\n\t   2) use user offloadgroup run workload on rdbmsv1,rdbmsv2 parallelly\n\t   3) rdbmsv1 should use smart scan and rdbmsv2 should use passthru mode\n\nUse rdbmsv1:RDBMS_MAIN_RELEASE and rdbmsv2:RDBMS_19.0.0.0.0_DATED for workload",
    "platform": null
  },
  {
    "test_name": "tsagnvsievict.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagnvsievict.tsc - Run si eviction test of multi-group in n-view env\n\nSet up n-view env, create tab in 2 version database, run si eviction test",
    "platform": null
  },
  {
    "test_name": "tsagobj2db.tsc",
    "setup": null,
    "flags": {
      "rowval": "2000",
      "errbasename": "^tst_tscname^tmp.log"
    },
    "description": "tsagobj2db.tsc - run with 2 databases",
    "platform": null
  },
  {
    "test_name": "tsagobj2nd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagobj2nd.tsc - test that active secondary is not kept",
    "platform": null
  },
  {
    "test_name": "tsagobjcachini.tsc",
    "setup": "tsagnini",
    "flags": {
      "nospdefault": "true",
      "my_lmhb": "'_small_table_threshold=1'",
      "cdb": "true"
    },
    "description": "tsagobjcachini.tsc - initializations for tsagobjcach\n\nparameters:\n     . num_flash_per_cell\n       default is 3\n     . flash_size\n       default is 224M, 48M of which will be used for celldisk metadata,\n       128M for flash griddisk, 16M for flash log (64M total), leaving 32M\n       on each disk for flash cache\n     . sage_mirror_mode\n       default is no mirroring\n     . extent_mgmt (local|dictionary)\n       default is local",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagobjcachsql.tsc",
    "setup": null,
    "flags": {
      "extent_mgmt": "local",
      "segsize": "1M",
      "initsize": "8M",
      "colsize": "1630"
    },
    "description": "tsagobjcachsql.tsc - SQL scripts for tsagobjcach\n\nSQL script for flash cache objects manipulation.",
    "platform": null
  },
  {
    "test_name": "tsagobjmir.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagcaudef",
      "tkrmlogother": "t_work:logcell",
      "errbasename": "^tst_tscname^tmp.log"
    },
    "description": "tsagobjmir.tsc - test drop griddisk, drop ASM disk\n                    - check that backup and 2ndary extents are not cached",
    "platform": null
  },
  {
    "test_name": "tsagobjmira.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagobjmira.tsc - add disks to diskgroups\n\nCalled by tsagobjmir.tsc to add disks to diskgroups",
    "platform": null
  },
  {
    "test_name": "tsagobjmirb.tsc",
    "setup": null,
    "flags": {
      "cmd": "^cmd^' '^argname^",
      "tmparg": "arg^i^",
      "argname": "^^tmparg^",
      "argval": "^^argname^",
      "lsext": "lsextent^dg^.log",
      "tst_var_mode": "true"
    },
    "description": "tsagobjmirb.tsc - relocate extent to new disks\n\nCalled by tsagobjmir.tsc to relocate an extent of a file in a diskgroup\n     to new disks\n\nrun tsagobjmirb dg=... [ext=...] [tbs=...] [newgd0 ...]\n\n     dg - diskgroup name\n     ext - extent number to relocate (default - 0)\n     tbs - tablespace whose extent will be relocated (default - 1st\n           tablespace file found by \"tsagextrelo.pl < lsextent$dg.log\")\n     newgd0 ... - new griddisks to relocate extent to (default - as\n                  determined by tsagextrelo.pl)",
    "platform": null
  },
  {
    "test_name": "tsagobjrbr.tsc",
    "setup": null,
    "flags": {
      "rowval": "1000"
    },
    "description": "tsagobjrbr.tsc -- flash cache test for RBR",
    "platform": null
  },
  {
    "test_name": "tsagobjrbr2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagobjrbr2.tsc - test rbr by shrinking, truncating and dropping tables\n\nThis script is forked by tsagobjrbr.tsc.",
    "platform": null
  },
  {
    "test_name": "tsagobjstat.tsc",
    "setup": null,
    "flags": {
      "objid": "^tsagobj^id",
      "maxobjid": "max^objid^"
    },
    "description": "tsagobjstat.tsc - get flash cache object stat",
    "platform": null
  },
  {
    "test_name": "tsagoclrh.tsc",
    "setup": null,
    "flags": {
      "tstname": "tsagoclrh"
    },
    "description": "tsagoclrh.tsc - Test Cases of OCL RH\n\nIt is a new feature called OCL Region Heuristics (RH) for OLTP and\n     Uncompressed tabled. The main idea with this feature is to improve\n     CC performance in write intensive queries. Consider a scenario where\n     we have a 1MB CC populated region and the following occurs read-write\n     sequence occurs to a region (write, read, write, read)\n\n     Currently our code works as follows\n   1. When 1st write to a region occurs. We invalidate CC for that region\n   2. When 1st read to the region occurs, CC is built again for the region\n   3. When 2nd write to the region occurs. We invalidate CC for that region\n      again\n   4. When 2nd read the region occurs, CC is built again for the region.\n   This constant invalidation and rebuilding is costly in terms of performance\n   The OCL RH feature is meant to stop constantly re building CC for a region\n   if writes to the region keeps occurring. The feature monitors each region\n   and builds CC only when say 10 consecutive reads to the region occurs.\n   That way we know it is read-intensive and CC wont be invalidated.\n   When the first write occurs, RH begins to monitor the region and say after\n   10 consecutive we unmonitor the region. During the time the region is\n   monitored we dont build CC.",
    "platform": null
  },
  {
    "test_name": "tsagoclrh_corrupt.tsc",
    "setup": null,
    "flags": {
      "tstname": "tsagoclrh_corrupt"
    },
    "description": "tsagoclrh_corrupt.tsc - Test for Population Failure Tracking\n\nCurrently when we perform CC2 population in the background, there could\n     be CC2 population failures due to data dependencies (Max numbero of CUs,\n     CU too big, Output size too big etc). FPLIB indicates to the celloflsrv\n     if such a failure has occured and informs the cellsrv to skip CC2\n     population retries.\n\n     For EHCC, this information is stored as a bit in the FC cache header which\n     prevents subsequent CC2 population retries.\n\n     for non-EHCC we dont have foreground CC population and hence the FC cache\n     header at this stage is not valid and hence there is no way to hold this bit\n     of preventing CC2 population retries\n\n     With OCL RH we introduce the concept of CC2 Population Tracking. The bit\n     sent by Fplib is cached in OCL RH for each 1 MB region and the region is\n     tracked by RH till a write to the region occurs. While the region is being\n     tracked, CC2 wont be populated for the given CC2 rewrite region. When a write\n     to the region occurs, we assume the data dependency is fixed and we\n     automatically untrack the region\n\n     Thus OCL RH will now perform two functions\n     1. Population Failure Tracking-Tracking those regions which previously failed\n     CC2 population\n     2. CC2 monitoring - Monitoring those regions that had a CC2 invalidation due\n     to a write",
    "platform": null
  },
  {
    "test_name": "tsagocrasm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagocrasm.tsc - Replicate has/test/tclsa/src/tclsaocrasm.tsc\n\nThis is a replica of tclsaocrasm.tsc but uses exadata specific\n    tclsaocrasm_init_exa to initialize the OCR and VD on ASM",
    "platform": null
  },
  {
    "test_name": "tsagocrcefl.tsc",
    "setup": null,
    "flags": {
      "OCR_DG2_ROOT": "'+OCREXDG2'",
      "OCR_DG2_NAME": "'OCREXDG2'",
      "OCR_DG12_ROOT": "'+OCREXDG12'  ## nomal redundancy diskgroup",
      "OCR_DG12_NAME": "'OCREXDG12'",
      "OCR_DG123_ROOT": "'+OCREXDG123'  ## high redundancy diskgroup",
      "OCR_DG123_NAME": "'OCREXDG123'",
      "OCR_DG3_ROOT": "'+OCREXDG3'",
      "OCR_DG3_NAME": "'OCREXDG3'",
      "testname": "tsagocrcefl",
      "lgfile": "^testname^11z.log"
    },
    "description": "tsagocrcefl.tsc - Cell failure test for OCR on ASM\n\nTests the Cell failure scenarios with OCR on ASM",
    "platform": null
  },
  {
    "test_name": "tsagocrcefl1.tsc",
    "setup": null,
    "flags": {
      "lgfile": "^testname^11d.log"
    },
    "description": "tsagocrcefl1.tsc - Cell failure with OCR on ASM\n\nTest Case - 1. Two DG on Cell 1",
    "platform": null
  },
  {
    "test_name": "tsagocrcefl2.tsc",
    "setup": null,
    "flags": {
      "lgfile": "^testname^11i.log"
    },
    "description": "tsagocrcefl2.tsc - Cell failure test with OCR on ASM",
    "platform": null
  },
  {
    "test_name": "tsagocrcefl3.tsc",
    "setup": null,
    "flags": {
      "lgfile": "checkocr_aftercelfl3.lst"
    },
    "description": "tsagocrcefl3.tsc - Cell failure TEst with OCR on ASM\n\nTest Case - 3 - Normal redundancy diskgroup",
    "platform": null
  },
  {
    "test_name": "tsagocrcefl4.tsc",
    "setup": null,
    "flags": {
      "lgfile": "^testname^11r.log"
    },
    "description": "tsagocrcefl4.tsc - Cell failure test with OCR on ASM\n\nTest Case - 4 High redundancy diskgroup",
    "platform": null
  },
  {
    "test_name": "tsagocrcefl5.tsc",
    "setup": null,
    "flags": {
      "VOTEDISKG": "OCREXDG12",
      "lgfile": "tsagocrcefl5e.log",
      "sep": "'/'"
    },
    "description": "tsagocrcefl5.tsc - Test for bug 9871522\n\nOCR and VD on normal redundancy DG on ASM\n     3 cells -> 1 cell is down\n     and then bounce CRS stack",
    "platform": null
  },
  {
    "test_name": "tsagocrdgcr.tsc",
    "setup": null,
    "flags": {
      "new_asm_compat": "19.0.0.0.0",
      "new_db_compat": "19.0.0.0.0"
    },
    "description": "tsagocrdgcr.tsc - Create diskgroup for OCR on ASM tests",
    "platform": null
  },
  {
    "test_name": "tsagoedaawsapi.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaawsapi.tsc - this file has unit tests for AwsApiTest.java\n\nUnit tests for AWS API testing\n\nrun test class methods",
    "platform": null
  },
  {
    "test_name": "tsagoedacellcli.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagoedachownperm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedachownperm.tsc  - this file has unit tests for changing permissions on ORACLE_BASE\n\nUnit tests for oeda changing permissions on ORACLE_BASE\n\nOEDA Test for OB - chown",
    "platform": null
  },
  {
    "test_name": "tsagoedacli01.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedacli01.tsc - TEST OEDACLI COMMANDS",
    "platform": null
  },
  {
    "test_name": "tsagoedacliaddswitch.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedacliaddswitch.tsc  - this file run oedacli script with add switch\n\ntest in invalid cases when using oedacli add switch commands\n\nadd switch tests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedaclicloneadminilomnet.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaclicloneadminilomnet.tsc - test clone guest/compute/newcell\n\nrun oedacli script to test admin/ilom network for clone newcell/clone\n    guest/clone compute\n\ndrop newcell to a cluster",
    "platform": null
  },
  {
    "test_name": "tsagoedaclicloneguests.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaclicloneguests.tsc  - this file run oedacli script with clone guest function\n\ntest in invalid cases when using oedacli clone guest commands\n\nclone guest tests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedacliclonenewcell.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedacliclonenewcell.tsc - test clone newcell\n\nrun oedacli script to add new cell from the cluster\n\nclone newcell from a cluster",
    "platform": null
  },
  {
    "test_name": "tsagoedaclijson.tsc",
    "setup": null,
    "flags": null,
    "description": "This test makes sure the oedacli command outputs in json format are\n     generated for the available commands",
    "platform": null
  },
  {
    "test_name": "tsagoedaclijvmerrorcode.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaclijvmerrorcode.tsc - test oedacli jvm error code\n\nteting oedacli failure case that jvm exit with error code 1\n\nhandle JVM error code",
    "platform": null
  },
  {
    "test_name": "tsagoedaclimain.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaclimain.tsc - main collection of tests for oedacli\n\nJUST THE wrapper for all  oedacli  tests\n\nCALLS existing standalone tests",
    "platform": null
  },
  {
    "test_name": "tsagoedaclinetwork.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaclinetwork.tsc - oedacli network testing\n\nunit testing for alter network\n\ntest oedacli alter network command",
    "platform": null
  },
  {
    "test_name": "tsagoedaclisshkey.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaclisshkey.tsc  - this file run oedacli script with ssh key funtion\n\nrun oedacli script with ssh key generation and deployment and passwordlogin\n     it need the rack nshc01adm03/04 and nshc01celadm04/05/06 to be up\n     please run this test once you have change to oedacli.\n     please check below to see how to run.\n\nssh key tests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedacliundoclonenewcell.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedacliundoclonenewcell.tsc  - run oedacli script to undo clone newcell\n\nrun oedacli script to remove new cell to a cluster\n\ndrop newcell to a cluster",
    "platform": null
  },
  {
    "test_name": "tsagoedaclonecompute.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaclonecompute.tsc  - this file has unit tests for oedacli clone compute  tests\n\nUnit tests for CLONECOMPUTE\n\ntests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedaclonefiles.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaclonefiles.tsc  - this file has unit tests for oeda clones\n\nUnit tests for oeda clone files\n\nClone file tests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedaclusterservice.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaclusterservice.tsc - Unit tests for cluster creation service",
    "platform": null
  },
  {
    "test_name": "tsagoedaconfigfile.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaconfigfile.tsc - Unit tests for oeda config file generation\n\nUnit tests for oeda config file generation",
    "platform": null
  },
  {
    "test_name": "tsagoedaconfigfilesselenium.tsc",
    "setup": null,
    "flags": {
      "cur_oeda_file_tmp": "oeda_selenium_file^i^",
      "cur_oeda_file": "^^cur_oeda_file_tmp^"
    },
    "description": "tsagoedaconfigfilesselenium.tsc - oeda selenium test which runs with webui with cdn enabled. This test uses the oeda webui which\n     gets the js and css from cdn.\n     When tsagrun_seleniumtest.sh is called from this test, a parameter local is passed. This is to make sure the test uses cdn\n\noeda selenium test for local runs\n\noeda selenium test",
    "platform": null
  },
  {
    "test_name": "tsagoedadatabasediscovery.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagoedadatabasehome.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedadatabasehome.tsc  - this file has unit tests for oeda domain groups\n\nUnit tests for databaseHome\n\nDatabase Home tests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedadiscovery.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagoedadiskgroup.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedadiskgroup.tsc\n\n   DESCRIPTION -Unit tests for testing OEDA diskgroup attributes",
    "platform": null
  },
  {
    "test_name": "tsagoedadiskgroups.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedadiskgroups.tsc - Unit test for diskgroup creation service",
    "platform": null
  },
  {
    "test_name": "tsagoedadiskgroups2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedadiskgroups2.tsc  - this file has unit tests for oeda domain groups\n\nUnit tests for diskgroups\n\nDskgroup tests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedadomaingroup.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedadomaingroup.tsc  - this file has unit tests for oeda domain groups\n\nUnit tests for domain groupfiles\n\nDomain Group tests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedaedvlibvolumeunits.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaedvlibvolumeunits.tsc -  unit testing for EDV volume, snapshot, and attachment\n\nUnit tests to parse the kommandOutput for volume, snapshot and attachment\n\nfor EDV volume library",
    "platform": null
  },
  {
    "test_name": "tsagoedaexascale.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaexascale.tsc  - this file has unit tests for oedacli exascle  tests\n\nUnit tests for EXASCALE\n\ntests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedaexascalediscovery.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagoedaexascaleselenium.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaexascaleselenium.tsc\n\noeda selenium test to convert asm config to exascale\n\noeda selenium test",
    "platform": null
  },
  {
    "test_name": "tsagoedaflashdg.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaflashdg.tsc - Test for oedacli commands to add/alter diskgroups with the diskgrouplocation token\n\nTest for add/alter diskgroup with diskgrouplocation - ER-30680929\n\nTest for add/alter diskgroup with diskgrouplocation - ER-30680929",
    "platform": null
  },
  {
    "test_name": "tsagoedageneralunits.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedageneralunits.tsc -  unit testing for all common OEDA junit methods testing\n\nUnit tests for all possible java methods\n\nrun test class methods, also run test classes parallel.",
    "platform": null
  },
  {
    "test_name": "tsagoedaguestcpumem.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaguestcpumem.tsc  - this file has unit tests for oeda guest cpu  and memory\n\nUnit tests for clone guest cpu  and mem\n\ntests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedaguestnat.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaguestnat.tsc  - this file has unit tests for oeda guest nat\n\nUnit tests for clone guest nat\n\ntests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedaimptst.tsc",
    "setup": null,
    "flags": {
      "cnt": "1"
    },
    "description": "tsagoedaimptst.tsc - OEDA import test",
    "platform": null
  },
  {
    "test_name": "tsagoedainstalltype.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedainstalltype.tsc - test for oedacli  commands to  alter installtype\n\ntest for InstallType ER\n\ntest for InstallType ER",
    "platform": null
  },
  {
    "test_name": "tsagoedajavadiskgroup.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedajavadiskgroup.tsc -  oeda diskgroup unit tests\n\noeda diskgroup unit tests\n\nOEDA Unit tests",
    "platform": null
  },
  {
    "test_name": "tsagoedalistcommandstests.tsc",
    "setup": null,
    "flags": null,
    "description": "Test for list commands in oedacli",
    "platform": null
  },
  {
    "test_name": "tsagoedamake.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedamake - make oeda\n\nOEDA unit tests\n\nOEDA Unit tests",
    "platform": null
  },
  {
    "test_name": "tsagoedamasklogpassword.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedamasklogpassword.tsc  - test mask password in log\n\nUnit tests for oeda to hide the password in oeda log file\n\nOEDA unit Testing to the password removal",
    "platform": null
  },
  {
    "test_name": "tsagoedamisctests.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagoedamoveguest.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedamoveguest.tsc - test for oedacli  commands to  alter MACHINE PARENT=\n\ntest for move guest\n\ntest for move guest",
    "platform": null
  },
  {
    "test_name": "tsagoedamtu.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedamtu.tsc  - this file has unit tests for oedacli  mtu tests\n\nUnit tests for MTU\n\nMTU tests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedanetworkservice.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedanetworkvalidation.tsc - Unit test for diskgroup creation service",
    "platform": null
  },
  {
    "test_name": "tsagoedanoexascale.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedanoexascale.tsc  - this file has unit tests for oedacli exascle  tests\n\nUnit tests for NOEXASCALE\n\ntests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedaocmdexceptiontest.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaocmdexceptiontest.tsc - this is unit testing to test OcmdException class\n\nrun OcmdException testing\n\nrun OcmdException  testing methods",
    "platform": null
  },
  {
    "test_name": "tsagoedaocrbackup.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaocrbackup.tsc  - this file has unit tests for OCR Backups\n\nUnit tests for oeda OCR Backups\n\nOEDA Test for OCR Backups",
    "platform": null
  },
  {
    "test_name": "tsagoedaovm.tsc",
    "setup": null,
    "flags": {
      "sep": "','",
      "domu_passwd": "welcome1"
    },
    "description": "tsagoedaovm.tsc - Helper script for running oeda on Exadata ovm",
    "platform": null
  },
  {
    "test_name": "tsagoedapasswdunit.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedapasswdunit.tsc -  unit testing for root/non-root password with encryption method (GCM)\n\nUnit tests for password\n\nrun test class methods",
    "platform": null
  },
  {
    "test_name": "tsagoedarackaltermachines.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedarackaltermachines.tsc - java unit testing for alter machines to rack\n\ntest oedacli new commands: add rack/alter switch/alter machine/alter machines to different rack\n\nXML change only, generate cli command files in T_WORK",
    "platform": null
  },
  {
    "test_name": "tsagoedasanitytst.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedasanitytst.tsc - Sanity test for OEDA.\n\nThis is Sanity test case for OEDA testing.\n     The test could only run on ol7 machines.\n\nThe script copies the dependent Firefox binaries and geckdriver from the specified location\n     /net/slc07dki/scratch/duazhu/oeda",
    "platform": null
  },
  {
    "test_name": "tsagoedasavefiles.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagoedaselenium.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedeaselenium.tsc - oeda selenium test\n\noeda selenium test\n\noeda selenium test",
    "platform": null
  },
  {
    "test_name": "tsagoedaseleniumimport.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedeaselenium.tsc - oeda selenium test\n\noeda selenium test\n\noeda selenium test",
    "platform": null
  },
  {
    "test_name": "tsagoedasqlutilsoratest.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedasqlutilsoratest.tsc  - this file has unit tests for Sql utils related tests.\n\nUnit tests for oeda Sql calls.\n\nOEDA Test for validating sql output.",
    "platform": null
  },
  {
    "test_name": "tsagoedasshkeyaccess.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedasshkeyaccess.tsc  - this file has unit tests for ssh key setup\n\nUnit tests for oeda ssh key setup/configuration\n\nOEDA Test for ssh key generation and setup ssh key access",
    "platform": null
  },
  {
    "test_name": "tsagoedastep1validation.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedastep1validation.tsc - run install.sh step1 validation\n\nrun install.sh step1 using all xml files in runallintegration runtable\n\nbetter to run this test after finished imaged for those racks in runtable;\n     otherwise, step 1 report errors",
    "platform": null
  },
  {
    "test_name": "tsagoedatestgridsetupcmd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedatestgridsetupcmd.tsc - test to check the the gridsetup command\n     for all gi versions\n\ngridsetup.sh",
    "platform": null
  },
  {
    "test_name": "tsagoedauloc.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedauloc.tsc  - this file has unit tests for oedacli  uloc tests\n\nUnit tests for LIST ULOC\n\nMTU tests for OEDA",
    "platform": null
  },
  {
    "test_name": "tsagoedaulocvalidation.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaulocvalidation.tsc -  validate uloc output\n\nvalidate the output of oedacli list uloc command\n\ncheck OedaCliOraTest.java",
    "platform": null
  },
  {
    "test_name": "tsagoedaupdatelimitsconf.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedaupdatelimitsconf.tsc  - this file has unit tests for updating limits.conf file\n\nUnit tests for oeda updating limits.conf file\n\nOEDA Test for OB - chown",
    "platform": null
  },
  {
    "test_name": "tsagoedavalidatediscoveredxml.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedavalidatediscoveredxml.tsc  - this file has unit tests for validating discovered es.xml\n\nUnit tests for oeda validating discovered es.xml\n\nOEDA Unit test.",
    "platform": null
  },
  {
    "test_name": "tsagoedavalidationutils.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedadiskgroups.tsc - Unit test for diskgroup creation service",
    "platform": null
  },
  {
    "test_name": "tsagoedavirtualmachine.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagoedavmcfgssh.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedavmcfgssh.tsc - java unit testing for vm.xml\n\ntest node access success/failure case with password/ssh key, also vm.xml generation\n\nrun after runallintegration. make sure the rack have vms configuration",
    "platform": null
  },
  {
    "test_name": "tsagoedavmunittests.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoedavmunitetests.tsc -  oeda vm unit tests\n\noeda vm unit tests\n\nOEDA Unit tests",
    "platform": null
  },
  {
    "test_name": "tsagoffclutab.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagccdef",
      "tablename": "tbchain"
    },
    "description": "tsagoffclutab.tsc - SAGE Exadata query offload tests clustered tables.\n\nExadata query offload tests w/ clustered tables\n\n- Short (unchained) vs chained rows\n     - PRE: With and without predicate filtering\n     - AGG: Aggregated vs not-aggregated function\n     - C0: Selecting the first columns (C0) of the table vs starting at a higher column number",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagoffloadcli.tsc",
    "setup": null,
    "flags": {
      "test_cmd_count": "0",
      "caseno": "0",
      "defaultcli": "bdscli",
      "defaultsrv": "bdsqlsrv"
    },
    "description": "tsagoffloadcli.tsc - Basic Test Cases for Offload Group\n\n< DESCRIPTION >\n\nKnowing bugs:\n   \t 1) Bug#15847533(CLOSED, fixed):\n      create offloadgroup all didn't return information\n      create offloadgroup longstring report confusing error information\n 2) Bug#15847767(CLOSED,not a bug):\n      case7 alter when shutdown offloadgroup, both succeed, expect:alter fail\n 3) Bug#14811593(CLOSED,invalid case):\n      case9 shutdown,drop offloadgroup didn't wait for workload complete",
    "platform": null
  },
  {
    "test_name": "tsagoflpscn.tsc",
    "setup": "srdbmsini",
    "flags": {
      "tsagofllog": "tsagoflpscntemp.log",
      "chkduration": "60",
      "midhookstart": "300",
      "workloadtime": "1800",
      "pfile_path": "^T_WORK^/t_init1.ora",
      "caseno": "pscn1",
      "caseoflgrp": "^sys_group_name^"
    },
    "description": "tsagoflpscn.tsc - test for bug 17721462\n\nrestart offloadgroup while parallel scan a big table\n     a) Start an offload group\n     b) Run a workload for say about 30 mns, where we are scanning\n         a reasonably large table (say a table of size 500MB). The scans\n         will be running in a loop. The table property should be altered\n         to run parallel scans (alter table <tab> parallel <num>)\n     c) Now in parallel, do a few alter table offload group shutdown and once\n          shutdown completes, restart the group, sleep 60 seconds, repeat",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagoflworkload.tsc",
    "setup": null,
    "flags": {
      "tsagofllog": "'tsagoflworkloadtemp.log'",
      "cur_osscellofl_package": "^ofl_rpm_name^",
      "old_osscellofl_package": "^ofl_rpm_name^",
      "chkduration": "20",
      "midhookstart": "150",
      "workloadtime": "360",
      "pfile_path": "^T_WORK^/t_init1.ora",
      "caseno": "28",
      "caseoflgrp": "^sys_group_name^",
      "casepackage": "^cur_osscellofl_package^"
    },
    "description": "tsagoflworkload.tsc - Test Cases that run workload for offloadgroup",
    "platform": null
  },
  {
    "test_name": "tsagonecommand.tsc",
    "setup": null,
    "flags": {
      "secure_password": "'hard;Root;Passw0rd'",
      "run_verifications": "true",
      "master_passwd": "'welcome1'",
      "asm_user": "grid",
      "db_user": "oracle",
      "asm_db_passwd": "welcome1",
      "script_name": "tsagonecinstall.sh"
    },
    "description": "tsagonecommand.tsc - Exadata Onecommand Setup",
    "platform": null
  },
  {
    "test_name": "tsagordsinstall.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "auto_local_undo": "true"
    },
    "description": "tsagordsinstall.tsc - Install ORDS on ExaScale\n\nThis file contains the scripts to install ORDS on ExaScale and\n     execute a few GET database APIs and then uninstall the ORDS setup\n\nConfluence Link - https://confluence.oraclecorp.com/confluence/display/EXC/Oracle+REST+Data+Services+%28ORDS%29+on+Exascale",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagordsuninstall.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagordsuninstall.tsc -This Script helps to Uninstall ORDS on ExaScale\n\nUninstall Oracle Rest Data Services on ExaScale",
    "platform": null
  },
  {
    "test_name": "tsagorigdskrpl.tsc",
    "setup": null,
    "flags": {
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagorigdskrpl.tsc - Original system disk relacement test\n\nNormal system disk removed and replaced with a system disk\n     from another cell and then the original system disk pushed back\n\nDEBUGCLI Test",
    "platform": null
  },
  {
    "test_name": "tsagorion.tsc",
    "setup": "tsaginit",
    "flags": {
      "m": "0",
      "n": "10",
      "runnum": "1",
      "nlarge": "10",
      "runtype": "rand",
      "ndisks": "1",
      "nsmall": "0",
      "size_large": "1024",
      "size_small": "0",
      "dur": "90",
      "write": "90",
      "thold": "30.00",
      "huge_no_need": "'-hugenotneeded'"
    },
    "description": "tsagorion.tsc - Performance benchmark test for SAGE using ORION (orion)\n\nRuns orion on ASM simulated disks\n\nStandard value to be determined after test is run\n     in a dedicated storage\n     The test can be run with differet parameters for numlarge & runtype\n     example: oratst -d tsagorion.tsc nlarge=20 runtype=seq\n              oratst -d tsagorion.tsc nlarge=40 runtype=rand",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagorion2.tsc",
    "setup": null,
    "flags": {
      "m": "0",
      "runnum": "1",
      "runtype": "rand",
      "dbid": "0",
      "cgid": "0",
      "ndisks": "1",
      "nsmall": "0",
      "nlarge": "0",
      "size_large": "1024",
      "size_small": "8",
      "dur": "180",
      "write": "0",
      "thold": "30.00",
      "huge_no_need": "'-hugenotneeded'",
      "advm": "'-is_advm'",
      "standby": "'-standby'",
      "gd1": "'datafile0'",
      "gd2": "'datafile1'",
      "gd3": "'datafile2'",
      "gd4": "'datafile3'",
      "gd5": "'datafile4'"
    },
    "description": "tsagorion.tsc - Performance benchmark test for SAGE using ORION (orion)\n\nRuns orion on ASM simulated disks\n\nStandard value to be determined after test is run\n     in a dedicated storage\n     The test can be run with differet parameters for numlarge & runtype\n     example: oratst -d tsagorion.tsc nlarge=20 runtype=seq\n              oratst -d tsagorion.tsc nlarge=40 runtype=rand",
    "platform": null
  },
  {
    "test_name": "tsagorionworkload.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagorionworkload.tsc - runs orion workload on real hardware\n\nThis test exercises following combinations of orion workloads:\n     - Single Orion run (solo workload)\n     - One Orion issuing small I/Os and other issuing large I/Os. (OLTP versus scan)\n     - Both Orions issuing scans.\n\nTest runs on real hardware",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagorphanfilecleanup.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "media_type": "XT",
      "vault_db": "DATA",
      "tst_name": "tsagorphanfilecleanup"
    },
    "description": "tsagorphanfilecleanup.tsc - EXASCALE: TEST CASES FOR\n                                 ORPHANED FILES' CLEANUP\n\nTest to verify that orphaned files are cleaned up\n      after failed DB operations\n\nFiles can be orphaned after database creates files but\n     cannot commit them due to various failure scenarios :\n     Case 1 : A process creates file but cannot proceed to commit\n              resulting in invoking abort which inturn deletes the\n              file.\n     Case 2 : Process death after create but before commit =>\n              resulting in file delete from PMON/CLMN process\n     Case 3 : Instance death - On instance fence due to death, EDS\n              should delete the orphaned files",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagoscsscan.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagoscsscan.tsc - Run OSCS scan against oss\n\nRun OSCS scan against OSS and flag Vulnerable Artifacts",
    "platform": null
  },
  {
    "test_name": "tsagossdth.tsc",
    "setup": "tsaginit",
    "flags": {
      "auto_undo_management": "true",
      "asm_ausize": "65536",
      "creatdev_file": "tkfgrddef",
      "compatible": "^def_compatibility^"
    },
    "description": "tsagossdth.tsc - SAGE OSS death test\n\nOSS comes down when IO's are in progress",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagossprofile.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagossprofile.tsc - OSS Wrapper for profile workload\n\nIt is OSS wrapper for profile workload script - tkvfoss.tsc",
    "platform": null
  },
  {
    "test_name": "tsagparfait.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagparfait.tsc - Automate Parfiat run and produce error report for user txns\n\nAutomate pd-parfait wrapper run against OSS source codes. It produces\n     error report for any detected coding error or warning for possible\n     memory issue at the end of the run, which generates a new diff file\n     each time it detects the error/warning.\n\nThis test is meant to catch parfait errors in the user txns. Parfait is\n     also run during nightly label build w/ report uploaded to parfait server.\n     The goal of this test is to catch parfait errors before they are introduced\n     in the label. Ideally nightly parfait report should not report any new errors.\n\n     Please note that any changes to thirdparty - fastcgi should not be in the same\n     txn as other code files. As parfait can only run against fastcgi or other non-cgi\n     code files. It can not do both since parfait itself does not know how to build for\n     fastcgi.",
    "platform": null
  },
  {
    "test_name": "tsagpasswdexpry.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagpasswdexpry.tsc - Test to check password expiry for exacli",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpdalrt.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagpdalrt.tsc - Test for bug 13376309\n\nTest for Bug 13376309. A brief summary of the test:\n      1. Delete Luns of specific slots and add in reverse order.\n      2. Check the LUNs, CDs ,PDs associations and status.\n      3. Simulate disk pridictive faliure.\n      4. Check the  LUNs, CDs ,PDs associations, status and generated alerts.",
    "platform": null
  },
  {
    "test_name": "tsagpdb_fcsize.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cell_with_pmem_cache": "true      ## create pmem cache",
      "creatdev_file": "tsagrddef4",
      "file_dest": "'+datafile'"
    },
    "description": "tsagpdb_fcsize.tsc - Test for Bug# 34146527 - PDB FCMAXSIZE SET\n                          TO 0 WHEN THE NORMALIZED MAX IS ZERO EVEN FOR ADB\n\nThis is about flashcachesize of PDBs in ADBS.\n    In ADBS, PDBs flashcachesize is calculated as CDB_flashcache_size X\n   PDB_max_size / Diskgroup_size in:\n   https://confluence.oraclecorp.com/confluence/pages/viewpage.action\n   ?spaceKey=~cecilia.gervasio@oracle.com&title=FC+Space+on+ADB-S\n   But it became zero which is impossible in ADBS, when there are some PDBs\n   whose sizes very small. When this happens, a normalization logic in IORM\n   kicks in and flashcachesize for some PDBs becomes zero.\n\nto be added in lrgdbcsapdbmaxsize",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpdbcc2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagpdbcc2.tsc - Check CC2=65 in CDB mode\n\nCheck CC2=65 in CDB mode\n     Create the CC2(imcfclv=65) table in PDB1 and PDB2 with same table name\n     1. Connect to PDB1\n     Turn on CC2=65 and create table tbl_65 cellmemory memcompress for query\n     2. Connect to PDB2\n     Turn on CC2=65 and create table tbl_65 cellmemory memcompress for query\n     3. Connect to PDB1\n     Query tbl_65, CC2 check expects SUCCESS\n     4. Connect to PDB2\n     Query tbl_65, CC2 check expects SUCCESS\n     5. Connect to PDB2\n     Turn off CC2 and query tbl_65, CC2 check expects FAILURE\n     6. Connect to PDB1\n     Query tbl_65, CC2 check expects SUCCESS",
    "platform": null
  },
  {
    "test_name": "tsagpdbclniorm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagpdbclniorm.tsc - clean intra-dbplan, inter-dbplan and cat-dbplan\n\nclean intra-dbplan, inter-dbplan and cat-dbplan for all CDBs/PDBs",
    "platform": null
  },
  {
    "test_name": "tsagpdbini.tsc",
    "setup": "srdbmsini",
    "flags": {
      "pdb_section": "1"
    },
    "description": "tsagpdbini.tsc - Setup n CDB with n PDBs on Exadata\n\npdb_section=1: Setup 1 cdb and 'n' pdb in each cdb with normal redundancy. Each PDB gets its own set of GD.\n            we setup 1 CDB with 6 PDBs in lrgdbcsapdb by default\n      pdb_section=2: Setup n cdb and 'n' pdb in each cdb with normal redundancy. Each PDB gets its own set of GD.\n            we setup 3 CDBs WITH 2 PDBs in each cdb in lrgdbcsapdb1 by default\n      pdb_section=3: Setup n-cdbs+n-pdbs on n-view, one for each DB version. Each PDB gets its own set of GD.\n            we setup 1 CDB WITH 2 PDBs in each RDBMS version in lrgdbcsamdbpdb by default",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpdbsetiorm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagpdbsetiorm.tsc - Create testing tbs, set intra-dbplan, inter-dbplan and cat-dbplan\n\ncreate testing tbs, users, consume groups, categories",
    "platform": null
  },
  {
    "test_name": "tsagpdbsmart.tsc",
    "setup": null,
    "flags": {
      "db_loop": "2",
      "disabled_pdb": "1",
      "log_file": "tsagpdbservice_excld.log"
    },
    "description": "tsagpdbsmart.tsc - Run smart scan tests on all CDBs/PDBs\n\nCurrently we have below PDB tests:\n         1). Test case 1: For all individual PDB's make sure Smart-scan works for all PDB's\n         2). Test case 2: Disable smart scan in the first PDB of each CDB, then smart scan\n                          should only fail on disabled PDBs and succeed on the rest.\n         3). Test case 3: Shutdown 1 PDB. Keep other PDB up. Smart-scan should complete\n         4). Test case 4: Failure test with GD offline\n         5). Test case 5: Failure test with cellsrv shutdown\n     And we run PDB tests as below\n         pdb_section=1: Run testcase 1-5 in lrgdbcsapdb\n         pdb_section=2: Run testcase 1-5 in lrgdbcsapdb1\n         pdb_section=3: Run testcase 1-2 in lrgdbcsamdbpdb",
    "platform": null
  },
  {
    "test_name": "tsagpdbxcellsetup.tsc",
    "setup": null,
    "flags": {
      "x5_af_setup": "true",
      "num_af_dev": "12",
      "nvme_size": "2272",
      "sage_mirror_mode": "normal",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "num_pdbs": "3",
      "pdb_gdsize": "384M",
      "cellname": "secondcell",
      "asmdisks_created": "2",
      "nflint": "1",
      "numlogfiles": "2",
      "do_not_set_numlogfile": "true"
    },
    "description": "tsagpdbxcellsetup.tsc - Sets up Griddisks on 12 EF cells\n\n2 Cells\n     Each cell has 12 EF disks\n     Extra GD created = 9 on each cell\n     3 GD used in 1 Failgroup for a new DG (for each PDB) ## Total 3 PDBs",
    "platform": null
  },
  {
    "test_name": "tsagpdbxdb.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagpdbxdb.tsc - Tests disk level isolation for PDB express\n\nCreates 3 PDBs on 1 CDB\n    Each PDB is on a different DiskGroup\n    Each such diskgroup has 2 Failgroups of 3 disks each\n    Creates 3 cell users, 1 for each PDB and assign required Privileges\n    Run some exacli commands using these users",
    "platform": null
  },
  {
    "test_name": "tsagpdebugclipmem.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagpdebugclipmem.tsc -  PMEM support  DebugCLI test\n\nFailure simulation tests:\n   1- predictive failure\n   2- fail disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpdpf_subheap.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagpdpf_subheap.tsc - test script for predicate disk/ predicate filter heap allocation limits\n\nThis script has 3 test cases\n     test case 1: simulate event to allcate a 100MB chunk on subheap. Run a SI query. Then check\n                  for ORA-700 soft assert and cellsrvstat have >0 stat\n     test case 2.1: simulate event to allocate 10 chunks of 10MB on subheap. Run SI query. Then check\n                  for ORA-700 soft assert and cellsrvstat have >0 stat\n     test case 2.2: same with test case 2, only that we are testing a fplib bug by using \"subheap_alf\"\n                  in the simevent\n     test case 3:\n       Step 1: set parameters _cell_pred_metadata_subheap_size_mb=1 and\n               _cell_pred_filt_max_subheap_size_mb=100\n       Step 2: set the table\n       Step 3: save the cell_stat file and check for stat\n               \"Number of Smart IO libinit nomem exceptions\"\n       Step 4: check for ORA-700 soft assert\n       Step 5: unset the parameters\n\nNone",
    "platform": null
  },
  {
    "test_name": "tsagpermsparsedg.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagpermsparsedg.tsc - Test for sparsedb\n\nTry to open sparsedb after removing write permission\n      on sparse diskgroup.",
    "platform": null
  },
  {
    "test_name": "tsagphydisk.tsc",
    "setup": null,
    "flags": {
      "trcname": "fsatrace.log"
    },
    "description": "tsagphydisk.tsc - Physical Disk tests\n\nIt tests the various operations that can be performed on Physicaldisk\n     and LUN through SAGE command line interface.",
    "platform": null
  },
  {
    "test_name": "tsagphydsk_hw.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagphydisk.tsc -\n\nIt tests the various operations that can be performed on Physicaldisk and LUN through SAGE command line interface.",
    "platform": null
  },
  {
    "test_name": "tsagphydskzap.tsc",
    "setup": "tsagnini",
    "flags": {
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagphydskzap.tsc - Test for powering off while disk failure simulation\n\nIf a normal system disk is pulled before disk was completely powered off\n    while simulating failure on disk. And the disk was replaced with a disk\n    which already has system partitions, then the newly inserted disk\n    imaging will fail as the newly inserted disk will not be zapped\n    (because the previous disk was not bad).",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpmemcachesizing.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagpmemcachesizing.tsc - PMEMCache sizing test\n\nThis is a stand alone test that verifies two things:\n     1. Drops existing PMEMCache and recreates it of a certain size\n        larger than 16M, then makes sure the cache is created of desired size.\n\n     2. Restart cellSrv after recreating PMEMCache and list griddisk detail to\n        verify that cachedBy attribute shows the celldisk name on which PMEMCache\n        has been recreated.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpmemcdbini.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "cell_with_pmem_cache": "1",
      "processes": "200",
      "sessions": "220",
      "OSS_ENABLE_FC_PERSISTENCE": "WriteBack",
      "OSS_ENABLE_NC_PERSISTENCE": "Off",
      "myoutbasename": "tsagpmemcdbinio.log",
      "mylogfile": "tsagpmemcdbini.log"
    },
    "description": "tsagpmemcdbini.tsc - sets up CDB env with IORM plan pmem and fc support\n\nThis script creates 2 CDBs having 4 PDBs in each\n     1. Default behavior: single cell setup with FlashCache WriteBack and PMEMCache WriteThrough\n     $ oratst tsagpmemcdbini.tsc\n\n     2. For single cell setup, FlashCache WriteThrough and PMEMCache WriteThrough\n     $ oratst tsagpmemcdbini.tsc fcmode=WriteThrough\n\n     3. For single cell setup, FlashCache WriteBack and PMEMCache WriteBack\n     $ oratst tsagpmemcdbini.tsc pmemmode=WriteBack\n\n     4. For two cell setup, FlashCache WriteBack and PMEMCache WriteThrough\n     $ oratst tsagpmemcdbini.tsc mirrormode=normal\n\n     5. For two cell setup, FlashCache WriteThrough and PMEMCache WriteThrough\n     $ oratst tsagpmemcdbini.tsc mirrormode=normal fcmode=WriteThrough\n\ntsagpmemcdbini sets up CDB env containing four PDBs with default flashcachemode\n     and pmemcachemode to WriteBack and IORM plan.",
    "platform": null
  },
  {
    "test_name": "tsagpmemfailure.tsc",
    "setup": null,
    "flags": {
      "tsaglogappend": "1",
      "myoutbasename": "^tst_tscname^o.log"
    },
    "description": "tsagpmemfailure.tsc - run db workload with pmemcache failure\n\n1. Bring up environment with 2 CDBs having 4 PDBs in each\n     2. Run db workload\n     3. Fail pmem cache in parallel\n     4. Check for crashes, there should not be any\n     5. Check for successful workload completion",
    "platform": null
  },
  {
    "test_name": "tsagpmemlog01.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tkfgcddef"
    },
    "description": "tsagpmemlog01.tsc - pmemlog cellcli tests\n\npmemlog cellcli tests\n\ntest cases present:\n1. perform various CELLCLI operations like create/drop/list/describe/alter on PMEMLOG\n   while DB workload is running concurrently. Ensure that DB workload finishes without failure.\n2. check that PMEMLOG persists across cell services restart\n3. try some CELLCLI operations on PMEMLOG while a cell service is down\n4. drop PMEM celldisk c9DXDDISK0 and check its impact on pmemlog\n5. add a new PMEM disk DXDDISKNEW and re-create PMEMLOG to use both the existing PMEM disks and new PMEM disk\n6. remove one PMEM disk DXDDISK0 and then bring the same PMEM disk back. Check its impact on PMEMLOG.\n7. remove one PMEM disk and replace it with a new PMEM disk. Check its impact on PMEMLOG.\n8. check PMEMLOG usage by ensuring Total number of PMEMLOG writes in cell alert logs should be non-zero.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpmemlog02.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagpmemlog02.tsc - pmemlog tests with DB workload\n\nPmemlog tests with DB workload\n\ntest cases present:\n   test for Bug 32011043: set _cell_pmemlog_flags=64 to generate saved redo, run DB workload and verify\n   saved redo is present. Apply_savedredo and confirm there is no more saved redo on PMEM Log\n1. With DB workload running in background, drop and re-create both the PMEM Celldisks.\n   Ensure that DB workload goes through fine.\n2. shutdown DB+ASM+cell, remove one PMEM disk DXDDISK0, open DB and perform DML,\n   shutdown db+asm+cell, bring the same PMEM disk back, open DB and perform DMLs.\n3. remove all the PMEM disks, perform DMLs, bring all the PMEM disks back, perform more DMLs.\n4. check PMEMLOG usage by ensuring Total # of PMEMLOG writes in cell alert logs should be non-zero.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpmemlog03.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagpmemlog03.tsc - PMEMLOG tests\n\nPMEMLOG tests\n\nThis test has 3 cells setup.\n1. cell 1 doesnt have PMEMLOG ; cell 2 and cell 3 have PMEMLOGs.\n   Ensure DMLs when PMEMLOG is not accessible. Shutdown cell 2 and cell 3 and perform DML operations.\n2. create/drop PMEMLOG in a loop and check freeSpace on PMEM celldisks\n3. Ensure that dropping PMEMLOG from the cells does not affect DMLs.\n   Drop pmemlog for all the cells one after another and verify DML operations.\n4. all 3 cells should have pmemcache and pmemlog. With DB workload running in background, crash cell 3.\n   Ensure that cell 1 and cell 2 are healthy and DB workload finishes without failure.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpmemlog04.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagpmemlog04.tsc - pmemlog tests\n\ntests for pmemlog\n\nThis test file uses pmemcache and flashcache in WriteBack mode.\n1. Shutdown DB+ASM+cell, remove 1 PMEM disk and then startup DB+ASM+cell.\n   PMEMLOG should be present with warning status.\n2. With DB workload running in background, simulate disk failure with failureType=predictivefailure\n   on all PMEM disks. Ensure that DB workload finishes without failure.\n3. With DB workload running in background, simulate I/O hang (BLOCKIO_ALL_HANG) on all the FLASH disks.\n   Ensure that DB workload finishes without failure.\n4. DB crash when DB workload is running. Ensure ASM and cell are healthy. DB can be started again.\n5. Generate a lot of redo and force drop/add ASM disk datafile0 from datafile diskgroup.\n   Ensure that DB workload finishes without failure. Do this in a loop.\n6. check PMEMLOG usage by ensuring Total # of PMEMLOG writes in cell alert logs should be non-zero.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpmemlog05.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagpmemlog05.tsc - pmemlog tests\n\npmemlog tests\n\nTestcase for bugs 33632844 and 33632754\n     Bug description:\n     One of the flash devices was experiencing very slow writes, hence\n     flushing writes for PMEM Log requests were getting stuck. Many PMEM\n     Log buffers were thus not being freed in a timely manner. LGWR\n     processses thus ended up getting RNR errors because of a lack of\n     free receive PMEM Log buffers in cellsrv.\n\n     Test case:\n     Step 1: Use 'simevent[BLOCKIO_ALL_HANG]' to simulate hangs on a flash device\n     Step 2: Configure orion to write to the same flash device, and then run orion\n     Step 3: After the orion test, confirm via cellsrvstat that the stat value for\n             num IOs cancelled before reaching OS is non-zero",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpmemlogoff.tsc",
    "setup": "srdbmsini",
    "flags": {
      "runfakeibsrv": "true"
    },
    "description": "tsagpmemlogoff.tsc - PMEM Log on/off capability test within IORMPLAN.\n\nPMEM Log on/off capability test within IORMPLAN.\n\nPMEM Log on/off capability test within IORMPLAN.\n\n     1) Start up all cell services, ASM, and db\n     2) Fork/run a db workload that will take about 5 minutes; this should\n        be some sort of PL/SQL loop that repeatedly updates a database table,\n        the loop needs periodic COMMIT statements to force log writes\n     3) Query PMEM Log writes from v$cell_state: value should be non-zero;\n     4) Run the following loop 10 times:\n        1. Disable PMEM Log:\n           alter iormplan dbplan = ((name = <db_name>, pmemlog=off));\n        2. Clear PMEM Log stats:\n           alter cell events = \"immediate cellsrv.cellsrv_resetstats('pmemlog')\"\n        - Sleep 5 seconds\n        - Query PMEM Log writes from v$cell_state: value should be zero\n          Enable PMEM Log:\n            alter iormplan dbplan = ((name = <db_name>, pmemlog=on));\n        - Sleep 5 seconds\n        - Query PMEM Log writes from v$cell_state: value should be non-zero\n\n     5) Wait for db workload to finish\n\n     SQL PMEM Log writes:\n\n     select to_number(extractvalue(xmltype(statistics_value),\n                                   '//stat[@name=\"num_writes\"]'))\n     \"num_writes\"\n     from v$cell_state\n     where statistics_type='PMEMLOG' and object_name like '%PMEMLOG';",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpmemoff.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef"
    },
    "description": "tsagpmemoff.tsc - test case for bug 33854422\n\nTest case for bug 33854422 to ensure IO request bypass pmemcache\n     if DB has pmemcache=off.\n\nTest steps:\n     (1) Create 2 test DBS (TEST5 & TEST6) and set default IORM plan.\n     (2) Run orion small read workload on 150MB disk area with dbid=6.\n         The entire 150MB region should get cached in pmemcache.\n         DB_PC_BY_ALLOCATED for TEST6 should be 150MB.\n     (3) Run orion small read workload on a separate 150MB disk area\n         with dbid=5. The entire 150MB region should get cached in\n         pmemcache. DB_PC_BY_ALLOCATED for TEST5 should be 150MB.\n     (4) Change IORM plan. Set pmemcache=off for TEST6.\n         This will trigger IORM policy enforcement. The TEST6 data\n         cached in pmemcache will get invalidated.\n     (5) Run orion small read workload on 300MB disk area again\n         with dbid=6. All reads should be NOCACHE reads that bypass\n         pmemcache. DB_PC_BY_ALLOCATED for TEST6 should be 0.\n         DB_PC_BY_ALLOCATED for TEST5 remains unchanged.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpmemspacemgmt.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagpmemspacemgmt.tsc - iorm db plan space management tests for PMEM\n\nPMEM IORM DB Plan tests - setting different pmemcachemin and pmemcachesize\n     pmem directive vlaues and see if IORM plan is successfully altered",
    "platform": null
  },
  {
    "test_name": "tsagpmemtest.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagpmemtest.tsc - Test for dax devices\n\nThis test checks the  following features on DAX devices\n  1- do orion  based reads/writes\n  2- detect whether a read from a local client is going through osslib (cellsrvstat)\n  3- metadata recovery --- delete the primary file, etc ,restart the cellsrv and checks\n     whether  files are created.\n  4- cellutil printing of metadata for dax\n  5- griddisk create/drop dxd disks\n  6- griddisk alter - groupname - new command\n  7- creation of dax diskgroup from dax griddisks",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagport.tsc",
    "setup": null,
    "flags": {
      "tmp_nowarn": "^tst_nowarn^",
      "has_hostname": "has_hostname_^j^",
      "tmpport": "oss_port^ossinst^",
      "tag": "cellsrv^ossinst^' tsagport caller='^tst_parent_name^",
      "oss_port": "^^tmpport^",
      "temp_has_num_node": "^has_num_nodes^"
    },
    "description": "tsagport.tsc - get free port number for use by cellsrv\n\nThis should be invoked as a macro or with runtest.\n     If adding virtual cell in the middle of the test, i.e. without\n     setting OSS_TESTING for the whole test, don't do it like this\n       ossinst 2\n       run tsagport\n       startoss\n     but like this\n       run tsagport ossinst=2                         # get port for 2nd cell\n       ossinst 2 no_startms                              # work with 2nd cell\n       EXECUTE ASIS T_SYSTEM:perl creatdev.pl -f devdef -d ^asm_raw_dir^\n       (or create disk files by some other means than creatdev.pl and run\n       startoss cleanstart      # point RS, MS to 2nd cell and start cellsrv)\n\n     This script also sets cellip.ora, cellinit.ora with new port,\n     sets oss_port, oss_port<ossinst> to new port,\n     sets oss_devdir, oss_devdir<ossinst>, asm_raw_dir to raw<port>,\n     sets raw to $T_WORK/raw<port>, and\n     sets sage_diskstring_dir, asm_diskstring_dir, raw_path<ossinst> to\n          o/<cellip>:<port>/",
    "platform": null
  },
  {
    "test_name": "tsagpostdbupg.tsc",
    "setup": null,
    "flags": {
      "db_con": "'sys/knl_test7 as sysdba'",
      "db_upgrade_version": "'19.0.0.0.0'"
    },
    "description": "tsagpostdbupg.tsc - Driver for post DB upgrade tests\n\nAny test that needs to be run on upgraded DB can be added here\n     This script is called from tklvossupg.tsc",
    "platform": null
  },
  {
    "test_name": "tsagpowercyclepmem.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cellconnstr": "root@^cell_name^"
    },
    "description": "tsagpowercyclepmem.tsc - iov + powercycle test with pmemcache\n\nsetup pmem disks and pmemcache\n   run iov on pmemdisk\n   powercycle the cell while iov is running\n   verify that pmemcache is normal and not degraded\n\ntest to be added in lrgrhx7sapmem",
    "platform": null
  },
  {
    "test_name": "tsagpq1mbalign.tsc",
    "setup": "srdbmsini",
    "flags": {
      "log_file": "tsagpq1mbalign.log"
    },
    "description": "tsagpq1mbalign.tsc - PQ 1MB alignment test case\n\nWhen the DB server sends data to the cell in Parallel Query mode, it send them as granules. A big table will have multiple granules.\n     CC2 handles data in 1MB chunks. Sometimes there is misalignment since the granules will not align to these 1MB chunks.\n     In these cases, there will be a flashcache miss since data is not stored in the cache. We can see this as misses in flashcache.\n     The number of granules has a dependency on the number of slaves (degree of parallelism-dop). So the test case Yali has mentioned will\n     have the following steps:\n\n       1) drop flashcache\n       2) create flashcache to a particular celldisk/girddisk (to isolate the test)\n       3) create a big table like lineitem table\n       4) select max(val) from table with dop 2 (which gets this to cache)\n       5) clear v$cellstate\n       6) Do select again.\n       7) Check the stats.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpqlv.tsc",
    "setup": null,
    "flags": {
      "tstname": "tsagpqlv"
    },
    "description": "tsagpqlv.tsc - Different PQ level simulation to CC test suite\n\n###########################################################\n     # Add different PQ level simulation to CC test suite\n     # Issue: When database decide to chop off extents into\n     #        smaller pieces and pushed down to cell where\n     #        we decide to do CC especially after we added\n     #        the less than 1mb CC support.\n     # Purpose: Does the different PQ level test in the cycles.\n     ###########################################################",
    "platform": null
  },
  {
    "test_name": "tsagprdd1.tsc",
    "setup": null,
    "flags": {
      "faildisk": "datafile0",
      "failparam": "_cell_disk_predfail_list",
      "faildesc": "'predictive failed disk'",
      "testdesc": "'Remove and replace predictive failed disk'",
      "tsagfailLog": "^tst_tscname^b.log",
      "myFailAct": "'run t_work:rmdsk'",
      "myRecoverAct": "'run t_work:repldsk'"
    },
    "description": "tsagprdd1.tsc - Exadata Predictive Disk Drop Test # 1\n\nBasic Predictive Drop test for a single predictive failure disk",
    "platform": null
  },
  {
    "test_name": "tsagprdd1f.tsc",
    "setup": null,
    "flags": {
      "faildisk": "FLASH0",
      "failparam": "_cell_disk_predfail_list",
      "faildesc": "'predictive failed FLASH disk'",
      "testdesc": "'Remove and replace predictive failed FLASH disk'",
      "tsagfailLog": "^tst_tscname^b.log",
      "myFailAct": "'run t_work:rmdsk'",
      "myRecoverAct": "'run t_work:repldsk'"
    },
    "description": "tsagprdd1f.tsc - Exadata Predictive Disk Drop Test with Flash Disks\n\nPredictive Drop test for a predicive failure on flash disks which have\n     Grid Disks and flashcache",
    "platform": null
  },
  {
    "test_name": "tsagprdd2.tsc",
    "setup": null,
    "flags": {
      "faildisk": "datafile1",
      "failparam": "_cell_disk_fail_list",
      "faildesc": "'failed disk'",
      "testdesc": "'Remove and replace failed disk'",
      "tsagfailLog": "^tst_tscname^b.log",
      "myFailAct": "'run t_work:rmdsk'",
      "myRecoverAct": "'run t_work:repldsk'"
    },
    "description": "tsagprdd2.tsc - test for failed disk\n\nBasic Predictive Drop test for a single failed disk",
    "platform": null
  },
  {
    "test_name": "tsagprdd2f.tsc",
    "setup": null,
    "flags": {
      "faildisk": "FLASH1",
      "failparam": "_cell_disk_fail_list",
      "faildesc": "'dead FLASH disk'",
      "workloadTbl": "flashdgtbl",
      "testdesc": "'Remove and replace dead disk",
      "tsagfailLog": "^tst_tscname^b.log",
      "myFailAct": "'run t_work:rmdsk'",
      "myRecoverAct": "'run t_work:repldsk'"
    },
    "description": "tsagprdd2f.tsc - Exadata Predictive Disk Drop Test with Flash Disks\n\nPredictive Drop test for a dead FLASH disk\n     which has a griddisk and flashcache on it.",
    "platform": null
  },
  {
    "test_name": "tsagprdd3.tsc",
    "setup": null,
    "flags": {
      "cell_perf_max_hd_proa_fail": "12",
      "cell_io_hang_drop_hard": "true",
      "slowdisk": "datafile2",
      "faildesc": "'slow disk'",
      "testdesc": "'Simulate slow disk then replace it'",
      "tsagfailLog": "^tst_tscname^.log",
      "myFailAct": "'run t_work:setslowevent'",
      "myRecoverAct": "'run t_work:reen_repldsk'"
    },
    "description": "tsagprdd3.tsc - test for slow disk\n\nBasic Predictive Drop test for a single slow disk",
    "platform": null
  },
  {
    "test_name": "tsagprdd3f.tsc",
    "setup": null,
    "flags": {
      "slowdisk": "FLASH2",
      "cell_perf_max_fd_proa_fail": "12",
      "cell_io_hang_drop_flash": "true",
      "faildesc": "'slow FLASH disk'",
      "tsagfailLog": "^tst_tscname^.log",
      "testdesc": "'Remove and replace slow FLASH disk'",
      "myFailAct": "'run t_work:setslowevent'",
      "myRecoverAct": "'run t_work:reen_repldsk'"
    },
    "description": "tsagprdd3f.tsc - Exadata Predictive Disk Drop Test with Flash Disks\n\nPredictive Drop test for a slow FLASH disk\n     which has a griddisk and flashcache on it.",
    "platform": null
  },
  {
    "test_name": "tsagprdini.tsc",
    "setup": "tsagnini",
    "flags": {
      "time2wait4alert": "59",
      "bar": "'|'",
      "oldtyp": "^dsktyp^",
      "dsktyp": "'('^dsktyp^')'",
      "errbasename": "tsagprdinitmp.log"
    },
    "description": "tsagprdini.tsc - initializations for tsagprd tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpredbupg.tsc",
    "setup": null,
    "flags": {
      "db_con": "'sys/knl_test7 as sysdba'"
    },
    "description": "tsagpredbupg.tsc - Driver for pre-DB upgrade tests\n\nAny test that needs to be run on pre-upgrade DB can be added here\n     This script is called from tklvoss2.tsc",
    "platform": null
  },
  {
    "test_name": "tsagpredpxyrd_awrstats.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagpredpxyrd_awrstats.tsc - tests AWR stats for Smart IO Partner Read\n\nTests Smart IO Partner Read AWR stats by querying v$cell_global table.",
    "platform": null
  },
  {
    "test_name": "tsagpredpxyrd_bug38087492.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagpredpxyrd_bug38087492.tsc - Test timeout cancellation for Smart IO Partner Read\n\nValidates Smart IO Partner Read behavior when the partner cell hangs\n     before responding to INIT or DATA requests. Uses simulation events to\n     delay partner cell responses and verifies that timeout cancellation\n     logic works as expected.\n\n     Simulation events used:\n       alter cell events = \"cellsrv_simevent[PRED_PXYRD_INITREQ_DELAY_RESP] ...\"\n       alter cell events = \"cellsrv_simevent[PRED_PXYRD_DATAREQ_DELAY_RESP] ...\"\n\nTest steps:\n       1. Connect to cellsrv1 and shut it down.\n       2. Execute a full table scan repeatedly. IOs will be directed to cellsrv2,\n          which will begin building the CC2.\n       3. Bring up cellsrv1, wait 60 seconds for synchronization to complete.\n       4. Enable the desired simulation event on cellsrv2.\n       5. Execute a full table scan again and check num_bytes_ptnr_hit and query execution time:\n            - For freq = 1: num_bytes_ptnr_hit should be 0\n            - For freq = 2: num_bytes_ptnr_hit should be > 0\n            - Execution time should be < 1s\n       6. Wait 60 seconds for the simulation event to finish and for stats to\n          update on both cells. Collect state dump on cellsrv1 and cellsrv2\n          to verify the statistics.\n\n     Four test cases:\n       PRED_PXYRD_INITREQ_DELAY_RESP freq = 1\n       PRED_PXYRD_INITREQ_DELAY_RESP freq = 2\n       PRED_PXYRD_INITREQ_DELAY_RESP freq = 1\n       PRED_PXYRD_INITREQ_DELAY_RESP freq = 2\n\n     Expected state dump statistics:\n       Primary cell (cellsrv1):\n         pred_pxyrd_read_jobs_succ                 = 0 (freq = 1); >0 (freq = 2)\n         pred_pxyrd_read_jobs_bytes_returned_toPD  > 0\n         pred_pxyrd_read_jobs_fail_initreq_timeout > 0 (for INIT simulation event); = 0 (for DATA simulation event)\n         pred_pxyrd_read_jobs_fail_datareq_timeout > 0 (for DATA simulation event); = 0 (for INIT simulation event)\n       Partner cell (cellsrv2):\n         pred_pxyrd_data_req_received              > 0\n         pred_pxyrd_data_req_success               = 0 (freq = 1); >0 (freq = 2)\n         pred_pxyrd_bufs_purged_timedout           > 0\n         pred_pxyrd_bufs_purged_timedout_bytes     > 0\n         pred_pxyrd_get_jobs_ptnr                  > 0\n         pred_pxyrd_get_jobs_bytes_ptnr            > 0\n         pred_pxyrd_get_jobs_succ_ptnr             > 0",
    "platform": null
  },
  {
    "test_name": "tsagpredpxyrd_bug38128750.tsc",
    "setup": "srdbmsini",
    "flags": {
      "oss_testing": "2"
    },
    "description": "tsagpredpxyrd_bug38128750.tsc - CELLSQLSTAT - SMARTSCAN PARTNERREAD STATS\n\nAdds test cases for cellsqlstat tool support for smartscan partnerread stats.\nv\n\nSteps for Test:\n       1. Setup exadata with 2 cells\n       2. Create a encrypted table\n       3. For _kdzf_columnar_cache: BLOCK CACHE, CC1 and CC2 repeat steps 4 - 10.\n 4. Create a pre snapshot of stats using cellsqlstast tool and capture process id\n       5. Switch to cell 1 and shutdown cellsrv services.\n 6.  Run full table scan\n 7.  Restart cellsrv services.\n 8. Create a post snapshot of stats using cellsqlstast tool and capture process id\n 9. execute full table scan and verify 'cell bytes read by partner cellsâ\n 10. Kill both cellsqlstast tool processes.\n 11. Check for existence of partner read section in stats for BLOCK CACHE\n 12. Check for existence of partner read section in stats for CC1\n       13. Check for existence of partner read section in stats for CC2",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpredpxyrd_bug38178155.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagpredpxyrd_bug38178155.tsc - kcfis partner read failure tests\n\nTestcases to check if kcfis handles proxy read translation failures\n     gracefully.\n\n     1.) Test that kcfis can handle the initial proxy read translation\n         failure correctly:\n\n         alter session set events=''27606 trace name context forever, level 0x10000000'';\n\n     2.) Test that kcfis can handle the proxy read translation failure\n         after IO completion gracefully:\n\n         alter session set events=''27606 trace name context forever, level 0x20000000'';\n\n     2.) Test kcfis session interrupt during proxy read:\n\n         alter session set events=''27606 trace name context forever, level 0x40000000'';\n\nAt the time of writing this test, the size of kcfis request batch is\n     fixed (128). To reproduce the original issue, one has to manually reduce\n     the size to a much smaller value in addition to the above simulation\n     events. If the size becomes configurable in the future, this test should\n     be updated to better exercise the code path.",
    "platform": null
  },
  {
    "test_name": "tsagpredpxyrd_bug38183370.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagpredpxyrd_bug38178155.tsc - FC deferred IO Partner Read Tests\n\nTestcases to check if Smart IO Partner Read works fine in case Cache\n     layer decides to deferred the local IO on the primary cell.\n\n     alter cell events=\"cellsrv_simevent[FLASHCACHE_DEFER_IOS] ...\"",
    "platform": null
  },
  {
    "test_name": "tsagprefetch_outofmem.tsc",
    "setup": "tsaginit",
    "flags": {
      "cell_with_flash_cache": "all",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagprefetch_outofmem.tsc - test for\n           Bug 38132880 - OCI: ORA-00752 DETECTED LOST WRITE ON PRIMARY\n           NEED HELP TO AVOID RECURRENCE AND RCA Opt-In Alert\n\npls see below\n\nadded in lrgsanvcache1",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprfen1.tsc",
    "setup": "tsagnini",
    "flags": {
      "auto_undo_management": "true",
      "asm_ausize": "65536",
      "creatdev_file": "tkfgrddef"
    },
    "description": "tsagprfen1.tsc - SAGE predicate fencing test # 1\n\nTests process death while doing predicate query\n\n- Bring up oss, diskmon, asm, rdbms\n     - Fork a process doing predicate query which will also do a HOLD\n     - Kill forgrouund doing the query\n     - Release\n     - Check alert*log and tables",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprfen2.tsc",
    "setup": "tsagnini",
    "flags": {
      "auto_undo_management": "true",
      "asm_ausize": "65536",
      "creatdev_file": "tkfgrddef"
    },
    "description": "Tests ASM death while doing predicate query\n\n- Bring up oss, diskmon, asm, rdbms\n     - Fork a process doing predicate query which will also do a HOLD\n     - Kill RDBMS\n     - Release using ASM instance\n     - Check alert*log and tables",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprfen3.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line"
    },
    "description": "tsagprfen3.tsc - SAGE ASM instance crash predicate fencing test\n\nCrashes ASM instance while doing predicate query\n     Needs RAC since second instance of ASM is used to release IO\n\n- set event to HOLD IO\n     - Fork sql which does predicate query\n     - shutdown abort ASM instance\n     - set event to RELEASE IO from another ASM instance\n     - check if predicate query fencing happened\n\n      **Note**\n     - Everything done through inst 2 (rdbms second instance) is commented out for now\n       since startup of inst 11 (ASM inst 11) after the shutdown abort hangs.\n       This is to be investigated later on .",
    "platform": null
  },
  {
    "test_name": "tsagprfen4.tsc",
    "setup": "tsagnini",
    "flags": {
      "cluster_database": "true",
      "maxinstances": "4",
      "auto_undo_management": "true",
      "asm_ausize": "65536",
      "creatdev_file": "tkfgrddef",
      "compatible": "^def_compatibility^"
    },
    "description": "tsagprfen4.tsc - SAGE implicit predicate fencing test\n\nKills CSS on a single node env when predicate quiery is being done\n\nThis test is not enabled due to the foll reason:\n     - tksaiofc3a.sql hangs after the instance comes up and checks for the table\n     - Only one alert*log shows the fencing msg. The other log does not the fencing msg.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprfensuit.tsc",
    "setup": "tsaginit",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "cluster_database": "true"
    },
    "description": "tsagprfensuit.tsc - SAGE Predicate Fencing tests\n\nDriver test for SAGE Predicate Fencing tests\n\nRun by oss_saprfe.tsc (lrgsaprfe)",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprmcp.tsc",
    "setup": "tsaginit",
    "flags": {
      "dangerousfg": "1",
      "differentszfg": "1",
      "creatdev_file": "tsagr3def"
    },
    "description": "tsagprmcp.tsc - Placing Primary copy on Outer tracks\n\nTests for \"Placing Primary copy on Outer tracks\".",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagprodd.tsc - Exadata Predictive disk drop test driver\n\nRuns predictive disk drop tests",
    "platform": null
  },
  {
    "test_name": "tsagprodd1.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "disk": "datafile0"
    },
    "description": "tsagprodd1.tsc - Exadata Predictive Disk Drop Test # 1\n\nBasic Predictive Drop test for a predicive failure for single disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd10.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagprodd10.tsc - Exadata Predictive Disk Drop Test # 10\n\nPredictive Drop test for a predicive failure\n     Start RDBMS workload\n     - For a normal redundancy mark\n     - one grid disk from cell1 and another griddisk from cell2 as predictive disk drop\n     - marked disks should go offline\n     - Workload should complete",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd11.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'"
    },
    "description": "tsagprodd11.tsc - Exadata Predictive Disk Drop Test with Flash Disks\n\n- All disks in both cells (with 3 disks each) have predictive disk drop\n     - Disks are marked to be dropped but since the redundancy rule\n       will be broken, not all disks in the DG are dropped.\n     - Now mark 3 disks in one cell as dead disks.\n     - These disks should be force dropped. But since at least once cell is left online\n       I/O's should compelete",
    "platform": null
  },
  {
    "test_name": "tsagprodd12.tsc",
    "setup": null,
    "flags": {
      "asm_ausize": "1048576",
      "SAGE_MIRROR_MODE": "normal",
      "vault_db": "DATA",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'"
    },
    "description": "tsagprodd12.tsc - Exadata Predictive Disk Drop Test with Flash Disks\n\n- Simulate predictive disk drop on all 3 disks on Cell # 1\n     - Disks are dropped. Disks are replaced.\n     - Simulate predictive disk drop on all 3 disks on Cell # 2\n     - Disks are dropped. Disks are replaced.\n     - Simulate slow disk on 1 disk on Cell # 2\n     - Simulate dead on 1 disk on Cell # 2\n     - Simulate predictive disk drop  on 1 disk on Cell # 2\n     - All of them should be dropped\n     - I/O's should compelete\n\nDisk used are FLASH disks",
    "platform": null
  },
  {
    "test_name": "tsagprodd13.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'"
    },
    "description": "tsagprodd13.tsc - Exadata Predictive Disk Drop Test with Flash Disks\n\n- In One cell in a dg (with 3 disks each) mark disks as dead disk\n    - In another cell (with 3 disks each) for the same dg\n      mark as slow disk or mark them as dead disk\n    - Disks should be brought offline and dg dismounted\n    - I/O's should not compelete",
    "platform": null
  },
  {
    "test_name": "tsagprodd14.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'"
    },
    "description": "tsagprodd14.tsc - Exadata Predictive Disk Drop Test with Flash Disks\n\n- Mark a disk slow\n     - Include that disk in create and mount diskgroup. This should succeed.\n     - After mounting marked disk goes offline",
    "platform": null
  },
  {
    "test_name": "tsagprodd14b.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'"
    },
    "description": "tsagprodd14b.tsc - Exadata Predictive Disk Drop Test with Flash Disks\n\n- Make all disks slow.\n     - Create and mount diskgroup with those disks. This should succeed.\n     - After mounting disks will be dropped and dg should be dismounted",
    "platform": null
  },
  {
    "test_name": "tsagprodd15.tsc",
    "setup": null,
    "flags": {
      "sage_mirror_mode": "high",
      "vault_db": "DATA",
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "asm_ausize": "1048576",
      "conn1": "'sys/knl_test7 as sysasm'"
    },
    "description": "tsagprodd15.tsc - Exadata Predictive Disk Drop Test with Flash Disks\n\nCell 1: Through cellcli mark two disks as inactive (alter griddisk\n    FLASH0 inactive) and mark 1 cell as slow performing.\n    The slow performing disk is taken offline\n    2) Meanwhile in Cell 1 the disks are marked active.\n    3) The slow performing disk is replaced.\n    4) At the end of the test, all disks are online\n\n    Note: Originally the test was marking disks in the second cell as slow\n    performing. But that brought the diskgroup down. So presently test\n    doesn't do anything with the second cell disks.",
    "platform": null
  },
  {
    "test_name": "tsagprodd16.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagprodd1.tsc - Exadata Predictive Disk Drop Test # 16\n\nUnit test case for flood control\n     Set slow disk event for more than 1 HD and more than 4 FD\n     For more than 1 HD and more than 4 HD the disks should not be dropped",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd17.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "sage_mirror_mode": "high",
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagprodd17.tsc - Exadata Predictive Disk Drop Test # 17\n\nBasic Predictive Drop test for two disks",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagprodd18.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "high",
      "disk": "datafile0",
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagprodd18.tsc - Exadata Griddisk Status Test #18\n\nAlter griddisk from online status to different status\n      -- 1.1 from online to online\n      -- 1.2 from online to replace\n      -- 1.3 from replace to online\n\nthe online status stands for the active status of griddisk\n     we do not use online command for asm directly\n     but only for cellside test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd19.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "high",
      "disk": "datafile0",
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagprodd19.tsc - Exadata Griddisk Status Test #19\n\nAlter griddisk from offline status\n       -- 1.1 from online to offline\n       -- 1.2 from offline to online\n       -- 1.3 from offline to offline\n       -- 1.4 from offline to replace\n       -- 1.5 from replace to offline\n\nthe online status stands for the active status of griddisk\n     we do not use online command for asm directly\n     but only for cellside test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd1f.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'",
      "disk": "FLASH0"
    },
    "description": "tsagprodd1f.tsc - Exadata Predictive Disk Drop Test with Flash Disks\n\nPredictive Drop test for a predicive failure on flash disks which have\n     Grid Disks and flashcache",
    "platform": null
  },
  {
    "test_name": "tsagprodd2.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "disk": "datafile0"
    },
    "description": "tsagprodd2.tsc - Exadata Predictive Disk Drop Test # 2\n\nBasic Predictive Drop test for a dead disk for a single disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd20.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "asm_power_limit": "11",
      "disk": "datafile0"
    },
    "description": "tsagprodd20.tsc - Exadata Griddisk Status Test #20\n\nAlter griddisk from offline status\n       -- 1.1 from offline to slow\n       -- 1.2 from offline to dead failure\n       -- 1.3 from offline to predictive failure\n\nthe online status stands for the active status of griddisk\n     we do not use online command for asm directly\n     but only for cellside test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd21.tsc",
    "setup": "tsagnini",
    "flags": {
      "ENABLE_CELLSRV_DUMP": "1",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "num_flash_per_cell": "8",
      "sage_mirror_mode": "high",
      "disk": "datafile0",
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "asm_power_limit": "11",
      "differentszfg": "1"
    },
    "description": "tsagprodd21.tsc - Exadata Griddisk Status Test #21\n\nAlter griddisk from predictive failure status\n       -- 1.1 from predictive failure to offline\n       -- 1.2 from predictive failure to online\n       -- 1.3 from predictive failure to slow\n       -- 1.4 from predictive failure to dead\n       -- 1.5 from predictive failure to predfail\n\nthe online status stands for the active status of griddisk\n     we do not use online command for asm directly\n     but only for cellside test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd22.tsc",
    "setup": "tsagnini",
    "flags": {
      "ENABLE_CELLSRV_DUMP": "1",
      "sage_mirror_mode": "high",
      "disk": "datafile0",
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagprodd22.tsc - Exadata Griddisk Status Test #22\n\nAlter griddisk from dead failure status\n       -- 1.1 from dead to predfail\n       -- 1.2 from dead to slow\n       -- 1.3 from dead to dead\n       -- 1.4 from dead to offline\n       -- 1.5 from dead to online\n\nthe online status stands for the active status of griddisk\n     we do not use online command for asm directly\n     but only for cellside test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd23.tsc",
    "setup": "tsagnini",
    "flags": {
      "disk": "datafile0",
      "file_dest": "@^vault_db^",
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "asm_power_limit": "11"
    },
    "description": "tsagprodd23.tsc - Exadata Griddisk Status Test #23\n\nAlter griddisk from poor performance\n       -- 1.1 from slow to offline\n       -- 1.2 from slow to online\n       -- 1.3 from slow to predfail\n       -- 1.4 from slow to slow\n       -- 1.5 from slow to dead\n\nthe online status stands for the active status of griddisk\n     we do not use online command for asm directly\n     but only for cellside test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd24.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagprodd24.tsc - Exadata Predictive Disk Drop Test # 17\n\nBasic Predictive Drop test for two disks",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd2f.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'",
      "disk": "FLASH1"
    },
    "description": "tsagprodd2f.tsc - Exadata Predictive Disk Drop Test with Flash Disks\n\nPredictive Drop test for a dead disk with the disk being a flash\n     disks which have Grid disks and flashcache",
    "platform": null
  },
  {
    "test_name": "tsagprodd3.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "disk": "datafile0"
    },
    "description": "tsagprodd1.tsc - Exadata Predictive Disk Drop Test # 3\n\nBasic Predictive Drop test for a slow disk for a single disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd3f.tsc",
    "setup": null,
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'",
      "asm_allow_sysdba": "true",
      "disk": "FLASH2"
    },
    "description": "tsagprodd3f.tsc - Exadata Predictive Disk Drop Test with Flash Disks\n\nPredictive Drop test for a slow disk with the disk being a flash\n     disks which have Grid disks and flashcache",
    "platform": null
  },
  {
    "test_name": "tsagprodd4.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "disk": "datafile0",
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "asm_allow_sysdba": "true"
    },
    "description": "tsagprodd4.tsc - Exadata Predictive Disk Drop Test # 4\n\nTests discovery code path for slow disks\n     - Simulate a Predictive Drop test for a slow disk\n     - Shutdown abort the ASM instance\n     - Startup ASM instance and make sure that the slow disk is not mounted\n     - Change the griddisk status from slow failure to replaced\n\nSince the discovery code path is the same this test is run\n     only with one type of disk drop (i.e slow disks-not predictive drop\n     or dead disk)",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagprodd5.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "disk": "datafile0",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "asm_allow_sysdba": "true",
      "oss_failgroup": "failalldbdg"
    },
    "description": "tsagprodd5.tsc - Exadata Predictive Disk Drop Test # 5\n\n- Have a RDBMS workload\n     - Simulate a Predictive Drop test for a dead disk\n     - Bounce one cell while the other is alive\n     - Workload should complete\n\n- Change griddisk status from dead failure to replaced",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagprodd5f.tsc",
    "setup": null,
    "flags": {
      "file_dest": "@^vault_db^",
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "conn1": "'sys/knl_test7 as sysasm'",
      "asm_allow_sysdba": "true",
      "log_file": "tsagprodd5f.log"
    },
    "description": "tsagprodd5f.tsc - Exadata Predictive Disk Drop Test with Flash Disks\n\nTests tries adding GD, FC, FL to a disk which is performing badly\n\nDoes not use RDBMS",
    "platform": null
  },
  {
    "test_name": "tsagprodd6.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "asm_allow_sysdba": "true",
      "disk": "datafile0"
    },
    "description": "tsagprodd6.tsc - Exadata Predictive Disk Drop Test # 6\n\n- Start RDBMS workload\n     - Drop force the same disk from ASM\n     - Simulate a prdicative disk drop. XDWK should say that disk not present\n     - Workload should complete",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd7.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "asm_allow_sysdba": "true",
      "cluster_database": "true",
      "maxinstances": "2",
      "disk": "datafile0"
    },
    "description": "tsagprodd7.tsc - Exadata Predictive Disk Drop Test # 7 on RAC\n\n- Test is in RAC with multiple ASM instances\n     - Simulate a prdicative disk drop\n     - Shutdown abort all ASM instances\n     - Start all ASM instances and mount dg\n     - Verify that the disk which was dropped stays dropped.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd8.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagprodd8.tsc - Exadata Predictive Disk Drop Test # 8\n\nPredictive Drop test for a predicive failure\n     Start RDBMS workload\n     For a normal redundancy mark\n     - Simulate one slow disk and 4 dead disks\n     - The marked disks should be dropped\n     - Since mirror disks are still alive dg is mounted\n     - Workload should complete\n     - Bring in news disks and they should be auto onlined",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd8a.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef",
      "disk": "datafile0"
    },
    "description": "tsagprodd8a.tsc - Exadata Predictive Disk Drop Test # 8a\n\nBasic Predictive Drop test for a slow disk for a single disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagprodd9.tsc",
    "setup": "tsagnini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagprodd9.tsc - Exadata Predictive Disk Drop Test # 9\n\nPredictive Drop test for a predicive failure\n     Start RDBMS workload\n     - For a normal redundancy mark\n     - Simulate predictive disk failure on 4 disks and slow performance on 1 disk\n     - The marked disks should be dropped\n     - Since mirror disks are still alive dg is mounted\n     - Workload should complete\n     - Bring in news disks and they should be auto onlined",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagproxyread.tsc",
    "setup": null,
    "flags": {
      "oss_testing": "2",
      "flash_size": "420",
      "creatdev_file": "tsagrh2def"
    },
    "description": "tsagproxyread.tsc - test case for proxy read feature\n\nThis tset has below steps:\n     Step 1: create the test tables\n     Step 2: populate FC with frist table - lineitem_1\n     Step 3: Switch to cell 1 and shutdown cellsrv\n     Step 4: populate FC with second table - lineitem_2\n     Step 5: Start up cell 1\n     Step 6: run scan on second table, cell 1 should be in proxy mode\n     Step 7: Check the cellsrvstats\n\nProxy Read for CC is not supported yet, when CC is used,\n     Proxy Read is not used. So CC is disabled on System level",
    "platform": null
  },
  {
    "test_name": "tsagproxyread_diskrep.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cdb": "true",
      "compatible": "^def_compatibility^",
      "dbconn": "'sys/knl_test7 as sysdba'",
      "asmconn": "'sys/knl_test7@inst11 as sysasm'"
    },
    "description": "tsagproxyread_diskrep.tsc - test case for ASM proxy read - disk replacement\n\nThis test requires 2 cells\n     Test steps:\n     Step 1: create test table and run in the background\n     Step 2: simulate failure on HardDisk\n     Step 3: cancel simulation of failure\n     Step 4: Ensure that the disk health factor goes from bad to good\n     Step 5: Check proxy read message from alert log and check cellsrv stats\n\nProxy Read for CC is not supported yet, when CC is used,\n     Proxy Read is not used. So CC is disabled on System level",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagproxyreadiov.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1"
    },
    "description": "tsagproxyreadiov.tsc - proxy read iov tests\n\niov testing for proxy read",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagproxyreadorion.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback"
    },
    "description": "tsagproxyreadorion.tsc - Orion Proxy Read Tests\n\nProxy read functional tests\n\n     Coverage:\n     - Proxy read local miss -> remote (fc/xrmem) hit\n     - Proxy read local miss -> remote miss\n     - Proxy read local hit\n     - Proxy read local flash hit -> xrmem pop\n     - Local read/write to flush while proxy read outstanding\n     - Proxy read while management operations\n     - Proxy read remote no prefetch\n     - Proxy read local no prefetch\n     - Proxy read bg pop prefetch\n     - Bg population redundancy check\n     - Proxy read error injections\n     - New Cell Key creation during Proxy read workload\n\n     Tips:\n     Run this test to generate orionproxyreadtest.sh\n     The script can be used for more controlled debugging",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagproxyreadorionmulti.tsc",
    "setup": "tsagnini",
    "flags": {
      "OSS_ENABLE_FC_PERSISTENCE": "writeback"
    },
    "description": "tsagproxyreadorionmulti.tsc - Orion Proxy Read Tests Multi Cell\n\nProxy read functional tests with multi cell\n\n     Coverage:\n\n     Tips:\n     Run this test to generate orionproxyreadtestmulti.sh\n     The script can be used for more controlled debugging",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpushback_offloadqueries.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagpushback_offloadqueries.tsc - Pushback predicate filtering\n\nPushback (Reverse offload) predicate filtering is now\n     fair for serial and parallel queries running together. Before txn\n     sidchoud_mpp_serialquery when a serial or low DOP query was run in\n     parallel with high DOP queries, there use to be no pushback for the\n     serial query. This was unfair for the high DOP queries since they\n     disproportionately would get impacted by pushback (reverse offload)\n     causing query time impact. This test validates pushback for a mixture of\n     queries to ensure the feature is working as intended",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagpwrcycle.tsc",
    "setup": "tsagnini",
    "flags": {
      "oss_auto_manage_disks": "true"
    },
    "description": "tsagpwrcycle.tsc - Debugcli test\n\nThe test checks for the following-\n     - Test -1 : Failed disk comes back normal after power-Cycle\n     - Test -2 : Failed disk don't comes back normal (it will remain failed) even after power-Cycle\n     - Test -3 : Simulate timeout for powercycle of a failed disk - disk will go to failed status\n     - Test -4 : Powercycle a normal physicaldisk - cli should throw an error\n     - Test -5 : Powercycle a normal physicaldisk with force- disk should be powercycled\n     - Test -6 : Power-off command fails - Physicaldisk should go to failed state\n     - Test -7 : Power-on command fails - Physicaldisk should go to failed state\n     - Test -8 : Test backdoor feature",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagqlcfailure.tsc",
    "setup": null,
    "flags": {
      "CELL_WITH_QLC_DISK": "1",
      "sage_mirror_mode": "normal",
      "differentszfg": "1",
      "QLC_DISK_SIZE_MB": "5120"
    },
    "description": "tsagqlcfailure.tsc - TEST FOR QLC DISKS FAILURE\n\nsimulate failures like resilver, predictive,confinement,pd fail,scrub\n      for more details, pls see below\n\nto be added in dbconsamsqlcdisk1",
    "platform": null
  },
  {
    "test_name": "tsagqlcfailure2.tsc",
    "setup": null,
    "flags": {
      "CELL_WITH_QLC_DISK": "1",
      "sage_mirror_mode": "normal",
      "QLC_DISK_SIZE_MB": "3072"
    },
    "description": "tsagqlcfailure2.tsc - TEST FOR QLC DISKS FAILURE\n\nsimulating confinement on qlc disk using fkdiskstats.py\n\nto be added in dbconsamsqlcdisk1\n\n   Bug:36604069\n     Add back the old flash disks missing/powered off before\n    1. Display all physical disks status and copy\n       cell_disk_config xml before dropping test TLC and QLC Physical disks.\n    2. Shutdown services and copy back cell_disk_config xml to OSSCONF\n    3. Start services and check for disk creation w/ normal status\n\n    Replace mixed type of flash disks w/ new disks\n    1. Display all physical disks status and copy\n       cell_disk_config xml before dropping test TLC and QLC Physical disks.\n    2. Shutdown services and copy back cell_disk_config xml to OSSCONF\n    3. Mask the serial number of the test TLC and QLC physical disk\n        in the cell_disk_config.xml\n    4. Start services and display disks are auto created.\n    5. Remove masked disks from xml and list Physical disks",
    "platform": null
  },
  {
    "test_name": "tsagqmall.tsc",
    "setup": null,
    "flags": {
      "log_file": "tsagqmehcc"
    },
    "description": "tsagqmall.tsc - Run fined grained quarantine with  Attribute tests\n\nqm_attribute - This is the quarantine attribute what you want to test\n        the available values: SI, EHCC, OLTP, TRACE, EVA_PRE,\n        FAST_PRE, CHAINEDROW, BLOOMFILTER, DECRYPTION\n\n     section attribute - define which case will be tested\n       section=1: All attribute values and the different quarantine types\n       section=2: Resolving conflicts within plans when quarantines\n                  are different types\n       section=3: Resolving conflicts between full and fine grained quarantine\n       section=4: Resolving conflicts across plans for full and fine grained quarantines\n       section=5: Resolving conflicts across plans and databases\n\n     Usage:\n       runtest tsagqmall section=1 qm_attribute=SI",
    "platform": null
  },
  {
    "test_name": "tsagqmcli.tsc",
    "setup": null,
    "flags": {
      "log_file": "tsagqmcli.log"
    },
    "description": "tsagqmcli.tsc - Run Basic Cellcli quarantine command test cases",
    "platform": null
  },
  {
    "test_name": "tsagqmsi.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagqmsi.tsc - Run fined grained quarantine with SI Attribute tests\n\nqm_attribute - This is the quarantine attribute what you want to test\n        the available values: SI, EHCC, OLTP, TRACE, EVA_PRE,\n        FAST_PRE, CHAINEDROW, BLOOMFILTER, DECRYPTION\n\n     section attribute - define which case will be tested\n       section=1: All attribute values and the different quarantine types\n       section=2: Resolving conflicts within plans when quarantines\n                  are different types\n       section=3: Resolving conflicts between full and fine grained quarantine\n       section=4: Resolving conflicts across plans for full and fine grained quarantines\n       section=5: Resolving conflicts across plans and databases\n\n     Usage:\n       runtest tsagqmsi section=1 qm_attribute=SI",
    "platform": null
  },
  {
    "test_name": "tsagqmsql.tsc",
    "setup": null,
    "flags": {
      "sqllog": "tsagqmsql^iteration^^instance^.log",
      "connect_as_sysdba": "'connect sys/knl_test7 as sysdba'",
      "user": "qmuser"
    },
    "description": "tsagqmsql.tsc - Run QM queries",
    "platform": null
  },
  {
    "test_name": "tsagqosfailover.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "cellconnstr": "root@^cell_node^",
      "dbconnstr": "root@^db_node^",
      "reflogfile": "^tst_tscname^_x7.lst"
    },
    "description": "tsagqosfailover.tsc - test for qos failover\n\nTest steps are:\n     1- setup cell and dbnode environment\n     2- run moss to make sure communication went fine\n     3- block lane 4 with repeat 100000 rds-stress -Q 4 -r <cellip> -s <dbnodeip> --reset\n     4- while 3) going on start communication using moss\n     5 make sure qos failover occur\n\nwe are using active bonding with only 1 ip in this test.",
    "platform": null
  },
  {
    "test_name": "tsagqosfailover1.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagqosfailover1.tsc - test for qos failover\n\nTest steps are:\n     1- setup simulated env\n     2- run moss to make sure communication went fine\n     3- block LANE0 I/O using simulated event\n     4- Run moss again while I/Os are blocked\n     5 make sure qos failover occur",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagracc.tsc",
    "setup": "tsaginit",
    "flags": {
      "noecho_testname": "true",
      "maxinstances": "2",
      "cluster_database": "true",
      "creatdev_file": "tsagrddef",
      "conn": "'connect sys/knl_test7 as sysasm'",
      "connd": "'connect sys/knl_test7 as sysdba'"
    },
    "description": "tsagracc.tsc - Exadata cell.smart_scan_capable update test\n\na) Loop 1\n  1) Start asm1 and 2,rdbms1,create a diskgroup.\n  2) Mount dg is asm 2.\n  3) asm1-update cell.smart_scan_capable.\n  4) check in asm 2 that the change is reflected.\n\n  b) Loop 2\n  1) Start asm1 and 2,rdbms1,create a diskgroup.\n  2) Mount dg is asm 2.Start rdbms 2 and create a tb on the diskgroup.\n  3) asm1-update cell.smart_scan_capable.\n  4) Check the change is reflected in rdbms 2",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagracini.tsc",
    "setup": "tsagnini",
    "flags": {
      "cluster_database": "true",
      "maxinstances": "^max_instance^",
      "_rdbms_internal_fplib_enabled": "false",
      "ainst": "asm_instance^i^",
      "file_dest": "^asmprefix^datafile",
      "conn": "'sys/knl_test7 as sysdba'"
    },
    "description": "tsagracini.tsc - run rdbmsini with RAC, ASM and SAGE\n\nCreate RAC db on SAGE disks\n\n1. Make ipc_g sage_on oracle_rac before running this test.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagracksiteid.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagracksiteid.tsc -  test for ext cluster site/rack ids\n\ntest for ext cluster site/rack ids\n     Test checks that rack and site id default is 0's.\n     Test checks that rack and site id are set when rack and site name is set.\n     Test checks that rack and site names and id's persist.\n     Test checks that renaming rack or site does not change id if it exists.\n     Test checks that arbiter disk discovery sees the same site and rack ids.\n     Test checks that setting rack and site names empty will zero the ids\n\n     Negative test: site and rack names must be up to 15 alphanumerics",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagramcacheiov.tsc",
    "setup": "tsaginit",
    "flags": {
      "max_areas": "5  # max number of areas on each device",
      "gd": "'datafile0,datafile1'    # datafile griddisks",
      "duration": "600  # run IOV for 10mins",
      "blk_szlst": "'32K'",
      "io_szlst": "'512,1K, 1.5K, 2K, 2.5K, 3K, 3.5K, 4K, 8K, 16K,32K,64K,128K,256K, 1M, 4M'       # to set the num of blocks",
      "threads": "10",
      "ramcache": "true",
      "align": "y",
      "offset": "4  # start offset to be 4M aligned",
      "area_sz": "4",
      "creatdev_file": "tsagddef"
    },
    "description": "tsagramcacheiov.tsc - IOV test for RAM CACHE\n\nThis is basic IOV test for ram cache",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrbrcellfail.tsc",
    "setup": null,
    "flags": {
      "cache_hint": "'/*+ cache */'",
      "dbusr": "'sys/knl_test7 as sysdba'"
    },
    "description": "tsagrbrcellfail.tsc - RBR with Cell Failure test\n\nRBR with Cell Failure test",
    "platform": null
  },
  {
    "test_name": "tsagrcadg.tsc",
    "setup": "tsaginit",
    "flags": {
      "connect_as_sysdba": "'connect sys/knl_test7 as sysdba'",
      "def_sga_target": "800M",
      "sga_target": "^def_sga_target^",
      "use_self_tune_sga": "true",
      "failalldbdg": "true",
      "redund": "normal",
      "flash_size": "240",
      "flash_griddisk_size": "160",
      "uniq_dsknames": "FLASH",
      "TKDG_SETUP_WALLET": "ON",
      "rac": "2",
      "sdg_asm": "yes",
      "sdg_asm_redund": "normal",
      "db_cache_size": "16M",
      "DB_KEEP_CACHE_SIZE": "16M",
      "DB_RECYCLE_CACHE_SIZE": "16M",
      "loop_cntr": "400"
    },
    "description": "tsagrcadg.tsc - Ramcache Rac Test for ADG (Active Data Gaurd)\n\nThis is rac test which checks ramcache pops for standby and primary databases. It also checks for gc_policy_time set/unset.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrcblocksize.tsc",
    "setup": null,
    "flags": {
      "sga_target": "^def_sga_target^",
      "use_self_tune_sga": "true",
      "sage_mirror_mode": "normal",
      "flash_size": "240",
      "flash_griddisk_size": "160",
      "uniq_dsknames": "FLASH",
      "loop_cntr": "400"
    },
    "description": "tsagrcblocksize.tsc - Ramcache test for different block sizes 4k,8k and 32k\n\nTest describes ramcache population for 4k,8k,16k and 32k block size tablespaces.\n\nTest basically checks ramcache population for different tablespaces 4k,8k,16k and 32k. Different workloads for different blocksizes are used and ramcache stats are population accordingly.",
    "platform": null
  },
  {
    "test_name": "tsagrcflflash.tsc",
    "setup": null,
    "flags": {
      "def_sga_target": "665M",
      "sga_target": "^def_sga_target^",
      "use_self_tune_sga": "true",
      "connect_as_sysdba": "'connect sys/knl_test7 as sysdba'",
      "format_long_identifier": "true",
      "cluster_database": "true",
      "maxinstances": "2",
      "SAGE_MIRROR_MODE": "normal",
      "allow_pmem_gd": "true",
      "failalldbdg": "true",
      "flash_size": "240",
      "flash_griddisk_size": "160",
      "uniq_dsknames": "FLASH",
      "cell_with_ram_cache": "1",
      "db_cache_size": "16M",
      "DB_KEEP_CACHE_SIZE": "16M",
      "DB_RECYCLE_CACHE_SIZE": "16M",
      "loop_cntr": "3600"
    },
    "description": "tsagrcflflash.tsc - This test is to check flash failure and brownout bug in various scenarios.\n\nTest is to check the ramcache ioctls,flash pop ioctls and flash brownout values in different cases. It is for flashcache brownout bug 28011238.",
    "platform": null
  },
  {
    "test_name": "tsagrcmode.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sga_target": "512M",
      "use_self_tune_sga": "true",
      "flash_size": "240",
      "flash_griddisk_size": "160",
      "uniq_dsknames": "FLASH",
      "sage_mirror_mode": "normal"
    },
    "description": "tsagrcmode.tsc - Test to check enable disable for ramcachemode and other ramcachemode attributes.\n\nThis test checks for enable disable feature of ramcachemode",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagrcnonrac.tsc",
    "setup": null,
    "flags": {
      "sga_target": "512M",
      "use_self_tune_sga": "true",
      "failalldbdg": "true",
      "sage_mirror_mode": "normal",
      "flash_size": "240",
      "flash_griddisk_size": "160",
      "uniq_dsknames": "FLASH",
      "loop_cntr": "400"
    },
    "description": "tsagrcnonrac.tsc - NON RAC TEST FOR RAM CACHE\n\nSingle db instance test for ramcache\n\nNON-RAC test for RAMCACHE containing filled buffer pool",
    "platform": null
  },
  {
    "test_name": "tsagrcnonrac_1.tsc",
    "setup": null,
    "flags": {
      "sga_target": "512M",
      "use_self_tune_sga": "true",
      "sage_mirror_mode": "normal",
      "flash_size": "240",
      "flash_griddisk_size": "160",
      "uniq_dsknames": "FLASH",
      "loop_cntr": "400"
    },
    "description": "tsagrcnonrac_1.tsc - Non-rac test to check ramcache disable feature\n\nThis test will check the ioctls after setting up disable event for ramcache",
    "platform": null
  },
  {
    "test_name": "tsagrcrac.tsc",
    "setup": null,
    "flags": {
      "def_sga_target": "800M",
      "sga_target": "^def_sga_target^",
      "use_self_tune_sga": "true",
      "connect_as_sysdba": "'connect sys/knl_test7 as sysdba'",
      "format_long_identifier": "true",
      "cluster_database": "true",
      "maxinstances": "2",
      "failalldbdg": "true",
      "sage_mirror_mode": "normal",
      "flash_size": "240",
      "flash_griddisk_size": "160",
      "uniq_dsknames": "FLASH",
      "creatdev_file": "tsagrddef",
      "loop_cntr": "400"
    },
    "description": "tsagrcrac.tsc - ramcache rac tests contains basic rac test and table drop test\n\nThis test consist of basic ramcache rac test and table drop rac test for ramcache.",
    "platform": null
  },
  {
    "test_name": "tsagrcracdrop.tsc",
    "setup": null,
    "flags": {
      "sga_target": "512M",
      "use_self_tune_sga": "true",
      "format_long_identifier": "true",
      "cluster_database": "true",
      "maxinstances": "2",
      "failalldbdg": "true",
      "sage_mirror_mode": "normal",
      "flash_size": "240",
      "flash_griddisk_size": "160",
      "uniq_dsknames": "FLASH",
      "loop_cntr": "400"
    },
    "description": "tsagrcracdrop.tsc - RAC Test to check ramcache during filled buffer pool\n\nTest is to check ioctls when buffer pool is filled.\n\nRAC test to check ramcache after dropping buffer pool",
    "platform": null
  },
  {
    "test_name": "tsagrcre.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true",
      "asm_ausize": "4194304",
      "creatdev_file": "tsagrcrez"
    },
    "description": "tsagrcre.tsc - Fast file creation optimization for REDO LOGs on SAGE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrcre2.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true",
      "asm_ausize": "4194304",
      "file_dest": "'+DATAFILE'",
      "file_dest1": "'+DATAFILE/'",
      "creatdev_file": "tsagrcrez2"
    },
    "description": "tsagrcre2.tsc - Fast file creation optimization for Standby REDO LOGs on SAGE",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrcscripts.tsc",
    "setup": null,
    "flags": {
      "vault_db": "DB^ORA_SID_UPPER^",
      "file_dest": "'+datafile'",
      "loop_cntr": "400"
    },
    "description": "tsagrcscripts.tsc - Ram cache script file\n\nScripts required to execute ramcache feature and its tests",
    "platform": null
  },
  {
    "test_name": "tsagrcset.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "normal",
      "flash_size": "240",
      "flash_griddisk_size": "160",
      "uniq_dsknames": "FLASH"
    },
    "description": "tsagrcset.tsc - Basic Ram Cache setup\n\nVery basic ram cache setup just to add, drop store and check stats\n\nSetup file to check ramcache operations",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrdma_crc.tsc",
    "setup": null,
    "flags": {
      "cell_with_flash_cache": "all",
      "cell_with_pmem_cache": "true",
      "creatdev_file": "tsagrddef"
    },
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagrdmasimu.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef"
    },
    "description": "tsagrdmasimu.tsc - Secure RDMA PMEMCache and simulation event test in one view\n\nSecure RDMA PMEMCache and simulation event test in one view\n\nSecure RDMA PMEMCache and simulation event test in one view\n     below is test steps:\n\n       1. Setup exadata without asm and db instance, and create 2 griddisks.\n       2. Create keys for multi-clusters.\n       3. alter griddisk to be availableTo specific cluster and restart\n          cellsrv/MS\n       4. Setup configuration for clusters. Use cluster1 for example.\n         (1) Under $T_WORK, create separate configuration directory for\n             cluster1 and cluster2.\n               $ mkdir cluster1_cfg",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrealhwimageinfo.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrealhwimageinfo.tsc - Real HW Image Version Information test\n\nSends out alerts for Real HW Cell/DBMCLI test nodes requiring reimage\n\nCalled from wrapper script tsagimageinfowpr.tsc",
    "platform": null
  },
  {
    "test_name": "tsagrealhwscripts.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrealhwscripts.tsc - Compilation of scripts used in realhw testcases",
    "platform": null
  },
  {
    "test_name": "tsagrecov.tsc",
    "setup": "srdbmsini",
    "flags": {
      "num_dbs": "2",
      "cdb_dd": "pdbs_across_cdbs",
      "cur_osscellofl_package": "^cur_ofl_rpm_name^",
      "pfile_path": "^T_WORK^/t_init2.ora",
      "caseno": "2_2",
      "caseoflgrp": "oflgrp2_2",
      "casepackage": "^cur_osscellofl_package^"
    },
    "description": "tsagrecov.tsc - Recovery test for RS\n\nUse simulation event to kill cellsrv/celloflsrv, test RS auto restart\n\nOffload template in tsaginitoflworkload.tsc",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrecparam.tsc",
    "setup": "srdbmsini",
    "flags": {
      "db_block_size": "8192",
      "db_block_checksum": "full",
      "setlang": "true"
    },
    "description": "tsagrecparam.tsc - This test set all the required ASM and RDBMS parameters\n                        according to the best practises doc to get the best\n\t\t  performance out of exadata\n\nThe RDBMS parameter \"use_large_pages\" is not added yet, as we are still figuring out\n      how to use it on real hardware.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrecreatexrmem_x11.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagxrmemcachex11.tsc - Test for cache drop and restore when\n                             FlashCache flush/cancel on x11\n\nTest steps :\n     1. We need to ensure that FlashCache, PmemCache, and XRMemCache\n        do not exist. If they do exist, drop all of them.\n        cellcli -e list <FlashCache/PmemCache/XRMemCache> detail\n        cellcli -e drop <FlashCache/PmemCache/XRMemCache> all\n     2. We need to change the cache mode to writeback to allow FlashCache\n        to be flushed.\n        cellcli -e alter cell FlashCachemode=writeback\n     3. The first test step is to create the FlashCache.\n        We can use the following command:\n        cellcli -e create FlashCache all\n     4. Run cellcli -e list <FlashCache/PmemCache/XRMemCache> detail.\n        We should see that all three caches are present.\n        PmemCache and XRMemCache both will show XRMemCache in the output.\n     5. Run cellcli -e alter FlashCache all flush\n     6. Repeat step 4. We should see FlashCache is flushed\n        and only FlashCache is present.\n     7. Run cellcli -e alter FlashCache all cancel flush\n     8. Repeat step 4. We should see FlashCache is normal\n        and all three caches are present.\n     9. Run cellcli -e drop FlashCache all\n    10. Repeat step 4. We should see all three caches are dropped.",
    "platform": null
  },
  {
    "test_name": "tsagrecreatexrmem_x9.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrecreatexrmem_x9.tsc - Test for re-create the XRMemCache when\n\t\t\tis flushed the Flash Cache on x9\n\nTest steps :\n     1. We need to ensure that FlashCache, PmemCache, and XRMemCache\n        do not exist. If they do exist, drop all of them.\n        cellcli -e list <FlashCache/PmemCache/XRMemCache> detail\n        cellcli -e drop <FlashCache/PmemCache/XRMemCache> all\n     2. We need to change the cache mode to writeback to allow FlashCache\n        to be flushed.\n        cellcli -e alter cell FlashCachemode=writeback\n     3. The first test step is to create the FlashCache.\n        We can use the following command:\n        cellcli -e create FlashCache all\n     4. Run cellcli -e list <FlashCache/PmemCache/XRMemCache> detail.\n     \t  We should see that only the FlashCache is present.\n     5. Run cellcli -e alter FlashCache all flush?\n     6. Repeat step 4. We should see FlashCache is flushed\n  \t  and only FlashCache is present.\n     7. Run cellcli -e alter FlashCache all cancel flush\n     8. Repeat step 4. We should see FlashCache is normal\n        and only FlashCache is present.\n     9. Run cellcli -e create PmemCache all\n    10. Repeat step 4. We should see FlashCache is present and\n        PmemCache will be shown for both list PmemCache detail and\n        list XRMemCache detail.\n    11. Run cellcli -e alter FlashCache all flush\n    12. Repeat step 4. We should see FlashCache is flushed, and\n        all three caches are present.\n    13. Run cellcli -e drop FlashCache all\n    14. Repeat step 4. We should see FlashCache is dropped but PmemCache\n        and XRMemCache will still output the PmemCache.\n    15. Run cellcli -e drop PmemCache all\n    16. Repeat step 4. We should see all three caches are not present.",
    "platform": null
  },
  {
    "test_name": "tsagredocache.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagredocache.tsc - Caching online redo log in FlashCache\n\nTest online redo log caching in FlashCache\n\nBelow is basic test steps:\n     (1) Create WriteBack FlashCache in the test.\n     (2) Have the table contain more rows, such as 128K rows, then the redo\n         log generation is much higher.\n     (3) Add \"alter system switch logfile\" to trigger archivelog write.\n     (4) Check cell side metrics. The following metrics shall be non-0.\n             \"Number of redo log read cache hit\",\n             \"Total size of redo log read cache hit (KB)\",\n             \"Number of redo log read population\",\n             \"Total size of redo log read population (KB)\",\n             \"Number of redo log write to cache\",\n             \"Total size of redo log write to cache (KB)\",\n     (5) Also check \"list flashcachecontent\" to check whether redolog is\n         cached in FC. There shall be 2 lines, one is cachedRedoContent for\n         primary database, the other is for physical standby database.\n   list flashcachecontent attributes dbUniqueName,dbID,tableSpaceNumber,\n         objectNumber,cachedSize where objectNumber = 4294967293;\n     For example, the result might look like:\n  CellCLI> list flashcachecontent attributes dbUniqueName,dbID,\n    tableSpaceNumber,objectNumber,cachedSize where objectNumber = 4294967293;\n           DGREDOLOGB      3009953870      0       4294967293      142036992\n           DGREDOLOG       2294494054      0       4294967293      140795904\n     (6) We also perform a FC stat dump for debugging reference purpose.\n alter cell events=\"immediate cellsrv.cellsrv_flashcache(dumpstats, 0, 0, 1)\"",
    "platform": null
  },
  {
    "test_name": "tsagreimage.tsc",
    "setup": null,
    "flags": {
      "SKIP_REIMAGE": "true",
      "FORCE_REIMAGE": "false",
      "cell_imagv": "\"\"",
      "cell_cellv": "\"\"",
      "dbnode_imagv": "\"\"",
      "view_imagv": "\"\"",
      "view_cellv": "^label^",
      "image_needed": "true",
      "reimage_fail": "true",
      "cell_reimage_fail": "true",
      "dbnode_reimage_fail": "true"
    },
    "description": "tsagreimage.tsc - File with code for re-imaging the cell ad dbnode\n\nThis script reimages based on some pre-checks.\n     Checks are as follows:\n\n     The script first determines (1) Image version on node\n                                 (2) Cell version on node\n                                 (3) Image version on dbnode\n                                 (4) Image version in view\n                                 (5) Cell version in view\n     Additionally, skip_reimage & force_reimage flags are imported.\n     If force_reimage is issued, re-imaging is done in all cases.\n     Else, if skip_reimage is issued, re-image is skipped.\n     If both skip_reimage and force_reimage are not issued,\n     re-imaging occurs if (1)!=(4) or (2)!=(5) or (3)!=(4). Else, it is skipped.\n\n     If re-imaging fails mid-way, the node is commented out\n     and added back to the pool.",
    "platform": null
  },
  {
    "test_name": "tsagreimage_db_cell.tsc",
    "setup": null,
    "flags": {
      "SKIP_REIMAGE": "true",
      "FORCE_REIMAGE": "false",
      "cell_imagv": "\"\"",
      "cell_cellv": "\"\"",
      "dbnode_imagv": "\"\"",
      "view_imagv": "\"\"",
      "view_cellv": "^label^",
      "image_needed": "true",
      "reimage_fail": "true",
      "cell_reimage_fail": "true",
      "dbnode_reimage_fail": "true"
    },
    "description": "tsagreimage_db_cell.tsc - File with code for re-imaging the cell ad dbnode\n\nThis script reimages based on some pre-checks.\n     Checks are as follows:\n\n     The script first determines (1) Image version on node\n                                 (2) Cell version on node\n                                 (3) Image version on dbnode\n                                 (4) Image version in view\n                                 (5) Cell version in view\n     Additionally, skip_reimage & force_reimage flags are imported.\n     If force_reimage is issued, re-imaging is done in all cases.\n     Else, if skip_reimage is issued, re-image is skipped.\n     If both skip_reimage and force_reimage are not issued,\n     re-imaging occurs if (1)!=(4) or (2)!=(5) or (3)!=(4). Else, it is skipped.\n\n     If re-imaging fails mid-way, the node is commented out\n     and added back to the pool.",
    "platform": null
  },
  {
    "test_name": "tsagreimgfailseq.tsc",
    "setup": null,
    "flags": {
      "mode": "quarantine",
      "qstring": "'Reimage_Failure'"
    },
    "description": "tsagreimgfailseq.tsc - Sequence for reimage failure\n\nSequence to be followed in case of reimage failure\n     Called from tsagnodereimage.tsc\n\nSends mail alert for reimage failure and returns back\n     the cell to pool with comment",
    "platform": null
  },
  {
    "test_name": "tsagreimgfailseq_db_cell.tsc",
    "setup": null,
    "flags": {
      "mode": "quarantine",
      "qstring": "'Reimage_Failure'"
    },
    "description": "tsagreimgfailseq_db_cell.tsc - Sequence for reimage failure on cell and/or dbnode\n\nSequence to be followed in case Cell and/or Dbnode failure\n     Called from tsagreimage_db_cel.tsc\n\nSends mail alert for reimage failure and returns back\n     the cell and dbnode to pool with comment",
    "platform": null
  },
  {
    "test_name": "tsagrmbas_iop1.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "resource_mgr": "1",
      "db_cache_size": "40M",
      "cpu_count": "8"
    },
    "description": "tsagrmbas_iop1.tsc - Interop version of tsagrmbas.tsc\n\nRun two user workloads (DSS and OLTP) under different resource\n     manager plans",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrelinkcellsrv.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrelinkcellsrv.tsc - relink the cellsrv if necessary\n\nThe tsagrelinkcellsrv.tsc relinks the oss/bin/cellsrv, if necessary.\n\n     If the cellsrv binary is newer than all the deplibs, then it does\n     nothing. Otherwise, a .dif file is created (see tsagrelinkcellsrv target\n     in oss/Makefile) to attract the developer's attention to the fact of the\n     rebuild, following with an attempt to relink cellsrv. If the relink is\n     failed, the test is aborted.",
    "platform": null
  },
  {
    "test_name": "tsagrenamedusrattribs.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrenamedusrattribs.tsc - Tests user attributes\n\nTests the user attributes that were renamed as part of txn - oxu_cleanup_user_visible_for_user\n      1) userType renamed to category\n      2) publicKeyFingerprint1/2/3 to pubKeyFingerprint1/2/3\n      3) remove userId",
    "platform": null
  },
  {
    "test_name": "tsagrenmcd.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagrenmcd.tsc - SAGE rename cell\n\nTests for rename of a cell. v$asm_disk should reflect the new name",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrepflash.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_no_asmdb": "true"
    },
    "description": "tsagrepflash.tsc - New Flash Disk Replace tests to be run on real hw\n\nthese are test cases related to Flash Disk (on real hw) and deal with FDOM replacement.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrephd.tsc",
    "setup": "tsagnini",
    "flags": {
      "auto_undo_management": "true",
      "asm_ausize": "4194304",
      "creatdev_file": "tkfgrddef",
      "redund1": "external",
      "nflint": "1"
    },
    "description": "tsagrephd.tsc - HarDisk Replacement Testcases on real hardware\n\nThis test script has testcases implemented for HDD testing on real hw.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrepparam.tsc",
    "setup": null,
    "flags": {
      "NXTARG": "0",
      "appendStr": "0",
      "ARGNAME": "ARG^NXTARG^",
      "ARGVALUE": "^^ARGNAME^",
      "LOCAL_ALL_ARG_LIST_CLI": "^LOCAL_ALL_ARG_LIST_CLI^' '^ARGVALUE^",
      "ALL_ARG_LIST": "^LOCAL_ALL_ARG_LIST_CLI^"
    },
    "description": "tsagrepparam.tsc - replace parameters provided in dat file for tcellcli\n\nParses arguments provided to tcellcli dat files\n\nUsage: When we want to use variables in dat files, currently it is not\n     supported, so this macro helps to replace parameters passed to tcellcli",
    "platform": null
  },
  {
    "test_name": "tsagrescueplan.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagrescueplan.tsc - RescuePlan test for exadata cell\n\nTest for verifying successful setup for attributes of cell after rescue.",
    "platform": null
  },
  {
    "test_name": "tsagresourceprofile.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "egs_deplmode": "sharedCloud"
    },
    "description": "tsagresourceprofile.tsc - Resource Profile Command Tests\n\nTest for resource profile commands -\n        mkresourceprofile  lsresourceprofile\n        chresourceprofile  rmresourceprofile",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagrestartcell.tsc",
    "setup": null,
    "flags": {
      "errbasename": "^outbasename^"
    },
    "description": "tsagrestartcell.tsc - kill cellsrv and let RS restart it",
    "platform": null
  },
  {
    "test_name": "tsagrestartexc_bg.tsc",
    "setup": null,
    "flags": {
      "loop_count": "1",
      "interval": "300",
      "excserver": "^command^",
      "file_to_stop": "^stopfile^",
      "dbg_msg": "'RESTART EXC:DEBUG:'",
      "valid_excserver": "'egs,syseds,usreds,eds,ers,bsw,bsm,cellsrv,esnp,random,'",
      "sep": ",",
      "option": "restart_esnp",
      "restartlog": "^logfile^.lst"
    },
    "description": "tsagrestartexc_bg.tsc - Restarts an EXC service in the background\n\ndefmacro restartessrv tsagrestartexc_bg\n     Following parameters can be passed:\n      loop_count=<n> -> Number of times ES server need to be restarted\n      interval=<n> -> Restart every n secs\n      excserver=<ES server name>, like egs, syseds, ers, etc\n      stopfile=<file> -> file marker in $T_WORK to stop the loop [optional]\n\nExample invocations:\n    (1) restartessrv loop_count=10 interval=500 excserver=egs\n         => Will restart EGS Leader 10 times in interval of 500 secs\n    (2) restartessrv loop_count=20 interval=90 excserver=egs stopfile=stop.log\n         => Restarts EGS Leader 20 times in interval of 90 secs, but will\n            stop if it finds $T_WORK/stop.log before/end of a loop.",
    "platform": null
  },
  {
    "test_name": "tsagresz.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "dangerousfg": "1",
      "differentszfg": "1",
      "cdb": "true",
      "no_dump_nvcache": "true"
    },
    "description": "tsagresz.tsc - test resizing SAGE disks\n\nResize in various ways:\n     . increasing size forward using contiguous segments\n     . increasing size backward using contiguous segments\n     . increasing size using non-contiguous segments\n     . decreasing size",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrh_12min_timeout.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_12min_timeout.tsc - roce switch test\n\nthis script is used to call roce switch maa\n     33297969 test on saturday",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_add_edv_volume.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_add_edv_volume.tsc - Add additional EDV volume to KVM guest\n\nEnhance oedacli to be able to manage EDV volumes on existing KVM guests\n\n     Different Test Scenarios listed in below confluence page is\n     covered to validate this feature\n\n     https://confluence.oraclecorp.com/confluence/display/~rajeev.k.jain@oracle.com/New+functionality+for+adding+additional+EDV+volumes+to+KVM+guests",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_amd_post_oeda_validations.tsc",
    "setup": null,
    "flags": {
      "compute_fqdn": "^compnode1^"
    },
    "description": "tsagrh_amd_post_oeda_validations.tsc - AMD validations test script",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_blockstore_workload.tsc",
    "setup": null,
    "flags": {
      "thread_no": "thread_^i^"
    },
    "description": "tsagrh_blockstore_workload.tsc - Helper script to run blockstore on real H/W LRG.\n\nThis script is used to run blockstore workload on Real H/W lrg.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug37582178.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug37582178.tsc - this test simulate M.2 system failure and make sure cell node reboots\n\nthis test simulate M.2 system failure and make sure cell node reboots",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug37829269.tsc",
    "setup": null,
    "flags": null,
    "description": "This test needs to be run in a miniCloud environment, where IPv6 packets are blocked by the Arista switch.\n     In such environments, ExaPortMon should exit and report ?IPv6 is not supported? to the journal.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_37219761.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_37219761.tsc - RH test driver for Bug 37219761\n\nPerform DIMM UE HWPOISON Fault on ESNPSRV",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_37256860.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_37256860.tsc - tsc script to run tsagrh_bug_37256860.sh\n\nPerform DIMM UE HWPOISON Fault on CELLSRV",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_cellrpm_upgrade.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_cellrpm_upgrade.tsc - Script for Cell RPM Upgrade",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_check_services.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_check_services.tsc - Check status of services running n nodes",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_clddbworkload.tsc",
    "setup": null,
    "flags": {
      "egsleader": "^TST_EXE_RESULT^",
      "n_clone_pdb": "3",
      "batch_size": "10",
      "src_pdb": "pdb1"
    },
    "description": "tsagrh_clddbworkload.tsc - RealHardware CLouD DB WORKLOAD driver script\n\nDriver file for driving workload and fault injection for provisioing lrg",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dbshutdown.tsc",
    "setup": null,
    "flags": {
      "compnode1": "scacad02dv0101.usdvm.oraclecorp.com",
      "compnode2": "scacad03dv0101.usdvm.oraclecorp.com",
      "compuser": "oracle",
      "oraclepasswd": "We1come$",
      "conn_string": "^compuser^@^compnode1^",
      "conn_string2": "^compuser^@^compnode2^",
      "oracle_home": "/u01/app/oracle/product/19.0.0.0/dbhome_1"
    },
    "description": "tsagrh_dbshutdown.tsc - Script for shutting down all DB process",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dbworkload.tsc",
    "setup": null,
    "flags": {
      "notification_script": "notification_policy.sh",
      "swtch_failure_script": "tsagrh_swtchfail1_maa.sh",
      "asm_home": "/u01/app/23.0.0.0/grid"
    },
    "description": "tsagrh_dbworkload.tsc - Runs DB workload tests on Quarter rack\n\nRuns DB workload tests on Exadata quarter rack after OEDA deployment",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_diagpack.tsc",
    "setup": null,
    "flags": {
      "compnode1": "^compute_node^",
      "compuser": "oracle",
      "oraclepasswd": "We1come$",
      "celluser": "root",
      "cellpasswd": "welcome1",
      "cell1f": "^cell1^",
      "cell2f": "^cell2^",
      "cell3f": "^cell3^",
      "compnode1f": "^compnode1^",
      "compnode2f": "^compnode2^",
      "domain": ".us.oracle.com",
      "conn_string": "^compuser^@^compnode1f^",
      "conn_string2": "^compuser^@^compnode2f^",
      "conn_cell1_str": "^celluser^@^cell1f^",
      "conn_cell2_str": "^celluser^@^cell2f^",
      "conn_cell3_str": "^celluser^@^cell3f^",
      "max_time": "4800"
    },
    "description": "tsagrh_diagpack.tsc - RealHardware validation for DIAGnostic PACK\n\nVerification scenario for diagpack on real hardware",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dimm_poison_test.tsc",
    "setup": null,
    "flags": {
      "vmuser": "root",
      "vmpasswd": "welcome1",
      "compnode1": "^compute_node^",
      "compnode1f": "^compnode1^",
      "compnode2f": "^compnode2^",
      "compuser": "root",
      "comppasswd": "welcome1",
      "cell1f": "^cell1^",
      "cell2f": "^cell2^",
      "cell3f": "^cell3^",
      "cellsf": "^cell1f^,^cell2f^,^cell3f^",
      "celluser": "root",
      "cellpasswd": "welcome1"
    },
    "description": "tsagrh_dimm_poison_test.tsc - Real Hardware fault injection test for DIMM in compute node\n                                   (bug-34538907 / 34548884 / 34572057)\n\nThis test injects failure into compute/guest node's DIMM and cause guest node to crash.\n     After the crash caused by the DIMM failure, we verify the guest node status and check\n     alerts and diagpack generation.  After that, we restart the guest node for recovery\n\n     Without above bug fixes (bug-34538907 / 34548884 / 34572057) the guest node continue\n     to run even if there is a DIMM failure and it will cause many different random errors\n     to the process/application running on the guest VM",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dimm_poison_test_ptlm.tsc",
    "setup": null,
    "flags": {
      "test_user": "root",
      "user_passwd": "welcome1"
    },
    "description": "tsagrh_dimm_poison_test_ptlm.tsc - Dimm Poison Test to be run on the PT LM Branch",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dimm_ue_egssrv.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_dimm_ue_egssrv.tsc\n\nOratst wrapper to invoke tsagrh_dimm_ue_egssrv.sh",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_disk_confinement.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_disk_confinement.tsc - cellsrv force powering off hung confined NVME disk\n\nWhen a NVME disk is confined hung, Cellsrv will force power it off.\n     This test simulates disk hang confinement and check if the power off\n     feature is triggered",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dstate_faultinj.tsc",
    "setup": null,
    "flags": {
      "vmuser": "oracle",
      "vmpasswd": "We1come$"
    },
    "description": "tsagrh_dstate_faultinj.tsc - D-state fault injection test\n\nThis script is replication sequence for MAA Bug-35784008, using Cgroup Freezer\n     Aims to find any corruption caused due to D-state processes on guest nodes.\n\n     Fatal bg processes in D-state causes the sick DB instance terminate, due to which after\n     the instance reconfiguration some stale write from this bg process leads to Corruption.\n\nRefer to wiki for details:\n     https://confluence.oraclecorp.com/confluence/display/~rajeev.k.jain@oracle.com/Bug+35784008+Reproduction+using+D+state+injection",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dstate_faultinj_cell.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_dstate_faultinj_cell.tsc - D-state fault injection on Cell node\n\nTesting out the D-state fault injection with CELLSRV, MS and RS process.\n     Def: State \"D\" indicates Uninterruptible sleep generally related to IO.\n     This state usually occurs when a process is waiting for data from\n     a device. Once in D state processes don't respond to any I/O?s, even\n     termination signal (KILL/SIGKILL). D-state processes exit their state\n     when the I/O they were waiting for completes, allowing them to resume.\n\nhttps://confluence.oraclecorp.com/confluence/x/JWzEGwI",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_enable_smart_storage.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_enable_smart_storage.tsc - test ENABLESMARTSTORAGE set to false\n\nTxn to verify ENABLESMARTSTORAGE is correctly set to false after new\n     cell rpm installation in real hw X11 cell.\n\nThis test runs in lrgrhx11sadiag",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_env_setup.tsc",
    "setup": null,
    "flags": {
      "MACH_SCHEDULE": "^MACH_SCHED_FROM_FILE^",
      "SKIP_TSAGEND": "true",
      "tmp_nowarn": "^tst_nowarn^",
      "USER_TXN_NAME": "na"
    },
    "description": "tsagrh_env_setup.tsc - Environment setup file for real hw lrgs\n\nEnvironment setup file for the real hardware lrgs\n\nsetup file",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_exascale_unit_tests.tsc",
    "setup": null,
    "flags": {
      "bug_35238299_disable": "true",
      "ip1": "^ip1^",
      "ip2": "^ip2^",
      "ip3": "^ip3^",
      "ip4": "^ip4^"
    },
    "description": "tsagrh_exascale_unit_tests.tsc  -  Wrapper file which executes docker tests\n\ntsagrh_exascale_unit_tests.tsc is a wrapper file which run docker test scripts in Real HW cells, the file cover below tests\n\n     Tests which runs with AEP enabled\n       1. Exascale cell & compute API\n       2. Running ls command and dbmcli commands using escli\n       3. Compute ERS repository test using curl - Negative test case\n       4. ERS failover in one cell\n       5. REST CONFIGURATION BUG TEST with AEP configs\n       6. Test for thasingh_bug-36443985\n\n     Test which runs with AEP disabled\n       1. ERS Multicell failover\n       2. BUG 31861580 Test\n       3. Test for bug 34325937 - Use escli command to make frontend value to True/False",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_fcstat.tsc",
    "setup": null,
    "flags": {
      "compnode1": "^compute_node^",
      "compuser": "oracle",
      "oraclepasswd": "We1come$",
      "celluser": "root",
      "cellpasswd": "welcome1",
      "cell1f": "^cell1^",
      "cell2f": "^cell2^",
      "cell3f": "^cell3^",
      "compnode1f": "^compnode1^",
      "compnode2f": "^compnode2^",
      "domain": ".us.oracle.com",
      "conn_string": "^compuser^@^compnode1f^",
      "conn_string2": "^compuser^@^compnode2f^",
      "conn_cell1_str": "^celluser^@^cell1f^",
      "conn_cell2_str": "^celluser^@^cell2f^",
      "conn_cell3_str": "^celluser^@^cell3f^"
    },
    "description": "tsagrh_fcstat.tsc - Real Hardware Lrg to print out Flashcache Stats\n\nVerification of FlashCache stats/metrics for Real Hardware LRG",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_fndd.tsc",
    "setup": null,
    "flags": {
      "compconnstr1": "root@^compnode1f^",
      "compconnstr2": "root@^compnode2f^"
    },
    "description": "tsagrh_fndd.tsc - Add FNDD Test on Real HW",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_getdb_files.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_getdb_files.tsc - Copy Required Files for Imaging/Upgrade/Workload",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_guest_migration.tsc",
    "setup": null,
    "flags": {
      "xml_file_name_bkp": "^xml_file_name^",
      "xml_file_name": "^xml_file_name_bkp^"
    },
    "description": "tsagrh_guest_migration.tsc - Guest offline migration from one kvm host to another\n\nAdded functional test to migrate guest vm from one KVM host to another\n     after a successful guest on edv deployment\n\n     New oedacli comamnd \"migrate guest\" used for migration and post migration checks\n     performed to make sure migrated guest vm working as expected in target KVM host\n     NOTE: OEDACLI guest migration input file created in tftp serve in below location\n      /net/10.32.19.91/export/exadata_images/dpant/exascale/guest_migration_oedacli_input_file\n\n     Test Scenario covered for guest migration:\n       1. Shutdown Guest VM scaqar04dv0801m\n       2. Migrate Guest VM scaqar04dv0701m from KVM Host scaqar04adm07 to\n          scaqar04adm08 using single oedacli command and perform migration verification checks\n       3. Migrate Guest VM scaqar04dv0801m from KVM Host scaqar04adm08 to scaqar04adm07\n          using individual migration oedacli command and perform migration verification checks\n       4. Verify DB works on both migrated Guest VM scaqar04dv0701m, scaqar04dv0801m by\n          running tpch workload with 5 PDBs for a duration of 5 minutes",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_hac_testsuite.tsc",
    "setup": null,
    "flags": {
      "nodename": "^cell1^",
      "cmdfilepath": "^T_WORK^/tsagrh_hac_testsuite.sh"
    },
    "description": "tsagrh_hac_testsuite.tsc - Exadata host_access_control test suit for real h/w\n\nThis scripts tests options related to EXADATA host_access_control on cell nodes\n   cmd: \"/opt/oracle.cellos/host_access_control\"\n   lrgname: lrgrhx7imonec\n     driver file: oss_rhx7imonec.tsc",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_hw_allocation.tsc",
    "setup": null,
    "flags": {
      "srch": "'.us.oracle.com'",
      "cells": "^cell1^",
      "cell1_f": "^cell1^.us.oracle.com",
      "cellconnstr": "root@^cell1_f^",
      "cell1connstr": "root@^cell1_f^",
      "cell2_f": "^cell2^.us.oracle.com",
      "cell2connstr": "root@^cell2_f^",
      "compute_node_tmp": "^compute_node^",
      "compute_node": "^compute_node_tmp^",
      "compute_node_f": "^compute_node^.us.oracle.com",
      "dbnodeconnstr": "root@^compute_node_f^"
    },
    "description": "tsagrh_hw_allocation.tsc - Used to allocate hw to real hw lrgs\n\nThis script helps to allocate the hw, validate the alloacted hw,\n    initialize hardware variables and set timezone of farmhost",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_hw_deallocation.tsc",
    "setup": null,
    "flags": {
      "FORCE_TEMP": "^FORCE_REIMAGE^",
      "FORCE_REIMAGE": "^FORCE_TEMP^",
      "mode": "normal",
      "qstring": "^USER_TXN_NAME^"
    },
    "description": "tsagrh_hw_deallocation.tsc - Used for dealloction of hw\n\nThis script helps to return the allocated hw to lrg back\n      pool file of hardware.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_ifd_checks.tsc",
    "setup": null,
    "flags": {
      "kvmhostnode1": "^compute_node^",
      "kvmhost_connstr": "root@^kvmhostnode1^"
    },
    "description": "tsagrh_ifd_checks.tsc - IFD instance check on all node and check IFD alerts in all monitoring nodes\n                             running EGS after a compute node reboot\n\nUsing exascale_provisioning real HW to test below IFD checks:\n        1. Check IFD enabled on all nodes by checking below EGS trace log content in all cell and compute nodes\n            skgzibr_ini: Finished skgzibr setup\n            skgzibr_start_node_monitoring: .* Start monitoring:\n        2. Reboot compute node and check IFD alerts on all monitoring nodes(cell and compute)\n           running EGS (below content to check in 3 cells and 1 compute node were EGS process runs)\n             \"detected by IFD\" on alert logs of each monitoring node where EGS runs\n            .*skgzibr_check_node_reachability_by_node.*Unreachable due to prev unreachable.*and prev tcp still up\n\nOn exascale_provisioning HW, EGS process running on 2 compute and 3 cell nodes",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_initialize.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_initialize.tsc - Script ot initialise parameters on Real HW lrgs\n\nThis Scripts helps to set Different Parameters for the View and farm\n     Allocate the nodes for real hw lrgs from respective scheduling dir\n     The list of variables getting initialised from this script are -\n     Transaction name, View Name,  Label Name, Farm Job ID, Results Dir,\n     Compute Nodes, Cell Nodes, wait-pid for nodes, db_shiphome_present\n     Also Includes LRG Start Email and Farm Abort Hook Script allocation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_iov_workload.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_iov_workload.tsc - Helper script to run IOv on real H/W lrg.\n\nThis script is used to run IOV workload on Real H/W lrg.\n   That will take the input of <compute>,<cell>,<lrg_name>",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_jetty_rpm_install.tsc",
    "setup": null,
    "flags": {
      "cell1_f": "^cell1^.us.oracle.com",
      "cellconnstr": "root@^cell1_f^",
      "comp1_f": "^compute1^.us.oracle.com",
      "compconnstr": "root@^comp1_f^"
    },
    "description": "tsagrh_jetty_rpm_install.tsc - txn to verify jetty rpm installation\n\nVerify jetty rpm installation which has been seperated from cell/db rpm\n\nThis test verifies jetty rpm installation which has been seperated from\n     cell/db rpm and now is avaialable as exadata-webserver rpm. Test steps:\n     1 : Shutdown db/cell services(drop cell on cellnode as well )\n     2 : Uninstall old cell/db rpm\n     3 : Verify old rpms are uninstalled\n     4 : Install new rpm, verify new rpms are correctly installed\n     5 : Restart db/cell services (recreate cell on cell node)\n     6 : Verify that MS is up",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_kvm_guest_irq_check.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_kvm_guest_irq_check.tsc - KVM guest IRQ check",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_livemig_run_unit_tests.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_livemig_run_unit_tests.tsc\n\nlive migration lrg unit test executor",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_logon_storm_test.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "node1": "^node^",
      "logfilename": "tsagrh_logon_storm_test",
      "node1f": "^node1^",
      "node_conn_str": "root@^node1f^"
    },
    "description": "tsagrh_logon_storm_test.tsc - test for logon storm on half rack rh\n\nThis transaction simulates a logon storm using Last Transaction Emulator\n     (LTE) utility in Exascale half rack envirnment\n\nThis test simulates logon storm and monitors CPU usage in the background\n     A diff file is generated if the IDLE CPU usage dips below 50%\n     The test collects the following data:\n       - CPU usage\n       - cumulative logon stat\n       - cell disk open\n       - cell single block physical read: xrmem cache\n       - cell single block physical read: flash cache\n       - cell smart table scan\n       - cell single block physical read\n     More details on this test can be found on the following confluence page:\n     https://confluence.oraclecorp.com/confluence/x/oUAdCQQ\n     Information about LTE utility which is used to simulate logon storm:\n     https://rdbms-frappe.oraclecorp.com/RDBMS_MAIN_LINUX.X64/definitions?d=tk_perf%2Frac_kit%2Flte%2FREADME.txt",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_lrgsuit.tsc",
    "setup": null,
    "flags": {
      "mode": "normal",
      "cellconnstr": "root@^cell1_f^",
      "amd_testing": "true",
      "exacli_env": "true",
      "oss_x9_testing": "true"
    },
    "description": "tsagrh_lrgsuit.tsc - LRG's tests suit\n\nLRG specific environment setup and checks prior\n     to running lrg's tests\n\n     Contains all real hw lrgs and tests to run. On the\n     basis of lrgname tests are executed.\n\n1. By default all the lrg run will copy exawatcher-files from cells back to hosted machine\n    2. If we want to skip the copying of exawatcher-files, then we should set below lines inside if condition\n        import save_debugstat nowarn\n        set save_debugstat false",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_make_install_asan.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_make_install_asan.tsc - Helper script used to make and\n       install asan RPM on storage nodes.\n\nHelper script used to make and install asan RPM on\n       storage nodes for x7imonec",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_multi_config_workload.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_multi_config_workload.tsc - Run Workload on HC/EF Vaults\n\nThis script Creates Vaults of 1T of EF and HC/EF Type and create template\n     Setups database tablespace and tables on top of those vaults\n     And executes Simple Workload",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_oeda_dropcells.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_oeda_dropcells.tsc - Test dropcells and deletecells oedacli cmd",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_rdmastats.tsc",
    "setup": null,
    "flags": {
      "compuser": "oracle",
      "oraclepasswd": "We1come$",
      "conn_string": "^compuser^@^compnode1^",
      "conn_string2": "^compuser^@^compnode2^",
      "oracle_home": "/u01/app/oracle/product/23.0.0.0/dbhome_1"
    },
    "description": "tsagrh_rdmastats.tsc - Collect RDMA Statistics data\n\nCollect RDMA Statistics data",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_reboot_roce_switch.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_reboot_roce_switch - roce switch test\n\nthis script is used to call roce switch maa\n     33297969 and 31991705 test",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_reimage.tsc",
    "setup": null,
    "flags": {
      "FORCE_TEMP": "^FORCE_REIMAGE^",
      "FORCE_REIMAGE": "^FORCE_TEMP^"
    },
    "description": "tsagrh_reimage.tsc - Reimage module for real hw driver",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_restart_cellsrv.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_restart_cellsrv.tsc - This is the helper script to stop and restart crs ,cellsrv\n     and save asan logs\n\nThis script used to stops crs,restart cellsrv , restart crs\n        and save asan logs to asan.sav",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_restore_dbmmgmt_rpm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_restore_dbmmgmt_rpm.tsc - Restore exadata-dbmmgmt RPM for yixiali_restore_dbmmgmt_rpm_keep_config\n\nThis is used to run tsagrh_restore_dbmmgmt_rpm.sh functiona test case for\n       yixiali_restore_dbmmgmt_rpm_keep_config. Restore exadata-dbmmgmt and check\n       MS attribute persistent.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_rollback_test.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_rollback_test.tsc - patch rollback test on exascale single cell\n\nTest to verify Exascale cold downgrade from a label of higher version\n     with newer features to an older label with lower version features\n\nThis test runs in lrgrhx9saexc1cell.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_rpm_install.tsc",
    "setup": null,
    "flags": {
      "cellconnstr": "^cell2connstr^"
    },
    "description": "tsagrh_rpm_install.tsc - Rpm installation module",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_runlrgchk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_runlrgchk.tsc - RealHardware RUN LRG CHecK logic\n\nCheck if RH lrg should be run based on day of the week.  The control is\n     determined by the lrg_run_day.lst file in each RH's reservation directory\n\n     This must be included under a RH LRG driver tsc and can't be run standalone",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_save_traces.tsc",
    "setup": null,
    "flags": {
      "cluster_nodes": "1",
      "lrgname_alert": "^LRG_NAME^",
      "mail_addr": "exadata_realhw_alerts_ww_grp@oracle.com"
    },
    "description": "collect alert histories, save cell traces, logs from cell\n     to farm host machine",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_savetrace.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_savetrace.tsc - Real Hardware SAVE TRACE driver\n\nThe purpose of this tsc is to standardize the trace/alert files\n     preservation for real hardware lrgs\n\n     This File helps to save following traces/logs-\n     1. OEDA / Upgrade Traces and logs work directory\n     2. Cell - ASM, EXC, cellos and var logs/traces\n     3. KVM Host - ASM, EXC, cellos, rdbms (Bare metal) and var logs/traces\n     4. KVM Guest - ASM, EXC, cellos, rdbms diag and var logs/traces\n     5. Preserve the Imaging, Patching and OEDA Log zips\n     6. Generate a property file - clusterinfo.sav",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_single_cell_test.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_single_cell_test.tsc - Single cell exascale test\n\nTest script for unit tests which run on single cell Exascale\n     deployment on real hardware.\n\nThe following tests are run using this script on real hw\n       1. Rest Config Upgrade\n       2. Nginx certificate/key upload\n       3. Check test for tsagalter_ersip.sh\n       4. Alter ERS IP in Priority cell\n       5. xiaohshe_bug-33125607 &  BUG 30646935 -\n          EXASCALE: RS SHUTDOWN THE ORIGINAL EGSSRV\n       6. Bug37093224 VIP lane stat compare in lrgrhx9saexc1cell",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_single_inst_db.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_single_inst_db.tsc - Script to create Single Instance DB on vault\n\nTests FLow -\n       1. Create a vault for the Database storage\n       2. Setup Database using DBCA (Single Instance Database)\n       3. Run Some workload on the Database.\n       4. Delete the database using DBCA and cleanup vault.\n       5. Validate workload and DBCA setup.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_single_inst_db_rhexcupgrade.tsc",
    "setup": null,
    "flags": {
      "ers_ip": "'10.95.129.125'"
    },
    "description": "tsagrh_single_inst_db_rhexcupgrade.tsc - Script to create Single Instance DB on vault\n\nTests FLow -\n       1. Create a vault for the Database storage\n       2. Setup Database using DBCA (Single Instance Database)\n       3. Run Some workload on the Database.\n       4. Delete the database using DBCA and cleanup vault.\n       5. Validate workload and DBCA setup.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_teardown.tsc",
    "setup": null,
    "flags": {
      "mode": "normal",
      "qstring": "^USER_TXN_NAME^"
    },
    "description": "tsagrh_teardown.tsc - Runs at the end of Real HW LRG\n\nChecks Services, Alerts, upload results and send emails, Deallocate nodes",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_tpcds_setup.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_tpcds_setup.tsc - Script to copy files and generate script for TPCDS Workload",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_ue_dimm_poison_test.tsc",
    "setup": null,
    "flags": {
      "vmuser": "root",
      "vmpasswd": "welcome1",
      "compnode1": "^compute_node^",
      "compnode1f": "^compnode1^",
      "compnode2f": "^compnode2^",
      "compuser": "root",
      "comppasswd": "welcome1",
      "cell1f": "^cell1^",
      "cell2f": "^cell2^",
      "cell3f": "^cell3^",
      "cellsf": "^cell1f^,^cell2f^,^cell3f^",
      "compnodesf": "^compnode1f^,^compnode2f^",
      "celluser": "root",
      "cellpasswd": "welcome1"
    },
    "description": "tsagrh_ue_dimm_poison_test.tsc - Real Hardware fault injection test for DIMM in cell node\n\nThis test injects failure into cell node's DIMM and cause cellsrv, ms, and cellrs\n     processes to have issue.\n     After the fault injected to the DIMM failure, we verify the cell node status and check\n     alerts",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_vm_edv_tests.tsc",
    "setup": null,
    "flags": {
      "cluster_xml_file": "/net/10.32.19.91/export/exadata_images/dpant/lrg_driver/OSS_MAIN/lrgrhexaprovcluster_onprem_multi_vm/lrgrhexaprovcluster_onprem_multi_vm_scacaf02adm0507_cluster3.xml"
    },
    "description": "tsagrh_vm_edv_tests.tsc - File contains all vm_edv test to run\n\nSingle tsc file to have all the vm_edv tests to run",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_vm_maker_add_disk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_vm_maker_add_disk.tsc - Functional test for Bug 35806941\n\n35806941 - VM_MAKER --CREATE --DISK-IMAGE FAILS, OSS_MAIN ONLY",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_workload.tsc",
    "setup": null,
    "flags": {
      "compuser": "oracle",
      "oraclepasswd": "We1come$",
      "celluser": "root",
      "cellpasswd": "welcome1",
      "cell1f": "^cell1^",
      "cell2f": "^cell2^",
      "cell3f": "^cell3^",
      "compnode1f": "^compnode1^",
      "compnode2f": "^compnode2^",
      "ora_home": "/u01/app/oracle/product/23.0.0.0/dbhome_1",
      "ora_sid": "cdb1db11",
      "ora_sid2": "cdb1db12",
      "max_time": "86400",
      "min_time": "10",
      "total_pdb": "100",
      "batch_size": "10",
      "node2_pdb": "50"
    },
    "description": "tsagrh_workload.tsc - Run workload on X3 with 2000 PDBs\n\nRun workload on X3 with 2000 PDBs",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_x7generateoedaxml.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrh_x7generateoedaxml.tsc - Generates x7imonec xml using OEDA UI\n\nThis file is the entry point for generating x7imonec xml using OEDA UI",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhcellinit.tsc",
    "setup": null,
    "flags": {
      "cellname": "cell^i^"
    },
    "description": "tsagrhcellinit.tsc - It initializes the installed cell\n\n<It initializes the installed cell>\n\n<can be called for cell initialization from any rh cell>",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhcellrpminstall.tsc",
    "setup": null,
    "flags": {
      "mode": "normal"
    },
    "description": "tsagrhcellrpminstall.tsc - Installs cell rpm on rh cell\n\nIt installs the rpm on rh cell\n\nIt installs the rpm on rh cell",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhcellsuit.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrhcellsuit.tsc - Test suit for cell only lrgs\n\nTest suit for cell only lrgs",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhcellsynctime.tsc",
    "setup": null,
    "flags": {
      "mach_date": "^mach_date^"
    },
    "description": "tsagrhcellsynctime.tsc - Macro to sync real hw cell time with farm\n\nHelps in resyncing cell time with machine node",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhcov.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrhcov.tsc - setup for coverage run on real hw\n\nsetup for coverage run on real hw",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhdbnodesynctime.tsc",
    "setup": null,
    "flags": {
      "mach_date": "^mach_date^"
    },
    "description": "tsagrhdbnodesynctime.tsc - Macro to sync real hw db time to farm\n\nHelps in resyncing dbnnode time with machine node",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhdisablessh.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrhdisablessh.tsc - test for ssh disable\n\nTest script added for cell user creation and exacli execution",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhdxddbworkload.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrhdxddbworkload.tsc - Real HW test with both database and cell nodes\n\nThis test sets up database on X5 real hardware on PMEM disks. In order to\n     set up db on PMEM disks,  PMEM PDs are created in-memory and then on top of them\n     celldisks and griddisks are created. tsagcrdxddisks.sh is used to create PMEM PDs.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhfndd.tsc",
    "setup": null,
    "flags": {
      "dbconnstr": "root@^compute_node_f^",
      "lstfile": "^tst_tscname^.lst"
    },
    "description": "tsagrhfndd.tsc - test for Fast Node Death Detection\n\nTEST FOR DETECTING CLIENT SIDE ISSUES FNDD\n     2 tests have been implemented -\n       i.) with interface down\n       ii.) with reboot of server node\n iii.) with setting a kernel panic in server node\n     More description at start of test.\n\nTest added in lrgrhx7safnddfailure",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhgdbclone_test.tsc",
    "setup": null,
    "flags": {
      "cluster_num": "c11",
      "cluster_nodes": "^cluster_node1^,^cluster_node2^"
    },
    "description": "tsagrhgdbclone_test.tsc - real hardware test for gdbclone\n\nFunctional test for gdbclone on real hw\n\nThis test runs in lrgrhexaprovcluster_onprem_multi_vm in the background\n     with tpch workload running. More information about gdbclone can be found at:\n     https://confluence.oraclecorp.com/confluence/display/~rajeev.k.jain@oracle.com/gDBClone+Tests",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhinit.tsc",
    "setup": null,
    "flags": {
      "oeda_xml": "'/net/10.32.19.91/export/exadata_images/dpant/Exa_QA_x9_qinq_failure_testing.xml'",
      "cur_cluster_compute1": "compnode1_clu^c^",
      "cur_cluster_compute2": "compnode2_clu^c^",
      "cur_aws_cell": "rh_cell^i^",
      "tmp_compute": "aws_compute^i^",
      "cur_scanlistener": "scanlistener_clu^c^",
      "cur_dg": "dg_clu^i^_db^p^",
      "cur_dg_val": "^^cur_dg^",
      "vaultprefix": "'@'"
    },
    "description": "tsagrhini.tsc - init the real hw env.\n\ninit the real hw env.\n\ninit the real hw env.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhipcdat_dxd.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true"
    },
    "description": "tsagrhipcdat_dxd.tsc - Test the new LGWR rdma write on dxd griddisk\n\nThis test runs on X7 real hardware.\n\nThis test can be run standalone with prior cell setup on storage cell.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhipcdatstress.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cellconnstr": "root@^cell_name^",
      "openmpi": "openmpi-1.10-x86_64"
    },
    "description": "tsagrhipcdatstress.tsc - ipcdat stress real hardware test\n\nipcdat stress real hardware test\n\nipcdat stress real hardware test",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhipcdatstresssrq.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cellconnstr": "root@^cell_name^",
      "openmpi": "openmpi-1.10-x86_64"
    },
    "description": "tsagrhipcdatstresssrq.tsc - ipcdat_stress_srq real hardware test\n\nipcdat_stress_srq real hardware test. This test runs on x5 hardware\n\nipcdat_stress_srq real hardware test.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhlgwrtestpmem.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cellconnstr": "root@^cell_name^"
    },
    "description": "tsagrhlgwrtestpmem.tsc - Test the new LGWR rma write on DAX griddisk\n\nThis test runs on X52 real hardware which has physical harddisks.\n\nThis test can be run standalone with prior cell setup on storage cell.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhnewipcdatiopath.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell1_f": "^cell1^.us.oracle.com"
    },
    "description": "tsagrhnewipcdatiopath.tsc - Test the new IPCDAT I/O path in Libcell - Cellsrv\n\nThis test runs on X52 real hardware which has physical harddisks. It tests\n     the new IPCDAT I/O path in Libcell - Cellsrv\n\nThis test can be run standalone with prior cell setup on storage cell.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhnewipcdatiopath1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrhnewipcdatiopath1.tsc - Test the new IPCDAT I/O path in Libcell - Cellsrv\n                                  on pmem griddisks\n\nThis test runs on X52 real hardware which has physical harddisks. It tests\n     the new IPCDAT I/O path in Libcell - Cellsrv on pmem griddisks\n\nThis test can be run standalone with prior cell setup on storage cell.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhnewipcdatiopath2.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cellconnstr": "root@^cell_name^"
    },
    "description": "tsagrhnewipcdatiopath2.tsc - Test the new IPCDAT I/O path in Libcell - Cellsrv\n\nThe test uses IOV to perform RDMA reads and writes and verifies that the I/Os\n     go over IPCDAT by looking at the stats. It also verifies the keep_alive ioctl feature.\n\nThis test can be run standalone with prior cell setup on storage cell.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhrunscript.tsc",
    "setup": null,
    "flags": {
      "callername": "^tst_current_name^",
      "tscname": "u^callername^_^rndm^^tsagrhrunscriptsuffix^",
      "append_prefix": "'>'",
      "output_list": "'> '^log^",
      "ALL_ARG_LIST": "^ALL_ARG_LIST^' '^ARGVALUE^'='^^ARGVALUE^",
      "NXTARG": "0",
      "ARGNAME": "ARG^NXTARG^",
      "ARGVALUE": "^^ARGNAME^",
      "value": "^^ARGVALUE^",
      "aws_cell": "^value^",
      "aws_cluster": "1",
      "aws_compute": "^value^",
      "aws_compute_asm": "true",
      "aws_compute_db": "1",
      "cellcli_prefix": "'@'",
      "aws_node": "^compuser^@^cur_compute^",
      "aws_node_twork": "^aws_compute_twork^",
      "fileprefix": "'.sql'",
      "script_file": "^cmdfile^",
      "outfilenm": "^tscname^.log",
      "macro_debug_file": "^tscname^_debug.log"
    },
    "description": "tsagrhrunscript.tsc - The wrapper of invoking sqlplus/perl/sh/cellcli\n         to run on AWS cell and compute nodes\n\nThe wrapper of invoking sqlplus/perl/sh/cellcli to run on AWS cell and compute nodes\n   it requires cell or compute parameter\n     cell=[1-3], mean run on cell[1-3] node\n     cluster=[1-3], mean run on cluster x\n     compute=[1-3], mean run on compute[1-3] node\n     db=[1-3], mean run on cluster[1-3], compute[1-3] node , db[1-3]\n\n\n   usage: how to run sql/perl/shell/cellcli\n\n     echo > cell.dat\n          > set echo on\n          > list cell detail\n     endecho\n     tcellcli cell.dat cell=1 > ^outbasename^\n     log append\n\n     echo > checkimage.sh\n          > imageinfo\n          > echo \"test value $1\"\n     endecho\n     tsh checkimage.sh test1 cell=2 > ^outbasename^\n     log append\n\n     echo > checkimage.pl\n          > system(\"imageinfo\");\n     endecho\n     tperl checkimage.pl cell=2 > ^outbasename^\n     log append\n\n     echo > tsagcrttb.sql\n          > set verify off\n          > connect sys/knl_test7 as sysdba\n          > set echo on\n          > set trimspool on\n          > set linesize 160\n          > drop table t1;\n          > 'create table &1 (c0 number, c1 date, c2 varchar2(2000), c3 nvarchar2(2000), c4 timestamp,'\n          >       c5 timestamp with time zone, c6 timestamp with local time zone, c7 float(20), c8 number,  c9 number);\n          > declare\n          >  i     NUMBER := 0;\n          >  k     timestamp:= systimestamp;\n          >  l     nvarchar2(30):= ''stage22299'';\n          > begin\n          >  for i in 1..80000 loop\n          > '  insert into &1 values (i,sysdate,''storage'',l,k+i,k+i,systimestamp,9999.99999,2,i); '\n          >  end loop;\n          >  commit;\n          > end ;\n          > /\n          > 'select count(*) from &1 ;'\n     endecho\n     sesql tsagcrttb t1 compute=1 > ^outbasename^\n     log append\n\nThe wrapper of invoking sqlplus/perl/sh/cellcli",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhsnmpv3test.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "node1": "^node^",
      "type_node": "^type_node^",
      "logfilename": "tsagrhsnmpv3test",
      "node1f": "^node1^.us.oracle.com",
      "node_conn_str": "root@^node1f^"
    },
    "description": "tsagrhsnmpv3test.tsc - snmpv3 trap detection test on rh\n\nThis to a test to validate snmpv3 trap delivery from rh\n     and trap reception at farm vm.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhtornioprevention.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell1_f": "^cell1_f^"
    },
    "description": "tsagrhtornioprevention.tsc - test for torn I/O prevention for DAX disk\n\nThis test sets up PMEM griddisk on real hardware and tests for\n     torn I/O prevention for DAX disk\n\nRun iov with 10 threads of locked wirte, 10 threads of locked\n     read, 10 threads of direct read. Keep the test running for 10 min",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhvarset.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrhvarset.tsc - To set var on rh\n\nThis tsc can be used to have persistent variable being set on\n     real hardware",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhwalrt.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrhwalrt.tsc - Configure mail on cell for alerts and alert related tasks",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhx9_ndctl.tsc",
    "setup": null,
    "flags": {
      "cell_node": "^cell_name^",
      "cell_full": "^cell_node^.us.oracle.com",
      "OSS_HW_TESTING": "1",
      "cellconnstr": "root@^cell_full^"
    },
    "description": "tsagrhx9_ndctl.tsc - ilom pmem fault testcases\n\nTo test dirty shutdown handling on PMEM\n\nto be added in lrgrhx9upgrade",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrlhwini.tsc",
    "setup": null,
    "flags": {
      "temp_nowarn": "^tst_nowarn^",
      "USER_TXN_NAME": "na"
    },
    "description": "tsagrlhwini.tsc - Common Init script for Real hw",
    "platform": null
  },
  {
    "test_name": "tsagrman.tsc",
    "setup": "tsagnini",
    "flags": {
      "scompatible.rdbms": "12.2.0.0.0",
      "tsagcln": "3",
      "creatdev_file": "tsagccdef",
      "tsagrmanlog": "tsagrmanffc.log",
      "file_dest": "^asmprefix^datafile",
      "asmprefix": "'+'"
    },
    "description": "tsagrman.tsc - RMAN short regress test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrmanini.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagrmanini.tsc - Initialization script used in RMAN lrgs, lrgsa11*",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrmansnap_file.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagrmansnap_file.tsc - Test for checking rman fuctionality on snapshots\n\n- Create tablespace on snapshot\n      - insert values and take incremental backup\n      - delete the datafile on which we created tablespace\n      - restore the datafile and try to run query on the table",
    "platform": null
  },
  {
    "test_name": "tsagrmansnap_table.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagrmansnap_table.tsc - RMAN test for multilevel exascale snapshots\n\n### TEST 3 SCENARIOS ####\n    1 - delete some rows from table 1 and recover those rows\n    2 - drop table 2 and recover it\n    3 - drop tablespace 3 and recover the tablespace and the contents on it",
    "platform": null
  },
  {
    "test_name": "tsagrmansparse_file.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrmansparse_file.tsc - Test for checking rman fuctionality on sparsedb\n\n- Create tablespace on Sparsedg\n      - insert values and take incremental backup\n      - delete the datafile on which we created tablespace\n      - restore the datafile and try to run query on the table",
    "platform": null
  },
  {
    "test_name": "tsagrmansparse_table.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrmansparse_table.tsc - Test to check rman functionality for sparsedb\n\n### TEST 3 SCENARIOS ####\n    1 - delete some rows from table 1 and recover those rows\n    2 - drop table 2 and recover it\n    3 - drop tablespace 3 and recover the tablespace and the contents on it",
    "platform": null
  },
  {
    "test_name": "tsagrmansql.tsc",
    "setup": null,
    "flags": {
      "tsagrmanlog": "tsagrman.log",
      "tsagrmanref": "^tsagrmanlog^",
      "file_dest": "^asmprefix^datafile",
      "backup_dest": "^asmprefix^datafile",
      "sysdba": "'sys/knl_test7 as sysdba'"
    },
    "description": "tsagrmansql.tsc - run just the backup & restore scripts\n\nCalled by tsagrman to execute the backup scripts.\n     This was split out from tsagrman.tsc so that it can be invoked\n     after having the database created by another script, say srdbmsini.\n\nNeed to have the database already created, oss and asm already running.\n     If this event does not exist in t_init1.ora and you want krb tracing,\n     add it\n         event '\"logon trace name krb_trace level 1\"'",
    "platform": null
  },
  {
    "test_name": "tsagrmbas.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "resource_mgr": "1",
      "nowarn": "^TST_NOWARN^",
      "db_cache_size": "40M",
      "cpu_count": "8"
    },
    "description": "tsagrmbas.tsc - Performance test version of tkrobas.tsc\n\nRun two user workloads (DSS and OLTP) under different resource\n     manager plans",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrmbasini.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrmbasini.tsc - Initialization and Setup for tsagrmbas.tsc\n\nLoads test data and creates resource manager plans for tsagrmbas.tsc.",
    "platform": null
  },
  {
    "test_name": "tsagrmbaswl.tsc",
    "setup": null,
    "flags": {
      "resource_mgr": "1"
    },
    "description": "tsagrmbaswl.tsc - Actual performance workload for tkrobas2.tsc.\n\nForks TPC-C and TPC-H like sessions and verifies results.",
    "platform": null
  },
  {
    "test_name": "tsagrmcfg.tsc",
    "setup": null,
    "flags": {
      "testdesc": "'Case 8: Simulate loss of hard disk while cell is down and \\",
      "dsk2repl": "datafile0"
    },
    "description": "tsagrmcfg.tsc - test lost of cell_disk_config.xml",
    "platform": null
  },
  {
    "test_name": "tsagrmmode.tsc",
    "setup": null,
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line"
    },
    "description": "tsagrmmode.tsc - Test SAGe Resource Manager MODE transitions\n\nThis test verifies the transitions between the various IORM modes:\n     e.g. solo workload, priority workload, priority DSS workload,\n     priority OLTP workload, priority write workload, overall dss, etc.\n\nUse tcellcli/tperl/tmvfile/tcpfile/trmfile while adding any new test",
    "platform": null
  },
  {
    "test_name": "tsagrmrac.tsc",
    "setup": "tsagnini",
    "flags": {
      "maxinstances": "4",
      "sql_executable": "sqlplus",
      "sql_exe_mode": "/nolog",
      "cluster_database": "true",
      "asm_allow_sysdba": "true",
      "creatdev_file": "tkfgrddef",
      "asminst": "asm_instance^i^"
    },
    "description": "tsagrmrac.tsc - This test verifies IORM plans on RAC instances\n\nThis test verifies IORM plans being set on RAC instances and\n     ensures correct serialization amongst the incoming plans in cellsrv.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrmwl.tsc",
    "setup": null,
    "flags": {
      "wlcg_cmd": "^wlcg^"
    },
    "description": "tsagrmwl.tsc - Test SAGe Resource Manager Workload Generator\n\nThis is a non-standalone test for generating workloads to\n     verify IO Resource Manager mode transitions.  It was originally\n     written to be used with tsagrmmode.tsc, but can be used with\n     other tests.\n\nExpects \"wltype\" to be set to:\n       heavy_large_read\n       heavy_small_read\n       heavy_direct_write\n       heavy_bg_write\n       light_small_read\n       light_large_read\n       light_small_write\n       invalid_iormtag\n       allocation_failure\n       flash_large_read\n       flash_small_read\n\n     Expects the resource plan to have the following consumer groups\n       sys_group:         level 1, 100%: login as sysdba\n       interactive_group: level 2, 100%: login as oltp/oltp\n       batch_group:       level 3, 45%: login as batch/batch\n       low_group:         level 3, 35%: login as low/low\n       other_group:       level 3, 20%: login as other/other\n\n     Use tcellcli/tperl/trmfile/tcpfile/tmvfile while adding any new test",
    "platform": null
  },
  {
    "test_name": "tsagrpmtest.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrpmtest.tsc - Exadata Install lrg helper script",
    "platform": null
  },
  {
    "test_name": "tsagrprtimertest.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrprtimertest.tsc - disk/cell repair timer test\n\n1. Setup 3 cell exastack\n      2. Verify diskrepairTimer/cellrepairTimer\n      3. Verify alerts for disk FORCE DROPPED as timeout expired\n      4. Verify alerts for disk back online before timeout expired\n      5. Verify alerts for disk not eligible for DROP when shutdown cellsrv in cell1 and cell2",
    "platform": null
  },
  {
    "test_name": "tsagrs.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_testing": "1",
      "log_file": "tsagbug14212136.log"
    },
    "description": "tsagrs.tsc - RS suite tests\n\nRS suite tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrs1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrs1.tsc - sanity test for RS operations\n\nPerforms sanity test for RS operations",
    "platform": null
  },
  {
    "test_name": "tsagrs2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrs2.tsc - stress test for RS operations\n\nPerforms stress test for RS operations",
    "platform": null
  },
  {
    "test_name": "tsagrs3.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrs3.tsc - test RS behavior when zombie Cellsrv exists\n\nCheck whether RS waits for zombie Cellsrv to go away before starting\n     a new Cellsrv\n\nThis test first creates a zombie cellsrv process: start a cellsrv\n     from a script, kill -STOP the script and force kill the cellsrv.\n     We check that in the presence of a zombie cellsrv, we cannot restart\n     cellsrv in cellcli successfully - RS will keep trying to remove the\n     zombie before starting a new cellsrv. We check the alert log to see\n     if after 60 seconds, RS will print a message to the alert log and\n     generate an RS-7445 incident. We then kill -CONT the script pid (the\n     parent process of the zombie cellsrv), and observe if the cellcli\n     command to restart cellsrv finally succeeds",
    "platform": null
  },
  {
    "test_name": "tsagrs4.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrs4.tsc - Create zombie Cellsrv exists\n\nCheck whether RS waits for zombie Cellsrv to go away before starting\n     a new Cellsrv\n\nThis test first creates a zombie cellsrv process: start a cellsrv\n     from a script, kill -STOP the script and force kill the cellsrv.\n     We check that in the presence of a zombie cellsrv, we cannot restart\n     cellsrv in cellcli successfully - RS will keep trying to remove the\n     zombie before starting a new cellsrv. We check the alert log to see\n     if after 60 seconds, RS will print a message to the alert log and\n     generate an RS-7445 incident. We then kill -CONT the script pid (the\n     parent process of the zombie cellsrv), and observe if the cellcli\n     command to restart cellsrv finally succeeds",
    "platform": null
  },
  {
    "test_name": "tsagrscellsrvalrt.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrscellsrvalrt.tsc - Test for allechan_restart_cellsrv_alert\n\nAfter adding tag to cellinit.ora , expect incident after rs restart",
    "platform": null
  },
  {
    "test_name": "tsagrsflood.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagrslv1.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagrslv1.tsc - Drop Griddisk/Celldisk being Resilvered.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrslv2.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagrslv2.tsc - Resilvering griddisk offline/online in Loop",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrslv3.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagrslv3.tsc - Resilvering Griddisk Drop/Add to Diskgroup in Loop",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrslv4.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef",
      "oss_asm_sec": "true"
    },
    "description": "tsagrslv4.tsc - Resilvering Griddisk resize with DB file resize - Shrink",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrslv5.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagrslv5.tsc - Resilvering Griddisk Resize with DB file resize - Expansion",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrslvini.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrslvini.tsc - Defines some common scripts used in Resilvering Tests",
    "platform": null
  },
  {
    "test_name": "tsagrsport.tsc",
    "setup": null,
    "flags": {
      "tmpport": "rs_port^ossinst^"
    },
    "description": "tsagrsport.tsc - get RS ports\n\nThis is invoked by tsaginit to get 2 free ports for RS\n     and populate cellinit.ora with them.",
    "platform": null
  },
  {
    "test_name": "tsagrsqpcleanup.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell1_f": "^cell1^.us.oracle.com"
    },
    "description": "tsagrsqpcleanup.tsc - test for BUG 31537035 - LONG CELLSRV BROWNOUT SEEN WITH 120K QPS\n\ntest the following cases :\n     1- To test whether runtime cleanup of QPs is still done in cellsrv.\n     2- To test whether QP cleanup is done in background on shutdown\n     3- To test whether QP cleanup is done in background on crash:\n\ntest to be added in lrgrhx7saipcdat",
    "platform": null
  },
  {
    "test_name": "tsagrsrestart_locale.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagrun.tsc",
    "setup": null,
    "flags": {
      "ALL_ARG_LIST": "^ALL_ARG_LIST^' '^ARGVALUE^'='^^ARGVALUE^",
      "NXTARG": "0",
      "ARGNAME": "ARG^NXTARG^",
      "ARGVALUE": "^^ARGNAME^"
    },
    "description": "tsagrun.tsc - run oratst command stored in variable passed in",
    "platform": null
  },
  {
    "test_name": "tsagruniormpmem.tsc",
    "setup": null,
    "flags": {
      "iormreflog": "tsagiormpmem_off_bug^xrmem_mode^.log"
    },
    "description": "tsagruniormpmem.tsc - Run unit test of IORM PMEM Cache\n\nIORM PMEM Cache includes below Basic TestCase 1 and TestCases 2 : flash cache and pmem cache\n     section - define which case will be tested\n       section = pmem_wr_fc_wr : WT PMEMCache + WT FC\n       section = pmem_wr_fc_wb : WT PMEMCache + WB FC\n       section = pmem_wb_fc_wr : WB PMEMCache + WT FC\n       section = pmem_wb_fc_wb : WB PMEMCache + WB FC\n       section = pmem_wr_fc_no : PMEMCache without FC\n\nA: Basic TestCase 1:\n\n     1. Use multiple cluster test framework to setup two clusters:\n          Cluster 1 is normal database, and Cluster 2 is 2 CDBs with 2 PDBs in each one\n\n     2. Configure inter-dbplan on cell side, focus on three attributes: pmemcachemin/pmemcachelimit/pmemcachesize\n        pmemcachelimit - soft limit , pmemcachesize - hard limit\n\n     3. Configure intra-dbplan on db server side:\n       a). Create two resource consumer groups on normal database in cluster 1\n       b). Map user sessions to consumer groups\n       c). Use CREATE_SIMPLE_PLAN to create and enable a resource plan for normal database\n       d). Use CREATE_CDB_PLAN_DIRECTIVE and CREATE_CDB_PLAN to create user directives and cdb plan in Cluster 2 CDB1,\n           config new_memory_min, new_memory_limit, memory_min, memory_limit\n       e). Use CREATE_CDB_PROFILE_DIRECTIVE and CREATE_CDB_PLAN to create user profiles and cdb plan in Cluster 2 CDB2,\n           config new_memory_min, new_memory_limit, memory_min, memory_limit\n       f). Enable intra-dbplan\n\n     4. Enable IORM on cell side\n        CellCLI> ALTER IORMPLAN objective=auto;\n\n     5. Fork to run different workloads via tsagworkload.sql on different consumer groups across normal database and CDBs\n        Also includes CC1/CC2 workloads\n         Consumer group 1 -> workload1\n         Consumer group 2 -> workload2\n\n     6. Verify result\n       1). All workloads should be successful\n       2). Check the stats in nvcache dump trace files, this need your help to clarify which stats should be verified.\n           CellCLI> alter cell events = \"immediate cellsrv.cellsrv_nvcache(dumpstats, 0, 0, 1)\"\n           CellCLI> alter cell events = \"immediate cellsrv.cellsrv_nvcache(dumpgroups, 0, 0, 0)\"\n       3). Check the pmem cache limits about the intra-dbplans and inter-dbplans in oss_iorm.txt\n       4). Display the pmem cache limits in list database and pluggabledatabase\n       5). Check pmemcache/flashcache and flashcachecontent\n\n     B: TestCases 2 : flash cache and pmem cache\n       1. Setup exadata with flash cache and pmem cache, there are five conbinations\n         1) WT PMEMCache + WT FC  : section = pmem_wr_fc_wr\n         2) WT PMEMCache + WB FC  : section = pmem_wr_fc_wb\n         3) WB PMEMCache + WT FC  : section = pmem_wb_fc_wr\n         4) WB PMEMCache + WB FC  : section = pmem_wb_fc_wb\n         5) PMEMCache without FC  : section = pmem_wr_fc_no\n\n       2. Rerun testcase1",
    "platform": null
  },
  {
    "test_name": "tsagruniormqm.tsc",
    "setup": null,
    "flags": {
      "message": "'QuarantineMgr: quarantining .*'",
      "default_catdbplan": "'(name=other, level=2, allocation=50)'",
      "iorm_user": "'iormuser2/iormuser2'",
      "qmprefix": "1",
      "qm_crash_freq": "1",
      "iormdb_inst": "1",
      "ref_log": "tsagiormqm20_^iormdb_inst^.log"
    },
    "description": "tsagruniormqm.tsc - Run unit test of IORM Quarantine\n\nqm_crash_freq - use it to define the cellsrv crash frequent\n     skip_qm_check - use it to define when to skip check for QM message\n     my_plan - This is the plan name of IROM on db side\n     enable_iormplan - enable or disable iormplan on cellsrv side\n     section - define which case will be tested\n       section = INTRA_DBPLAN_SOFTLIMIT : use intradbplan quarantine to isolate soft intradbplan\n       section = INTRA_DBPLAN_HARDLIMIT : use intradbplan quarantine to isolate hard intradbplan\n       section = INTER_DBPLAN_HARDLIMIT : use iormplan quarantine to isolate soft interdbplan\n       section = INTER_DBPLAN_SOFTLIMIT : use iormplan quarantine to isolate hard interdbplan\n       section = CATEGORY_DBPLAN_SOFTLIMIT : use iormplan quarantine to isolate category plan\n       section = MIXED_INTRA_INTER_DBPLAN : use intradbplan and iormplan quarantine to mixed plans\n       section = MIXED_ALL_DBPLAN :  use intradbplan and iormplan quarantine to all mixed plans",
    "platform": null
  },
  {
    "test_name": "tsagrunmtciormqm.tsc",
    "setup": null,
    "flags": {
      "default_catdbplan": "'(name=other, level=2, allocation=50)'",
      "iorm_user": "'iormuser2/iormuser2@inst1'",
      "qm_crash_freq": "1",
      "iormdb_inst": "1",
      "excld_mode": "'_excld'"
    },
    "description": "tsagrunmtciormqm.tsc - Run unit test of IORM Quarantine\n\nqm_crash_freq - use it to define the cellsrv crash frequent\n     skip_qm_check - use it to define when to skip check for QM message\n     my_plan - This is the plan name of IROM on db side\n     enable_iormplan - enable or disable iormplan on cellsrv side\n     section - define which case will be tested\n       section = INTRA_DBPLAN_SOFTLIMIT : use intradbplan quarantine to isolate soft intradbplan\n       section = INTRA_DBPLAN_HARDLIMIT : use intradbplan quarantine to isolate hard intradbplan\n       section = INTER_DBPLAN_HARDLIMIT : use iormplan quarantine to isolate soft interdbplan\n       section = INTER_DBPLAN_SOFTLIMIT : use iormplan quarantine to isolate hard interdbplan\n       section = CATEGORY_DBPLAN_SOFTLIMIT : use iormplan quarantine to isolate category plan\n       section = MIXED_INTRA_INTER_DBPLAN : use intradbplan and iormplan quarantine to mixed plans\n       section = MIXED_ALL_DBPLAN :  use intradbplan and iormplan quarantine to all mixed plans\n\nWe run a set of unit tests about IORM QM in tsagrunmtciormqm.tsc from\n       cell view like :\n         runtest tsagrunmtciormqm\n\n       in cell view, there is no asm/db instances in it, so we use mtcruninrdbms.tsc\n       to rewrite the macro to support sesql/sesqlfp to run in cell view, this can\n       allow us to run the sql files from cell view. below is the examples:\n         sesql tsagset_cdbplan 0 iorm_plan log=tsag_setdbplan_clu1.log clusterid=cid1\n       It means we have tsagset_cdbplan.sql in cell view, then it will be copied to\n       cluster 1 view, and use pcw_remote_exec to run sesql in cluster 1 view,\n       when it is finished, copy the output file ^log^ to t_work under cell view\n\n       There are two parameters in mtcruninrdbms.tsc:\n         - clusterid, the default is cid1, it can be set to cid1 or cid2, it means\n           that the sql file will be run in cluster 1 or cluster 2\n         - log, the name of output file, if don't set, it is the same as the name of sql file\n\n      For \"sesqlfp\", we should pass the connect string to sql file, eg.\n        fork sesqlfp tsagwkld iormuser1/iormuser1@inst1 log=tsagwkld_sub_cid1.log clusterid=cid1\n        fork sesqlfp tsagwkld iormuser1/iormuser1@cdb1_pdb1 log=tsagwkld_sub_cid1.log clusterid=cid2\n        wait\n\n      We have another wrapper - pcw_remote_exec_wrapper which can help us to run tsc file\n      in cluster views asynchronously\n       examples:\n    \t  for cid 1 ^max_cluster_num^\n    \t    CPFILE T_WORK:tsagstartasmdb.tsc HAS_MCL_HASW_^cid^:\n    \t    fork T_WORK:pcw_remote_exec_wrapper.tsc pcw_exec_tsc=tsagstartasmdb.tsc clusterid=^cid^\n    \t  endloop\n    \t  wait\n\n      If you want to run tsc file in cluster views in sequence, please do:\n    \t  for cid 1 ^max_cluster_num^\n    \t    CPFILE T_WORK:tsagstartasmdb.tsc HAS_MCL_HASW_^cid^:\n    \t    pcw_remote_exec T_HAS_WORK:tsagstartasmdb.tsc node_nums=0 clusterid=^cid^ wait_for_complete\n    \t  endloop",
    "platform": null
  },
  {
    "test_name": "tsagrunqm.tsc",
    "setup": null,
    "flags": {
      "qm_attribute": "'SI'",
      "plan_name": "'test'",
      "upper_rang": "^qm_attr_val^",
      "qm_temp": "qm_value^i^"
    },
    "description": "tsagrunqm.tsc - Run fined grained quarantine tests\n\nqm_attribute - This is the quarantine attribute what you want to test\n        the available values: SI, EHCC, OLTP, TRACE, EVA_PRE,\n        FAST_PRE, CHAINEDROW, BLOOMFILTER, DECRYPTION\n\n     sys_quarantine - how to create quarantines in system plan\n        NONE  - Do not create a quarantine;\n        FULL  - Full quarantines;\n        FINED - Fined Grained quarantines\n        FULL_FINED - Full + Fined quarantines\n     user_quarantine - how to create quarantines in user plan\n        NONE  - Do not create a quarantine;\n        FULL  - Full quarantines;\n        FINED - Fined Grained quarantines\n        FULL_FINED - Full + Fined quarantines\n\n  For parameter quarantine_tp:\n       1 - ALLDB Fined Grained Quarantine\n       2 - database Fined Grained Quarantine\n       3 - OBJID Fined Grained Quarantine\n       4 - SQLID Fined Grained Quarantine\n       5 - SQL PLAN Fined Grained Quarantine\n       6 - ALLDB Full Quarantine\n       7 - database Full Quarantine\n       8 - OBJID Full Quarantine\n       9 - SQLID Full Quarantine\n      10 - SQL PLAN Full Quarantine",
    "platform": null
  },
  {
    "test_name": "tsagrunqm1.tsc",
    "setup": null,
    "flags": {
      "qm_attribute": "'SI'",
      "plan_name": "'test'",
      "upper_rang": "^qm_attr_val^",
      "qm_temp": "qm_value^i^"
    },
    "description": "tsagrunqm1.tsc - Run fined grained quarantine tests\n\nqm_attribute - This is the quarantine attribute what you want to test\n        the available values: SI, EHCC, OLTP, TRACE, EVA_PRE,\n        FAST_PRE, CHAINEDROW, BLOOMFILTER, DECRYPTION\n\n     sys_quarantine - how to create quarantines in system plan\n        NONE  - Do not create a quarantine;\n        FULL  - Full quarantines;\n        FINED - Fined Grained quarantines\n        FULL_FINED - Full + Fined quarantines\n     user_quarantine - how to create quarantines in user plan\n        NONE  - Do not create a quarantine;\n        FULL  - Full quarantines;\n        FINED - Fined Grained quarantines\n        FULL_FINED - Full + Fined quarantines\n\n  For parameter quarantine_tp:\n       1 - ALLDB Fined Grained Quarantine\n       2 - database Fined Grained Quarantine\n       3 - OBJID Fined Grained Quarantine\n       4 - SQLID Fined Grained Quarantine\n       5 - SQL PLAN Fined Grained Quarantine\n       6 - ALLDB Full Quarantine\n       7 - database Full Quarantine\n       8 - OBJID Full Quarantine\n       9 - SQLID Full Quarantine\n      10 - SQL PLAN Full Quarantine",
    "platform": null
  },
  {
    "test_name": "tsagrunrhorion.tsc",
    "setup": null,
    "flags": {
      "testname": "test1",
      "runtype": "rand",
      "dbid": "0",
      "cgid": "0",
      "ndisks": "1",
      "nsmall": "0",
      "nlarge": "0",
      "size_large": "1024",
      "size_small": "32",
      "dur": "360",
      "write": "0",
      "thold": "30.00",
      "huge_no_need": "'-hugenotneeded'"
    },
    "description": "tsagrunrhorion.tsc - Runs orion on ASM disks\n\nRuns orion on ASM disks on real hardware\n\nStandard value to be determined after test is run in a dedicated storage\n     The test can be run with differet parameters for numlarge & runtype\n     example: oratst -d tsagrunrhorion testname=largeio dbid=5 ipaddress=^ipaddress^ ndisks=5 nlarge=1000 nsmall=0",
    "platform": null
  },
  {
    "test_name": "tsagsacellsigmask.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagsacellsigmask.tsc - test for cellsrv sigmask\n\nTest for cellsrv to check the output of grep SigBlk\n     before and after running cellsrvstat command (Bug: 36888230)\n\nTest performs following Operations\n   1. It checks for sigmask of all threads\n   2. Add OSS_EL_SEGV_KGETRY with 1 occurrence\n   3. After the simulation event is triggered, check if all threads' sigmasks are\n      normal after the signal is dealt with by comparing it with the values taken in step 1.\n\n   It checks for SigBlk by providing cellsrv pid.\n\n    === Flow of test for Bug:36888230 ===\n    1. Grep for SigBlk by providing cellsrv pid\n    2. Run cellsrvstat -stat=exec_threadstate\n    3. Again, grep for SigBlk by providing cellsrc pid\n    4. Comapare values before and after running cellsrvstat are same",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsacpc.tsc",
    "setup": null,
    "flags": null,
    "description": "oss_sacpc.tsc - Cross-Platform Compatibility Test\n\nto make OSS source compilable by g++/gcc",
    "platform": null
  },
  {
    "test_name": "tsagsaexaclddrl.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "phase_dur": "120",
      "FLASH_SIZE": "1024M",
      "vault_db": "DATA",
      "vault_log": "DATA",
      "sage_mirror_mode": "high"
    },
    "description": "tsagsaexaclddrl.tsc - Basic DRL test\n\nTests basic DRL behavior with snapshots and everything\n\nTestadded in lrgsaexaclddrl",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagsaexaclddrl2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "cmd_mirror_mode": "normal",
      "vault_db": "DATA",
      "vault_log": "DATA",
      "egs_trace": "highest",
      "sage_mirror_mode": "high"
    },
    "description": "tsagsaexaclddrl2.tsc - DRL serialization test  Tests racing write/delete cases\n\nDescription mentioned below\n\ntest added in lrgsaexacldvesdrl2",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagsaexcdisktrans1.tsc",
    "setup": null,
    "flags": {
      "create_template_with_high_redund": "true",
      "mixed_workload": "true",
      "maxpdb": "2"
    },
    "description": "tsagsaexcdisktrans1.tsc - test for covering disk state transitions\n\ntest covers disk state transition cases\n     1. ACCESSIBLE_GOOD -> ACCESSIBLE_NEEDS_TRIAGE -> INACCESSIBLE_TRAIGING- > ACCESSIBLE_GOOD\n     2. ACCESSIBLE_GOOD -> ACCESSIBLE_NEEDS_TRIAGE -> ACCESSIBLE_GOOD\n     3. ACCESSIBLE_GOOD <-> DEAD\n     4. ACCESSIBLE_GOOD -> INACCESSIBLE_TRIAGING -> INACCESSIBLE_UNKNOWN -> ACCESSIBLE_GOOD",
    "platform": null
  },
  {
    "test_name": "tsagsafefile.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagsafefile.tsc - Safe file test",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagsafwfail_rh.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "MACH_NAME": "^cell^",
      "MACH_PASSWD": "welcome1",
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagsafwfail_rh.tsc - cx3 cards fw upgrade/downgrade tests\n\npls see at the starting of the tests\n\nTest to be added in lrgrhx5safwfail",
    "platform": null
  },
  {
    "test_name": "tsagsahangman.tsc",
    "setup": null,
    "flags": null,
    "description": "oss_sahangman.tsc - Cellsrv unit test for hangman utility\n\nRuns test-hm (which runs oss/utl/hangman) unit test",
    "platform": null
  },
  {
    "test_name": "tsagsahfact.tsc",
    "setup": "srdbmsini",
    "flags": {
      "cdb": "true",
      "differentszfg": "1",
      "compatible": "^def_compatibility^",
      "dbconn": "'sys/knl_test7 as sysdba'",
      "asmconn": "'sys/knl_test7@inst11 as sysasm'"
    },
    "description": "tsagsahfact.tsc - testcase for bug 34074579 where a brownout is seen\n     due to increased disk latency while rebalance is running for a replaced disk.\n\ntestcase for bug 34074579 where a brownout is seen due to increased\n     disk latency while rebalance is running for a replaced disk.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsandboxaltersp1.tsc",
    "setup": null,
    "flags": {
      "add_drop_time": "300",
      "min_cell": "12",
      "max_cell": "24",
      "max_iters_input": "3600",
      "iters_before_resize": "600",
      "drop_time": "600",
      "test_name": "^tst_tscname^d"
    },
    "description": "tsagsandboxaltersp1.tsc - EGS Sandbox Alter SP Reconfig Tests\n\nTest scenarios included are - SP is created with x no. of cells\n         and (y-x) cells are added to the SP to resize to y cells.\n         Again, the cells added are dropped and SP is altered.\n     The cells have 10TB, 14TB and 18TB sized disks.\n      the different disk sizes depend on the cell_no\n      if (cell_no%3) is 0 then 10 TB, if 1 then 14 TB and if 2 then 18 TB\n      1. 6 cells -> 12 cells -> 6 cells\n      2. 3 cells -> 12 cells -> 3 cells\n      3. 3 cells -> 6 cells -> 3 cells\n      4. 12 cells -> 24 cells -> 12 cells",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp10.tsc",
    "setup": null,
    "flags": {
      "add_drop_time": "100",
      "x": "179",
      "y": "7",
      "max_iters_input": "2700",
      "iters_before_resize": "1800",
      "drop_time": "900",
      "test_name": "^tst_tscname^c"
    },
    "description": "tsagsandboxaltersp10.tsc - SP Reconfig Test\n\nContains test cases where -\n   1. Create SP with x cells\n   2. Alter SP to go from x cells to y cells\n   3. Alter SP to go from y cells back to x cells\n   Storagepool is shrunk first and then grown",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp11.tsc",
    "setup": null,
    "flags": {
      "add_drop_time": "100",
      "x": "200",
      "y": "15",
      "max_iters_input": "2700",
      "iters_before_resize": "1800",
      "drop_time": "900",
      "test_name": "^tst_tscname^b"
    },
    "description": "tsagsandboxaltersp11.tsc - SP Reconfig Test\n\nContains test cases where -\n   1. Create SP with x cells\n   2. Alter SP to go from x cells to y cells\n   3. Alter SP to go from y cells back to x cells\n   Storagepool is shrunk first and then grown",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp12.tsc",
    "setup": null,
    "flags": {
      "min_cell": "3",
      "max_cell": "500",
      "max_iters_input": "3600",
      "iters_before_resize": "600",
      "test_name": "^tst_tscname^"
    },
    "description": "tsagsandboxaltersp12.tsc - EGS Sandbox Alter SP Reconfig Tests\n\nTest scenarios included are - SP is created with x no. of cells\n         and (y-x) cells are added to the SP to resize to y cells.",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp2.tsc",
    "setup": null,
    "flags": {
      "min_cell": "27",
      "max_cell": "120",
      "max_iters_input": "3600",
      "iters_before_resize": "900",
      "drop_time": "900",
      "add_drop_time": "600",
      "test_name": "^tst_tscname^c"
    },
    "description": "tsagsandboxaltersp2.tsc - EGS Sandbox Alter SP Reconfig Tests\n\nTest scenarios included are - SP is created with x no. of cells\n         and (y-x) cells are added to the SP to resize to y cells.\n         Again, the cells added are dropped and SP is altered.\n     The cells have 10TB, 14TB and 18TB sized disks.\n      the different disk sizes depend on the cell_no\n      if (cell_no%3) is 0 then 10 TB, if 1 then 14 TB and if 2 then 18 TB\n      1. 6 cells -> 35 cells -> 6 cells\n      2. 16 cells -> 195 cells -> 16 cells\n      3. 27 cells -> 120 cells -> 27 cells",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp3.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagsandboxaltersp3.tsc - EGS Sandbox Alter SP Reconfig Tests\n\nTest Scenario - SP is created with 3 cells and then it resized to\n        6 cells, to 12 cells and 24 cells.\n      Cells have disks of sizes 10TB, 14TB and 18TB\n      the different disk sizes depend on the cell_no\n      if (cell_no%3) is 0 then 10 TB, if 1 then 14 TB and if 2 then 18 TB",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp4.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^a"
    },
    "description": "tsagsandboxaltersp4.tsc - EGS SANDBOX TESTING\n\nSetup SP with 12 cells at start and reduce to 6 and then to 3 cells\n     Fail and bringup disks in between.",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp5.tsc",
    "setup": null,
    "flags": {
      "x": "12",
      "y": "6",
      "max_iters_input": "1800",
      "drop_time": "600",
      "test_name": "^tst_tscname^b"
    },
    "description": "tsagsandboxaltersp5.tsc - SP reconfig Test",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp6.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagsandboxaltersp6.tsc - Alter SP scenario\n\nSP Resize test from 11 cells to 140 cells, where cells have\n      multiple sized disks",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp7.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagsandboxaltersp7.tsc - Alter SP scenario\n\nSP is started with 10 cells, later 1 cell is dropped and again\n     it is added back",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp8.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagsandboxaltersp8.tsc - SP Reconfig Test\n\nSP is created with 3 cells, later 252 cells are added and\n      it is resized to 255 cells",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp9.tsc",
    "setup": null,
    "flags": {
      "add_drop_time": "100",
      "x": "37",
      "y": "4",
      "max_iters_input": "2400",
      "iters_before_resize": "1250",
      "drop_time": "600",
      "test_name": "^tst_tscname^c"
    },
    "description": "tsagsandboxaltersp9.tsc - SP Reconfig Test\n\nContains test cases where -\n   1. Create SP with x cells\n   2. Alter SP to go from x cells to y cells\n   3. Alter SP to go from y cells back to x cells",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp_ad.tsc",
    "setup": null,
    "flags": {
      "add_drop_time": "100"
    },
    "description": "tsagsandboxaltersp_ad.tsc - Alter StoragePool Add and Drop cells\n\nIncludes common steps to alter the storagepool twice\n        First, N no. of cells are added and SP is altered\n        Then, N no.of cells are dropped and SP is altered",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp_add.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsandboxaltersp_add.tsc - Alter StoragePool Add cells\n\nIncludes common steps to alter the storagepool\n        N no. of cells are added and SP is altered",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp_da.tsc",
    "setup": null,
    "flags": {
      "add_drop_time": "100",
      "min_cell": "^x^"
    },
    "description": "tsagsandboxaltersp_da.tsc - Alter StoragePool Drop and Add cells\n\nIncludes common steps to alter the storagepool twice\n        First, N no. of cells are dropped and SP is altered\n        Then, N no.of cells are added and SP is altered",
    "platform": null
  },
  {
    "test_name": "tsagsandboxaltersp_drop.tsc",
    "setup": null,
    "flags": {
      "add_drop_time": "100",
      "max_cell": "^x^",
      "min_cell": "^x^"
    },
    "description": "tsagsandboxaltersp_drop.tsc - Alter StoragePool Drop Test",
    "platform": null
  },
  {
    "test_name": "tsagsandboxcreatesp12d_hc.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^",
      "x": "95",
      "y": "100",
      "size_of_disks": "22000000000000",
      "no_of_disks": "12",
      "media_type": "4"
    },
    "description": "tsagsandboxcreatesp12d_hc.tsc\n\nEGS Sandbox Dynamic SP Creation Tests\n      Configuration: 22TB Disks, HC config, 12 disks per cell",
    "platform": null
  },
  {
    "test_name": "tsagsandboxcreatesp4d_ef.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^",
      "x": "95",
      "y": "100",
      "size_of_disks": "6000000000000",
      "no_of_disks": "4",
      "media_type": "2"
    },
    "description": "tsagsandboxcreatesp4d_ef.tsc\n\nEGS Sandbox Dynamic SP Creation Tests\n      Configuration: 6.0TB Disks, EF config, 4 disks per cell",
    "platform": null
  },
  {
    "test_name": "tsagsandboxcreatesp6d_hc.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^",
      "x": "95",
      "y": "100",
      "size_of_disks": "22000000000000",
      "no_of_disks": "6",
      "media_type": "4"
    },
    "description": "tsagsandboxcreatesp6d_hc.tsc\n\nEGS Sandbox Dynamic SP Creation Tests\n      Configuration: 22TB Disks, HC config, 6 disks per cell",
    "platform": null
  },
  {
    "test_name": "tsagsandboxcreatesp8d_ef.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^",
      "x": "95",
      "y": "100",
      "size_of_disks": "15000000000000",
      "no_of_disks": "8",
      "media_type": "2"
    },
    "description": "tsagsandboxcreatesp8d_ef.tsc\n\nEGS Sandbox Dynamic SP Creation Tests\n      Configuration: 15TB Disks, EF config, 8 disks per cell",
    "platform": null
  },
  {
    "test_name": "tsagsandboxreconfigadd.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagsandboxreconfigadd.tsc - Using reconfig_cell_add,\n\tincrease cells from 8 to 9, 9 to 12 and\n\t12 to 13 to see split ring model and\n\t cell partner transitions in sandbox environment\n\nTakes min cell as 8  and max cell as 13.\n     Then using reconfig_cell_add function, increase\n     cells from 8 to 13 in 3 steps to check split ring\n     model and cell partner transitions in  sandbox environment",
    "platform": null
  },
  {
    "test_name": "tsagsandboxreconfigcells.tsc",
    "setup": null,
    "flags": {
      "test_name": "^tst_tscname^"
    },
    "description": "tsagsandboxreconfigcells.tsc - Using reconfig_cell_add,\n\tincrease cells from 8 to 13 and then drops them from 13 to 8 one by one\n              in sandbox environment\n\nTakes min cell as 8  and max cell as 13.\n     Then using reconfig_cell_add function, increase\n     cells from 8 to 13 in 3 steps to check split ring\n     model and cell partner transitions in  sandbox environment",
    "platform": null
  },
  {
    "test_name": "tsagsaparam.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "ix": "0",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagparam - Exadata parameters\n\nThis test file provides a place to add all test cases needed to test\n     exadata related parameters",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsarestfuzz.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagsarestfuzz.tsc - Fuzzing test on exadata rest apis\n\nRestfuzz tool is used for fuzzing test.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsaupdate.tsc",
    "setup": null,
    "flags": {
      "skip_each_boot": "true",
      "fresh_image_for_patch": "true"
    },
    "description": "tsagsaupdate.tsc - Exadata test for Upgrade/downgrade",
    "platform": null
  },
  {
    "test_name": "tsagscbugsuit.tsc",
    "setup": "srdbmsini",
    "flags": {
      "asm_ausize": "1048576",
      "oss_no_asmdb": "true",
      "asm_allow_sysdba": "true"
    },
    "description": "tsagscbugsuit.tsc - Smart Scan specific bug test suite\n\nPlease DO NOT add any MS related bug test in this suite.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagscrubtst_db.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "compute_node_f": "^compute_node^.us.oracle.com",
      "dbnodeconnstr": "root@^compute_node_f^"
    },
    "description": "tsagscrubtst_db.tsc -  Test to cover bug 35551562 for\n     compute node\n\nSteps:\n   1. Run alter dbserver harddiskscrub attributes to see scrubbing error",
    "platform": null
  },
  {
    "test_name": "tsagscrubtst_ef.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "cell1_f": "^cell1^.us.oracle.com",
      "cellconnstr": "root@^cell1_f^"
    },
    "description": "tsagscrubtst_ef.tsc - Test to cover bug 35551562 for\n     extreme flash cell\n\nSteps:\n   1. set _disk_scrubbing_bypass_asm_check=true in cellinit.ora\n   2. Run alter cell harddiskscrub attributes to see scrubbing error\n   3. Wait and check for zero message like Begin scrubbing in alert.log",
    "platform": null
  },
  {
    "test_name": "tsagsdd.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagr2def",
      "log_file": "tsagsdd.log"
    },
    "description": "tsagsdd.tsc - Test DROP CELL/CELLDISK/GRIDDISK with erase option\n                    under ASM/DB instance\n\nThis script is used to test erase data for CELL/CELLDISK/GRIDDISK\n         with ASM/DB instance, ERASE can be zero,nnas and dod.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsdd1.tsc",
    "setup": "tsaginit",
    "flags": {
      "log_file": "tsagsdd1.log"
    },
    "description": "tsagsdd1.tsc - Test DROP CELL/CELLDISK/GRIDDISK with erase option\n                    under no ASM/DB instance\n\nThis script is used to test erase data for CELL/CELLDISK/GRIDDISK\n         without ASM/DB instance, ERASE can be zero,nnas and dod.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsddfc.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagr2def",
      "asm_ausize": "1048576",
      "tk_parsearg_force_compatible": "^max_compatibility^",
      "log_file": "tsagsddfc.log"
    },
    "description": "tsagsddfc.tsc - Drop erase for griddisk,celldisk and cell with flashdisk\n\nSDD support flashdisks, but only erase=7pass to be finished",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsddfcini.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsddfcini.tsc - create celldisks/griddisks on flashdisks\n\noss_sdd_fc = 1 - create all celldisks on flashdisks, flashcache\n                      in a separate flashdisk\n     oss_sdd_fc = 2 - create all celldisks on flashdisks, flashcache\n                      in a same flashdisk\n     oss_sdd_fc = 3 - create celldisks on both flashdisks and harddisks\n                      flashcache in a separate flashdisk\n     oss_sdd_fc = 4 - create celldisks on both flashdisks and harddisks\n                      flashcache in the same celldisks\n     when force_listener is true, will use lintener to connect rdbms to\n         get rid of invalid ipaddress error.",
    "platform": null
  },
  {
    "test_name": "tsagsddini.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsddini.tsc - Initialize some data before issuing drop erase\n\nThis script is used to create some celldisks and griddisk in Cell 2 and 3,\n     Cell 3 is a mirror of Cell 2 by creating normal redundancy DG.",
    "platform": null
  },
  {
    "test_name": "tsagsddiorm.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagsdddef",
      "dgattr": "'attribute '''compatible.asm'''='''^scompatible.asm^''','''compatible.rdbms'''='''^scompatible.rdbms^''','''cell.smart_scan_capable'''='''true''",
      "log_file": "tsagsddiorm.log"
    },
    "description": "tsagsddiorm.tsc - OSS drop erase IORM test\n\nThis script is going to test read/write IOs and erasing IOs, which is managed by IORM Plan, so fork\n       to run two cases:\n       1). Generate random IO to GridDisk gd1\n       2). Submit erase=7pass to GridDisk gd2\n     #1 should finish firstly due to high priority",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsddmct.tsc",
    "setup": null,
    "flags": {
      "log_file": "tsagsddmct.log"
    },
    "description": "tsagsddmct.tsc - metadata corruption tests of SDD project\n\ncorrupt metadata of cellsrv while issuing drop erase command to the disks",
    "platform": null
  },
  {
    "test_name": "tsagsddmt.tsc",
    "setup": null,
    "flags": {
      "log_file": "tsagsddmt.log"
    },
    "description": "tsagsddmt.tsc - multiple tests of Project 33295\n\nErase and recreate disk multiple times",
    "platform": null
  },
  {
    "test_name": "tsagsddpt.tsc",
    "setup": null,
    "flags": {
      "log_file": "tsagsddpt.log"
    },
    "description": "tsagsddpt.tsc - persistence tests of SDD project\n\nrestart ms/cellsrv while issuing drop erase command to the disks",
    "platform": null
  },
  {
    "test_name": "tsagsddrac.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagr2def",
      "maxinstances": "3",
      "cluster_database": "true",
      "log_file": "tsagcksddrac.log"
    },
    "description": "tsagsddrac.tsc - RAC tests of Secure data deletion project\n\nSetup 3 cells and 3 asm/db instances, then fork to simulate mass IOs into the disks\n           in different instances and erase the disks.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsddsimu.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagr2def",
      "log_file": "tsagsddsimua.log"
    },
    "description": "tsagsddsimu.tsc - Drop erase tests with simulation event\n\nDrop erase tests with simulation event, it includes below test cases:\n\n     Test Suite A : Erase for Griddisk - the output is tsagsddsimua.log\n        TestCase 1 - GridDisk is in HEALTH_GOOD state\n        TestCase 2 - GridDisk is in HEALTH_GOOD state ¨C erase with force option\n        TestCase 3 - GridDisk is in worse state than BAD_ONLINE\n        TestCase 4 - GridDisk is in worse state than BAD_ONLINE ¨C erase with force option\n\n     Test Suite B : Celldisk with one or more griddisks - the output is tsagsddsimub.log\n        TestCase 5 - CellDisk is in HEALTH_GOOD state ¨C erase with force option\n        TestCase 6 - CellDisk is in worse state than BAD_ONLINE\n        TestCase 7 - CellDisk is in worse state than BAD_ONLINE ¨C erase with force option\n\n     Test Suite C : Celldisk with no griddisks - the output is tsagsddsimuc.log\n        TestCase 8 - CellDisk is in HEALTH_GOOD state ¨C erase with force option\n        TestCase 9 - CellDisk is in worse state than BAD_ONLINE\n        TestCase 10 - CellDisk is in worse state than BAD_ONLINE ¨C erase with force option\n\n     Test Suite D : Erase disks with nowait option - the output is tsagsddsimud.log\n        TestCase 11 - GridDisk is in HEALTH_GOOD state ¨C erase with nowait option\n        TestCase 12 - CellDisk with no griddisks is in worse state than BAD_ONLINE\n                      and erase with nowait\n        TestCase 13 - CellDisk with no griddisks is in worse state than BAD_ONLINE\n                      and erase with force and nowait\n\nWe don't start asm and db instance in these tests.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsdsh.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsdsh.tsc - test distributed shell for sage\n\ndistributed shell for sage\n\ndcli is a python script used to run commands on multiple remote\n     cells in parallel.",
    "platform": null
  },
  {
    "test_name": "tsagsdsh3.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsdsh3.tsc - regression test for dcli on python 3\n\ndcli regression test for python 3",
    "platform": null
  },
  {
    "test_name": "tsagsdwarmio.tsc",
    "setup": null,
    "flags": {
      "sga_target": "^def_sga_target^",
      "use_self_tune_sga": "true",
      "connect_as_sysdba": "'connect sys/knl_test7 as sysdba'",
      "format_long_identifier": "true",
      "cluster_database": "true",
      "maxinstances": "2",
      "SAGE_MIRROR_MODE": "normal",
      "failalldbdg": "true",
      "flash_size": "240",
      "flash_griddisk_size": "160",
      "uniq_dsknames": "FLASH",
      "db_cache_size": "16M",
      "DB_KEEP_CACHE_SIZE": "16M",
      "DB_RECYCLE_CACHE_SIZE": "16M",
      "loop_cntr": "360"
    },
    "description": "tsagsdwarmio.tsc - Test for sparse disk creation during warm up of flash disk",
    "platform": null
  },
  {
    "test_name": "tsagsec.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagr2def",
      "max_instance": "1"
    },
    "description": "tsagsec.tsc - security tests",
    "platform": null
  },
  {
    "test_name": "tsagsec1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsec1.tsc - SAGE security test with ASM\n\nThis is called by tsagsec.\n     Start ASM, mount disks, and access them via asmcmd with the key given.\n     It is not a good key, the disks won't be seen.\n\n     Input parameters:\n      - tstlog - log name\n      - reflog - reference log to compare against",
    "platform": null
  },
  {
    "test_name": "tsagsec2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsec2.tsc - SAGE security test with tools other than ASM and DB\n\nThis is called by tsagsec.\n     Run different client tools with the key given.\n     It is not a good key, the clients will fail to run properly.\n\n     Input parameters:\n      - tstlog - log name\n      - reflog - reference log to compare against",
    "platform": null
  },
  {
    "test_name": "tsagsec3.tsc",
    "setup": "tsagnini",
    "flags": {
      "MY_CELL_ANT_PORT_ACTIVITY_THRESHOLD": "20000",
      "creatdev_file": "my_tsagrddef",
      "sage_mirror_mode": "^sage_mirror_mode^",
      "oss_failgroup": "failalldbdg",
      "compatible": "^max_compatibility^",
      "asm_ausize": "1048576",
      "nflint": "1",
      "kxdbio_ut_ctl": "4",
      "REMOTE_CELL_MGR_INSTANCES": "4",
      "mytest": "^tst_tscname^_high",
      "logfile1": "^mytest^.log"
    },
    "description": "tsagsec3.tsc - The goal of this test is to verify C2C offload and\n                    HTW w/ ASM-scoped and DB-scoped securities. Test\n                    veries that both the features use Smart Scan code path.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsec4.tsc",
    "setup": "tsagnini",
    "flags": {
      "cdb": "true",
      "creatdev_file": "my_tsagrddef",
      "sage_mirror_mode": "high",
      "compatible": "^max_compatibility^",
      "nflint": "1",
      "kxdbio_ut_ctl": "4",
      "REMOTE_CELL_MGR_INSTANCES": "4",
      "mytest": "^tst_tscname^",
      "logfile1": "^mytest^.log"
    },
    "description": "tsagsec4.tsc - The goal of this test is to verify C2C offload and\n                    HTW w/ ASM-scoped security with a CDB.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsec5.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "my_tsagrddef",
      "sage_mirror_mode": "^sage_mirror_mode^",
      "oss_testing": "2",
      "compatible": "^max_compatibility^",
      "nflint": "1",
      "mytest": "^tst_tscname^",
      "logfile1": "^mytest^.log"
    },
    "description": "tsagsec5.tsc - ASM scoped security test - drop and assign key on cell2\n\nASM scoped security test - drop and assign key on cell2\n\nTest steps:\n       1. setup ASM scoped security for 2 cells\n       2. remove IP of Cell2 from cellip.ora\n       3. drop key on Cell2\n       4. shutdown Cell2\n       5. restart Cell2\n       6. assign key for Cell2\n       7. add IP of Cell2 in cellip.ora\n       8. check disks on Cell2 are online in ASM diskgroup",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsecasm.tsc",
    "setup": null,
    "flags": {
      "osskey": "12345",
      "asmid": "myasm"
    },
    "description": "tsagsecasm.tsc - Test ASM-scoped security\n\nTest ASM-scoped security with bad key, no key (open mode), and good key",
    "platform": null
  },
  {
    "test_name": "tsagsecdb.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagrh2def",
      "max_instance": "1",
      "osskey": "12345",
      "clusterid": "'+asm'",
      "realmid": "thisrealm",
      "db1key": "my_db1_key",
      "db2key": "my_db2_key",
      "file_dest": "^asmprefix^standby",
      "db_recovery_file_dest": "^asmprefix^standby",
      "backup_dest": "^asmprefix^flint",
      "run_in_tsagsec": "1   # tell tsagrmansql to create only a small number of rows",
      "tsagsec_negative_case": "1"
    },
    "description": "tsagsecdb.tsc - Test DB-scoped security\n\nTest DB-scoped security with 2 different DBs",
    "platform": null
  },
  {
    "test_name": "tsagsecdb2.tsc",
    "setup": null,
    "flags": {
      "creatdev_file": "tsagrh2def",
      "tmp_nowarn": "^tst_nowarn^",
      "max_instance": "2",
      "max_db_num": "6"
    },
    "description": "tsagsecdb2.tsc - Test DB-scoped security\n                    - extension structures for griddisk records to\n                    - store bigger entries\n\nTest DB-scoped security with 64 different DBs",
    "platform": null
  },
  {
    "test_name": "tsagsecini.tsc",
    "setup": "tsagnini",
    "flags": {
      "nflint": "0"
    },
    "description": "tsagsecini.tsc - initialization script for security tests\n\nInitialization script for security tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsecuritykey.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagsecuritykey.tsc - security certificate test for exacli\n\nThis test checks uploading a security certificate on exacli",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagselinux.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagselinux.tsc - SELinux audit log sniffer testing.\n\nTests that SELInux audit.log sniffer can generate alerts properly",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsendmail.tsc",
    "setup": null,
    "flags": {
      "foundsucfile": "false",
      "mail_help": "1",
      "num_arg": "1",
      "c_tst_debug": "^TST_DEBUG^",
      "mail_sleep": "^^argvalue^",
      "mail_summary": "1",
      "mail_from_email": "^mail_email^",
      "mail_email": "^^argvalue^",
      "mail_to_email": "^mail_email^",
      "mail_count": "1",
      "underscore": "'_'",
      "mail_wait": "1",
      "mail_check_dif": "1",
      "mail_ifarm": "1",
      "succ": "'.lsuc'",
      "diff": "'.ldif'",
      "dbgprfx": "'mailexa:::'",
      "argname": "arg^num_arg^",
      "argvalue": "^^argname^",
      "mail_testname": "^tst_tscname^",
      "exitfile": "^mail_testname^.exit",
      "reqfile": "^mail_testname^",
      "mail_sub": "'Sleeping in test '",
      "TXN_USER": "^USER",
      "loopCount": "1",
      "total_sleep": "0"
    },
    "description": "tsagsendmail.tsc - Test utility to run a given test in a loop.\n\nThe utility accepts few parameters, checks for dif(s) and sends email\n\nUsage: tsagsendmail [testname=<testname>]\n                        [sleep=<sleep> | wait | diffcheck]\n                        [ifarm]\n                        [[email=<email_id] |\n                         [[from_email=<email id>]\n                          [to_email=<email id>]]]\n\n     testname    -  The test for which the dif will be checked. If not provided\n                    will use ^tst_tscname^\n     diffcheck   -  Will wait or sleep only if there is a failure\n     sleep       -  wait for 'sleep' seconds before exiting\n     wait        -  Keep waiting till user enters and creates a file called\n                    ^tst_tscname^.exit\n     ifarm       -  will mail only for ifarm\n\n\n    EXTRA options if required\n    ----------------------------\n     email       -  Both from and to email will be set to this email address\n     from_email  -  To filter using the email id, if nothing is provided,\n                    default will be username@oracle.com\n     to_email    -  A comma separated list of email id to send the summary\n                    mail to. Pass this within single quotes\n\n   Example\n       When running as part of user farm job, comment it in case you\n       want to get notification after some step or just before a step is to\n       executed use\n\n          mailexa wait\n\n       When running as part of user farm job, comment it in case you\n       want to get notification about failure of the test and wait use the\n       following option\n\n          mailexa diffcheck wait\n\n       If you want the test to sleep for sometime after some steps and\n       recieve a notification\n\n          mailexa sleep=x\n\n            note: x in seconds",
    "platform": null
  },
  {
    "test_name": "tsagservmonitor.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true"
    },
    "description": "tsagservmonitor.tsc - service monitor test\n\nPlease see below:\n\nto be added in lrgrhx7saipcdatqos",
    "platform": null
  },
  {
    "test_name": "tsagservmonitor_db.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1",
      "compute_node_f": "^compute_node^.us.oracle.com",
      "dbnodeconnstr": "root@^compute_node_f^"
    },
    "description": "tsagservmonitor_db.tsc - systemd service monitor test for db\n\nPlease see below:\n\nto be added in lrgrh7sasrvmondb",
    "platform": null
  },
  {
    "test_name": "tsagservmonitor_livemig.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagservmonitor_livemig.tsc - Monitors services for LM Nodes\n\nCurrently monitors injected failures in the following 2 services\n          exadata-virtmon (on KVM host)\n          exadata-qmpmon (on KVM host and cell)\n        For each script, inject a failure in the service script and restart the service\n        Wait for sometime and then look for a service down alert\n        Then, remove the failure injection and restart service again. We should then see a clear alert.\n\nRuns in lrgrhexaprovcluster_livemig",
    "platform": null
  },
  {
    "test_name": "tsagsetmediatype.tsc",
    "setup": null,
    "flags": {
      "media_type": "CF",
      "sep": ",",
      "numtype": "4",
      "tmpmt": "media_type^i^",
      "xt_media": "true",
      "ef_media": "true",
      "hc_media": "true",
      "cf_media": "true",
      "CELL_WITH_QLC_DISK": "1",
      "QLC_DISK_SIZE_MB": "2560",
      "CF_SP": "OPC_CF_STORAGE",
      "CELL_WITH_XRMEM_ONLY": "1",
      "NUM_FLASH_PER_CELL": "12",
      "FLASH_SIZE": "2096M",
      "FLASH_GRIDDISK_SIZE": "1536",
      "only_flash": "true",
      "PMEM_SIZE": "256",
      "EF_SP": "OPC_EF_STORAGE",
      "HC_SP": "OPC_HC_STORAGE",
      "CELL_WITH_FLASH_CACHE": "1",
      "CELL_WITH_PMEM_CACHE": "0",
      "XT_SP": "OPC_XT_STORAGE"
    },
    "description": "tsagsetmediatype.tsc - Parses media_type passed to tsagexastackup\n\nHandles media_type passed to tsagexastackup/xrdbmsini etc\n     It is comma separated list of any combination out of 3 valid media types\n     media_type=EF,HC\n     media_type=all  << will create 3 storage pools >>",
    "platform": null
  },
  {
    "test_name": "tsagsetpci.tsc",
    "setup": "tsaginit",
    "flags": {
      "num_flash_per_cell": "16",
      "flash_griddisk_size": "512",
      "flash_size": "1024",
      "SAGE_MIRROR_MODE": "normal",
      "shared_pool_size": "300M"
    },
    "description": "tsagsetpci.tsc - TEST for Bug 13850114 - FLASH DISK RETURNING TO STATUS NORMAL NEEDS TO RECREATE GD/FC/FL\n\nThis test does IDT Riser reset simulation due to which half of the flash devices go offline.\n     When they come back online after a cell reboot, CD/GD/FC/FL should be automatically recreated",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsetsize.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsetsize.tsc - set size of cell components\n\nIt is called from tsagupgflog.tsc.\n     tsagupgflog.tsc passes the size of flashlog  ,flashcache , and two\n     griddisks and this script creates these components as per the size.",
    "platform": null
  },
  {
    "test_name": "tsagsetupadg.tsc",
    "setup": "srdbmsini",
    "flags": {
      "standby_ctl": "'+CONTROLFILE/dbs/'^dbs2_controlfile_1_name^",
      "file_dest_standby": "'+DATAFILE'",
      "log_dest_standby": "'+LOGFILE'",
      "file_name_convert": "(\"DATAFILE/dbs\",\"DATAFILE/phsty\",\"LOGFILE/dbs\",\"LOGFILE/phsty\")",
      "creatdev_file": "tsagmtcdef",
      "standby_db_opt": "db_create_file_dest=^file_dest_standby^",
      "file_dest": "'+DATAFILE'"
    },
    "description": "tsagsetupadg.tsc - Setup Exadata Active Dataguard\n\nSetup Exadata Active Dataguard, it reuse srdbmsini to setup cell serices and primary database\n     So all valid parameters of srdbmsini can be used for cell/GI/ASM/DB instance\n\nSetup Exadata Active Dataguard, currently we add a new parameter - oss_asm_adg to\n     support below three modes:\n       1) oss_asm_adg=adg_indifferent_cell : Primary/Standby DB are created in different diskgroups across cells\n                           this is default setting\n       2) oss_asm_adg=adg_insame_cell : Primary/Standby DB are created across diskgroups but in same cells\n       3) oss_asm_adg=adg_insame_asmdg : Primary/Standby DB are created in same diskgroups but different directories",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsetupspdb.tsc",
    "setup": "srdbmsini",
    "flags": {
      "datafile_redund": "normal",
      "SAGE_MIRROR_MODE": "^datafile_redund^",
      "oss_testing": "3",
      "scompatible.rdbms": "19.0.0.0",
      "compatible": "^scompatible.rdbms^",
      "dif_sparse": "tsagaddsparsegd.dif",
      "lognum": "a"
    },
    "description": "tsagsetupspdb.tsc - Setup File for Sparse DB For Corruption Tests\n\nSets up base DB and creates clone for corruption Tests on SParse",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagshort.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagshort.tsc - SAGE short regress test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagshutmn.tsc",
    "setup": null,
    "flags": {
      "md": "0",
      "otemp": "asm_instance^m^",
      "oinst": "^^otemp^",
      "num_nodes": "2"
    },
    "description": "tsagshutmn.tsc - Shutdown emmulated nodes\n\nShutdown clusterware, ASM and RDBMS instances on emmulated nodes\n     And shutdown Exascale stack for oss_exascale_testing",
    "platform": null
  },
  {
    "test_name": "tsagsichk.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagsichk.tsc - Quarantine test for SI check\n\nQuarantine test for SI check\n\nCorresponding to PredicateOflExec.cpp::execSICheckMsg(),PredicateDiskRead.cpp::siCheck()",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsidumpusage.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsidumpusage.tsc - new test for cellcli SI dump usage command",
    "platform": null
  },
  {
    "test_name": "tsagsievict.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagsievict.tsc - test SI eviction feature\n\nAfter simulate SI Eviction, storage index value should not increased",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsievict2.tsc",
    "setup": "srdbmsini",
    "flags": {
      "asm_ausize": "1048576"
    },
    "description": "tsagsievict2.tsc - test SI eviction feature\n\nSimulate SI Eviction and parameters in dump file should not be none",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsievict3.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagsievict3.tsc - test SI eviction feature\n\ntest SI eviction optimization",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsiminmaxdr.tsc",
    "setup": null,
    "flags": {
      "logmsg": "'[Test 40] Check SI while executing min, max query for col8 (FLOAT) with query low compression'",
      "logmsg2": "'Table is populated in tsagstrindxminmaxquery_low.log and SI is primed by a query'"
    },
    "description": "tsagsiminmaxdr.tsc - SI Min/Max test cases wrapper script",
    "platform": null
  },
  {
    "test_name": "tsagsiminmaxdr2.tsc",
    "setup": null,
    "flags": {
      "logmsg": "'[Test 48] Check SI while executing min, max query for col8 (FLOAT) with archive high compression'",
      "logmsg2": "'Table is populated in tsagstrindxminmaxarchive_high.log and SI is primed by a query'"
    },
    "description": "tsagsiminmaxdr2.tsc - SI Min/Max test cases wrapper script",
    "platform": null
  },
  {
    "test_name": "tsagsiminmaxdr_sage6.tsc",
    "setup": null,
    "flags": {
      "logmsg": "'[Test 48] Check SI while executing min, max query for col8 (FLOAT) with archive high compression'",
      "logmsg2": "'Table is populated in tsagstrindxminmaxarchive_high.log and SI is primed by a query'"
    },
    "description": "tsagsiminmaxdr_sage6.tsc - SI Min/Max test cases wrapper script for lrgsage6\n\nThis script runs a subset of SI MIN/MAX tests as a part of lrgsage6.",
    "platform": null
  },
  {
    "test_name": "tsagsiminmaxopt.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagsiminmaxopt.tsc -  SI minmax optimization test\n\nThis test script has SI minmax optimization test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsiobjidzero.tsc",
    "setup": "srdbmsini",
    "flags": {
      "log_name": "tsagsiobjidzero.log"
    },
    "description": "tsagsiobjidzero.tsc - testcase for SI objid zero\n\nSI should not be built when object id is zero.\n\nTest steps:\n       a. Setup Exadata\n       b. Initialize columnar cache package for exadata feature validation in oratst\n       c. Purge existing SI in cellcli - alter cell events = \"immediate cellsrv.cellsrv_storidx(purge, ALL, 0, 0, 0)â\n       d. Test write path\n          a. Turn on new simulation event in cellcli - alter cell offloadgroupEvents=\"cellsrv_simevent[SI_OBJD_ZERO] frequency=1, count=20\"\n          b. Create a table and load some data\n             create table t1 (col1 number, col2 date, col3 varchar2(2000), col4 nvarchar2(2000), col5 timestamp, col6 timestamp with time zone, col7 timestamp with local time zone, col8 float(20), col9 char(2000)) ;\n             declare\n             i NUMBER := 0 ;\n             j NUMBER := 0 ;\n             k timestamp:= systimestamp ;\n             l nvarchar2(30):= 'stage' ;\n             begin\n                 for i in 1..30200 loop\n                 insert into t1 values (i,sysdate+i,' storage',l || i,k + i,k + i,k + i,9999.99999+i, 'A') ;\n                 commit ;\n                 end loop ;\n             end ;\n             /\n             commit ;\n          c. Use checkCellStat to check cell stat - \"nm_idx_not_built_objd_is_zero\"\n          d. Run cellsrvstat to check for one cell stat - \"SI #idx not built objd zero\"\n       e. Purge existing SI in cellcli - alter cell events = \"immediate cellsrv.cellsrv_storidx(purge, ALL, 0, 0, 0)â\n       f. Test read path\n          a.Turn on new simulation event in cellcli - alter cell offloadgroupEvents=\"cellsrv_simevent[SI_OBJD_ZERO] frequency=1, count=40\"\n          b.Attempt to build SI by reading from existing table - select count(*) from t1 where col1 < 0;\n          c. Use checkCellStat to check cell stat - \"nm_idx_not_built_objd_is_zero\"\n          d. Run cellsrvstat to check for one cell stat - \"SI #idx not built objd zero\"\n       g. simevent turn off at the end of test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsipersfailure.tsc",
    "setup": null,
    "flags": {
      "table_name": "tsagrectab",
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'",
      "pre_checksum": "'_checksum'"
    },
    "description": "tsagsipersfailure.tsc - Failure tests for SI Persistence Project\n\nFailure tests for SI Persistence Project\n\n     When kill -9 cellsrv, we will not save/load imgs. So there will be no imgs.\n     Upon graceful shutdowns, we save and load imgs.\n     When imgs are created, SHMEM is destroyed. When imgs are loaded, imgs are destroyed.\n\nFailure tests for SI Persistence Project\n\n       run_srdbmsini: 1. setup exadata by running srdbmsini, 0. skip it when it is up\n       restart_type : we have below kinds of cellsrv/cellolfsrv restart\n         cellsrv_restart - cellcli restart for cellsrv\n         celloflsrv_restart - cellcli restart for celloflsrv\n         cellsrv_startup - cellcli shutdown/startup for cellsrv\n         celloflsrv_startup - cellcli shutdown/startup for celloflsrv\n         cellsrv_kill - use \"kill -9\" to kill cellsrv, RS will restart it\n         celloflsrv_kill - use \"kill -9\" to kill celloflsrv, RS will restart it\n         cellsrv_simulate - use use simulation event to crash cellsrv, RS will restart it\n         celloflsrv_simulate - use use simulation event to crash celloflsrv, RS will restart it\n         all - run all above testcases\n\n       mode : run this file with different mode\n          testcase  - run basic cases\n            sitab_creation: whehter recreate testing table\n\n          restart   - restart cellsrv/celloflsrv, then rerun SI workload\n            workload - user defined SI workload\n            workload_ref : the workload ref log\n\n          loopstart - Run testcase mode in a N loop\n            loop_num: the count of restart times\n\n           randomloop - Random loop tests for cellsrv/celloflsrv kill/restart\n\n       area : areas about SI persistent failure tests\n          si_store_crash : SI restore Recovery crash quarantine\n          si_rec_crash : SI recovery after oflsrv crash\n          si_checksum : SI checksumming\n          si_identity : SI store identity verification\n          si_loader_crash : osssistoreloader crash simulation\n          si_loader_save : osssistoreloader \"save\" function\n          si_loader_load : osssistoreloader \"load\" function\n          si_write : pending write consistency check\n          si_ridx_chain : RIDX chain\n          si_miss_state : missing/corrupted loader state file\n\n      if you want to test SI Persistence with a speical SI workload, and use event to crash celloflsrv\n         runtest  tsagsipersfailure mode=restart restart_type=celloflsrv_simulate \\\n              workload=tsagsichk workload_ref=tsagsiwkld\n\n      if you want to test SI Persistence in a loop\n         a). run all restart types in a loop\n           runtest  tsagsipersfailure mode=loopstart run_srdbmsini=0\n         b). run cellsrv_kill in a loop\n           runtest  tsagsipersfailure mode=loopstart run_srdbmsini=0 restart_type=cellsrv_kill\n\n      if you want to test SI Persistence\n         a). run all restart types testcases\n           runtest  tsagsipersfailure mode=testcase run_srdbmsini=0\n         b). run cellsrv_restart testcase\n           runtest  tsagsipersfailure mode=testcase run_srdbmsini=0 restart_type=cellsrv_restart",
    "platform": null
  },
  {
    "test_name": "tsagsipersini.tsc",
    "setup": null,
    "flags": {
      "restart_type": "all",
      "tablename": "tsagsipersist",
      "loop_num": "3"
    },
    "description": "tsagsipersini.tsc - Init script for SI Persistent Project\n\nInit script for SI Persistent Project\n\nInit script for SI Persistent Project",
    "platform": null
  },
  {
    "test_name": "tsagsipersist.tsc",
    "setup": null,
    "flags": {
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'",
      "restart_type": "all",
      "tablename": "tsagsipersist",
      "loop_num": "3",
      "log_prefix": "tsagsipersist",
      "encalgo": "AES256",
      "pfile_path": "^T_WORK^^s^worklv2^s^t_init1.ora"
    },
    "description": "tsagsipersist.tsc - Basic tests for SI Persistence Project\n\nBasic tests for SI Persistence Project\n\nBasic tests for SI Persistence Project\n\n       run_srdbmsini: 1. setup exadata by running srdbmsini, 0. skip it when it is up\n       restart_type : we have below kinds of cellsrv/cellolfsrv restart\n         cellsrv_restart - cellcli restart for cellsrv\n         celloflsrv_restart - cellcli restart for celloflsrv\n         cellsrv_startup - cellcli shutdown/startup for cellsrv\n         celloflsrv_startup - cellcli shutdown/startup for celloflsrv\n         cellsrv_kill - use \"kill -9\" to kill cellsrv, RS will restart it\n         celloflsrv_kill - use \"kill -9\" to kill celloflsrv, RS will restart it\n         cellsrv_simulate - use use simulation event to crash cellsrv, RS will restart it\n         celloflsrv_simulate - use use simulation event to crash celloflsrv, RS will restart it\n         all - run all above testcases\n\n       mode : run this file with different mode\n          testcase  - run basic cases\n            sitab_creation: whether recreate testing table\n\n          restart   - restart cellsrv/celloflsrv, then rerun SI workload\n            workload - user defined SI workload\n            workload_ref : the workload ref log\n\n          loopstart - Run testcase mode in a N loop\n            loop_num: the count of restart times\n\n      if you want to test SI Persistence with a speical SI workload, and use event to crash celloflsrv\n         runtest tsagsipersist mode=restart restart_type=celloflsrv_simulate \\\n              workload=tsagsichk workload_ref=tsagsiwkld\n\n      if you want to test SI Persistence in a loop\n         a). run all restart types in a loop\n           runtest tsagsipersist mode=loopstart run_srdbmsini=0\n         b). run cellsrv_kill in a loop\n           runtest tsagsipersist mode=loopstart run_srdbmsini=0 restart_type=cellsrv_kill\n\n      if you want to test SI Persistence\n         a). run all restart types testcases\n           runtest tsagsipersist mode=testcase run_srdbmsini=0\n         b). run cellsrv_restart testcase\n           runtest tsagsipersist mode=testcase run_srdbmsini=0 restart_type=cellsrv_restart",
    "platform": null
  },
  {
    "test_name": "tsagsipurge.tsc",
    "setup": "srdbmsini",
    "flags": {
      "sys_group_name_11233": "'TEST_GROUP'",
      "log_file": "tsagsipurge.log"
    },
    "description": "tsagsipurge.tsc - Basic tests for SI purge\n\nInclude below basic tests for SI purge:\n     (1) single purge with wrong name\n     (2) single group purge for an offload group with correct name\n     (3) single purge for \"ALL\"\n     (4) purge and queries running together:\n\nWe would like to merge the SI purge transaction along with some tests.\n     In all cases, we should see the message \"StorageIdx action params: PURGE group_name 0 0 0\" in the alert log.\n\n     We could add the following test cases for an environment where we have\n     finished some queries and have non-zero number of pages (but without active queries running):\n     (1) single purge with wrong name\n       e.g. alter cell events = \"immediate cellsrv.cellsrv_storidx(purge, wrong_group_name, 0, 0, 0)\"\n       We should observe the message in the trace file:\n         \"OCL SI PURGE: invalid offload group name wrong_group_name. Error:10 (Offload group name not found)\"\n\n     (2) single group purge for an offload group with correct name\n       e.g. alter cell events = \"immediate cellsrv.cellsrv_storidx(purge, SYS_121210_140217, 0, 0, 0)\"\n            alter cell offloadgroupEvents = \"immediate cellsrv.cellsrv_storidx(dumpridx, ALL, 0, 0, 0)\"\n       We should observe 0 num_pages in the dump file and message:\n          \"OCL SI GROUP PURGE: purged * (total *) pages for group * number of iterations *\"\n       The first two numbers should be equal.\n\n     (3) single purge for \"ALL\"\n       The general case of Test (2) in a 3-view setup. The messages should be observed in trace files of all offload groups.\n\n     We could also add the following concurrency tests:\n     (4) purge and queries running together:\n       continuous purge and heavy-load queries running side-by-side\n\n     Here, to be a bit more specific, please do the followings:\n     1) create a table with 100MB\n     2) make 10 copies of the table\n     3) have one query run against each table with each query having at least a predicate, in addition to having continuous purge.\n     4) Enable SI diag mode and run scans on tables\n       - alter session set \"_kcfis_storageidx_diag_mode\" =1\n       - scan the table again (if problems happen, celloflsrv will assert)\n\n     1) create a table with 100MB\n     2) make 10 copies of the table\n     3) have DML modifying each the table (e.g. update t1 set col1 = col+1),\n     in addition to having continuous purge.\n     4) Enable SI diag mode and run scans on tables\n       - alter session set \"_kcfis_storageidx_diag_mode\" =1\n       - scan the table again (if problems happen, celloflsrv will assert)\n\n     (5) purge and recovery running together\n       continuous purge and \"kill celloflsrv\" running side-by-side\n     Please do this on top of 4 (i.e. kill celloflsrv peridically while doing 4).\n\n     (6) purge and griddisk drop running side-by-side\n       continuous purge and \"drop griddisk\" running side-by-side",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsireb.tsc",
    "setup": null,
    "flags": {
      "log_name": "tsagsireb.log"
    },
    "description": "tsagsireb.tsc - actual SI rebalance test steps, need two cells\n\n1. remove some datafile disk under cell 1\n     2. create table, insert data\n     3. populate SI, check SI saving, should increase by SI in cell 1 or 2\n     4. purge SI in cell 1\n     5. add back datafile disk to cell 1, wait asm rebalance finish\n     6. purge SI in cell 2\n     7. check SI saving, should increase by SI in cell 1",
    "platform": null
  },
  {
    "test_name": "tsagsirebini.tsc",
    "setup": null,
    "flags": {
      "db_con": "'sys/knl_test7 as sysdba'",
      "hidestr": "'##'"
    },
    "description": "tsagsirebini.tsc - init script for SI rebalance test",
    "platform": null
  },
  {
    "test_name": "tsagsirec.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagsirec.tsc - SI recovery test\n\nThis test will exercise the SI recovery code: RIDXes become free when oflsrv dies and new incarnation\n      of celloflsrv can use them again since free() put freed pages in the beginning of the free list.\n\n    The below is the details in this test:\n      1) Create the testing table\n      2) wait for 5 minutes for SI summary\n      3) issue the following query twice.\n        sql> select count(*) from tb_name where col1 < 0;\n      4) In the same session, please run\n         select name,value from v$sesstat a, v$statname b where\n            (a.STATISTIC# = b.STATISTIC#) and\n             (a.sid) = userenv('sid') and\n             (name in ( 'cell physical IO bytes saved by storage index')) order by name;\n         We should see non-zero value with: 'cell physical IO bytes saved by storage index'\n      5) issue a query in a loop for 10 minutes such as :\n         sql> select count(*) from tb_name where col1 < 0;\n         Meanwhile, bounce celloflsrv.\n      6) In the same session, run the sql in step4, should see the value of\n         \"cell physical IO bytes saved by storage index\" that is greater than the one that see in step4.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsirgnhdlre.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagsirgnhdlre.tsc - Storage Index region handle id reuse test\n\nCreate and drop griddisks for 5000 iterations. Each iteration\n     creates a griddisk for each celldisk, so 12 griddisks total.\n     We create and drop as many as 60000 griddisks in this test.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsiscm.tsc",
    "setup": "srdbmsini",
    "flags": {
      "log_file": "tsagsiscm.log"
    },
    "description": "tsagsiscm.tsc - Run SI short column summary on Sparc DB\n\nAdd new test to cover optimization for byte swapping for SPARC DB endianess\n     using Storage Index short column summary.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsiupdate.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagsiupdate.tsc - storage index test\n\nPlease see below\n\ntest added to lrgdbconsascbug10",
    "platform": null
  },
  {
    "test_name": "tsagskip_dropsp.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagskip_dropsp.tsc - skip drop SP during cleanup\n\nskip drop SP during cleanup if lrg is in skip_dropsp_cleanup.dat",
    "platform": null
  },
  {
    "test_name": "tsagslowspaction.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagslowspaction.tsc - Take action for slow StoragePool creation\n\nIf SP creation was really slow (exceeds 23 mins), this script will\n      send out a mail to pbahl to take further actions",
    "platform": null
  },
  {
    "test_name": "tsagsmartio.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsmartio.tsc -  adding a support for XT config to disable/enable smart IO on the fly on cell side\n\nThe test cannot run alone. Should run it after tsaginitcc2.tsc",
    "platform": null
  },
  {
    "test_name": "tsagsmartrekey.tsc",
    "setup": "srdbmsini",
    "flags": {
      "creatdev_file": "tsagldsdef",
      "cdb": "true",
      "file_dest": "'+DATAFILE'"
    },
    "description": "tsagsmartrekey.tsc - ONLINE REKEY OPERATION WITH SMART SCAN QUERY\n\nPls see below.\n\nto be added in lrgdbconsascbug23",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsmartscanpartnerread.tsc",
    "setup": null,
    "flags": {
      "bkc_mode": "'BLOCK CACHE'",
      "cc1_mode": "'CC1'",
      "cc2_mode": "'CC2'",
      "bkc_compression": "'disabled'",
      "cc1_compression": "'enabled,pop_cc1'",
      "cc2_compression": "'enabled'",
      "ossmetricid_blk": "'241'",
      "ossmetricid_cc1": "'837'",
      "ossmetricid_cc2": "'835'",
      "SYS_GROUP_NAME": "^sysoflgrp^",
      "sysdba": "'sys/knl_test7@inst1 as sysdba'"
    },
    "description": "tsagsmartscanpartnerread.tsc - Test for proxy read feature\n\nThis test verifies ASM Proxy Read for better Smart Scan Performance.\n\nThe steps followed in test are:\n     1. Connect to cellsrv1, and shut it down\n     2. Execute full table scan repeatedly, the IOs will go to cell2 at this step and start building CC on cell2.\n     3. Bring up cellsrv1, and wait for sync to be completed and wait for 180 seconds.\n     4. Execute full table scan again, this time, we should see \"num_bytes_ptnr_hit\" to be non-zero.",
    "platform": null
  },
  {
    "test_name": "tsagsmrebal1.tsc",
    "setup": "srdbmsini",
    "flags": {
      "OSS_AUTO_MANAGE_DISKS": "true",
      "oss_testing": "3",
      "SAGE_MIRROR_MODE": "high",
      "creatdev_file": "tsagsmartrebal_creatdev_file"
    },
    "description": "tsagsmrebal1.tsc - Functional test for smart rebalance\n\nFailure path test cases. That is , disk is simulated to be failed\n     in various conditions and is checked if rebalance will be triggered or not.\n\nSmart Rebalance - intelligently infers whether the rebalance will succeed and\n                       triggers the rebalance operation.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsmrebal2.tsc",
    "setup": "srdbmsini",
    "flags": {
      "SAGE_MIRROR_MODE": "high",
      "oss_testing": "3",
      "nflint": "1",
      "differentszfg": "1",
      "dbconn": "'sys/knl_test7 as sysdba'",
      "asmconn": "'sys/knl_test7@inst11 as sysasm'"
    },
    "description": "tsagsmrebal2.tsc - Functional test for smart rebalance\n\nIf the diskgroup does not  have enough free space\n     ie.  ( FREE_MB < 15% of TOTAL_MB ) and a disk is\n     dropped for replacement , the disk will not be dropped\n     at asm side even after disk_repair_time.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsmrebal3.tsc",
    "setup": "srdbmsini",
    "flags": {
      "SAGE_MIRROR_MODE": "high",
      "oss_testing": "3",
      "nflint": "1",
      "differentszfg": "1",
      "creatdev_file": "tsagimdef",
      "dbconn": "'sys/knl_test7@inst1 as sysdba'",
      "asmconn": "'sys/knl_test7@inst11 as sysasm'"
    },
    "description": "tsagsmrebal3.tsc - Loop test that fills upa and frees up DG space\n\nThe test for bugs :\n        35271102 -  STARTUP OF DATABASE FAILS IN CTWR BACKGROUND PROCESS, ORA-00600: MEMORY LEAK\n        35271083 -  ALERT \"INSUFFICIENT FREE SPACE TO REBALANCE THE ASM DISK GROUP: RECOC1\" NOT CLEARED ON ANY NODE\n\n     Test steps:\n        Loop 5 times :\n          1) Fill up the DG space\n          2) Restart DB + ASM\n          3) Trigger smart rebalance by drop disk for replacement from the DG\n          4) Check asm disk status + warning alerts for insufficient free space\n          5) Reenables the disk back in DG\n          6) Frees up space in DG\n          7) Look for clear alert",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsnap_ehcc.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagsnap_ehcc.tsc - EHCC test on multi level snapshots\n\nCreates an EHCC table on Encrypted/Non encrypted Tablespace\n     multilevel Exascale snapshots",
    "platform": null
  },
  {
    "test_name": "tsagsnap_si.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagsnap_si.tsc - Storage Index test on multi level snapshots\n\nTest to verify that storage index works on multilevel\n     Exascale snapshots",
    "platform": null
  },
  {
    "test_name": "tsagsnapchainedrow.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagsnapchainedrow.tsc - Chained row test on snapshots\n\nTest to verify that chained row work on multilevel\n     Exascale snapshots",
    "platform": null
  },
  {
    "test_name": "tsagsnapdelparal.tsc",
    "setup": "xblockini",
    "flags": {
      "python_path": "^ADE_VIEW_ROOT^/python/bin",
      "ledvniscsi": "1",
      "edvcleanup_only": "1"
    },
    "description": "tsagsnapdelparal.tsc - parallel delete of snapshots\n\nTests parallel delete of snapshots",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagsnapdelparaltest.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagsnapdelparaltest.tsc - Test script for checking parallel deletion of snapshots\n\n1. Test multiple deletion of snapshot in parallel while creating multiple snapshots\n\n      a. create 10 snapshots from a volume\n      b. Delete 5 of them asynchronously\n      c. Delete 5 of them synchronously which should fail (negative test case)\n      d. Create 5 more snapshot from same volume\n      e. Delete all the 10 remaining snapshot asynchronously.\n      f. Clean up and Delete volume at the end\n\n     2. Multiple snap is being created while multiple snap is being deleted\n\n      a. Create 5 snapshots from a volume\n      b. set delay simulation for snap creation and snap deletion\n      c. Delete all snapshots asynchronously\n      d. Create a snapshot which should fail (negative test case)\n      c. Restart bsw\n      d. Clean up and Delete volume at the end",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagsnapdroptest.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagsnapdroptest.tsc - datafiles drop test on drop snapshot\n\nTests that datafiles are dropped when snapshot is dropped\n       including datafiles",
    "platform": null
  },
  {
    "test_name": "tsagsnapfeaturetest.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "cdb": "true",
      "auto_local_undo": "true",
      "vault_db": "DATA",
      "PWFILE_ON_EXC": "false"
    },
    "description": "tsagsnapfeaturetest.tsc - exascale features test on snapshot\n\nLrg to test multiple Exascale features on multi level snapshot\n\nFeatures tested :",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagsnapfeaturetest_2.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "cdb": "true",
      "auto_local_undo": "true",
      "vault_db": "DATA",
      "PWFILE_ON_EXC": "false"
    },
    "description": "tsagsnapfeaturetest_2.tsc - exascale features test on snapshot\n\nLrg to test multiple Exascale features on multi level snapshot\n\nFeatures tested :\n       - smart scan\n       - range scan + index range scan + scan with smart scan disabled\n       - rman delete-recover datafile",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagsnapfeaturetest_3.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "cdb": "true",
      "auto_local_undo": "true",
      "vault_db": "DATA",
      "PWFILE_ON_EXC": "false",
      "dsk_size": "xl"
    },
    "description": "tsagsnapfeaturetest_3.tsc - exascale features test on snapshot\n\nLrg to test multiple Exascale features on multi level snapshot\n\nFeatures tested :\n       - RMAN tables, tablespace recover\n       - RMAN backup in multipel ways and recover\n       - OLTP workload on exascale snapshots",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagsnapfeaturetest_functions.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsnapfeaturetest_functions.tsc -  Helper functions for tsagsnapfeaturetest\n\nScript to create helper functions for tsagsnapfeaturetest.\n\nThis script will be common script to create functions for snapshot\n     operations in tsagsnapfeaturetest* series of tests",
    "platform": null
  },
  {
    "test_name": "tsagsnapoltp.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "Test to check OLTP workload on a exascale snapshots",
    "platform": null
  },
  {
    "test_name": "tsagsnaprange_scan.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagsnaprange_scan.tsc - scan tests for exascale snapshots\n\ntest for range scan + index range scan (vector IO)\n     + scan with smart scan disabled - (batch IO) on multilevel\n     exascale snapshots",
    "platform": null
  },
  {
    "test_name": "tsagsnaprman.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagsnaprman.tsc - RMAN tests for exascale snapshots\n\nThe test checks  backup and restore command on 4 (sparse, densily sparse,sparse\n    child, chunk) tables on multilevel exascale snapshots.\n    tab_sparse    - nearly entirely SPARSE (save perhaps the first block or two)\n    tab_s_and_p_d - \"Salt AND Pepper Dense\"  - every  10th block updated in snap.\n    tab_s_and_p_s - \"Salt AND Pepper Sparse\" - every 100th block updated in snap.\n    tab_chunk     - one contiguous CHUNK of updated data representing a %10 section.\n    For each of these four types of data we are going to test\n\n     backup as sparse    copy       ....\n     backup as nonsparse copy       ....\n     backup as sparse    backupset  ....\n     backup as nonsparse backupset  ....\n\n      finally we will also test the restore\n     portion of the puzzle with commands resembling\n\n     restore (datafile ) from datafilecopy;\n     restore (datafile ) from backupset;\n\n      The outline goes aomething like below :\n\n      1- create the snapshot test environment.\n      2- populate the tables with 25000 rows, one 8K block each for approximately\n         195M of test data\n      3- build some indexes and clean out the base table blocks.\n      4- create multilevel snapshots\n      5 - testing a backup\n        The cross product of all these tests looks like\n\n        [backup to  , restore from ]                            X\n        [sparse     , nonsparse    ]                            X\n        [copy       , backupset    ]                            X\n        [tab_sparse , tab_s_and_p_d, tab_s_and_p_s, tab_chunk ]\n\n       6- delete backed up file\n       7- testing a restore.\n            restore the deleted file and run query to verify everything is correct",
    "platform": null
  },
  {
    "test_name": "tsagsnapscan.tsc",
    "setup": null,
    "flags": {
      "cdb": "true"
    },
    "description": "tsagsnapscan.tsc - Smart scan test for snapshots\n\nSmart scan test for multilevel exascale snapshots",
    "platform": null
  },
  {
    "test_name": "tsagsnapshot.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsnapshot.tsc - Test for sparsedb\n\nIn a loop, create snap shots and see how many can we create",
    "platform": null
  },
  {
    "test_name": "tsagsocket.tsc",
    "setup": "tsagnini",
    "flags": null,
    "description": "tsagsocket.tsc - set port limits for cellsrv connection requests\n\nartificially set port limits for cellsrv accepting new connection",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsocketmap.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsocketmap.tsc - Test to compute FlashDisk socket mapping based on its PCI address/NUMA information.\n\ncompute FlashDisk socket mapping based on its PCI address/NUMA information",
    "platform": null
  },
  {
    "test_name": "tsagsourcekey.tsc",
    "setup": "srdbmsini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagsourcekey.tsc - Check no leak\n\nAfter simulation the arbiter should hang",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsparse1.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "redund": "external"
    },
    "description": "tsagsparse1.tsc - Tests SPARSE DB, Smart Scan Tests",
    "platform": null
  },
  {
    "test_name": "tsagsparse1_loop.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "redund": "external"
    },
    "description": "tsagsparse1_loop.tsc - test for sparsedb\n\nIn a loop create and drop including datafiles a sparse DB.",
    "platform": null
  },
  {
    "test_name": "tsagsparse_arbiter.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagsparse_arbiter.tsc - Arbiter test for Sparse disks\n\ntests unit tests on SPARSE didks using Arbiter",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsparse_chainrow.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparse4.tsc - Chained ROws on SPARSE DB\n\nCreates a WIDE table with 260 columns on SPARSE DB",
    "platform": null
  },
  {
    "test_name": "tsagsparse_ehcc.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparse3.tsc - EHCC table on SPARSE DB\n\nCreates an EHCC table on Encrypted/Non encrypted Tablespace on SPARSE DB",
    "platform": null
  },
  {
    "test_name": "tsagsparse_ehcc_chain.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparse_ehcc_chain.tsc - Create EHCC table and a table with Chained row and clone the DB on Exadata\n\nThis test creates a Base PDB. Creates EHCC table, and a table with chained rows\n      It then create a snapshot of this PDB and manipulates EHCC table and Wide table data.\n\ntest",
    "platform": null
  },
  {
    "test_name": "tsagsparse_ioerrors.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_auto_manage_disks": "true",
      "creatdev_file": "tsagrddef",
      "uniq_dsknames": "all",
      "max_areas": "5  # max number of areas on each device",
      "gd": "'sparsefile0,sparsefile1'    # sparse griddisks",
      "duration": "7200  # run IOV for 2 hours",
      "blk_szlst": "'64K'",
      "io_szlst": "'512,1K, 1.5K, 2K, 2.5K, 3K, 3.5K, 4K, 8K, 16K,32K,64K,128K,256K, 1M, 4M'       # to set the num of blocks",
      "threads": "10",
      "sparse": "true",
      "align": "y",
      "offset": "4  # start offset to be 4M aligned",
      "area_sz": "80",
      "use_sparse_free_ioctl": "true"
    },
    "description": "tsagsparse_ioerrors.tsc - Sparsedb test\n\nTest to check IO hang errors while iov running in the background",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsparse_m_cdb.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "redund": "external",
      "creatdev_file": "tsagr2def",
      "compatible": "^scompatible.rdbms^"
    },
    "description": "tsagsparse_m_cdb.tsc - Sparsedb test\n\nmove a SPARSE DB from 1 CDB to another CDB and make sure that we can\n       still perform scan queries.",
    "platform": null
  },
  {
    "test_name": "tsagsparse_mediarec.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "SAGE_MIRROR_MODE": "normal",
      "sparsedg_redund": "normal"
    },
    "description": "tsagsparse_mediarec.tsc - Test for sparsedb ( lrgc26dbcsasparse25)\n\nTest for media recovery . Steps are as follows:\n     - create base table in parent with one row per block\n     - range scan.... to clean indexs\n     - now close the parent\n     - create the kid now , and modify every 10th block\n     - corrupt block instantiated in the kid and do read stuff with smart scan off\n     - inject error on the 37th block in mirror #1 on parent.  and do the same read  query",
    "platform": null
  },
  {
    "test_name": "tsagsparse_realhwd.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1",
      "oss_hw_testing": "1"
    },
    "description": "tsagsparse_realhwd.tsc - Sparsedb - real hardware test\n\nTest to check megacli pd offline/online while iov running in background",
    "platform": null
  },
  {
    "test_name": "tsagsparse_si.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "redund": "external"
    },
    "description": "tsagsparse2.tsc - Storage Index test on SPARSE DB\n\nTests the Storage Index feature works on SPARSE DB\n\ntest",
    "platform": null
  },
  {
    "test_name": "tsagsparse_si_cross_cdb.tsc",
    "setup": "srdbmsini",
    "flags": {
      "logon": "'sys/knl_test7 as sysdba'"
    },
    "description": "tsagsparse_si_cross_cdb.tsc - SI test on SPARSE DB across CDBs\n\nVerify the Storage Index entries for the parent are shared among the\n     snapshots within the same CDB and across different CDBs.\n\n     Test overview:\n      1) Create 2 CDBs\n      2) Add sparse disks and disk-group\n      3) Bring the CDBs up with  _kcfis_cell_passthru_enabled=true.\n         This is to prevent SI from being populated by other DB activities.\n      4) Create a table in the CDB1_PDB1, which will be the parent for\n         snapshots\n      5) Create 4 snapshots off the CDB1_PDB1\n      6) Unplug TEST_SNAP3 and TEST_SNAP4 from CDB1 and plug it into CDB2\n      7) Modify half of the table in TEST_SNAP2 and TEST_SNAP4, so that they\n         get some RIDXes of their own.\n      8) Run the scan on all snaphosts and verify the results.\n         TEST_SNAP1 and TEST_SNAP3 should should have zero valid RIDXes (all\n         should be coming from the parent). TEST_SNAP2 and TEST_SNAP4 should\n         have non-zero number of valid RIDXes.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsparse_sp.tsc",
    "setup": null,
    "flags": {
      "conn1": "'sys/knl_test7 as sysdba'",
      "conn2": "'sys/knl_test7@cdb1_pdb1 as sysdba'",
      "exa_cdb": "true",
      "compatible": "^scompatible.rdbms^"
    },
    "description": "tsagsparse_sp.tsc - Helper script to tsagsparse3.tsc and tsagsparse4.tsc\n\nChecks the existence of a SParse DB and connect to it or creates a new\n      one if it doesn't exist",
    "platform": null
  },
  {
    "test_name": "tsagsparse_stats.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparse_stats.tsc - Test SPARSE db, stats and wait events\n\nThis tests the stats (sysstat) and wait events for sparse db",
    "platform": null
  },
  {
    "test_name": "tsagsparseall_sparse.tsc",
    "setup": "srdbmsini",
    "flags": {
      "exa_cdb": "true",
      "compatible": "^scompatible.asm^"
    },
    "description": "tsagsparseall_sparse.tsc - sparsedb test\n\nTest to create different types of files ( voting,password,spfile,controlfile etc..)\n    on SPARSEDG",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsparseasmcmdcp.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "redund": "external",
      "max_instance": "2",
      "maxinstances": "^max_instance^     # number of asm instances"
    },
    "description": "tsagsparseasmcmdcp.tsc - Test for asmcmd cp --sparse and setsparseparent",
    "platform": null
  },
  {
    "test_name": "tsagsparseasmcmdinout.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparseasmcmdinout.tsc - Test for sparsedb\n\nThe test checks the functionality of inplace and out of place asmcmd copy commands",
    "platform": null
  },
  {
    "test_name": "tsagsparseautoflash.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "redund": "external"
    },
    "description": "tsagsparseautoflash.tsc - Sparsedb test\n\nThis test checks the functionality of autoflashcache",
    "platform": null
  },
  {
    "test_name": "tsagsparseautoflash2.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "redund": "external",
      "tst_tscname": "tsagsparseautoflash"
    },
    "description": "tsagsparseautoflash2.tsc - Sparsedb test\n\nThis test checks the functionality of autoflashcache",
    "platform": null
  },
  {
    "test_name": "tsagsparsebase.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "redund": "external"
    },
    "description": "tsagsparsebase.tsc - Test for sparsedb\n\n- Try to open sparse DB when the base files are not mounted",
    "platform": null
  },
  {
    "test_name": "tsagsparsecellsrvstat.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparsecellsrvstat.tsc - Sparsedb test\n\nTest to check new sparse cellsrvstat",
    "platform": null
  },
  {
    "test_name": "tsagsparsedb.tsc",
    "setup": "tsaginit",
    "flags": {
      "compatible": "^scompatible.rdbms^",
      "sparse_disk_per_cell": "5",
      "lognum": "a"
    },
    "description": "tsagsparsedb.tsc - Creates a DB on Exadata and setup for SPARSE DB",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsparsedg_resize.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparsedg_resize.tsc - Sparsedb test\n\nThis test checks the following functionalities-\n      - Try to rename files in the snapshot, and make sure we can rename file names.\n      - Resize Sparse diskgroup and check virtual size.",
    "platform": null
  },
  {
    "test_name": "tsagsparsedropgd.tsc",
    "setup": null,
    "flags": {
      "differentszfg": "1",
      "exa_cdb": "true",
      "SAGE_MIRROR_MODE": "normal",
      "sparsedg_redund": "normal"
    },
    "description": "tsagsparsedropgd.tsc - sparsedb test\n\ndrop a disk and let rebalance to finish with\n\n   - no IO\n   - a workload of reads\n   - a workload of writes\n   - a loop of create/delete a large file\n   - a loop of create/delete a hundred small files.",
    "platform": null
  },
  {
    "test_name": "tsagsparsegdcheck.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "SAGE_MIRROR_MODE": "normal",
      "sparsedg_redund": "normal"
    },
    "description": "tsagsparsegdcheck.tsc - sparsedb test\n\noffline/online of a disk with (resync test)\n\n   no IO\n   a workload of reads\n   a workload of writes\n   a loop of create/delete a large file\n   a loop of create/delete a hundred small files.",
    "platform": null
  },
  {
    "test_name": "tsagsparsegdreplace.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagddef"
    },
    "description": "tsagsparsegdreplace.tsc - Sparsedb test\n\nDisk replacement test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsparseiov.tsc",
    "setup": "tsaginit",
    "flags": {
      "max_areas": "5  # max number of areas on each device",
      "gd": "'sparsefile0,sparsefile1'    # sparse griddisks",
      "duration": "9000  # run IOV for 1.5 hours",
      "blk_szlst": "'64K'",
      "io_szlst": "'512,1K, 1.5K, 2K, 2.5K, 3K, 3.5K, 4K, 8K, 16K,32K,64K,128K,256K, 1M, 4M'       # to set the num of blocks",
      "threads": "10",
      "sparse": "true",
      "align": "y",
      "offset": "4  # start offset to be 4M aligned",
      "area_sz": "4",
      "creatdev_file": "tsagddef"
    },
    "description": "tsagsparseiov.tsc - sparsedb test\n\ncheck iov function on sparsedb",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsparsemap.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparsemap.tsc - Test to check functionality of table x$kxdspphysmp which stores the mapping\n                      between logical extents and their physically allocated rgns in sparse griddisk.\n\n- The test is basically to find a logical extent and its materialized rgn , find file index, disk name , file name , put all the information in a temp file , confirm that it is\n     indeed materialized in oradebug. Now delete the file , the materialized rgn should be gone , confirm that it is no longer a materialized rgn in oradebug.",
    "platform": null
  },
  {
    "test_name": "tsagsparsemap2.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "redund": "external"
    },
    "description": "tsagsparsemap2.tsc - Sparsedb Test\n\nTest to check functionality of table x$kxdspphysmp which stores the mapping\n     between logical extents and their physically allocated rgns in sparse griddisk.",
    "platform": null
  },
  {
    "test_name": "tsagsparsemedia.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparsemedia.tsc - fast/instant create of normal files on sparse media.\n\n1) Create one of  new files on sparsedg and confirm that it does not use\n     full space on sparse diskgroup (since it is not materialized yet)\n  2) Confirm these files seem to work by create tables in them...\n     inserting rows and query them back out.\n  3) Confirm dbverify (dbv) works on this new file.\n  4) Take Rman sparse and non sparse backups\n  5) insert few more rows , moving the file forward in time.\n  6) delete the newly created file.\n  7) Restore an RMAN backup of one of these new files... recover it...\n     confirm the recently inserted rows, i.e. the rows inserted since\n     the backup, are still there.",
    "platform": null
  },
  {
    "test_name": "tsagsparsemetric.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparsemetric.tsc - sparsedb test\n\nTest to check sparsedb metrics and also GD creation from virtualsize",
    "platform": null
  },
  {
    "test_name": "tsagsparsemetric2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparsemetric2.tsc - Sparsedb Test\n\nTest to check sparsedb metrics",
    "platform": null
  },
  {
    "test_name": "tsagsparsemetric3.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparsemetric3.tsc - Sparsedb test\n\nTest to check out of space alert and then 0.0% disk utilization",
    "platform": null
  },
  {
    "test_name": "tsagsparseoltp.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagsparseoltp.tsc - sparsedb test\n\nTest to check OLTP workload on a sparse pdb.",
    "platform": null
  },
  {
    "test_name": "tsagsparseorion.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_auto_manage_disks": "true",
      "creatdev_file": "tsagr2def",
      "uniq_dsknames": "all",
      "max_areas": "5  # max number of areas on each device",
      "gd": "'sparsefile0,sparsefile1'    # sparse griddisks",
      "duration": "1200  # run IOV for 0.5 hours",
      "blk_szlst": "'64K'",
      "io_szlst": "'512,1K, 1.5K, 2K, 2.5K, 3K, 3.5K, 4K, 8K, 16K, 32K,64K,128K,256K, 1M, 4M'     # to set the num of blocks",
      "threads": "10",
      "sparse": "true",
      "align": "y",
      "offset": "4  # start offset to be 4M aligned",
      "area_sz": "4"
    },
    "description": "tsagsparseorion.tsc - Sparsedb test - orion\n\nCheck orion functionality with sparse griddisk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsparseparent.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparseparent.tsc - Sparsedb test.\n\nTest for renaming of a child's parent file.\n\n- Create normal sparse environment with test_snap1, test_snap2, etc..\n   - locate/remember all the parent file names in cdb_pdb1.\n   - locate/remember all the child file names in test_snap2.\n   - close test_snap1, test_snap2, and cdb_pdb1 (if opened)\n   - ASMCMD cp all the files associated with cdb_pdb1 to a new directory (say) \"+DATAFILE/PARENT_COPY/\".\n   - ASMCMD rm all the orig parent files from wherever they were.\n   - now the part we want to test - RENAME all the kid's parents to their respective new name in +DATAFILE/PARENT_COPY\n      via oradebug command , also rename's parent file location in controlfile\n   - open test_snap1, and execute some queries on tables in test_snap1.  Be sure he can find new parent,\n     opens parent db in read only\n   - Also, confirm parent is in new location with a query from x$ksfdss_cloneinfo.",
    "platform": null
  },
  {
    "test_name": "tsagsparseperm.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparseperm.tsc - Test for sparsedb\n\nTry to open Sparsedb after removing read permission on DATAFILE diskgroup  #     using ACL",
    "platform": null
  },
  {
    "test_name": "tsagsparseplug_loop.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparseplug_loop.tsc - test for sparsedb\n\n- In a loop unplug and plug the sparsedb",
    "platform": null
  },
  {
    "test_name": "tsagsparserange_scan.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparserange_scan.tsc - sparsedb test\n\ntest for range scan + index range scan (vector IO) + scan with smart scan disabled - (batch IO)",
    "platform": null
  },
  {
    "test_name": "tsagsparserename.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparserename.tsc -  test for sparsedb\n\nTry to open sparse DB when the base files are not\n      available for read",
    "platform": null
  },
  {
    "test_name": "tsagsparsereplacegd.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "SAGE_MIRROR_MODE": "normal",
      "si_snap": "true",
      "sparsedg_redund": "normal"
    },
    "description": "tsagsparsereplacegd.tsc - sparsedb test\n\nAdd and  Replace a disk and let rebalance to finish with\n\n  - no IO\n  - a workload of reads\n  - a workload of writes\n  - a loop of create/delete a large file\n  - a loop of create/delete a hundred small files.",
    "platform": null
  },
  {
    "test_name": "tsagsparserman.tsc",
    "setup": null,
    "flags": {
      "exa_cdb": "true",
      "creatdev_file": "tsagr2def",
      "redund": "external"
    },
    "description": "tsagsparserman.tsc - Test for sparsedb - rman test\n\nThe test checks  backup and restore command on 4 (sparse, densily sparse,sparse\n    child, chunk) tables .\n    tab_sparse    - nearly entirely SPARSE (save perhaps the first block or two)\n    tab_s_and_p_d - \"Salt AND Pepper Dense\"  - every  10th block updated in snap.\n    tab_s_and_p_s - \"Salt AND Pepper Sparse\" - every 100th block updated in snap.\n    tab_chunk     - one contiguous CHUNK of updated data representing a %10 section.\n    For each of these four types of data we are going to test\n\n     backup as sparse    copy       ....\n     backup as nonsparse copy       ....\n     backup as sparse    backupset  ....\n     backup as nonsparse backupset  ....\n\n     to either sparse media or non-sparse media (e.g. \"+DATAFILE\" or \"+SPARSEGD\") and\n      finally we will also test the restore\n     portion of the puzzle with commands resembling\n\n     restore (datafile ) from datafilecopy;\n     restore (datafile ) from backupset;\n\n      The outline goes aomething like below :\n\n      1- create the test sparsedb environment.\n      2- populate the tables with 25000 rows, one 8K block each for approximately\n         195M of test data\n      3- build some indexes and clean out the base table blocks.\n      4- close and re-open the base pluggable and create test_snap1\n      5- Rename asm files - because of the ASMCMD mkalias issue.\n         One cannot simply (re)move a fully qualified ASM name so\n         we need to mkalias for them so that we CAN (re)move the names.\n      6 - testing a backup\n        The cross product of all these tests looks like\n\n        [backup to  , restore from ]                            X\n        [sparse     , nonsparse    ]                            X\n        [copy       , backupset    ]                            X\n        [tab_sparse , tab_s_and_p_d, tab_s_and_p_s, tab_chunk ] X\n        [+DATAFILE  , +SPARSEDG    ]\n\n       7- testing a restore.\n\n       backup file as describe in foreach loop\n       rm the alias describing the file\n       restore file as describe in foreach loop\n       open the snapshot and run a query on restored table - close snaphot ......\n       rm restored file\n       reestablish asm alias back the original good child.",
    "platform": null
  },
  {
    "test_name": "tsagsparserslvr.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparserslvr.tsc - sparse disk resilvering test\n\nsparse disk resilvering test",
    "platform": null
  },
  {
    "test_name": "tsagsparsescan.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparsescan.tsc - Smart scan test for sparsedb\n\nsmart scan test in combination where DATAfFILE is 4M AU and SPARSEDG is 4M AU",
    "platform": null
  },
  {
    "test_name": "tsagsparsesgds.tsc",
    "setup": "tsaginit",
    "flags": {
      "max_areas": "5  # max number of areas on each device",
      "gd": "'sparsefile0,sparsefile1'    # sparse griddisks",
      "duration": "3800  # run IOV for 1 hours",
      "blk_szlst": "'64K'",
      "io_szlst": "'512,1K, 1.5K, 2K, 2.5K, 3K, 3.5K, 4K, 8K, 16K, 32K,64K,128K,256K, 1M, 4M'       # to set the num of blocks",
      "threads": "10",
      "sparse": "true",
      "reslv": "true",
      "align": "y",
      "offset": "4  # start offset to be 4M aligned",
      "area_sz": "4",
      "creatdev_file": "tsagddef"
    },
    "description": "tsagsparsesgds.tsc - Sparsedb test\n\nTest to check foolowing things while iov running -\n     1- alter griddisk command\n     2- cellsrv kill/restart\n     3- create griddisk and run iov parallely",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsparsesgds_failure.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "normal",
      "oss_testing": "2",
      "oss_auto_manage_disks": "true",
      "creatdev_file": "tsagrddef",
      "uniq_dsknames": "all",
      "num_flash_per_cell": "16",
      "oss_devdir1": "^T_WORK^/raw^oss_port^",
      "oss_devdir2": "^T_WORK^/raw^oss_port2^",
      "max_areas": "5  # max number of areas on each device",
      "gd": "'sparsefile0,sparsefile1'    # sparse griddisks",
      "duration": "3600  # run IOV for 1.5 hours",
      "blk_szlst": "'64K'",
      "io_szlst": "'512,1K, 1.5K, 2K, 2.5K, 3K, 3.5K, 4K, 8K, 16K,32K,64K,128K,256K, 1M, 4M'       # to set the num of blocks",
      "threads": "10",
      "sparse": "true",
      "align": "y",
      "offset": "4  # start offset to be 4M aligned",
      "area_sz": "4"
    },
    "description": "tsagsparsesgds_failure.tsc - Sparsedb test\n\nTest to check failure injections on HD/FLASHDISK while iov running in background",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsparsespace.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparsespace.tsc - Sparsedb test\n\nTest to check virtual and physical space from view x$kffil_sparse",
    "platform": null
  },
  {
    "test_name": "tsagsparsestat.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsparsestat.tsc - sparsedb test\n\nsession_stats (v$mystat) to measure the exact number of IOs on a scan sql on a sparse table - test1",
    "platform": null
  },
  {
    "test_name": "tsagspcorrupt_getvars1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagspcorrupt_getvars1.tsc - Helper script for Corruption Tests",
    "platform": null
  },
  {
    "test_name": "tsagspcorrupt_getvars2.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagspcorrupt_getvars2.tsc - Helper script for Corruption Tests",
    "platform": null
  },
  {
    "test_name": "tsagspcorrupt_getvars3.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagspcorrupt_getvars3.tsc - Helper script for Corruption Tests",
    "platform": null
  },
  {
    "test_name": "tsagspcorruptsql.tsc",
    "setup": null,
    "flags": {
      "parent_pdb": "CDB1_PDB1",
      "child_pdb": "TEST_SNAP1",
      "child2_pdb": "TEST_SNAP2"
    },
    "description": "tsagspcorruptsql.tsc - Defines SQL scripts to be used in Sparse\n                            Corruption tests",
    "platform": null
  },
  {
    "test_name": "tsagspgdtest.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_auto_manage_disks": "true",
      "exa_cdb": "true",
      "redund": "external"
    },
    "description": "tsagspgdtest.tsc - Test for lrgsamsbug23\n\nTest for shrinking the griddisk and ASM disk",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsplint.tsc",
    "setup": null,
    "flags": {
      "CVorLV": "LV"
    },
    "description": "tsagsplint.tsc - Run OSS lrgs on demand in interop mode\n\nCalled from tsaginit when INTEROP_RUN_FROM is set.",
    "platform": null
  },
  {
    "test_name": "tsagspocli.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagspocli.tsc - tests spool and start verbs\n\nThis tests the spool and START commands of CellCli",
    "platform": null
  },
  {
    "test_name": "tsagspocli_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagspocli_nls.tsc - tests spool and start verbs\n\nThis NLS tests the spool and START commands of CellCli",
    "platform": null
  },
  {
    "test_name": "tsagspresourcechk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagspresourcechk.tsc - Test to verify the resources associated with the\n     HC Storage Pool are properly calculated in hybrid environment\n\nTest to verify the resources associated with the HC Storage Pool are\n     properly calculated in hybrid environment. Resources we will verify\n     are \"Total SP space\", \"Total SP IOPS\" and \"Total SP Flash Cache\".",
    "platform": null
  },
  {
    "test_name": "tsagspswtchfail.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagspswtchfail.tsc - spine switch reboot and subnet manager reboot test\n\nRuns workload on a normal redundancy diskgroup and reboot\n     (1) Spine switch\n     (2) Subnet manager",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsqlmntr.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsqlmntr.tsc - Exadata sql test monitor\n\nRuns tman sql tmansqmonrws.sql with exadata",
    "platform": null
  },
  {
    "test_name": "tsagsrcgen.tsc",
    "setup": null,
    "flags": {
      "tsagsrcgen_status": "^TST_EXE_STATUS^"
    },
    "description": "tsagsrcgen.tsc - run tsagsrcgen.pl\n\nRun tsagsrcgen.pl on behalf of the test that contains this boilerplate:\n\n     #-----------------------------------------------------------------\n     # tsagsrcgen: a utility that allows {script|endscript}\n     #           THESE THREE ORA*TST COMMANDS MUST APPEAR EXACTLY AS IS\n     #-----------------------------------------------------------------\n     runtest tsagsrcgen ^tst_tscname^\n     include T_WORK:^tst_tscname^tsagsrcgenout.tsc\n     exit 0\n\nExpected to be run at the top of another .tsc file that contains\n     {script|endscript} statements. The enclosed source text will become\n     command files to be executed by the ^tst_tscname^tsagsrcgenout.tsc\n     ORA*TST source file generated below by tsagsrcgen.pl.\n\n     In order to function properly, the three ORA*TST source lines MUST\n     be included vebatim. tsagsrcgen.pl relies on their being present.\n\n     Check test tsagmacrosusg.tsc for more use cases",
    "platform": null
  },
  {
    "test_name": "tsagssb.tsc",
    "setup": null,
    "flags": {
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'",
      "ssb_conn": "'cdb1_pdb1'"
    },
    "description": "tsagssb.tsc - Exadata Star Schema Benchmark\n\nSSB workload on Exadata\n\nStar Schema Based on TPC-H",
    "platform": null
  },
  {
    "test_name": "tsagssst.tsc",
    "setup": "tsagnini",
    "flags": {
      "sage_mirror_mode": "normal"
    },
    "description": "tsagssst.tsc - Unit test for Smart Scan simulation tool\n\nUnit test for Smart Scan simulation tool.\n     This test doesn't use a database instance.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstat1.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagstat1.tsc - v$sysstats statistics for Exadata\n\nv$sysstats statistics for Exadata",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstatcollection.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagstatcollection.tsc - driver script to periodically collect stats\n\nThis is the driver script to periodically collect stats in\n     Exadata/Exascale environment on farm runs. The stat results are saved\n     in <farm result location>/exadata_stats.tar.gz.sav file\n\nTo use this utility in farm jobs, use the following config parameters:\n      farm submit <lrgname> -config \"EXADATA_STAT_COLLECTION=true;EXADATA_STAT_NAME=CELLSRVSTAT;INTERVAL=10\"\n      Explanation of parameters :\n      EXADATA_STAT_COLLECTION=true : Turn on stats collection\n      EXADATA_STAT_NAME=CELLSRVSTAT/ECSTAT\n      INTERVAL=10 : specify the interval of stat collection in seconds, for example-\n                    INTERVAL=10 will invoke and save cellsrv output every 10 seconds.\n\n     OR to enable it in integration runs please add the following entry in\n     tsagstatcollection_lrglist.dat\n      LRG:<lrgname>:<EXADATA_STAT_NAME>:<INTERVAL> - to enable stat collection in single lrg\n      SUITE:<SUITE_NAME>:<EXADATA_STAT_NAME>:<INTERVAL> - to enable stat collection in all lrgs in a suite",
    "platform": null
  },
  {
    "test_name": "tsagstclus1.tsc",
    "setup": null,
    "flags": {
      "DG_NAME": "DGP_01"
    },
    "description": "tsagstclus1.tsc - Stretch Cluster Test Case for Exadata(Part I)\n\nRun after VD/OCR is configured on ASM(tclsaocrasm_init.tsc)",
    "platform": null
  },
  {
    "test_name": "tsagstclus2.tsc",
    "setup": null,
    "flags": {
      "testuser": "iormuser",
      "numrows": "5",
      "tbs_name": "tbs_1"
    },
    "description": "tsagstclus2.tsc - Stretch Cluster Case for Exadata(Part II)\n\nRun after tsagstclus1.tsc, in continuation of the stretch cluster case",
    "platform": null
  },
  {
    "test_name": "tsagstclus3.tsc",
    "setup": null,
    "flags": {
      "tmp_nowarn": "^tst_nowarn^"
    },
    "description": "tsagstclus3.tsc - Stretch Cluster Failure Cases for Exadata\n\nRuns failure scenarios for stretch cluster on Exadata",
    "platform": null
  },
  {
    "test_name": "tsagstclusd.tsc",
    "setup": "tsaginit",
    "flags": {
      "disable_newhome": "false",
      "OSS_AUTO_MANAGE_DISKS": "true",
      "stclust": "true"
    },
    "description": "tsagstclusd.tsc - Exadata Stretch Cluster with OCR/VD on ASM\n\nThe stretch cluster test case with OCR and Voting Disks on the Exadata Diskgroup(lrgsage7).\n   The setup does the following:\n\n   a) Set oss_testing to 4.\n   b) Replicate Oracle Home(the setup uses the 2 home setup for ASM and DB).\n   c) Configure OCR and Voting Disks on ASM(tclsaocrasm_init.tsc)\n   d) Connect to ASM instance and create a diskgroup which has 2  regular failgroups\n      and 3 quorum failgroups.The regular failgroups have 4 disks each with cellsrv 1\n      and 2 serving one FG and cellsrv 3 and 4 the other.The quorum failgroups have\n      one asm disk(not Exadata) each.\n   e) Replace OCR from the default dg created by tclsaocrasm_init.tsc to the new diskgroup.\n   f) Replace Voting Disks from the default dg to the new dg.\n   g) Configure DB on ASM(tclsaocrasm_dbinit.tsc).\n   h) Create tablespace,table on the new dg and run a predicate query.\n   i) Run failure scenarios.\n      1) Fork a workload and kill cellsrv 3 and 4 while it is doing inserts.\n         See if it completes successfully.The FG served by cellsrv 3 and 4 goes offline.\n      2) Restart cellsrv 3 and 4,the disks come online automatically.\n      3) Switch back to cellsrv 1 and drop a regular and a quorum disk.\n      4) Add back the dropped disks.\n      5) Run read/write queries on the tablespace.\n   j) Do cleanup.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstoragepoolattributeupdate.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagstoragepoolattributeupdate.tsc -\n     Tests to verify update in lsstoragepool\n\n1. rename cellOfflineTimer to cellOfflineTimerInMins\n\n     2. rename diskOfflineTimer to diskOfflineTimerInMins\n\n     3. hide attributes FlashCacheSize and XRmemCacheSize\n        from escli \"lsstoragepool <storagePool name>\" and\n        \"lsstoragepool <storagePool name> --detail\" outputs\n\n     4. validate that lsstoragepool --detail includes\n        highDataStatus and highRecoStatus\n\n     5. validate that highDataStatus and highRecoStatus\n        can't be modified",
    "platform": null
  },
  {
    "test_name": "tsagstoragepooloperr.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagstoragepooloperr.tsc - New Test for vtambi_better_sp_op_error_messages\n\nTest Overview: This test verifies the behavior of the EGS leader process\n     and Raft log handling when a storage pool creation command fails and the\n     EGS leader process is restarted.\n     Test Environment: Requires an Exadata system with ESCli access and EGS (Exadata Grid Service)\n     running. The environment must allow killing and monitoring of the EGS leader process\n     and access to Raft logs.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagstrindx1.tsc",
    "setup": "tsaginit",
    "flags": {
      "tsagcln": "3",
      "logsuff": "archivehigh",
      "creatdev_file": "tsagxddef",
      "asm_ausize": "4194304",
      "nflint": "1",
      "file_dest": "'+DATAFILE'",
      "file_dest1": "'+datafile'",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''",
      "reffile": "tsagrman.log"
    },
    "description": "tsagstrindx1.tsc - Storage Index unit test w/ various datatype columns",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx1a.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagxddef",
      "asm_ausize": "4194304",
      "nflint": "1",
      "file_dest": "'+DATAFILE'",
      "file_dest1": "'+datafile'"
    },
    "description": "tsagstrindx1a.tsc - Test for bug - 9783818\n\nTest case for bug - INCORRECT RESULT DUE TO CHAINED ROW IN STORAGE INDEX",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx1b.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagxddef",
      "asm_ausize": "4194304",
      "nflint": "1",
      "file_dest": "'+DATAFILE'",
      "file_dest1": "'+datafile'"
    },
    "description": "tsagstrindx1b.tsc - Test for bug 9792513\n\nBug - INCORRECT RESULT WITH STORAGE INDEX DUE TO TRAILING ZEROS IN COLUMN VALUES",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx1bf.tsc",
    "setup": null,
    "flags": {
      "logsuff": "^logsuff^_mixed"
    },
    "description": "tsagstrindx1bf.tsc - Run Bloom filter tests for SI feature\n\nRun Bloom filter tests for SI feature",
    "platform": null
  },
  {
    "test_name": "tsagstrindx1c.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1",
      "creatdev_file": "tsagxddef",
      "asm_ausize": "4194304",
      "file_dest": "'+DATAFILE'",
      "file_dest1": "'+datafile'"
    },
    "description": "tsagstrindx1c.tsc - Test for bug 9824799\n\nUPDATING COLUMN VALUES TO NULL CAUSES STORAGE INDEX TO PRODUCE WRONG RESULT",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx1d.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1",
      "creatdev_file": "tsagxddef",
      "asm_ausize": "4194304",
      "file_dest": "'+DATAFILE'",
      "file_dest1": "'+datafile'"
    },
    "description": "tsagstrindx1d.tsc - Test for bug 10046323\n\nSTORAGE INDEX WRONG RESULT DUE TO CONCURRENT READ/ WRITE ON THE SAME 1MB REGION",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx1e.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1",
      "creatdev_file": "tsagxddef",
      "asm_ausize": "4194304",
      "file_dest": "'+DATAFILE'",
      "file_dest1": "'+datafile'"
    },
    "description": "tsagstrindx1e.tsc - Test for bug 11060702",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx1f.tsc",
    "setup": "tsaginit",
    "flags": {
      "tmp_nowarn": "^tst_nowarn^",
      "logsuff": "archivelow",
      "creatdev_file": "tsagxddef",
      "asm_ausize": "4194304",
      "nflint": "1",
      "file_dest": "'+DATAFILE'",
      "file_dest1": "'+datafile'",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx1f.tsc - Storage Indx test for CHAR datatype",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx1g.tsc",
    "setup": "tsaginit",
    "flags": {
      "nflint": "1",
      "creatdev_file": "tsagxddef",
      "asm_ausize": "4194304",
      "file_dest": "'+DATAFILE'",
      "file_dest1": "'+datafile'",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx1g.tsc - Test for New stats for storage index",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx2.tsc",
    "setup": "tsaginit",
    "flags": {
      "logsuff": "querylow",
      "creatdev_file": "tsagxddef",
      "file_dest": "'+DATAFILE'",
      "asm_ausize": "4194304",
      "nflint": "1",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''",
      "db_alert": "^ORACLE_HOME^/log/diag/rdbms/^ORACLE_SID^/^ORACLE_SID^/trace/alert'*'"
    },
    "description": "tsagstrindx2.tsc - More storage index tests\n\nTest predicates with ORs",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx2a.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagxddef",
      "file_dest": "'+DATAFILE'",
      "nflint": "1",
      "asm_ausize": "4194304",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx2a.tsc - Test for bug - 9783818\n\nTest case for bug - INCORRECT RESULT DUE TO CHAINED ROW IN STORAGE INDEX",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx2b.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagxddef",
      "file_dest": "'+DATAFILE'",
      "nflint": "1",
      "asm_ausize": "4194304",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx2b.tsc - Test for bug 10623494\n\nTests the fix for bug 10623494 - STORAGE INDEX WRONG RESULT FOR NOT-EQUAL",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx2c.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagxddef",
      "file_dest": "'+DATAFILE'",
      "nflint": "1",
      "asm_ausize": "4194304",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx2c.tsc - Test for bug - 10415382\n\nBug 10415382 - STORAGE INDEX NOT USED FOR EQUALITY PREDICATE(S)",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx3.tsc",
    "setup": "tsagnini",
    "flags": {
      "logsuff": "queryhigh",
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagsidef",
      "file_dest": "'+DATAFILE'",
      "asm_ausize": "4194304",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx3.tsc - Multi cell Test case for storage index\n\n1) Load a table on a normal redundancy diskgroup\n    2) run a query on the table that would use storage index\n    3) Offline the primary disk\n    4) update table (which makes the mirror dirty and hence require a rebalance)\n    5) run a query on the updated table\n    6) online the primary disk (this should trigger ASM rebalance)\n    7) run a query on the table (the scan should go to the primary. ASM rebalance should make the SI on the primary disk consistent with the secondary disk)",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx4.tsc",
    "setup": "tsaginit",
    "flags": {
      "logsuff": "archivehigh",
      "creatdev_file": "tsagxddef",
      "file_dest": "'+DATAFILE'",
      "nflint": "1",
      "asm_ausize": "4194304",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx4.tsc - Storage Index tests for IN- LISTS",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx5.tsc",
    "setup": "tsaginit",
    "flags": {
      "logsuff": "queryhigh",
      "creatdev_file": "tsagxddef",
      "file_dest": "'+DATAFILE'",
      "asm_ausize": "4194304",
      "nflint": "1",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx5.tsc - Tests SI usability with IS NULL/IS NOT NULL\n\nThis test executes some queries containing 'IS NULL' and 'IS NOT NULL'\n      predicates and see if SI is used.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx6.tsc",
    "setup": "tsaginit",
    "flags": {
      "logsuff": "archivelow",
      "creatdev_file": "tsagxddef",
      "file_dest": "'+DATAFILE'",
      "asm_ausize": "4194304",
      "nflint": "1",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx6.tsc - Tests SI functionality\n\nThis test script executes queries with datatypes such as number,\n     float, nvarchar2 and all missing operators from the SI coverage matrix.\n\nNone",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx7.tsc",
    "setup": "tsaginit",
    "flags": {
      "logsuff": "querylow",
      "creatdev_file": "tsagxddef",
      "file_dest": "'+DATAFILE'",
      "asm_ausize": "4194304",
      "nflint": "1",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx7.tsc - Tests for SI functionality\n\nThis test script executes queries with datatypes such as date, timestamp,\n     timestamp with timezone and all missing operators from SI coverage matrix\n\nNone",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx8.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagxddef",
      "file_dest": "'+DATAFILE'",
      "nflint": "1",
      "asm_ausize": "4194304",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx8.tsc - make sure cellsrv doesn't need SI pages across celloflsrv restarts.\n\n- shutdown ASM/DB, restart cellsrv, do state dump of ocl si and check that Page desc list:SI Ctl Free List #pages:\n     - run queries that use SI\n      - on db side, run \"alter system set cell_offload_processing = false\"\n      - kill -9 celloflsrv\n      - Do a state dump of ocl SI again  and verify  that Page Desc List value is same as SI\n         Ctl Free List i.e same number of Free Pages",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx_bug12618136.tsc",
    "setup": "tsaginit",
    "flags": {
      "tsagcln": "3",
      "creatdev_file": "tsagxddef",
      "file_dest": "'+DATAFILE'",
      "asm_ausize": "4194304",
      "nflint": "1",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx_bug12618136.tsc - Test for bug 12618136\n\nBloom filter between a EHCC table on non encrypted tablespace\n      and encypted table",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx_bug_11657444.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagxddef",
      "file_dest": "'+DATAFILE'",
      "asm_ausize": "4194304",
      "nflint": "1",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx_bug_11657444.tsc - Tests SI usability with IS NULL/IS NOT NULL\n\nTest script for a bug fix - 11657444",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindx_bug_12928840.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagxddef",
      "file_dest": "'+DATAFILE'",
      "nflint": "1",
      "asm_ausize": "4194304",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''"
    },
    "description": "tsagstrindx_bug_12928840.tsc - Test for bug 12583906\n\nTests for some dump messages during SI read/write errors",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindxchar.tsc",
    "setup": "tsaginit",
    "flags": {
      "cell_storage_index_diag_mode": "6",
      "logsuff": "archivelow",
      "creatdev_file": "tsagxddef",
      "asm_ausize": "4194304",
      "nflint": "1",
      "file_dest": "'+DATAFILE'",
      "dbs1_ctrlf": "''^dbs1_controlfile_1_fname^''",
      "reffile": "tsagrman.log"
    },
    "description": "tsagstrindxchar.tsc - Storage Index unit test char datatype columns",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrindxminmax.tsc",
    "setup": null,
    "flags": {
      "tst_no": "test8_tc",
      "qtxt": "'select min(col8), max(col8) from t1 - SI is primed by query'"
    },
    "description": "tsagstrindxminmax.tsc - SI Min/Max test cases\n\nThis test script has various scenarios to test SI Min/Max feature.",
    "platform": null
  },
  {
    "test_name": "tsagstrindxminmax_ehcc.tsc",
    "setup": null,
    "flags": {
      "tst_no": "test33_tc",
      "col_val": "col1",
      "qtxt": "'select min(col1), max(col1) from t1 - with QUERY LOW Compression and SI is primed by query'"
    },
    "description": "tsagstrindxminmax_ehcc.tsc - SI Min/Max test cases with various\n     EHCC compression techniques\n\nThis test script has various scenarios to test SI Min/Max feature",
    "platform": null
  },
  {
    "test_name": "tsagstrindxminmax_setup.tsc",
    "setup": "tsaginit",
    "flags": {
      "tsagcln": "3",
      "creatdev_file": "tsagxddef",
      "file_dest": "'+DATAFILE'",
      "asm_ausize": "4194304",
      "nflint": "1",
      "pga_aggregate_limit": "4G"
    },
    "description": "tsagstrindxminmax_setup.tsc - Test setup, table population for            #      SI MIN/MAX test cases\n\nThis test script has various scenarios to test SI Min/Max feature",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagstrmap.tsc",
    "setup": "tsagnini",
    "flags": {
      "nflint": "1",
      "use_self_tune_sga": "true",
      "file_dest": "+DATAFILE",
      "dbusr": "'scott/tiger'"
    },
    "description": "tsagstrmap.tsc - Test functions of dbms_storage_map\n\nTests for bug  8835785, 8836400",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsuit.tsc",
    "setup": "tsaginit",
    "flags": {
      "section": "ALL",
      "creatdev_file": "tsagaudef",
      "asm_compat_pre22": "true"
    },
    "description": "tsagsuit.tsc - Driver for lrgsacli and lrgsage4\n\nIt calls different tests for CELLCLI.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsundiagcases.tsc",
    "setup": null,
    "flags": {
      "kvm_host": "true",
      "compute_node_set": "^compute_node_f^^",
      "tst_tscname": "tsagsundiagcases_comp"
    },
    "description": "tsagsundiagcases.tsc - Test for sundiag.sh\n\nTest for hanging the script\n   Test for running time of script\n   Invalid arguments\n   Test for generated tar file size\n   Verify directories which should be present in Uncompressed resultant file\n   Test for griddisk-asm-status.out\n   Test for sosreport*.tar.xz\n   Compare real and collected log sizes(ms-odl/alert) - disabled for now as\n   disruptive, will run on single node\n   Verify that exawatcher charts are present\n   Runs on two modes\n    - single_cell -on one cell\n    - multi_node - on whole rack in parallel\n      eg - tsagsundiagcases.tsc mode=single_cell\n   Enable disruptive file content check test\n   Add args and combination of args test\n   verify metrics for capacity-optimized flash\n   verify ilom logs have timestamp\n   Asm info test and new param asm_info\n     uses ssh root@host \"bash -x /opt/oracle.SupportTools/sundiag.sh\"",
    "platform": null
  },
  {
    "test_name": "tsagswitchinst.tsc",
    "setup": null,
    "flags": {
      "ossgeninst": "ossgen^instcell^.lst",
      "ossgen_tmplog": "ossgentmp.log"
    },
    "description": "tsagswitchinst.tsc - Generates script to source in view to switch cell\n\nGenerates 3/4 scripts named - ossinst1.sh, ossinst2.sh, etc\n     which can be sourced locally to switch to that cell.\n     For example - source ossinst2.sh\n        All cellcli commands executed now will go to cell 2",
    "platform": null
  },
  {
    "test_name": "tsagswitchinst_esnp.tsc",
    "setup": null,
    "flags": {
      "dbossconf": "^DBOSSCONF^"
    },
    "description": "tsagswitchinst_esnp.tsc - Generates script to switch to ESNP node\n\nGenerates ossinstesnp.sh that can be sourced in local view\n    before running any dbmcli command in ESNP node's context\n\nExample - source ossinstesnp.sh\n              dbmcli -e list dbserver detail",
    "platform": null
  },
  {
    "test_name": "tsagswtchdf.tsc",
    "setup": "tsagnini",
    "flags": {
      "compatible": "11.2.0.0"
    },
    "description": "tsagswtchdf.tsc - Switch Datafile on Exadata + cell_config tests\n\nSwitch Datafile on Exadata",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagswtchfail.tsc",
    "setup": "tsaginit",
    "flags": {
      "sage_mirror_mode": "normal",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagswtchfail.tsc - Leaf switch Reboot Test\n\nTEST:1 ## Run a workload on a table created on a normal redundancy\n     \t      ## diskgroup, and reboot leaf switches",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagswupdate.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true"
    },
    "description": "tsagswupdate.tsc - UI tests for upgrade at scale\n\nCellCLI tests for softwareupdate object",
    "platform": null
  },
  {
    "test_name": "tsagswupdate_db.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true"
    },
    "description": "tsagswupdate_db.tsc - UI tests for upgrade at scale\n\nDBMCLI tests for softwareupdate object",
    "platform": null
  },
  {
    "test_name": "tsagsymlinkdcli.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagsymlinkdcli.tsc - Test for txn rirallab_symlinkdcli\n\nThis test implementation of changes made to allows dbmcli and cellcli to be used interchangeably on\nstorage and compute nodes, simplifying the use of common CLI commands like list\nphysicaldisk in scenarios such as dcli. This eliminates the need to\ndifferentiate between cell nodes and DB nodes, enabling a single command like\ncellcli -e list physicaldisk or dbmcli -e list physicaldisk to run uniformly\nacross all machines.",
    "platform": null
  },
  {
    "test_name": "tsagsyslogconf.tsc",
    "setup": "tsaginit",
    "flags": {
      "mach_name": "^cell^.us.oracle.com",
      "mach_passwd": "welcome1",
      "remote_node": "slcm01cel12",
      "remote_node_password": "welcome1"
    },
    "description": "tsagsyslogconf.tsc - Test for Exadata Syslogconf Feature",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagsyslogconf_upgrade.tsc",
    "setup": null,
    "flags": {
      "remote_node": "dadzaa04",
      "remote_node_password": "welcome1"
    },
    "description": "tsagsyslogconf_upgrade.tsc - Test for Exadata Syslogconf Feature",
    "platform": null
  },
  {
    "test_name": "tsagsysloginput.tsc",
    "setup": null,
    "flags": {
      "node_name": "^node^.us.oracle.com"
    },
    "description": "tsagsysloginput.tsc - Functional test for sysloginput\n\nThis test addes the functional test for txn xiaohshe_bug-32054460 (log relay support for auditd and other log files..)\n     Documentation : https://confluence.oraclecorp.com/confluence/display/EXD/SysLogInput+for+Cells+and+Computes\n       * Its a cellcli and dbmcli feature similar to syslogconf and syslogformat.\n       * SyslogInput is a cell/dbserver attribute which can be set to configure system logging.\n       * It makes persistent change to /etc/rsyslog.conf, so that upgrades will not overwrite the logging configuration change.\n       * It is independent of syslogconf and syslogformat in that it can be specified with or without these settings.\n       * The purpose is to extend log forwarding beyond just system messages (/var/log/messages).\n       * This can be used to also forward audit, aide, and yum messages",
    "platform": null
  },
  {
    "test_name": "tsagtargetwattneg.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagtargetwattneg.tsc - Negative testing for some dbmserver commands\n\nNegative testing for cpuTargetWatts and lowPowerMode commands\n   by running on non x11 computes\nRuns on lrgrhx9sadbmcli",
    "platform": null
  },
  {
    "test_name": "tsagtargetwatts.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagtargetwatts.tsc - Test for CPU TargetWatts Project on compute\n\nTestcases:\n    1. Initial setup - check socket, PowerLimit, cputargetwatts and\n                       cputargetwattsmaxvalue\n\n    2. Valid - Test each value incrementally, starting from the minimum(400)\n               to the maximum (800), in steps of 40, to confirm that\n               sockpower changes as expected using the e_smi_tool.\n\n    3. Persistent - Check PowerLimit remains same before ms shutdown and\n\t      after ms startup\n\n    4. Reset - Reset cpuTargetWatts using cpuTargetWatts=null\n\n    5. Underscore - Set the underscore parameter _cpu_target_watts_max_value\n                    in cellinit.ora to 500 and this will be new maxValue and\n                    all the  values not in the range of maxValue /2 and\n                    maxValue are invalid.\n\n    7. Invalid - Attempt to set cpuTargetWatts to a non-numeric string\n\nFull test description:\n    https://confluence.oraclecorp.com/confluence/display/~rishideep.\n    rallabandi@oracle.com/Test+Spec+for+CPU+TargetWatts+Project",
    "platform": null
  },
  {
    "test_name": "tsagtargetwatts_cell.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagtargetwatts_cell.tsc - Negative test for CPU TargetWatts Project\n                                on cell\n\nTry to do list cpuTargetWatts and cpuTargetWattsMaxValue\n    it should error output",
    "platform": null
  },
  {
    "test_name": "tsagtclsabcvfy.tsc",
    "setup": null,
    "flags": {
      "docker_suff": "'_dock'",
      "nowarn": "^TST_NOWARN^",
      "node_num": "0",
      "host_name": "has_hostname_^node_num^",
      "hname": "^^host_name^",
      "afdsupported": "0",
      "current_node_type": "has_nodetype_^HAS_NODE_NUM^",
      "current_node_type_value": "^^current_node_type^",
      "dsk0": "diskgroupreserrcrs5000.log",
      "dsk0_ref": "diskgroupreserrcrs5000_ref.log"
    },
    "description": "tsagtclsabcvfy.tsc - Sanity checks for BC\n\nTests for verifying the cluster status after starting BC mode CRS stack\n     This is a clone of tclsabcvfy.tsc",
    "platform": null
  },
  {
    "test_name": "tsagtcpmonloop.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "egs_deplmode": "custom",
      "eds_encryption_mode": "on",
      "sage_mirror_mode": "high"
    },
    "description": "tsagtcpmonloop.tsc - TCP Monitor loop test\n\ntests TCP Monitor thread and verifies that EGS detects cellsrv down in the loop",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagtelemetrylog.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagtelemetrylog.tsc - Test for telemetry log check for nvmecli\n\nNVMECLI command:\n     ./nvmecli --get --device=<devicename> --log=<logfilename> --type=[host_telemetry | controller_telemetry",
    "platform": null
  },
  {
    "test_name": "tsagtempalert.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagtempalert.tsc -  CELL Temperature Alert\n\nThis test calls the tsagtempalert.sh script to test the alert generation in cell if\n     the cell temperature crosses the threshold value of max and min limit\n\nThis test assumes the following:\n         a. Any latest cell RPM is already installed on the machine",
    "platform": null
  },
  {
    "test_name": "tsagtestbug37159551.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagtestbug37159551.tsc - test for 37159551 - X11M CELL M2NVME CELLCLI FAILURE SIMULATION IS NOT WORKING\n\nTest added to check failure simulation on M.2 NVME disks\n     (Test for txn rakkashy_bug-37159551_enable_pd_failure_simulation_for_m2_nvme)",
    "platform": null
  },
  {
    "test_name": "tsagthrashfclarge.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagfclargethrash.tsc - Test for large fc write\n\nIndividual CG thrashing case , i.e., globally we are\n     not thrashing, but some individual cache group is thrashing\n\nTest steps are below.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagthrashfclarge2.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagr2def"
    },
    "description": "tsagthrashfclarge2.tsc - test added in lrgsafclarge2\n\nThe test verifies that if global thrashing is detected,\n   LW is not throttled for IOs that belong to a hardmax FC group.\n   2 Test case included:\n     1. with temp IO LW. Temp IO is considered as high priority LW, which will bypass\n        the thrashing check, so we expect it won't be rejected by thrashing concern.\n     2. with EDV LW. EDV IO is considered as important but non high priority LW(important\n        LW is parent set of high priority LW), which won't bypass the thrashing check. So\n        we expect to see LW rejected due to thrashing concern.\n\ntest steps are below",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagthrsh.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagthrsh.tsc - Thresholds on IORM Metrics\n\nTests CREATE/ALTER/LIST/DROP THRESHOLD commands of cellcli",
    "platform": null
  },
  {
    "test_name": "tsagthrsh_nls.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagthrsh_nls.tsc - describe and help command of CellCLI\n\nTests CREATE/ALTER/LIST/DROP THRESHOLD commands of cellcli",
    "platform": null
  },
  {
    "test_name": "tsagtktgifcoss.tsc",
    "setup": null,
    "flags": {
      "maxinstances": "2",
      "create_instances": "^maxinstances^",
      "rdbms_internal_fplib": "true",
      "oltp_compress": "false",
      "num": "'_nofplib'",
      "imc_size": "200M",
      "db_opt": "'db_block_checking=true'",
      "stimcfclv": "1",
      "enable_small_tabs": "true"
    },
    "description": "tsagtktgifcoss.tsc - clone of EHCC test tktgifc.tsc\n\nClone of tktgifc.tsc\n\nClone of tktigfc.tsc added for easier addition of Exadata specific tests\n     1. Omit comparison for tsagchsi_nofplib_4.log due to intemittent difs and\n        tsagchsi_nofplib_5.log and tsagchsi_nofplib_6.log are running the same workloads.\n     2. Comment out comparison for tsagchksi_nofplib.log due to not have SI built.",
    "platform": null
  },
  {
    "test_name": "tsagtopcpu.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagtopcpu.tsc - topcpu cellcli commands\n\ntests topcpu cellcli commands",
    "platform": null
  },
  {
    "test_name": "tsagtopology.tsc",
    "setup": null,
    "flags": {
      "rack": "quarterrack"
    },
    "description": "tsagtopology.tsc - Test for verify topology commands on machine\n\nUSAGE: tsagtopology.tsc hw_type=<real|virtual>\n\nThe test calls tsagtopology.sh script to execute commands and store outputs.",
    "platform": null
  },
  {
    "test_name": "tsagtorniodxd.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_testing": "1",
      "creatdev_file": "tsagdiskdef"
    },
    "description": "tsagtorniodxd.tsc - test for tornio cellsrv crash recovery\n\nFind the offset of the DXD griddisk\n   Do orion to write to the griddisk with small disk_end_offset\n   Dump the full hexdump of the 40K to some file for reference\n   Set simulation event to crash cellsrv (also corrupt the data) at next DXD IO:\n   Use arbiter to issue 10 write 4K I/O to disk:\n   cellsrv will crash, and arbiter will exit with error after some wait.\n   Dump the full hexdump of the 40K to some file for reference again:\n   Now bring up rs, cellsrv, and it should auto recover the torned blocks:\n   Now we need to verify the 40K (4K x 10 blocks) region to verify if the recovery is successful or not.\n\ntest added to lrgsapmem3",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagtornioprevention.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "This test validates torn I/O prevention for DAX(DXD) disks\n\nRun iov with 10 threads of locked wirte, 10 threads of locked\n     read, 10 threads of direct read. Keep the test running for 10 min",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagtouchcountioctl.tsc",
    "setup": "tsaginit",
    "flags": {
      "cell_with_flash_cache": "all",
      "creatdev_file": "tsagrddef",
      "cell_with_pmem_cache": "true"
    },
    "description": "tsagtouchcountioctl.tsc - validates increment the touch count on NVCache\n\nThis test runs on fake hardware and validates increment the touch count on\n     NVCache cache lines using a side channel (IOCTLs), when the client is mostly\n     doing reads of the cache via RDMA (so that the IOs never reaches cellsrv and\n     hence cellsrv cannot increment the touch count normally).\n\nTest uses NVCache",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagtpcc.tsc",
    "setup": null,
    "flags": {
      "target_cluster": "1",
      "target_database": "1",
      "target_compute": "1",
      "target_node": "compute=^target_compute^",
      "target_clu": "cluster=^target_cluster^",
      "target_db": "db=^target_database^",
      "warehouses_num": "50",
      "tpcc_runtime": "20",
      "tpcc_terminals": "16",
      "base_dir": "^aws_compute_twork^",
      "tpcc_conn": "'clu'^target_cluster^'_inst'^target_compute^'_cdb'^target_database^'_pdb1'",
      "tpcc_prefix": "clu^target_cluster^_comp^target_compute^_db^target_database^"
    },
    "description": "tsagtpcc.tsc - TPC-C workload on Exadata real hw\n\nTPC-C workload on Exadata\n\nTPC-C workload on Exadata",
    "platform": null
  },
  {
    "test_name": "tsagtpcc_setup.tsc",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": null
  },
  {
    "test_name": "tsagtpcds.tsc",
    "setup": null,
    "flags": {
      "dg_name": "@vault1",
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'",
      "tpcds_conn": "cdb1_pdb_tpcds",
      "tpch_conn": "cdb1_pdb_tpch",
      "tpcds_scan": "CDB1_PDB_TPCDS_SCAN",
      "tpch_scan": "CDB1_PDB_TPCH_SCAN"
    },
    "description": "tsagtpcds.tsc - TPC-DS workload on Exadata\n\nTPC-DS workload on Exadata\n\nTPC-DS workload on Exadata",
    "platform": null
  },
  {
    "test_name": "tsagtpchwkld.tsc",
    "setup": null,
    "flags": {
      "sysdba": "'sys/knl_test7@cdb1_pdb1 as sysdba'",
      "aws_compute_twork": "^T_WORK^"
    },
    "description": "tsagtpchwkld.tsc - Run TPCH workload\n\nRun TPCH workload, if set enable_cc, will run tpch with cc too\n\nRun TPCH workload",
    "platform": null
  },
  {
    "test_name": "tsagtrcevent.tsc",
    "setup": null,
    "flags": {
      "trc_lvl_low": "'l'",
      "trc_lvl_med": "'m'",
      "trc_lvl_high": "'h'",
      "disk_trc_lvl": "lowest",
      "mem_trc_lvl": "high",
      "eds_trc_event": "'\\\"trace[Exc_Eds_Layer.*] disk=medium,memory=medium\\\"'",
      "egslib_trc_event": "'\"trace[libcell.EGS_Library] disk='^disk_trc_lvl^',memory='^mem_trc_lvl^'\"'",
      "bsw_trc_event": "'\\\"trace[Exc_Bsw_Layer.*] disk='^disk_trc_lvl^',memory='^mem_trc_lvl^'\\\"'",
      "bsm_trc_event": "'\\\"trace[Exc_Bsm_Layer.*] disk='^disk_trc_lvl^',memory='^mem_trc_lvl^'\\\"'",
      "osslib_trc_event": "'\"trace[libcell.Client_Library.*] disk='^disk_trc_lvl^',memory='^mem_trc_lvl^'\"'"
    },
    "description": "tsagtrcevent.tsc - helper script to parse trace event parameters\n\nUsed by tsaginit",
    "platform": null
  },
  {
    "test_name": "tsagtrcksc_intrpt.tsc",
    "setup": "srdbmsini",
    "flags": {
      "SAGE_MIRROR_MODE": "normal",
      "oss_testing": "2",
      "oss_auto_manage_disks": "true  # for XDMG to startup",
      "creatdev_file": "tsagaudef"
    },
    "description": "tsagtrcksc_intrpt.tsc - Test for cellsrv stat \"cell num map elem cancellation\"\n\nIn the test IO hang event is simulated on the celldisk.\n     Then smart scan query is executed and then in the same sql session value of cellsrv stat is\n     checked. The test expects its value to be greater than zero.\n\ncdpolicy.dat is modified inorder to reduce the IO cancellation threshold for simulated hardware.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagtrclimit.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagtrclimit.tsc - test case for cellsrv/cellsrv tracing infrastructure\n\ntest case for cellsrv/cellsrv tracing infrastructure",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagtstchk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagtstchk.tsc - Macro to conduct health check between the tests\n\nTo conduct health check between the tests\n\nTo conduct health check between the tests",
    "platform": null
  },
  {
    "test_name": "tsagtstchk_db.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagtstchk_db.tsc - Macro to conduct health check between the tests on dbnode\n\nTo conduct health check between the tests on dbnode\n\nTo conduct health check between the tests on dbnode",
    "platform": null
  },
  {
    "test_name": "tsagtstchk_db_cell.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagtstchk_db_cell.tsc - Macro to conduct health check between the tests\n\nTo conduct health check between the tests for cell and db\n\nTo conduct health check between the tests",
    "platform": null
  },
  {
    "test_name": "tsagtstoclcache.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagtstoclcache.tsc - Exadata test ocl cache\n\nTest for test_ocl_cache cell event",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaguncommittedfile1.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsaguncommittedfiles_phase1.tsc- Uncommitted Files Test - PHASE 1\n\nPhase 1: Explicit Deletion - This phase implements the EDSLIB APIs\n     to create, open, commit and abort uncommitted files.\n     It ensures that EDS has a way to differentiate between\n     a committed and an uncommitted file on the storage end.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsaguncommittedfile2.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsaguncommittedfile2.tsc - Phase 2 of Uncommitted Files Test\n\nAdditional: various uncommitted file unit test cases using edstool",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsaguncommittedfile3.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsaguncommittedfile3.tsc - PHASE 3 OF UNCOMMITTED FILE TESTS\n\nThe test validates that an uncommitted file gets deleted after it becomes stale",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagunifiedwait1.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_testing": "1",
      "creatdev_file": "tsagdiskdef"
    },
    "description": "tsagunifiedwait1.tsc - unified wait test\n\nunified wait test to check how concurrent skgxp and IPCDAT I/Os\n     are handled successfully by libcell. This test uses orion to issue\n     I/Os against PMEM and non-PMEM GDs concurrently. Test includes both\n     read, write, and -lgwr_rdma writes, so that all I/O paths can be exercised.\n\nNone",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagupdatecellxml.tsc",
    "setup": null,
    "flags": {
      "skip_pre_patch_setup": "true",
      "is_rolling": "true",
      "start_stop_orcl_components": "true",
      "exit_post_patch": "true",
      "ovs": "active"
    },
    "description": "tsagupdatecellxml.tsc - Exadata cell upgrade/downgrade using OEDA XML",
    "platform": null
  },
  {
    "test_name": "tsagupdatedbxml.tsc",
    "setup": null,
    "flags": {
      "dbpatch_label": "^TST_EXE_RESULT^",
      "skip_pre_patch_setup": "true",
      "pxe_server": "na",
      "base_release": "na",
      "is_rolling": "true",
      "exit_post_patch": "true",
      "ovs": "active",
      "testfile": "'tsagimpatchmgr_db_xml.tsc'"
    },
    "description": "tsagupdatedbxml.tsc - Exadata compute upgrade/donwgradeusing OEDA xml",
    "platform": null
  },
  {
    "test_name": "tsagupdateperf.tsc",
    "setup": null,
    "flags": {
      "skip_each_boot": "true",
      "fresh_image_for_patch": "true",
      "exit_post_patch": "true",
      "tsagupdateperf": "true",
      "delete_patch_dir": "false"
    },
    "description": "tsagupdateperf.tsc - Exadata upgrade performance testing",
    "platform": null
  },
  {
    "test_name": "tsagupdflash.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "true",
      "cell": "slcm01cel12",
      "cellconnstr": "root@^cell^"
    },
    "description": "tsagupdflash.tsc - Test for peer failure flash disks on X3\n\nThe test is for fix for re-enabling Aura2 flash disks and it goes through all the physical disks instead of LUNs to generate the list of flash disks to re-enable in updateFlashDisks so MS doesn't skip peer disks when a flash disk to re-enable doesn't have a present LUN.",
    "platform": null
  },
  {
    "test_name": "tsagupgflog.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagupgfldef"
    },
    "description": "tsagupgflog.tsc - test for flashlog upgrade/downgrade\n\ntest for flashlog upgrade/downgrade",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagusrgrp.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagusrgrp.tsc - Test for userdefined offloadgroup startup after cell reboot\n\n- User defined offloadgroup should be up and running after cell reboot\n       if the group was running before the reboot\n     - User defined offloadgroup should be stopped after cell reboot\n       if the group was stopped before the reboot\n     - System offloadgroup should be stopped after cell reboot\n       if the group was stopped before the reboot",
    "platform": null
  },
  {
    "test_name": "tsaguvscan.tsc",
    "setup": null,
    "flags": null,
    "description": "tsaguvscan.tsc - McAfee scan against OSS binaries in a given ADE view\n\nMcAfee scan against OSS binaries in a given ADE view",
    "platform": null
  },
  {
    "test_name": "tsaguwcache1.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0"
    },
    "description": "tsaguwcache1.tsc - test case for unaligned write caching\n\nTest case for caching unaligned write into clean page for WBFC.\n\nTest steps:\n\n     (1) Run orion 100% small read workload on 100MB disk region.\n         The small read is 1KB, i.e. not 4KB aligned. The small read would\n         trigger FC prefetch and populatiion. At the end of workload.\n         The data would get 100% cached.\n\n     (2) Run orion 100% small write workload on 100MB disk region.\n         The small write is 1KB, not 4KB aligned. All the unaligned writes\n         should go to cache. There should be zero nocache write.\n\n     (3) Repeat small read in (1). All reads should get cache hit.\n         There should be no cache miss, and no delta change in population.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaguwcache2.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0",
      "cell_with_write_cache": "1",
      "write_cache_size": "256"
    },
    "description": "tsaguwcache2.tsc - test case for unaligned write caching\n\nTest case for WriteCache (aka FlashCache for unaligned write caching).\n\nTest steps:\n\n     (1) Run orion 100% small write workload on 10MB disk region.\n         The small write is 1KB, not 4KB aligned. All the unaligned writes\n         should go to writecache. There should be zero nocache write.\n\n     (2) Repeat small write in (2). All writes should be redirty.\n\n     (3) Run orion 100% small read workload on 10MB disk region.\n         The small read is 1KB. All reads should get cache hit.\n         There should be no cache miss, and no delta change in population.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaguwcache3.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0"
    },
    "description": "tsaguwcache3.tsc - test case for unaligned write caching\n\nTest case for unaligned write caching via poisoned write.\n\nTest steps:\n\n     (1) Run orion 100% small write workload on 10MB disk region.\n         The small write is 1KB, not 4KB aligned. All the unaligned writes\n         should go to flashcache. There should be zero nocache write.\n\n     (2) Repeat small write in (2). All writes should be redirty.\n\n     (3) Run orion 100% small read workload on 10MB disk region.\n         The small read is 1KB. All reads should get cache hit.\n         There should be no cache miss, and no delta change in population.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsaguwcache4.tsc",
    "setup": "tsaginit",
    "flags": {
      "asm_disk_def": "tsagrdmadef",
      "cell_with_ram_cache": "0",
      "cell_with_pmem_cache": "0",
      "cell_with_xrmem_cache": "0"
    },
    "description": "tsaguwcache4.tsc - test case for unaligned write caching\n\nTest case for unaligned write into recovered clean page\n\nTest steps:\n\n     (1) Run orion 100% small read workload on 10MB disk region.\n         The entire 10MB disk region should get cached.\n     (2) Restart cellsrv.\n         All the clean cache pages are marked as recovered after restart.\n     (3) Run orion 100% small write workload on 10MB disk region.\n         The small write is 1KB, not 4KB aligned. All the unaligned writes\n         should go to flashcache. There should be zero nocache write.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagvdcefl.tsc",
    "setup": null,
    "flags": {
      "DG_VD123": "VDEXDG123",
      "lgfile1": "tsagvdcefl1.log"
    },
    "description": "tsagvdcefl.tsc - Voting disk on ASM - cell failure test",
    "platform": null
  },
  {
    "test_name": "tsagvdcflset.tsc",
    "setup": null,
    "flags": {
      "oss_testing": "3",
      "tmp_port": "^free_port_number^",
      "tag": "'cellsrv[2-'^oss_testing^'] tsagvdcflset'"
    },
    "description": "tsagvdcflset.tsc - sets up two cells for OCR on ASM",
    "platform": null
  },
  {
    "test_name": "tsagvdset.tsc",
    "setup": null,
    "flags": {
      "oss_testing": "3",
      "size": "160",
      "size1": "512",
      "tmp_port": "^free_port_number^",
      "tag": "'cellsrv[2-'^oss_testing^'] tsagvdset'"
    },
    "description": "tsagvdset.tsc - creates disks for cell 2 and cell 3\n\nvd on ASm setup script, prepares cell 2 and cell 3",
    "platform": null
  },
  {
    "test_name": "tsagver1.tsc",
    "setup": "tsagnini",
    "flags": {
      "noecho_testname": "true   ## prevents rdbmsini from duplicating the above line",
      "creatdev_file": "tsagrddef",
      "nflint": "0"
    },
    "description": "tsagver1.tsc - Exadata Module Version Mismatch\n\nExadata Module Version Mismatch test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagver2.tsc",
    "setup": "tsagnini",
    "flags": {
      "creatdev_file": "tsagrddef",
      "nflint": "0"
    },
    "description": "tsagver2.tsc - timezone tests for ExaData\n\ntimezone tests for ExaData",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagverifystoragetype.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagverifystoragetype.tsc\n\nHelper script to verify storage type used by ASM.\n\nThis script queries for storage.type attribute in v$asm_attribute table.",
    "platform": null
  },
  {
    "test_name": "tsagvers.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagvers.tsc - basic sanity check \"-v -- print version string\"\n\ncell binaries will print version string, with build date\n     when started with the -v | -version option. This test\n     does a simple sanity check: see if the label= matches.",
    "platform": null
  },
  {
    "test_name": "tsagvescollapsedel.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagvescollapsedel.tsc - Test for handling collapse in delta\n\nTest runs base ves xmls handles collapse in delta while IOV running in\n     the background\n\nThis lrg runs with encryption by default, see\n     tsage/data/lrglist_cloudservice.dat",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvesdisk3.tsc",
    "setup": null,
    "flags": {
      "phase_dur": "120",
      "sage_mirror_mode": "high",
      "ves_sim": "128",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagvesdisk3.tsc - Test for lrgsaexacldves3\n\nTest runs base ves xmls with egslib true",
    "platform": null
  },
  {
    "test_name": "tsagvesdisk4.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "phase_dur": "60",
      "FLASH_SIZE": "1024M",
      "sage_mirror_mode": "high",
      "ves_sim": "128"
    },
    "description": "tsagvesdisk4.tsc - test for lrgsaexacldves4\n\nThis test runs lrgsaexacldves3 with flashcache .\n     Couple of testcases :\n     1- Flashcache size is 20%(200M) of the testarea(1G).\n     2- Flashcache size is 90%(900M) of the testarea(1G).",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvesdisk5.tsc",
    "setup": null,
    "flags": {
      "phase_dur": "120",
      "sage_mirror_mode": "high",
      "vault_db": "DATA",
      "vault_log": "DATA"
    },
    "description": "tsagvesdisk5.tsc - Test for lrgsaexacldves5\n\nTest runs base ves xmls with egslib true",
    "platform": null
  },
  {
    "test_name": "tsagvesgdresize.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "egs_trace": "highest",
      "dsk_size": "large",
      "ves_sim": "64"
    },
    "description": "tsagvesgdresize.tsc - Test for ves griddisk resize\n\n1- Create ves griddisk of 900M\n     2- Run iov(ves_resize.xml) in background with read/write\n     3- shrink/expand the gd based on allocatedsize  while iov is running",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvesgdresize_db.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "egs_deplmode": "cloudatcustomer",
      "cmd_mirror_mode": "normal",
      "egs_dsk_fences_threshold": "true",
      "cdb": "true",
      "egs_trace": "highest",
      "egs_trace_file_rolling_disabled": "true",
      "dsk_size": "large",
      "sage_mirror_mode": "high",
      "vault_db": "DB^ORA_SID_UPPER^"
    },
    "description": "tsagvesgdresize_db.tsc - ves gd resize with active db workload\n\n- setup ves gds\n    - start db workload in background\n    - shrink/expand ves gd based on allocatedsize in a loop(count3)\n    - verify allocatedSize validation test\n    - make sure db workload ran fine with no crash\n\ntest to be added in lrgdbcsaexcvesgdresize_db",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagveshotsnap.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagveshotsnap.tsc - Test for taking snapshots in loop while IOV running\n\nTest runs base ves xmls and taking snapshots in loop while IOV running in\n     the background\n\nThis lrg runs with encryption by default, see\n     tsage/data/lrglist_cloudservice.dat",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagveshotsnap2.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "cmd_mirror_mode": "normal",
      "delta_sync": "true",
      "egs_dsk_fences_threshold": "true"
    },
    "description": "tsagveshotsnap2.tsc - Test for taking snapshots in loop while IOV running\n\nTest runs base ves xmls and taking snapshots in and deleting in random\n     order loop while IOV running in the background",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagviewsparse.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagviewsparse.tsc - Views for the Sparse Disks and Diskgroups\n\nThis test cases test various views introduced for Sparse Grid disks and Diskgroups\n\n/* X$KFDSK_SPARSE */\n     /* GV$ASM_DISK_SPARSE */\n     /* V$ASM_DISK_SPARSE */\n\n     /* X$KFDSK_SPARSE_STAT */\n     /* GV$ASM_DISK_SPARSE_STAT */\n     /* V$ASM_DISK_SPARSE_STAT */\n\n     /* X$KFGRP_SPARSE */\n     /* GV$ASM_DISKGROUP_SPARSE */\n     /* V$ASM_DISKGROUP_SPARSE */",
    "platform": null
  },
  {
    "test_name": "tsagvipdelete.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagvipdelete.tsc - Test vip deletion\n\nAdditional: Tests for blockstore VIP deletion",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvltprovision.tsc",
    "setup": null,
    "flags": {
      "vault_provision_space": "128GB",
      "substr_var": "G",
      "media_type_vault": "ALL",
      "sep": ",",
      "numtypev": "3",
      "tmpmt": "media_type_vault^i^",
      "provision_space_xt": "'spaceProvXT='^vault_provision_space^",
      "provision_space_ef": "'spaceProvEF='^vault_provision_space^",
      "provision_space_hc": "'spaceProvHC='^vault_provision_space^",
      "vault_provision_space_option": "^provision_space_xt^",
      "vault_provision_iops": "^iopsvalue^",
      "provision_iops_hc": "',iopsProvHC='^vault_provision_iops^",
      "provision_iops_ef": "',iopsProvEF='^vault_provision_iops^",
      "provision_iops_xt": "',iopsProvXT='^vault_provision_iops^",
      "vault_provision_iops_option": "^provision_iops_xt^",
      "vlt_options": "^vault_provision_space_option^"
    },
    "description": "tsagvltprovision.tsc - Parses media_type_vault parameter\n\nHelper script to parse media_type_vault parameter\n    This parameter denote which media type to be craeetd for the defult Vault\n    Pass single media type EF, HC or XT, or ALL or combo (comma separated)\n    media_type_vault=EF,HC",
    "platform": null
  },
  {
    "test_name": "tsagvolbackup.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagvolbackup.tsc - Volume and Backup test\n\nvolume or backup is used to create a volume, and a subsequent mkvolume\n     request with similar attributes but only with different external filename\n     or backup id arrives too soon, then mkvolume shouldn't returns the\n     previously created volume# id and create a new one.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolbkpfilepreserve.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagvolbkpfilepreserve.tsc - Test for rmvolumebackup with --keep-data option for preserving EDS file.\n\nTest for rmvolumebackup with --keep-data option for preserving EDS file.\n      Here are the test steps:\n        1. Create volume and volume snapshot\n        2. Create volume backup with datacopy type\n        3. Create normal volume backup\n        4. Remove normal  volume backup\n        5. Remove datacopy volume backup using --keep-data attribute\n        6. Check file existence and file ready only permission for datacopy volume backup",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolclone1.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagvolclone1.tsc - Test case for volume clones\n\n1) Test invalid inputs for volumeSnapshot attribute\n     2) Test a positive case by supplying correct volume snapshot value\n     3) List volume detail, to see the attribute values\n     4) Use vault option while creating clones\n     5) Test inputs for iopsInherited attribute\n     6) Tried cloning after deleting parent volume.\n     7) Tried deleting parent volume, when clone exists.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolclone2.tsc",
    "setup": "xblockini",
    "flags": {
      "bug_34584126": "open",
      "iscsi": "true"
    },
    "description": "tsagvolclone2.tsc - Test case for volume clones\n\nThis test file containd metadata sanity tests and clone of clones test.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolclonecurl1.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagvolclonecurl1.tsc - This test adds block store volume operations using only curl calls\n\nBlock store operations using curl - Loop test",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolclonedirect.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagvolclonedirect.tsc - Functional Test for bug 36978654\n\nFunctional Test for bug 36978654.\n     Now we create volume clone directly from a volume\n     instead of volume snapshot. The information available on a\n     volume clone should be the same as the parent volume\n     snapshot's information. Once the volume clone is created,\n     it will be treated as any other volume.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolclonedirect2.tsc",
    "setup": "xblockini",
    "flags": {
      "bug_34584126": "open",
      "iscsi": "true"
    },
    "description": "tsagvolclonedirect2.tsc - Test case for volume thin clones\n\nThis test file contains metadata sanity tests and clone of clones test\n     using volume thin clone method",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolcloneiorm.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagvolcloneiorm.tsc - IORM tests for volume clone\n\nThis test performs IORM side testing of volume clones. The following\n     methods are used to check IORM plans for any operation:\n     1. Checking the traces for any IORM plan update\n     2. Checking the output of cellcli list volume command\n     3. Checking IORM state dumps from oss_iorm.txt\n\n     The test cases can be checked from the plan below.\n\nTest plan is updated here: https://confluence.oraclecorp.com/confluence/display/EXC/Testing+Volume+Clone",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolclonerestore.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true",
      "run_vol_thin_clone": "false",
      "testname": "^tst_tscname^_thinclone"
    },
    "description": "tsagvolclonerestore.tsc - Create clone from restored volume and perform chvolume operations.\n\nCreate clone from restored volume and perform chvolume operations.\n     After updating volume attributes run io sanity checks and make sure\n     that changing volume attributes does not crash the volume data.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolclonestability1.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true"
    },
    "description": "tsagbsstability1.tsc - check stability during creation/deletion of volume clones\n\nSimulates bsm/bsw crash at different job states during\n     the creation/deletion of volume clones",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolclonestress.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagvolclonestress.tsc - Using CURL, concurrently create clones for around 30 minutes\n\nUsing CURL, concurrently create clones for around 30 minutes. (Stress Testing)\n\n     Test steps:\n       1. Create vault, volume and volume snapshot using curl commands\n       2. Assign execution time to be 1800 as we need to create volume clones parallely for 30 minutes.\n       3. Take the batch value as 10. (this creates 10 clones parallely)\n       4. Keep creating volume clones using curl command.\n       5. After 30 minutes when we exit from the loop, wait for 5 minutes.\n       6. Now check how many actual volumes have been created.\n       7. Delete all the volume clones that has been created.\n       8. Delete volume snapshot, volume and finally delete vault.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolgrplooptest.tsc",
    "setup": "xblockini",
    "flags": {
      "oss_memleak_skip_restart": "1"
    },
    "description": "tsagvolgrplooptest.tsc - This test script runs a loop test case with 100 iterations where we create and delete volume group and other storage entites in every iteration.\n\nThis test script runs a loop test case with 100 iterations wehere we create and delete volume group and other storage entities in every iteration.\n     In a loop of 100, run the same script - one after the other :\n       1. Create vault and volume\n       2. Create a volume group\n       3. Add volume to volume group\n       4. Delete volume group which would fail as volume group is not empty\n       5. Remove volume from volume group\n       6. Delete volume group which will succeed now\n       7. Increase size of volume\n       8. Create another volume group\n       9. Add old volume to newer volumegroup, add vol group snapshot as well\n      10. Repeat 4-6\n      11. Decrease size of volume\n      12. Repeat 8-10\n      13. Delete volume snapshots\n      14. Delete volume\n      15. Delete vault\n      16. Delete volume group",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolgrpstresstest.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagvolgrpstresstest.tsc - Test for stress testing of volume group feature.\n\nTest for stress testing of volume group feature. Here are test steps:\n       1. Create 500 volume groups with 1 volume each\n       2. Create snapshots in parallel for 20 volume groups at a time",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolgrpstresstest2.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagvolgrpstresstest2.tsc - Test for stress testing of volume group feature.\n\nTest for stress testing of volume group feature. Here are test steps:\n       1. Create a volume group with iopsProvisioned=100000\n       2. Create 250 volumes with contentType=data\n       3. Create 250 volumes with contentType=RECO\n       4. Create backup and restore of first 5 volumes.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolgrptests.tsc",
    "setup": "xblockini",
    "flags": {
      "python_path": "^ADE_VIEW_ROOT^/python/bin",
      "ledvniscsi": "1",
      "edvcleanup_only": "1"
    },
    "description": "tsagvolgrptests.tsc - This script contains basic, advance and AI generated test cases for Volume Group feature.\n\nThis script contains basic 6 test cases for Volume Group feature",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvollatencytest.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagvollatencytest.tsc - Test case to check latency for lsvolume and lsvolumesnapshot\n     is less than 4 minutes for 15000 volumes and each of its snapshots.\n\nTest case to check latency for lsvolume and lsvolumesnapshot\n     is less than 4 minutes for 15000 volumes and each of its snapshots.\n\n     Test steps:\n       1. Create 15000 volumes from same vault\n       2. Run lsvolume command and check latency is less than 4 minutes.\n       3. Create one volume snapshots from each of 15000 volumes.\n       4. Run lsvolumesnapshots and check latency is less than 4 minutes.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolsnappagination.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagvolsnappagination.tsc - Pagination test case for volume snapshots\n\nTest contains :\n      1) Test case for volume snapshot pagination\n      2) Test case for volume pagination\n      3) Test case for volumes, with volumegroup filter and pagination feature",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolumebackup.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true",
      "tsaglogappend": "1"
    },
    "description": "tsagvolumebackup.tsc - volume backup and snapshot tests\n\nvalidate volume backup and snapshot\n\nvalidate volume backup and snapshot",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolumebackup5.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagvolumebackup5.tsc - Tests volumebackup creation accross volume resizes.\n\nTest steps:\n      1. Create volume 1 (1G), and fill the full volume using dd\n\t2. Make volumesnapshot 1\n\t3. resize the volume to 512M, then resize again to 1G\n\t4. Make volumesnapshot 2\n\t5. Make volumebackup 1 from snapshot 1, wait for completion\n\t6. Make volumebackup 2 from snapshot 2, wait for completion\n\t7. Restore volumebackup 2 to new volume 2\n\t8. Compare contents of volume1 and volume 2",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolumebackupbug.tsc",
    "setup": "xrdbmsini",
    "flags": {
      "setup_blockstore": "true",
      "tsaglogappend": "1"
    },
    "description": "tsagvolumebackupbug.tsc - volumebackup deletion and bug test\n\nvalidate volume backup and snapshot deletion along other bug tests",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolumebackupbug2.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true",
      "setup_blockstore": "true"
    },
    "description": "tsagvolumebackupbug2.tsc - volume backup bug tests\n\nvalidate volume backup and snapshot bug tests",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolumebackupbug3.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true",
      "setup_blockstore": "true",
      "egs_mode": "sharedCloud"
    },
    "description": "tsagvolumebackupbug3.tsc - will have sharedCloud bug tests\n\nAdditional: blockstore tests for volume backups - contains mostly bug tests",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvolumemetrics.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagvolumemetrics.tsc - Curl test for volume metrics\n\nAdditional: tests for volume metrics",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagvos.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagvos.tsc - SAGE version of VOS short regress\n\nSAGE version of VOS short regress\n\nNot coded to run with multiple oss servers",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagvsaf12.tsc",
    "setup": null,
    "flags": {
      "x5_af_setup": "true",
      "num_af_dev": "12"
    },
    "description": "tsagvsaf12.tsc - Virtual Slot Tests for AF with 12 NVMe",
    "platform": null
  },
  {
    "test_name": "tsagvsaf8.tsc",
    "setup": null,
    "flags": {
      "x5_af_setup": "true",
      "num_af_dev": "8"
    },
    "description": "tsagvsaf8.tsc - Virtual slot test for AF configuration with 8 NVMe devices\n\nVirtual slot tests for All Flash configuration with 8 NVMe devices",
    "platform": null
  },
  {
    "test_name": "tsagvsn_tst_asm_setup.tsc",
    "setup": "tsaginit",
    "flags": {
      "sparsegds": "', virtualSize=10G'",
      "hd_size": "1040",
      "fd_size": "256",
      "gd_data": "800",
      "gd_fd": "48",
      "gd_reco": "160",
      "gd_dbfs": "80",
      "sage_mirror_mode": "high",
      "oss_auto_manage_disks": "true",
      "oss_failgroup": "failalldbdg",
      "asmdisks_created": "2",
      "creatdev_file": "tsagvsndef",
      "raw_path": "^sage_diskstring_dir^"
    },
    "description": "tsagvsn_tst_asm_setup.tsc - set up script for Enabling appliance mode in LRGs\n\nThis script sets up the test environment for Virtual Slot Number -\n    ASM Fixed  Partnership test cases.\n\n    USAGE:\n    tsagvsn_tst_asm_setup hd_size=<size> fd_size=<size> gd_data=<size>\n    gd_fd=<size> gd_reco=<size> gd_dbfs=<size>\n\n    where hd_size is the size of hard disks (DISK0 to DISK11)\n          fd_size is size for FLASH disks\n          gd_data is size of datafile GD's\n          gd_fd is size for flash GD's\n          gd_reco is size for reco GD's\n          gd_dbfs is size for dbfs GD's\n\n    The gd_<..> parameters are reused to determine whether the respective disk#     groups are to be created or not. Note that if gd_data parameter is passed,#     CONTROLFILE, LOGFILE and STANDBY diskgroups will also be created along\n    with DATAFILE diskgroup.\n\n   If none of the above parameters is passed, it uses default sizes and does\n   not create RECO and DBFS diskgroups.\n\nThis feature will only work in appliance mode. i.e\n    1. There are 12 disks and 16 Flash disks\n    2. Uses High Redundancy - 3 cells\n    3. DATA and RECO disk groups are created on all 12 HD's (slots 0 -11)\n    4. DBFS diskgroup is created on all of the 10 non-system HD's (slots 2-11)\n    5. a diskgroup spanning all of the Flash Disks\n    6. FlashLog and Writeback Flashcache on all the FD's\n    7. ASM/Grid disks in each disk group are of same size\n\n    Following are various constraints to enable appliance mode:\n\n    1. Appliance mode is not explicitly disabled. (_disable_appliance_check,\n       _disable_appliance_partnering)\n    2. Diskgroup compatibility is 11.2.0.4 or higher. Note 12.1.0.1 is not\n       compatible with appliance mode.\n    3. cell.smart_scan_capable (sageonly) is enabled.\n    4. All disks in the diskgroup are of the same type - hard/flash disks.\n    5. All disks have the same size.\n    6. All failure groups in the diskgroup have equal number of disks.\n    7. No. of disks in each failgroup should be either numslots or (numslots -\n       numdisks_system) where numslots is the maximum number of disk slots in #        the cell (info provided during oss discovery) and numdisks_system is\n       number of system disks carved out of the cell. Today, default for\n       numdisks_system is 2.\n    8. There are atleast 3 failure groups and 4 disks per failure group.\n    9. There are no offline disks.\n    10. All disks in the failure group have sequential and non-overlapping\n        slot numbers. Also, the first slot number is same across failure\n        groups. (guaranteed by OSS)",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagvsnasm.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagvsnasm.tsc - Test cases for Virtual Slot No - ASM Fixed Partnership\n\nThis script has various test scenarios to test Virtual Slot Number and\n     ASM Fixed Partnership project.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagvsnasm1.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagvsnasm1.tsc - Physical Disk Code Path Test Cases\n\nThis script has Physical Disk Replacement and Swapping test cases.",
    "platform": null
  },
  {
    "test_name": "tsagvsnasm_imptc.tsc",
    "setup": "tsaginit",
    "flags": null,
    "description": "tsagvsnasm_imptc.tsc - Physical Disk Import Tests",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagvtableprivilege.tsc",
    "setup": "xrdbmsini",
    "flags": null,
    "description": "tsagvtableprivilege.tsc - Test for Update in v$ table Access\n\nThe file tests the following updates in access to v$ tables -\n1. Non EXACS/On-Prem Exascale Stack:\n\n         excdbusr has cluster level privileges - Accessible\n         excdbusr doesnot have cluster level privileges - Accessible\n2. EXACS/ CloudService Exascale Stack:\n\n         excdbusr has cluster level privilege - Accessible\n         excdbusr doesnot have cluster level privilege - Not Accessible\n\n      The tests check access incase of 3 cluster level privilege options:\n      cl_monitor < cl_operator < cl_admin",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagworkloadcapture.tsc",
    "setup": "tsaginit",
    "flags": {
      "cell_with_flash_cache": "all",
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagworkloadcapture.tsc - Test for workload capture script\n\nSanity test for workload capture script\n\nThe workload capture will try to capture the IOs at the cell and\n     store it for analysis using a query tool. The information should also\n     allow running the similar workload using the captured info with a\n     potential replay tool.\n\n     https://confluence.oraclecorp.com/confluence/pages/viewpage.action?pageId=12625015447",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagwrdg.tsc",
    "setup": null,
    "flags": {
      "tsagwrdglog": "tsagwrdg_^action^^tsagwrdg^.log",
      "test_dg": "datafile",
      "numiter": "1",
      "numfiles": "1",
      "filesiz": "16                               # file size in term of 4K blocks"
    },
    "description": "tsagwrdg.tsc - script to create and drop files in a diskgroup\n\nLong running script to write to a diskgroup.",
    "platform": null
  },
  {
    "test_name": "tsagwrongres.tsc",
    "setup": null,
    "flags": {
      "conn_str": "'sys/knl_test7 as sysdba'"
    },
    "description": "tsagwrongres.tsc - Check which layer could cause the wrong result\n\nCheck which layer could cause the wrong result\n\nCheck which layer could cause the wrong result, if you want to debug some\n   queries, please refer to below demo:\n     echo  > tsagwrongresult_sql.sql\n           > 'select max(c1) from tb1 x where instr(C3,'''X''',1,1) = 1 and c1=100010;'\n           > 'select max(c1) from tb1 x where to_char(c3) > '''a''' and c1=100010;'\n           > 'select max(c1) from tb1 x where decode(instr(c3,'''X'''), 1, c3) is not null and c1=100010;'\n           > 'select max(c1) from tb1 x where regexp_instr(c3,'''X''')=1 and c1=100010;'\n     endecho\n     let conn_str 'tklocopu1/tklocopu1'\n     runtest tsagwrongres",
    "platform": null
  },
  {
    "test_name": "tsagx10efconfig.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_testing": "1",
      "oss_failgroup": "failalldbdg",
      "CELL_WITH_XRMEM_ONLY": "1",
      "flashsize": "512M",
      "creatdev_file": "tsagqlcdef",
      "CELL_WITH_QLC_DISK": "1",
      "NUM_QLC_PER_CELL": "8",
      "QLC_DISK_SIZE_MB": "2048",
      "scompatible.asm": "^scompatible18.asm^",
      "tmp_port": "^free_port_number^",
      "tag": "'cellsrv[2-'^oss_testing^'] tsagfgini'",
      "asmdisks_created": "2",
      "nflint": "1",
      "numlogfiles": "2",
      "do_not_set_numlogfile": "true"
    },
    "description": "tsagx10efconfig.tsc - Create X10 EF config\n\nUsing this file, a test can configure X10EF like system\n    with 4 TLC PERFORMANCE optimized FLASH devices (used for flashcache,etc)\n    and 4 QLC CAPACITY FLASH devices that can host 2 celldisks each\n    This script can be used in following ways\n    (1) include tsagx10efconfig\n        - Set sage_mirror_mode appropriately prior to calling this script\n        - set only_disks, if you do not want to setup ASM\n        - This script does not setup DB - It is responsibility of the caller\n          (This script is kind of replacement of tsagnini, if called directly)\n    (2) Second way to set up X10 EF is to set CELL_WITH_QLC_DISK=1\n        In that case, tsagnini will invoke tsagx10efconfig by itself\n    Example - To setup DB on QLC disks in X10EF mode, using normal redundancy:\n       let CELL_WITH_QLC_DISK 1\n       let sage_mirror_mode normal\n       include srdbmsini",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagx3dbcorecap.tsc",
    "setup": null,
    "flags": {
      "dbnode": "^compute_node^",
      "dbnodeconnstr": "^dbconnstr^"
    },
    "description": "tsagx3dbcorecap.tsc - Test for X3 DB Core Capping - CoD and IaaS\n\nThe transaction adds test for X3 DB Core Capping - CoD and IaaS.\nCoD is supported with values 8, 12 and 16 for X3-2 DB node.\nIaaS is supported for X3-2 and X3-8.\nCoD with IaaS is not supported for X3-2.\n\nThese cases have been scripted in tsagx3dbcorecap.tsc for X3-2.",
    "platform": null
  },
  {
    "test_name": "tsagx5afdejstat.tsc",
    "setup": null,
    "flags": {
      "x7_ef_setup": "true"
    },
    "description": "tsagx5afdejstat.tsc - OSS test for X5 all flashdisk eject status\n\nTest checks the eject status property for the all flashdisks setup for X5 using debugcli in various failure scenarios.\n\nto be added in lrgdbconsax7debugcli2",
    "platform": null
  },
  {
    "test_name": "tsagx5config.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_testing": "1",
      "cell_with_flash_cache": "0",
      "x5_hc_setup": "true",
      "nvme_size": "512",
      "creatdev_file": "tsagdebugclidef",
      "tmp_port": "^free_port_number^",
      "tag": "'cellsrv[2-'^oss_testing^'] tsagfgini'",
      "cellname": "thirdcell",
      "num_af_dev": "12"
    },
    "description": "tsagx5config.tsc - Configures an X5 system using DEBUGCLI\n\nTwo types of X5 systems can be set\n    1) High Capacity - 12 HDs and 4 NVMe devices\n    let x5_hc_setup true (Required, and is also default)\n          let nvme_size <size in MB>  (optional, takes 512M as default)\n\n    2) All Flash - 8 or 12 NVMe devices\n          let x5_af_setup true (Required)\n          let nvme_size <size in MB> (optional, takes 512M as default)\n          let num_af_dev <8 or 12>   (default is 8 devices)\n\nTo use this script, include it in the beginning of the test after setting\n     up required variables (as described above) for the desired setup",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagx5dbcorecap.tsc",
    "setup": null,
    "flags": {
      "dbnode": "^compute_node^",
      "dbnodeconnstr": "root@^compute_node_f^"
    },
    "description": "tsagx5dbcorecap.tsc - Core Count test for Exadata X5 DB Node\n\nTests for DB core count changes on X5 DB Node",
    "platform": null
  },
  {
    "test_name": "tsagx5dbcorecap1.tsc",
    "setup": null,
    "flags": {
      "dbnode": "^compute_node^",
      "dbnodeconnstr": "^dbconnstr^"
    },
    "description": "tsagx5dbcorecap1.tsc - Test for X5 DB Core Capping - CoD and IaaS\n\nThe transaction adds test for X5 DB Core Capping - CoD and IaaS.\nCoD is supported with values 18, 28 and 36 for X5-2 DB node.\nIaaS is supported for X5-2 and X5-8.\nCoD with IaaS is not supported for X5-2.\n\nThese cases have been scripted in tsagx5dbcorecap1.tsc for X5-2.",
    "platform": null
  },
  {
    "test_name": "tsagx6config.tsc",
    "setup": "tsaginit",
    "flags": {
      "cell_with_flash_cache": "0"
    },
    "description": "tsagx6config.tsc - Configures X6 HC system using DEBUGCLI",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagx6dskrep.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagx6dskrep.tsc - Test case for AURA 8 flash card\n\nAs AURA7 and AURA8 cards are very similar, Replace AURA6 with AURA7 flash card for the test.",
    "platform": null
  },
  {
    "test_name": "tsagx7config.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_testing": "1",
      "cell_with_flash_cache": "0",
      "x7_hc_setup": "true",
      "gdsize": "2048",
      "tmp_port": "^free_port_number^",
      "tag": "'cellsrv[2-'^oss_testing^'] tsagfgini'",
      "cellname": "thirdcell"
    },
    "description": "tsagx7config.tsc - Configures X7 EF/HC system using DEBUGCLI",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagx7dbcorecap.tsc",
    "setup": null,
    "flags": {
      "dbnode": "^compute_node^",
      "dbnodeconnstr": "root@^compute_node_f^"
    },
    "description": "tsagx7dbcorecap.tsc - Core Count test for Exadata X7 DB Node\n\nTests for DB core count changes on X7 DB Node non-eighthRack\n     1. Decrease pendingcorecount without force\n     2. Decrease pendindcorecount with force\n     3. Set _dbserver_allow_core_count_below_limit=true\n     4. Wait for reboot for new changes to be effective\n     5. Decrease pending core count below required minimum core count\n     6. Increase pendingcodecount\n     7. Wait for reboot for new changes to be effective\n     8. Undo 3.\n\nAdded in lrgrh7sasrvmondb",
    "platform": null
  },
  {
    "test_name": "tsagx7dskrep.tsc",
    "setup": null,
    "flags": {
      "x7_ef_setup": "true"
    },
    "description": "tsagx7dskrep.tsc - X7 Disk replacement Tests\n\nTests FD replacement on X7 cell (DEBUGCLI)\n\nFake hardware lrg using DEBUGCLI",
    "platform": null
  },
  {
    "test_name": "tsagx7rofs.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagx7rofs.tsc - Test to fix the alert message when File Systems\n\t\tare changed to Read-Only\n\ntsagx7rofs.tsc - Test to fix the alert message when File Systems\n\t\tare changed to Read-Only\n\nTest Steps:\n1) Cleanup Cell\n2) Copy lib/libcell19.so to /opt/oracle/cell/cellsrv/lib/libcell19.so\n3) Unmount /opt/cellconf and remount it in Read-Only Mode\n4) Wait for reboot\n5) Sleep for 5 minutes to let MS raise alert\n6) List alert history for checking the alert\n7) Unmount /opt/cellconf and remount it in Read-Write Mode",
    "platform": null
  },
  {
    "test_name": "tsagx9dbmssuit.tsc",
    "setup": null,
    "flags": {
      "section": "ALL"
    },
    "description": "tsagx9dbmssuit.tsc - Test suite for DBMCLI for x9",
    "platform": null
  },
  {
    "test_name": "tsagx9dskrplchk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagx9dskrplchk.tsc - Drop disk replacement check for x9\n\n1. drop a disk for replacement\n     2. drop another disk from same raid - should fail\n     3. reenable dropped disk\n     4. drop another disk from same raid - should succeed\n     5. Reenable dropped disk\n     6.a. drop a disk for replacement\n       b. drop another disk from another raid - should succeed\n       c. reenable disk from raid 1\n       d. reenable disk from Raid 2",
    "platform": null
  },
  {
    "test_name": "tsagx9dskrplchk_non_amd.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagx9dskrplchk_non_amd.tsc - Drop disk replacement check for x9\n\t\t\t     (non-AMD version)\n\n1. drop a disk for replacement\n     2. drop another disk from same raid - should fail\n     3. reenable dropped disk\n     4. drop another disk from same raid - should succeed\n     5. Reenable dropped disk\n     6.a. drop a disk for replacement\n       b. drop another disk from another raid - should succeed\n       c. reenable disk from raid 1\n       d. reenable disk from Raid 2",
    "platform": null
  },
  {
    "test_name": "tsagx9dskrplchk_singledisk.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagx9dskrplchk_singledisk.tsc - Drop disk replacement check for x9-AMD\n\t\t\t\twith single disk dropping\n\n1. drop a disk for replacement\n     2. drop another disk from same raid - should fail\n     3. reenable dropped disk\n     4. drop another disk from same raid - should succeed\n     5. Reenable dropped disk",
    "platform": null
  },
  {
    "test_name": "tsagxdbc.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagxdbc.tsc - Exadata XDB Install Script\n\nHelps to create XDB from scratch,backup and restore files\n     for Exadata XDB lrgs.",
    "platform": null
  },
  {
    "test_name": "tsagxlate.tsc",
    "setup": "srdbmsini",
    "flags": {
      "reflog": "tsagxlate.log"
    },
    "description": "tsagxlate.tsc - XLATE WITH SI test",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagxlcellresync1.tsc",
    "setup": null,
    "flags": {
      "num_cells": "15",
      "mixed_workload": "true",
      "maxpdb": "1",
      "oss_multims_testing": "true"
    },
    "description": "tsagxlcellresync1.tsc - Test for 15 cells for Resync\n\nFlow of test:\n     1. Start with 15 cells and drop one cell.\n     2. Shutdown  cellsrv from cells 4 and 10\n     3. Reboot EGS leader and startup services on cell 4 and 10\n     4. Check cellsrvstatus\n\nTest for Resync with large number of cells",
    "platform": null
  },
  {
    "test_name": "tsagxlcellresync2.tsc",
    "setup": null,
    "flags": {
      "num_cells": "12",
      "oss_multims_testing": "true",
      "mixed_workload": "true",
      "maxpdb": "1"
    },
    "description": "tsagxlcellresync2.tsc - Large config test for Resync\n\nStarts with 12 cells\n     Triggers:\n           StoragePool Reconfig - drop cells\n           Pooldisk offline - inactive GD\n           cell down  - shutdown services all\n     Ends at 8 cells\n\nNeeds larger VMs",
    "platform": null
  },
  {
    "test_name": "tsagxlcellsuite.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagxlcellsuite.tsc - This suite contains tests for special pool\n\nThis suite contains tests for eXtra Large cell configurations\n      that runs on special farm pool with high RAM and more cores",
    "platform": null
  },
  {
    "test_name": "tsagxrccreplace.tsc",
    "setup": null,
    "flags": {
      "sga_target": "3072M",
      "flash_size": "2000",
      "OSS_ENABLE_FC_PERSISTENCE": "writeback",
      "creatdev_file": "tsagrh2def",
      "ENABLE_XRMEM_CC_CHECK": "1"
    },
    "description": "tsagxrccreplace.tsc - test case for CC replacement in XRMEMCache\n\nThis test has below steps:\n     Step 1: Create test tables\n     Step 2: Flush FC and drop XRMEMCache\n     Step 3: Run all 3 workloads to populate FC\n     Step 4: recreate XRMEMCache\n     Step 5: Run CC2 workload 1(65)\n     Step 6: Run CC2 workload 2(97) several times so that it is hotter\n     Step 7: Check that columnarCacheSize in XRMEMCache from first\n             workload has been replaced\n     Step 8: Run CC1 workload 3(33) several times so that it is hotter\n     Step 9: Check taht cloumnarCacheSize in XRMEMCache from second\n             workload has been replaced\n\nIn this test, we set up 4 tables:\n     CC2 for query(65):    lineitem_1\n     CC2 for capacity(97): lineitem_2\n     CC1 table(33):        lineitem_3",
    "platform": null
  },
  {
    "test_name": "tsagxshjson.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagxshjson.tsc - Test for support for JSON output in XSH\n\nTest for support for JSON output in XSH",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagxshtest.tsc",
    "setup": "xblockini",
    "flags": null,
    "description": "tsagxshtest.tsc - This test validates how xsh handles errors after XSH\n                       I/O code path enhancement\n\nThis test validates xsh errors handling after XSH I/O code path\n     enhancement.  Here are the test steps:\n       1. Create file @test/file with some data\n       2. Simulate OSS_ERRCODE_MIR_OFFLINED on read requests\n       3. Check how xsh handles errors:\n          expect message âeshFS2ExascaleErrReapAll_0â from the step 3.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagxtefiormplan.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagxtefiormplan.tsc - IORM plan tests for EF + XT\n\nThe test does the following -\n   i)   Creates/Updates a vault and volume IOPS and validates the results.\n   ii)  Associate Resource Profiles a the vault\n   iii) Set Resource Profile to DB and validate that the DB receives the resources and per the profile.\n   iv)  Validate that updating the resource profile updates the DB\n   v)   Set PDB IORM plan and list PDB details",
    "platform": null
  },
  {
    "test_name": "tsagxwatchercdmdcheck.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagxwatchercdmdcheck.tsc - Exawatcher celldiskmd sampler check test",
    "platform": null
  },
  {
    "test_name": "tsagxwatchgraphtest.tsc",
    "setup": null,
    "flags": {
      "OSS_HW_TESTING": "1"
    },
    "description": "tsagxwatchgraphtest.tsc - Exawatcher Graph Test\n\nExawatcher test for verifying result retreival and graphs.",
    "platform": null
  },
  {
    "test_name": "tsagzstdcc2.tsc",
    "setup": null,
    "flags": {
      "log_name": "^tst_tscname^.log"
    },
    "description": "tsagzstdcc2.tsc - test case for ZSTD compression function\n\ntest case for ZSTD compression function\n\ntest steps are as follows:\n     step 1: turn off cc2 system wide, restart cellsrv\n     step 2: initialize temp table for checking stats 'columnar_cache_cc2_with_zstd'\n     step 3: turn on session CC2, run a query and check stat, expect failure\n             since ZSTD is not turned on\n     step 4: check for table id and obtain columnarcachesize\n     step 5: turn on ZSTD by setting _cell_high_comp_algo_level_cc2 to 1\n     step 6: run a query and check for stat, expect success\n     step 7: compare columnarcachesize for ZSTD and LZO, ZSTD should be smaller\n     step 8: run another query to check cc hits, expect success",
    "platform": null
  },
  {
    "test_name": "tsagbug38291892.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug38291892.tsc - Bug Test for bug - 38291892\n\n1.Create various diagpacks (with and without compression)\n      and verify their files as user escsmamon\n    2.Confirm escsmamon can access all diagpack files in\n      /var/log/oracle/deploy.\n    3.As root, fill up /var/log/oracle/deploy and verify that\n      old diagnostic files are purged.",
    "platform": null
  },
  {
    "test_name": "tsagbug38343734.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug38343734.tsc\n\nTest for Bug-38343734 : AIM4EXA:RS-7445 [SERV MS IS ABSENT] - OSSRSUTL_MONITOR_MONPR_THD\n\nThis test verifies that MS startup fails when MS deployment\n     is in progress which is done by script setup_dynamicDeploy",
    "platform": null
  },
  {
    "test_name": "tsagbug38363313.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagbug38363313.tsc - Test for Bug 38363313\n\nPlease see below\n\nAdded in lrgsaexacldbsfail15",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38366131.tsc",
    "setup": "tsaginit",
    "flags": {
      "creatdev_file": "tsagrddef"
    },
    "description": "tsagbug38366131.tsc - Bug 38366131 - OCI:: PHX::MULTIPLE HARDDISKS\n                     CONFINED DURING SCRUB DUE TO SOME IO HANG OVER 90S\n\nBug 38366131: remove undrained cancelled IOs confinement trigger.\n     As long as there are IO completions, do not confine the disk for hang.\n\n     The test starts read worklaod and simulates BLOCKIO_READ_HANG to trigger\n     disk temporary hang which would generate undrained cancelled read IO.\n     Then the test starts write workload which would clear the disk's hang\n     but the disk still has the undrained cancelled read IO. As long as\n     the write workload is active, Cellsrv will not confine the disk for hang.",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug38370389.tsc",
    "setup": null,
    "flags": {
      "oss_hw_testing": "1"
    },
    "description": "tsagbug38370389.tsc - Test for bug 38370389\n\nCustomer security team detected unprotected passwords in the system processes.\n     Hence the transaction rirallab_bug-38370389 makes it hidden.\n     This test case verifies that passwords are not visible.",
    "platform": null
  },
  {
    "test_name": "tsagbug38375591.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug38375591.tsc - Functional test for bug 38375591\n\nVerify correct handling of SEL events in MS: skip empty alerts for\n     non-matching GUIDs and trigger IO hang alert for matching GUIDs on\n     celldisks.\n\n1) Change cell trace level to finer.\n     2) Shut down MS.\n     3) Store output of get_SEL.sh in /tmp/file1.\n     4) Add a line to file1 with non-matching GUID, incrementing ID and time.\n     5) Modify get_SEL.sh to output content of file1.\n     6) Start up MS.\n     7) Wait for 'Alert message is empty, skipping...' in ms-odl.trc.\n     8) Change cell trace level back to fine.\n     9) Shut down MS.\n     10) Modify file1 by adding another line with matching GUID.\n     11) Start up MS.\n     12) Wait for IO hang alert on expected celldisk in ms-odl.trc.\n     13) Restore get_SEL.sh.\n     14) Remove /tmp/file1.\n\n\n\n    MODIFIED  (MM/DD/YY)\n   budkumar 10/31/25   - Creation",
    "platform": null
  },
  {
    "test_name": "tsagbug38397771.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug38397771.tsc - testcase for bug 38397771\n\nTest Overview: This test validates the behavior and recovery of SYSEDS\n     fence metadata files when they are deleted after an ERS restart\n\nSteps to Reproduce:\n      - Identify the Exascale node ID of ERS (for example, node ID 81).\n      - Locate and open any ERS trace file under trace dir to confirm the\n        assigned node ID.\n      - Restart the ERS service to trigger EGS to broadcast a fence to SYSEDS\n      - In the SYSEDS fence directory, find the fence metadata file\n        corresponding to the target node ID (e.g., fence_metadata_197).\n      - Manually remove the .copy file and empty the .header file for the\n        identified metadata.\n      - Restart the ERS service again to trigger another fence event.\n      - Verify that SYSEDS remains active and that the previously removed\n        metadata files (.copy and .header) have been recreated and repopulated\n      - Check system logs for an ORA-00700 error indicating recovery of the\n        fence metadata file.\n      - Confirm that the copy1 or copy2 file and the header file for the\n        concerned metadata are present and populated in the fence directory.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38414502.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug38414502.tsc - Test for bug 38414502\n\nDue to bug 38414502, if MS restarts during a disk failure on a multi-FDOM\n     flash card, only one FDOM may get the failure flag. This leaves FDOM states\n     inconsistent, causing the flash card to be stuck in \"powering off\" and preventing\n     a failure alert. The fix ensures all FDOMs correctly track pending failures or\n     degradations, even after an MS restart.",
    "platform": null
  },
  {
    "test_name": "tsagbug38460835.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug38460835.tsc - Test for Bug 38460835\n\nPlease see below\n\nAdded in lrgsaexacldeds",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38481864.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug38481864.tsc - Test for BUG 38481864\n\nTest for Bug 38481864 - LNX64-26.1-EXASCALE,EGS KEEP CRASHING WITH\n     ORA-00600[EGSCRCELL::PROCESSCONFIG:MISMATCHEDDROPPERM]",
    "platform": null
  },
  {
    "test_name": "tsagbug38485623.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug38485623.tsc - Bug Test for bug 38485623\n\nThis test verifies that stale and active MS pid files are properly\n   handled, ensuring any leftover pid file is removed before service startup,\n   confirming pid file cleanup on graceful shutdown, and validating that critical\n   files on the DB node have correct permissions for secure and reliable operation.#",
    "platform": null
  },
  {
    "test_name": "tsagbug38509417.tsc",
    "setup": "tsaginit",
    "flags": {
      "oss_no_asmdb": "true"
    },
    "description": "tsagbug38509417.tsc - Functional testcase for bug 38262726 -\n        ORA-600 [DISKIOSCHED::GETNEXTREQSTOLAUNCHFLASH:MAX_CONC_IO]\n\nFunctional testcase for bug 38262726",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagbug38512040.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagbug38512040.tsc - Bug Test for bug 38038546\n\nThis tests DB management RPM install and dbmcli behavior with invalid,\n   old, and fake JAVA_HOME values, ensuring tools handle misconfiguration\n   and work as expected",
    "platform": null
  },
  {
    "test_name": "tsagbug38512915.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "egstrcdir": "^ADR_BASE^^s^diag^s^EXC^s^exc^s^^HOSTNAME^^s^trace^s^"
    },
    "description": "tsagbug38512915.tsc\n\nTest for BUG-38512915 : EXASCALE: EDS SHOULD RAISE WARNING OTHER THAN MARKING \"BAD DIRECTORY STATE\" IF THE MEMORY BUDGET LIMIT IS TOO SMALL FOR LOADING FILE METADATA\n\nSteps followed in test :\n\n      1. Modify _eds_server_dir_cache_size_limit_MB to 10 to reset usreds's memory limit from the default 2000MB to 10MB\n      2. Restart USREDS on cell 1\n      3. Shutdown USREDS on cell 2 and cell 3\n      4. Run create file workload using exaloadgen\n      5. Verify we have warning like \"CLUSTER_ALERT.*CALRT-00101.*USREDS.*MemoryMonitor-MetadataStore\" in cell alert logs\n      6. Run create file workload again using exaloadgen but with different vault, to make sure we can still allocate memory\n      7. List datasets for both the vaults, should be listed correctly\n      8. Cleanup : Remove both the vaults, and startup the USREDS on cell2 and cell 3",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38555826.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagbug38555826.tsc - Test for 38555826\n\nPlease see below\n\nAdded in lrgsaexacldeds",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagbug38584590.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": null,
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcdos_flood.tsc",
    "setup": "xblockini",
    "flags": {
      "ledvsetup": "true",
      "setup_blockstore": "true",
      "edvcleanup_only": "1",
      "skip_tsagend": "true"
    },
    "description": "tsagexcdos_flood.tsc - flooding ddos test for all exascale services\n\nTest for flooding ddos attack on all exascale services for bug-38511517\n\nThis test does a flooding ddos attack using malformed packets on the\n     following exascale services:\n     CELLSRV, EGS, ERS, ESNP, IFD, MS, SYSEDS, USREDS, BSM, BSW, EDV",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcegsdiskrebuild.tsc",
    "setup": "tsagexastackup",
    "flags": {
      "sage_mirror_mode": "high",
      "max_rpm": "20000",
      "delta_sync": "true",
      "exc_normal": "disable",
      "exc_in_performance_testing": "true",
      "oss_multims_testing": "true"
    },
    "description": "tsagexcegsdiskrebuild.tsc - TEST For EGS_DISK_REBUILD Feature\n\nThe txn tests the EGS_DISK_REBUILD Feature\n     The EGS_DISK_REBUILD feature restricts each disk to be used for the\n      rebuild COPY operations of the earliest disk failure. This way, there\n      should be less disk resource contention because a smaller number of\n      disk IOs will be issued to each disk at a given time.\n\n     Testing deals with simulation of failures on multiple disks and ensure\n      the disk rebuild operations are based on the timestamp of failure one\n      by one with the help of script - tsagexcrebuildtest.sh\n\n     In summary, the rebuildtest.sh checks to ensure that any REBUILD COPY\n     operations that belong to a later disk failure event must be\n     processed after all REBUILD COPY operations that belong to an earlier\n     disk failure event have been processed, for a given disk that is being\n     used as either the source disk or the target disk of the REBUILD COPY\n     operations. Specifically, there are some operations that are force\n     started due to some dependency logic. These force started operations\n     are ignored.",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcerssecurity_zap1.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcerssecurity_zap1.tsc - security test on ERS endpoint using ZAP\n\n--------------------------------------------------------------------------\nZAP Automated Security Test:\nThis script automatically runs a full security scan on API endpoints.\nIt installs Java + ZAP if needed, sets up SSL trust, and runs:\n  1. Spider   - explores and maps all hidden URLs.\n  2. Active Scan - sends crafted attacks (SQLi, XSS, etc.) to find issues.\n  3. Fuzzer   - injects malicious payloads to test input validation.\nIt authenticates via admin:welcome1, fetches Bearer tokens dynamically,\nruns all endpoints from a YAML file, and generates HTML + JSON reports.\n--------------------------------------------------------------------------",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcerssecurity_zap2.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcerssecurity_zap2.tsc - SECURITY TESTING ON ERS ENDPOINTS USING ZAP\n\n# --------------------------------------------------------------------------\nZAP Automated Security Test Summary\nThis script automatically runs a full security scan on API endpoints.\nIt installs Java + ZAP if needed, sets up SSL trust, and runs:\n  1. Spider   - explores and maps all hidden URLs.\n  2. Active Scan - sends crafted attacks (SQLi, XSS, etc.) to find issues.\n  3. Fuzzer   - injects malicious payloads to test input validation.\nIt authenticates via admin:welcome1, fetches Bearer tokens dynamically,\nruns all endpoints from a YAML file, and generates HTML + JSON reports.\n--------------------------------------------------------------------------",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcerssecurity_zap3.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcerssecurity_zap3.tsc - SECURITY TESTING ON ERS ENDPOINTS USING ZAP\n\n--------------------------------------------------------------------------\nZAP Automated Security Test\nThis script automatically runs a full security scan on API endpoints.\nIt installs Java + ZAP if needed, sets up SSL trust, and runs:\n  1. Spider   - explores and maps all hidden URLs.\n  2. Active Scan - sends crafted attacks (SQLi, XSS, etc.) to find issues.\n  3. Fuzzer   - injects malicious payloads to test input validation.\nIt authenticates via admin:welcome1, fetches Bearer tokens dynamically,\nruns all endpoints from a YAML file, and generates HTML + JSON reports.\n--------------------------------------------------------------------------",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcrestfuzz_auth.tsc",
    "setup": "tsagexastackup",
    "flags": null,
    "description": "tsagexcrestfuzz_auth.tsc - Test for JWT token fuzzing using RestFuzz\n\nThis is a RESTFuzz test for Fuzzing the JWT bearer token\n       in the authorization path of ERS",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagexcvoldelsnapdelreacecond.tsc",
    "setup": "xblockini",
    "flags": {
      "iscsi": "true"
    },
    "description": "tsagexcvoldelsnapdelreacecond.tsc\n\ncheck the fix for a rare race scenario between volume delete, volume snapshot delete and bsw restart\n   more details :ssprasad_rti-32062409\n   https://orareview.oraclecorp.com/transaction/ssprasad_rti-32062409",
    "platform": "EXASCALE"
  },
  {
    "test_name": "tsagossmalloc.tsc",
    "setup": "srdbmsini",
    "flags": null,
    "description": "tsagossmalloc.tsc\n\nTest the basic ossmalloc functionality:\n        - configure num_frames parameter in ossmalloc.conf\n        - restart cellsrv to use the new param\n        - run a simple workload that will use some memory\n        - check the ossmalloc heap summary to confirm we are showing the\n          correct number of frames\n        - dynamically change monitor threshold in ossmalloc.conf\n        - (no restart needed)\n        - run the workload a few more times to use even more memory\n        - check the heap summary to confirm that some stacks are\n          now monitored (tagged with '<!>')",
    "platform": "EXADATA"
  },
  {
    "test_name": "tsagrhcustomssh.tsc",
    "setup": null,
    "flags": null,
    "description": "tsagrhcustomssh.tsc\n\nTest execution for custom shell on single cell\n\nhttps://confluence.oraclecorp.com/confluence/display/~rishideep.rallabandi@oracle.com/Test+spec+for+Custom+Shell",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagxrmem_forcenocache.tsc",
    "setup": null,
    "flags": {
      "num_flash_per_cell": "2",
      "flash_size": "256",
      "num_gd": "2",
      "gdsz": "256",
      "creatdev_file": "tkfgmydef"
    },
    "description": "tsagxrmem_forcenocache.tsc - functional tests to ensure certain iohints\n                                  and filetypes to NOT land on XRMemCache\n\nThe general test steps are:\n     1. start IO workload with iohint KG_ASM_STALE_FILE and filetype\n        ASM Stalefile\n     2. verify data is NOT cached on XRMemCache\n     3. start IO workload with iohints and filetypes which are not on the\n        force no XRMemCache list\n     4. verify data can be cached on XRMemCache",
    "platform": null
  },
  {
    "test_name": "tsagrh_12min_timeout.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_12min_timeout.sh\n\ntsagrh alerts after 12 min once roce switch is rebooted",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_acfs_post_volume_test.sh",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_acfs_pre_volume_test.sh",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_aep.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_aep.sh - Add testcases for AEP on Real HW\n\nDetails about AEP - https://confluence.oraclecorp.com/confluence/x/pj2ddgE\n     Details about AEP Tests - https://confluence.oraclecorp.com/confluence/x/J_JFDgI",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_alerthistory_allnodes.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_alerthistory_allnodes.sh\n\n1.) Save alerthistory for all nodes in alerthistory_allnodes.lst\n    2.) Send email if anything critcal or warning found in alerthistory\n\nHow to use : sh tsagrh_alerthistory_allnodes.sh <cell_nodes> <compute_nodes> <cluster_nodes> <user_who_send_email> <lrgname> <txn_name> <label>\n\n   MODIFIED   (MM/DD/YY)\n   sgangu      09/28/25 - Setup passwordless ssh for NAT envs\n   dpant       09/25/25 - add changes for jumphost based setup\n   suragraw    06/23/25 - Add file for success/failure case\n   suragraw    05/23/25 - Revamp the code to run in parallel for all nodes.\n   saheranj    04/29/25 - Change to remove INFO alerts in x9upgrade alert email\n   suragraw    04/02/25 - Add clusteralert alerts check\n   suragraw    07/25/24 - Check for Critical and Warning Alerts\n   suragraw    08/17/22 - Add Farm JOB ID and fix cleared alerts\n   pkuttam     08/12/22 - Add mail alert for exadata_realhw_mail_alerts_ww_grp\n                          for non-QR real HW LRGs\n   lknguyen    06/28/22 - send_mail only critical alerts\n   sajkansa    05/04/22 - Fixed filter logic of critical alerts\n   sajkansa    03/25/22 - Added filter fault alerts function\n   sajkansa    03/02/22 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_asan_traces.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_asan_traces.sh - Install asan RPM and its related dependencies from view.\n\nThis file installs the asan RPM from view along with\n     required libasan dependencies on storage nodes.\n     After installation, it restarts cellsrv services to enable traces.\n\nRun this script with a comma-separated list of cell nodes as argument.\n     Example:  ./tsagrh_asan_traces.sh \"cell01,cell02,cell03\"\n\n   MODIFIED   (MM/DD/YY)\n   arassing    09/24/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_assign_rdbms_label.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_assign_rdbms_label.sh - Script to assign RDBMS Label for Real hw lrgs",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_blockstore_afterupgrade.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_blockstore_afterupgrade.sh - script for block store sanity test after upgrade is completed\n\nAfter upgrade workload sanity test\n     1) create snapshot using volumegroups\n     2) Create backup and restore after upgrade\n     3) clone of volume after upgrade\n     4) 3 level incremental backup with data",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_blockstore_beforeupgrade.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_blockstore_beforeupgrade.sh - Involves setting up, managing\n       like vaults, volumes, volumegroup beforeupgrade to constant variable\n\nAdd a Blockstore control operations workload which runs before rolling upgrade.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_blockstore_functions.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_blockstore_functions.sh - Involves setting up, managing, and cleaning up resources\n       like vaults, volumes, snapshots, and backups\n\nAdd a Blockstore control operations workload which runs in parallel with rolling upgrade.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_blockstore_workload.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_blockstore_functions.sh - Involves setting up, managing, and cleaning up resources\n       like vaults, volumes, snapshots, and backups\n\nAdd a Blockstore control operations workload which runs in parallel with rolling upgrade.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug34557348.sh",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug35338688.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug35338688.sh - PURGE FILES WHEN INODES USAGE EXCEEDS THRESHOLD\n\nAdd functional test to validate the changes in bug 35338688\n     Fix description:\n      Besides the FS space usage check, MS needs to regularly (every 30 minutes\n      by default) check the Inodes usage for each mount, and purge the files if the\n      usage is over 80%. For testing purpose, setting files purge time to 3 minute",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug35824385.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug35824385.sh\n\nThis is a testcase to check net.ipv4.ip_forward consistency on KVM hosts pre/post upgrade (Bug #35824385).\n     But net.ipv4.ip_forward should be set to 1 only on NAT'd system (Bug #35852020).\n     For non NAT deployment vm_maker will correct this if it was set wrong, post upgrade when vm_maker kicks\n     in it sees that the setting is wrong for this machine and corrects it\n\n     Testcase brief:\n       Before the compute upgrade, we set parameters as described in Bug testcase.\n        Set in-memory net.ipv4.ip_forward = 1\n        Set in-disk net.ipv4.ip_forward = 0\n\n       Upgrade KVM Hosts\n\n       After Upgrade check for in-memory net.ipv4.ip_forward, should be 0 for non Nat'd system\n        Check in-memory net.ipv4.ip_forward = 0\n\nRelated Bugs: 36130407, 36764199\n\n   MODIFIED   (MM/DD/YY)\n   sadwe       10/24/24 - Fix diff due to 25.1 image version\n   sadwe       08/20/24 - Update the behaviour post upgrade net.ipv4.ip_forward=1 for non NAT'd system\n   sadwe       07/26/24 - Update location of /etc/sysctl.conf for 24.1 to /etc/sysctl.d\n   sadwe       12/19/23 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug36263733.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug36263733.sh - test script for bug 36263733 (aka 35496061)\n\nThis test verifies the proper functioning of the vm_maker utility's bridge management feature\n     on KVM guest nodes. Specifically, it validates the creation and removal of a network bridge with\n     a VLAN, ensuring that bridge removal works correctly even if the bridge definition file has been\n     renamed. The test is applicable to all SE branches and both fresh and upgraded environments.\n\nSteps to Reproduce:\n      - Use vm_maker to create a guest VM with a bridge on a guest node [EXPECT PASS].\n      - Use vm_maker to add a single bridge with 'vm_maker --add-single-bridge [BRIDGE_NAME] --vlan [VLAN_ID]'\n          on the guest node [EXPECT PASS].\n      - Locate a bridge definition file under /EXAVMIMAGES/GuestImages/[GUEST_NAME];\n          rename 'vmbondeth0.xml' to 'vmbondeth0.[NEW_VLAN_ID].xml' on the guest node [EXPECT PASS].\n      - Use vm_maker to remove the bridge with 'vm_maker --remove-bridge [BRIDGE_NAME] --vlan [VLAN_ID]'\n          on the guest node [EXPECT PASS].\n\n   MODIFIED   (MM/DD/YY)\n   kuashen     09/08/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug37582178.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug37582178.sh - test case for M2 Sytem failure and cell node power cycle\n\ntest case for M2 Sytem failure and cell node power cycle",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug38224682.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug38224682.sh - Validate IFB and TC bandwidth control setup\n\nThis test automates the configuration and verification of network\n     traffic shaping on compute nodes using Linux 'tc' (traffic control)\n     and IFB (Intermediate Functional Block) interfaces.\n\n- Sets up egress (upload) and ingress (download) bandwidth limits\n       on eth0 using HTB and IFB devices respectively.\n     - Verifies that tc qdiscs and classes are configured correctly.\n     - Checks runtime counters to ensure shaping is active.\n\n   MODIFIED   (MM/DD/YY)\n   kuashen     10/09/25 - Creation\n\nSource helper and logging functions",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_36110067.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_36110067.sh - RH test for bug 36110067 and bug 34096679\n\nTest 36110067 follows below flow:\n     Step1 - Add vmbogus.xml file with junk in it on one of the guest node.\n     Step2 - Shutdown all guest nodes\n     Step3 - Start all guest nodes\n\n     Test for bug 34096679-PLEASE ADD NIGHTLY KVM OVERSUBSCRIPTION\n        TESTS FOR BUG 33952779\n     Step1 - Stop guest vm\n     Step2 - Try to oversubscript the vcpu count for guest vm\n     Step3 - Check that over subscription should fail\n\nRefer to bug 36110067\n\n   MODIFIED   (MM/DD/YY)\n   schavhan    06/17/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_37203698.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_37203698.sh - validation of \"vm_maker --list-migration --detail --id %d\"\n\nThis test is to validate the vm_maker --list-migration --detail --id %d along with Xinaming's\n   fix on bug 37203698. This test performs a forward migration from source to dest, checks the id\n   and use vm_maker --list-migration --detail --id %d to compare the migration completion time vesus\n   the last phase end time on both source and destination hosts, and it then performs a backward migration\n   and validate the same.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_37219761.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_37219761.sh - ESNPSRV DIMM UE HWPOISON Test\n\nPerform DIMM UE HWPOISON Fault on ESNPSRV - Bug 37219761\n     Validate the corrupted segments are released and\n     Veridy new instance of ESNPSRV has booted\n\nMODIFIED   (MM/DD/YY)\n   sadwe       12/16/24 - Add '-o ServerAliveInterval=10 -o ServerAliveCountMax=4' in ssh edstool list\n   sadwe       11/13/24 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_37251550.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_37251550.sh - Real hardware test for Bug 37251550\n\n1. Stop EDV guests on kvm host\n        (perform step 2 and step 3 30 times, until we see the error)\n     2. DBMCLI -e ALTER DBSERVER SHUTDOWN SERVICES ALL\n     3. DBMCLI -e ALTER DBSERVER STARTUP SERVICES ALL\n     4. Start EDV guests on kvm host\n\nUsage: sh tsagrh_bug_37251550.sh <kvm_host>\n\n   MODIFIED   (MM/DD/YY)\n   arassing    01/29/25 - Test will run after drop_cell test\n   sadwe       12/18/24 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_37256860.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_37256860.sh - RH CELLSRV DIMM UE HWPOISON Test\n\nPerform DIMM UE HWPOISON Fault on CELLSRV - Bug 37256860\n     Validate the corrupted segments are released and\n     Veridy new instance of CELLSRV has booted\n\nShared Memory Key used: 0x72324112, 0x0025fb92\n\n   MODIFIED   (MM/DD/YY)\n   sadwe       10/27/25 - Drop RS-700 [CELLSRV START time exceeded] - Bug:38208903\n   sadwe       07/23/25 - Add Test 3 for allechan_bug-38218716, look for power cycle alert\n                        - Restart RS after Test 1 so RS will not power cycle cell\n                        - Drop RS-700 [CELLSRV RESTART time exceeded] - Bug:38208903\n   sadwe       04/01/25 - Add code to poison all pfn at once and unpoison in end\n                        - Add cap to poison first 5 address attached to EGG key.\n   sadwe       02/17/25 - Add code in Test 2 to drop expected ablerthistory\n   sadwe       11/12/24 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_37494873.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_37494873.sh - RH functional test for Bug 37494873.\n\nPreupgrade we need to have more then 6 disk attached to a domain\n     grep for lsilogic in the /etc/libvirt/qemu/<guest-name>.xml directory.\n     The presence of the lsilogic controller component is critical to reproducing the issue.\n\n     Upgrade should be successful without pci device missing (Refere Bug: 37494873)\n     Post upgrade check if lsilogic and pcie-to-pci-bridge cotroller should be removed\n\nMODIFIED   (MM/DD/YY)\n   sadwe       03/20/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_37680823.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_37680823.sh - This test case is for Buug-37680823\n    exascale on cc/cs : escli should be blocked on roce network from guest vm\n\nThis script automates the validation of ESCLI connectivity between\n    compute and cell nodes in an Exadata environment. It executes escli lscluster\n    commands over multiple control ports (5052/8080).",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_37894213.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_37894213.sh - Test Case for\n      Bug 37894213 - EXADATA-DBPROC-BIND SHOULD TARGET ALL LMS THREADS\n\nThreads with running status ora_lms.* or ora_lm[1-9].* or ora_rs[0-2].*  or ora_lgwr.* or ora_vktm.* or ora_ctwr.*,\n   check the exadata-dbbind-proc show_dbproc_affinity to make sure it appears in the list.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_38081703.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_38081703.sh - Check rocelinkinfo after switch SSH setup after `sudo su` on a non-root user\n\nThis script is related to bug 38081703. It ensures that running\n     rocelinkinfo works correctly after setting up switch SSH equivalency\n     from a root shell. The issue occurred when setup was done as a non-root\n     user and then rocelinkinfo failed even after switching to root.\n\n   REPRO STEPS (prior to fix):\n     1. ssh <non-root user>@scaqai13adm07.us.oracle.com\n     2. sudo su -    # switch to root\n     3. /opt/oracle.SupportTools/ibdiagtools/setup_switch_ssh_equiv.sh <switch_list>\n     4. /opt/oracle.SupportTools/ibdiagtools/rocelinkinfo\n\n     The issue was that rocelinkinfo would still fail even when run as root if\n     setup_switch_ssh_equiv.sh was run after switching to root from a non-root user.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_38139496.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_38139496.sh - Verify each disk has exactly one of Target or Serial in vm_maker output\n\nThis test checks that every disk entry returned by 'vm_maker --list --disk --detail' for a domain\n     contains either a Target field (e.g., sda, sdb) or a Serial field (cluster volume name), but not both\n     and not neither.\n\n- Domain suffixes vm01 and vm02 must exist for the host prefix.\n\n   MODIFIED   (MM/DD/YY)\n   kuashen     07/02/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_38338865.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_38338865.sh - Resilver metadata handling during image upgrade\n\nFunctional test for TXN vtambi_bug-38338865.\n     Before image upgrade, creates a dummy file in each of $OSSCONF/lostdata,\n     $OSSCONF/scrub, and $OSSCONF/resilver directories on a cell node.\n     After upgrade, verifies these dummy files still exist in their respective directories.\n     The test fails if any dummy file is missing post-upgrade.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_call_blockstore.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_call_blockstore.sh - Call Blockstore workload for real H/W lrg\n\nMaintains LRG-specific Blocsktore workload flow, customizable per LRG requirements.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_celld.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_celld.sh - Celld Test on Real HW\n\nThe test helps to check for the max locked memory limit to prevent running\n    into ipcdat_create_cqs error on the cell node.\n    The check is after different scenarios- Restarting EGS, RS services",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_celld_dbserverd.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_celld_dbserverd.sh - Driver file to run Celld and dbserverd Test\n\nThis is a helper script to run the celld and dbserverd test.\n     Testcase for Bug-34098092",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_celld_helper.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_celld_helper.sh - Driver file to run Celld test.\n\nThis is a helper script to run the celld test.\n     Testcase for Bug-34098092.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_cellfail_maa.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_cellfail_maa.sh - restart controller 0 on the cell\n\nFault injection script (taken from MAA scripts)\n     restarts controller 0 using storcli",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_cellilomstart_maa.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_cellilomstart_maa.sh - Real Hardware script to start Cell node via ilom cmd\n\nCell node startup via ilom command to make it online",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_celloffline_maa.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_celloffline_maa.sh - Real Hardware script for Cell Shutdown\n\nCell node shutdown command to make it offline",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_cellrebot_maa.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_cellrebot_maa.sh - Real Hardware script for Cell Reboot\n\nReboot a cell node's all services by issuing a restart command",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_change_flashcachemode.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_change_flashcachemode.sh - RH test for Bug: 38157786\n\nThis script validates the exascale scale-out operation with\n     different flashCacheMode param for cluster cells and scale-\n     our cells. Following config has been tested in this script:\n     1. Cells already part of the cluster should have\n        flashCacheMode: WriteBack\n     2. Cells used for scaleout operation should have\n        flashCacheMode: WriteThrough\n     Then perform scale-out/Add cell operation using OEDA.\n     OEDACLI > ALTER CLUSTER ADDCELLS=<> WHERE CLUSTERNUMBER=<>\n\nJira: Exadata Exascale Testing Team / EXESL-508\n\n   MODIFIED   (MM/DD/YY)\n   sadwe       07/11/25 - Add change to have cell4: WriteBack and cell5: WriteThrough\n   sadwe       07/08/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_check_ers_attrs.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_check_ers_attrs.sh - To check the value populated by restBackendServerList.\n\nAfter the cell expansion ,This is the test to check the esnode attributes restBackendServerList\n     only on cells where ers is running.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_check_ghost_egs.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_check_ghost_egs.sh - To check the ghost egs after restart of egs leader in exaprov.\n\nDeploy Exascale on a 1/4 rack using OEDA, then scale out the cluster by adding three more cells.\n     Verify there are no ghost EGS services. dentify the EGS leader with , restart it,\n     and confirm the leader has changed. Check again ,where ghost EGS services should now appear",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_check_iops.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_check_iops.sh - This is the functional test case for zalrmaih_bug-38086910\n     To check the no. of IOs are landing on QLC\n\nScript creates vault, volume, attaches to compute, runs Orion and checks IOPS range.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_check_services.sh",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_check_sudo.sh",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_chk_guest_gid.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_chk_guest_gid.sh - Check Guest node's GID table content\n\nThis script gets the GID table content from guest/DB nodes passed in and\n     generates a output file for the content of the GID.  The file name will be\n     append with either a \"pre\" or \"post\" state to note when the file was captured\n     either before or after the ROCE switch reboot",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_clean_pmem.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_clean_pmem.sh\n\nThis script deletes pmemlog, pmemcache\n     and re create pmemlog and pmemcache\n\nMODIFIED   (MM/DD/YY)\n   sajkansa    02/18/22 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_clone_basedb_vm.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_clone_basedb_vm.sh - RH test script for clone basedb vm\n\nThis script pefroms following steps to create a basedb vm clone:\n     Prereq:\n     Create a single guest XML, with network IPs for the cloned guest, and convert the XML to a basedb config.\n     This is a edv volume level clone of a basedb virtual machine\n     Steps:\n     1. List the volume details that will be used by the cloned guest\n     2. List the volumes used by the parent guest that will be used for snapshot and clone\n     3. Shut down the parent and guest for a consistent Snapshot\n     4. Create snapshots of volumes used by the parent guest listed in Step 3, for both system and basedb volumes\n     5. Use the snapshot created above to create volume clones and name them the same as listed in Step 2\n     6. Attach Cloned volumes using the same deviceName as listed in Step 2 with KVM host initiator of the parent KVM host\n     7. List the devices to verify if those are reflected on the target KVM host\n     8. For install.sh to reuse the EDV volume without recreate it, set up the property REUSEEDVVOLUMEIFEXIT=true for non-cloud\n     9. Finally, run the Create Basedb step using install.sh to create cloned guest\n\n     We also perform database clone validation here, we write some sql data to parent VM before the cloning\n     and we validate the DB on the cloned VM by looking for same data is accessible from cloned VM.\n\n     Addtional Bug tests added as part of the automatiom:\n     37916820, 38225309 (38199281)\n\nWiki: https://confluence.oraclecorp.com/confluence/x/ofYixgM\n\n   MODIFIED   (MM/DD/YY)\n   sadwe       09/13/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_create_awr_report.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_create_awr_report.sh - Script to create db snapshot and awr report",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_db_workload.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_db_workload.sh - helper script to rh_run_workload.sh\n\nThis is the main helper script to rh_run_workload.sh that actually do the work\n#    It has functions to setup, run and stop the workload using sql files.\n\nIntended to be used by rh_run_workload.sh on remote compute node\n      after OEDA deployment\n\n   MODIFIED   (MM/DD/YY)\n   prabajpa    01/15/25 - Changes to check for aide alerts\n   adasokan    11/14/23 - fix user/pass for asm on x7\n   lknguyen    12/11/22 - add inmemory_size\n   suragraw    08/22/22 - Add function for Enabling EGS Tracing\n   suragraw    08/26/22 - Add function for Set_IO_Max Size\n   suragraw    08/26/22 - Add function for PDB open-close Loop\n   msubrahm    07/11/22 - Add functions for GI FNDD tests\n   sajkansa    05/13/22 - Added clone_pdbs function\n   suragraw    03/04/22 - Resolve merge conflicts\n   sajkansa    01/31/22 - Passed pdbs as variable and shifted pdb creation logic\n   pbahl       01/27/22 - Restart DB after processes increase\n   suragraw    01/23/22 - Fix intermittent db workload failure\n   lknguyen    01/21/22 - Add support running workload from localhost\n   suragraw    10/21/21 - add functions to maximise DB workload\n   suragraw    01/04/22 - Add TPCH workload on all PDBs (20)\n   adasokan    11/10/21 - Add generate_cdb_quarantine\n   suragraw    10/04/21 - Add enable_wallet function\n   adasokan    09/28/21 - Add asmqueryredundancy\n   suragraw    07/20/21 - Fix DB workload errors by adding wait\n   suragraw    07/08/21 - Check workload errors in every iteration\n   suragraw    07/13/21 - Increase sleep time in setup for table creation\n   suragraw    07/08/21 - Check workload errors in every iteration\n   suragraw    05/19/21 - Update setup function to pass-in vault_name\n   mwhwa       05/12/21 - Add function to collect RDMA stats\n   suragraw    05/10/21 - Add function for DB_shutdown\n   pbahl       04/14/21 - parallel pdb setup\n   suragraw    03/23/21 - Added more queries and tables on db workload\n   mwhwa       02/16/21 - Update setup function to pass-in oracle_sid for wallet setup\n   pbahl       02/14/21 - Add asmquery\n   suragraw    01/11/21 - Modify the paramters for rh_run_workload.sh\n   pbahl       10/20/20 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dbserverd.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_dbserverd.sh - DBserverd test on real HW\n\nThe test helps to check for the max locked memory limit to prevent running\n    into ipcdat_create_cqs error on the compute node.\n    The check is after different scenarios- Restarting EDV, RS services",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dbserverd_helper.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_dbserverd_helper.sh - Driver file to run dbserverd Test\n\nThis is a helper script to run the dbserverd test.\n     Testcase for Bug-34098092",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dcli_test.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_dcli_test.sh - Test dcli command on cell and compute nodes\n\nTest added as part of dev txn rohansen_bug-35828305 changes\n      to test dcli cmd on cell and compute behind jumphost",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_diagpackc.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_diagpackc.sh - RealHardware DIAGnostic PACKage Cleanup\n\nThe script is to clean up the user and role created in the preperation phase.\n     This is required so that next time the test is run, we will not get error for existing user/role error.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_diagpackp.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_diagpackp.sh - RealHardware DIAGnostic PACKage Preperation\n\nThis script prepares the specific cell node to genreate diagpack.  The node info is passed by cell_conn\n       1. Create user and grant permission\n       2. Generate the diagpack\n       3. Wait for 120 sec to make sure diagpack file gets generated",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_diagpackv.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_diagpackv.sh - RealHardware DIAGnostic PACKage Validation\n\nThe script uses Expect to perform EXACLI command to list the diagpack on one specific cell node(scas11celadm12)\n     to make sure a new diagpack is generated successfully",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dimm_poison.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_dimm_poison.sh - Real Hardware fault injection test for DIMM in compute node\n\nThis test injects failure into compute/guest node's DIMM and create guest node crash.\n     After the crash caused by the DIMM failure, we verify the guest node status and check\n     alerts and diagpack generation.  After that, we restart the guest node for recovery",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dimm_poison_diagpack.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_dimm_poison_diagpack.sh - Test to check Diagpack Contents\n\nTest checks the Contents of the Diagpack generated from Dimm Test",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dimm_ue_egssrv.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_dimm_ue_egssrv.sh - RH DMM_UE fault test on EGSSRV\n\nThis test injects DIMM_UW fault on the EGSSRV using hwpoison utility\n     Follows below steps:\n     1. Disable secureboot and reboot Cell1 to apply changes\n     2. Make the EGS Leader to switch to Cell1\n         2.1- Shutdown EGS servers on two cells (do not shutdown EGS on node X).\n         2.2- Wait 5 seconds.\n         2.3- Shutdown EGS servers on two remaining nodes (do not shutdown EGS on node X).\n         2.4- Startup EGS on the two cells from step (1).\n         2.5- Wait 5 seconds -> Node X should become the leader.\n         2.6- Startup the remaining EGS servers.\n     3. Inject fault to physical address of EGS\n         3.1- Get EGSSRV PID\n         3.2- Fetch virtual adddresser mapped to PID\n         3.3- Convert virtual address to physical addresses\n         3.4- Echo addresses to /sys/kernel/debug/hwpoison/corrupt-pfn\n     5. Restart CELLSRV to trigger I/O's on EGS\n     6. Verify if EGS is stared with new PID\n     7. Enable secureboot and reboot Cell1 to apply changes\n\nIf there are any crashes seen in the stimulation then watson should catch them.\n\n   MODIFIED   (MM/DD/YY)\n   sadwe       10/27/25 - Increase sleep to 120 after DIMM fault, as alert generation\n                          sometimes take more time and cellcli drop alerthistory fail\n   sadwe       10/07/25 - Ignore redundancy while EBS & EGS shutdown\n   sadwe       01/10/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_disable_drl_cells.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_disable_drl_cells.sh - Script to disable DRL on Cloudservice LRGs",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_disable_secureboot.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_disable_secureboot.sh - Script to Disable Secureboot From Real HW Node",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_disk_confinement.sh",
    "setup": null,
    "flags": {
      "-n": "cdisknum=0"
    },
    "description": "tsagrh_disk_confinement.sh - cellsrv force powering off hung confined NVME disk\n\nWhen a NVME disk is confined hung, Cellsrv will force power it off.\n     This test simulates disk hang confinement and check if the power off\n     feature is triggered.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_dstate_freezer.sh",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_edv_fault_injection.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_edv_fault_injection.sh - Fault injeciton tests for VM on EDV\n\nBelow use cases are covered to inject fault on EDV\n       * On hypervisor while EDV is serving VM images and DB workload running\n           a) kill -9 edvsrv service on compute node\n           b) Verify that guests stay up and no DB workload errors\n       * Restarting CRS on guest node of a GI cluster while IO is occurring\n          on clustered EDV volume\n            - IO should continue uninterrupted on surviving guest node and\n              CRS, DB service should be up and running\n            - This validates proper fencing semantics of Exascale and GI.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_edv_resize_test.sh",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_egs_ddscvr.sh",
    "setup": null,
    "flags": {
      "-n": "gdisknum=0"
    },
    "description": "tsagrh_egs_ddscvr.sh - Script to test event DDSCVR_SYSTEM_DEATH",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_elu_executor.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_elu_executor.sh - script to generate testcase execution per json input\n\ntsagrh_elu_executor.sh calling the helper script elu_testcases_driver.py to generate <testcase_name>.sh file\n\t\t    it will invoke the <testcase_name>.sh to perform the test case steps per the json input.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_enable_secureboot.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_enable_secureboot.sh - Script to enable Secureboot on Real HW Node",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_escli_snapshot.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_escli_snapshot.sh - Script to create and drop escli snapshot in loop",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_escs_profile.sh",
    "setup": null,
    "flags": null,
    "description": "i     tsagrh_escs_profile.sh - Enable escs profile on all cell and guest node and verify using escs users\n\nsetup_escs_profile - Method to activate escs profile on all cell and guest node\n     check_escs_profile_users_access - Method to verify ssh access on all cell and guest node\n     using escs users escsmaadm,escsmamon\n     verify_passwordless_ssh - Private method to verify passwordless configuration for escs users",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_exaportmon_switch.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_exaportmon_switch.sh - This is a helper script for\n\t\texaportmon tests.\n\nIt has following functions:\n   1. port_shutdown - shuts down port on specified switch\n   2. port_startup - Start port on specified switch\n   3. remove_vlan - Remove vlan from specified port\n   4. add_vlan - Add vlan to specified port\n   5. remove_vlan_secure - Remove vlan from specified port for sf\n   6. add_vlan_secure - Add vlan to specified port for sf",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_excsfixpython.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_excsfixpython.sh - Exascale clould integration Fix python3",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_extract_cluster_info.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_extract_cluster_info.sh\n\nThis script extracts guest nodes\n\nMODIFIED   (MM/DD/YY)\n   suragraw    12/02/24 - Fix cluster_nodes Var to contain guests + App VM\n   suragraw    04/25/23 - Add variable domu_list which contains only guest\n   sajkansa    03/02/22 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_failfd_maa.sh",
    "setup": null,
    "flags": {
      "-n": "disknum=0"
    },
    "description": "tsagrh_failfd_maa.sh - Fails 1 FD + Write Error on 1 FD\n\nThis script is developed with help from scripts that MAA team uses\n     and is a combination of multiple scripts that they have for individual\n     failure.\n\nCurrent failures in this script:\n       1. Simulate BLOCK_IO WRITE Error on 1 FD\n       2. Simulate failure on 1 FD\n\n   MODIFIED   (MM/DD/YY)\n   saheranj    12/12/22 - Changes as per the new X5 hardware migration\n   sajkansa    04/02/22 - Added sleep to give gap b/w resilvering\n   mwhwa       09/09/21 - reduce loop to save execution time\n   juwawang    09/02/21 - add maxloop check for FC resilvering\n   mwhwa       03/02/21 - add 30s sleep after restart MS\n   mwhwa       02/15/21 - Update celldiskName to support X9\n   pbahl       02/02/21 - list CD/GD before & after\n   pbahl       11/17/20 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_fcstat.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_fcstat.sh - Real Hardware FlashCache STATS print out\n\nThis script print out the stats for flashcache before and after the workload\n     to verify the usage flashcache for all 3 cell nodes.\n          CellCLI> list cell detail | grep flashCacheMode\n          CellCLI> list griddisk where asmDiskGroupName=datac1 detail | grep cachingPolicy\n          CellCLI> list metriccurrent where objecttype=flashCache | grep FC_BY_USED\n          CellCLI> list metriccurrent where objecttype=flashCache | grep FC_IO_RQ_W_POPULATE\n          CellCLI> list metriccurrent | grep DB_FC_IO_RQ",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_fegsleader.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_fegsleader.sh - TSAG Real Hardware Find EGS LEADER script\n\nIn Real Hardware LRG, this script is to find the EGS leader node among 3 cell nodes\n      then set egs_leader in egs_leader.tsc\n\nUse this script as : sh tsagrh_fegsleader.sh <scaqae14celadm09,scaqae14celadm10,scaqae14celadm11>\n     Prereq: must have already setup passwordless SSH to cell nodes\n\n   MODIFIED   (MM/DD/YY)\n   mwhwa       10/24/21 - add ilom node info\n   mwhwa       07/23/21 - Add identification of non-egs leader node\n   mwhwa       04/01/21 - Script to find EGSLeader among 3 cells\n   mwhwa       04/01/21 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_fetch_debug_level_messages.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_fetch_debug_level_messages.sh - Test Case for Bug 37305410\n      /VAR/LOG/MESSAGES IS FILLED UP WITH USELESS SESSION MESSAGES,\n          RATE LIMITING KICKS IN CAUSING USEFUL MESSAGE LOSS\n        Fetch and analyze debug-level messages\n\nThis script checks if a specific process (exadata-dbproc-bind) is running and\n     retrieves debug-level messages from it. The messages are saved and analyzed\n     for further debugging.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_fetch_test_script.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_fetch_test_script.sh - This script is used to fetch tests\n          from the lrg farm run\n\nThis script is used to fetch comma sperated tests from oss_<lrg>_tests.\n     Create a tsc file(execute_fetched_tests), and include only those test\n     which are provided in the command.\n     (farm submit <LRG_NAME> -config \"RUN_TEST=<XYZ>,<ABC>,<DEF>\")\n\nMODIFIED   (MM/DD/YY)\n   arassing    06/05/24 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_find_mr_cache.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_find_mr_cache.sh - Extract the mr_cache value for order 2 and 3",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_fndd_diskmon.sh",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_functional_test_37873474.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_functional_test_37873474.sh - Functional for the txn vtambi_bug-37873474\n\nAfter an exadata-dbmmgmt upgrade, this file checks if the egs directory exists or not in the path /opt/oracle/dbserver_<old_version>/dbms/deploy/config. This test will work if the txn vtambi_bug-37873474 is present on the base label.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_gdinactive_maa.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_gdinactive_maa.sh - GD inactive and active\n\nTaken from MAA script -Added loop to GD inactive and active on cell",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_genoedacliinput.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_genoedacliinput.sh - Generate tsagoedacli.sh input file to change vcpu setting\n\nThis file generates the input file to tsagoedacli.sh to change vcpu setting for a specific guest node\n     passed in as first paramemter passed in\n     Ex:  sh tsagrh_genoedacliinput.sh <guest_node_name>",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_get_escli_locally.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_get_escli_locally.sh - This script is to download escli locally\n          and setting up oci for blockstore operations\n\nDownload escli locally and setup oci for blockstore operations",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_get_vmmaker_final_statistics.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_livemigration_final_statistics.sh - Script to capture live-migration statistics on compute node\n     and save it to /root/<node_name>_livemigration_final_statistics.lst\n\nThe script will take in a list of compute nodes comma separated list\n\nThe <node_name>_livemigration_final_statistics.lst will get copy back\n     to the $T_WORK/ after all live-migration completed from the LRG\n\n\n   MODIFIED   (MM/DD/YY)\n   dongyzhu    02/06/25 - Script to gather all live-migration IDs statistics\n                          from Compute node\n   dongyzhu    02/06/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_hac_testsuite.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_hac_testsuite.sh\n\nThis script covers host_acccess_control test modules and  bug tests.\n\nMODIFIED (MM/DD/YY)\n   sadwe     08/29/25   - Change 'opc' user to 'cuser' to avoid username coflict\n   sadwe     11/08/24   - Add change to run Bug 36262579 test on cell2\n   sadwe     10/27/24   - Add timeout and fix expect block for Bug 36262579\n   sadwe     09/09/24   - Add test for Bug 36611674 & 36262579\n   sadwe     06/11/24   - Add test for new option INACTIVE added in pam-auth\n   sadwe     04/19/24   - Add test for idle-timeout --stopidlesessionsec option\n   sadwe     03/02/24   - Add test for idle-timeout and testcase for Bug 36036942 & 36082502\n   sadwe     12/09/23   - Add test for ilom-configure, ilom-password, banner, sshciphers and testcase for Bug 34918838 & 34914691\n   sadwe     11/19/23   - Add test for pam-auth and testcase for Bug 35374320\n   sadwe     09/26/23   - Add test for password-policy and password-aging\n   sadwe     09/14/23   - Add test for access-export\n   sadwe     09/14/23   - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_incremental_backup.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_incremental_backup.sh - script for incremental backup workload during rolling ugrade\n\ncreates new vault -> creates new volume -> take snapshot1 -> take backup1 -> add data to vol ->\n     take snapshot2 -> take backup2 -> use backup2 to restore volume -> compare both vol with md5sum",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_inficheck.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_inficheck.sh - Infinicheck test the connectivity and bandwidth between nodes on the fabric\n\nThis file will perform Infinicheck test the connectivity and bandwidth between nodes on the fabric and it done by running the command /opt/oracle.SupportTools/ibdiagtools/infinicheck -g db_hosts -c cell_hosts with flags like -s, -b, -p.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_install_libasan_dependency.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_install_libasan_dependency.sh - Install libasan dependency to install asan RPM\n\nThis file is used to install libasan dependency on storage node\n     for asan RPM installation.\n\nUsed by tsagrh_asan_traces.sh for rhx7imonec\n\n   MODIFIED   (MM/DD/YY)\n   arassing    09/24/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_iohang_flash_maa.sh",
    "setup": null,
    "flags": {
      "-n": "fdisknum=0"
    },
    "description": "tsagrh_iohang_flash_maa.sh - script simulates failure on a flash disk\n\nscript simulates failure on a flash disk",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_iohang_maa.sh",
    "setup": null,
    "flags": {
      "-n": "fdisknum=0"
    },
    "description": "tsagrh_iohang_maa.sh - Simulates BLOCKIO_HANG on 1 FD and 1 CD\n\nModified MAA script for BLOCKIO_HANG to use in our env\n#      It simulates HANG on 1 FD and 1 HD 5 times in a gap of 2 mins",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_iohang_maa_x9.sh",
    "setup": null,
    "flags": {
      "-n": "fdisk2num=0"
    },
    "description": "tsagrh_iohang_maa_x9.sh - Real Hardware blockIO HANG test for X9\n\nModified MAA script for BLOCKIO_HANG to use in our env\n     It simulates HANG and STALL on 2 FlashDisk 5 times in a gap of 2 mins",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_iohang_physical_maa.sh",
    "setup": null,
    "flags": {
      "-n": "fdisknum=0"
    },
    "description": "tsagrh_iohang_physical_maa.sh - script simulates failure on two physical disks\n\nscript simulates failure on two physical disks",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_iorm_multivm_test.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_iorm_multivm_test.sh - IORM Test Improvement for onprem_multiVM\n\nAssign a size for each vault - (nothing can be unlimited).\n    Create 5 volumes assign to that vault and validate.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_iov_workload.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_iov_workload.sh - Create a vault and drl , no-drl files\n          to run IOV workload during online upgrade\n\nTransfer edstool, iov and iov libraries to the storage node and run iov workload\n       Phase 1 â Compute Node Execution:\n      Initially, the rolling upgrade begins with the Cell + BSW components. During\n       this phase, IOV workloads will be triggered from a compute node.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_iov_workload_functions.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_iov_workload_functions.sh - Create a common plugin for Compute\n     and storage node IOV Workload\n\nThis file contains all utilities to call IOV workload on compute and cell",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_kill_iov.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_kill_iov.sh - Kill the IOV workload on node\n\nKill Both DRL and Non-DRL IOV workload on node\n     Helper script used in tsagrh_iov_workload_functions.sh",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_label_status.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_label_status.sh - Real Hardware LRG to check if the current label is BROKEN",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_list_ers_ip.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_list_ers_ip.sh - Verifies that the result of command `escli lswallet --attributes restEndPoint` matches the expected ERS IP.\n\nThis script connects to the first compute node in the cluster, retrieves the restEndPoint value\n     from the EGS wallet using escli, and compares it against the expected ERS_IP. It is intended to\n     validate proper EGS registration in Exascale environments.\n\n     Steps:\n       1. Parse input arguments: log suffix and compute node list.\n       2. Skip execution if not running in an Exascale environment.\n       3. Connect to the first compute node.\n       4. Retrieve the restEndPoint value from escli lswallet.\n       5. Compare it with the expected global ERS_IP.\n       6. Log the result and return appropriate status.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_live_migration.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_live_migration.sh - Script to setup and prepare node with UDP for live migration\n\nThis script assumes that input cells compute_node\n     and guest/gi/db cluster info from $T_WORK/clusterinfo.sav\n     This script is based on the wiki https://confluence.oraclecorp.com/confluence/x/9_Set\n     In section VIRTIO Live Migration Steps (From PT_LM Branch - No Secure Fabric)\n\n   Steps :\n\n    1. Provision the env with PTLM label series, with database is up and operational\n       Reimaging the environment is done by doimageoeda.sh -xml  <xml file path>\n       In the current configuration file, the oeda deploy xml\n       2 compute nodes, 3 cell nodes, and 1 cluster.\n\n    ***   tsagrh_live_migration.sh starts from step 2 ***\n\n    2. Update RAC communication to use UDP instead of RDS\n       stop database and shutdown clusterware on each guests\n\n    3. Shutdown all guests\n\n    4. Configure Virtio network interfaces\n\n       a)Create bridge interfaces vmre0 and vmre1 on the KVM host using re0/re1 as\n         the physical interfaces and plumb IPs to the vmre0/vmre1 interfaces\n         Modify re0/re1 interface to use bridge vmre0/vmre1 interface\n\n       b)Using virsh domiflist <guest_domain_name> and note the hostdev and mac address\n         Replace hostdev interfaces from the guest with type bridge,virtio interface\n         vmre0/vmre1 while keeping same mac addresses as before\n\n   5) Update Cells node for UDP\n\n      a) Disable cellwall\n\n      b) Append _skgxp_dynamic_protocol=2 and _skgxp_ant_options=1\n          to  $OSSCONF/cellinit.ora\n\n      c) Restart rs and cellsrv\n\n\n   6) Update guest to recognize new network inteface from step 4b.\n\n       a)Check on guest and grab new internet mac to set in 70-persistent-net.rules\n\n       b)Update 70-persistent-net.rules with new mac from re0 / re1\n\n   7) Configure guest with UDP mode\n\n      a) Append _skgxp_dynamic_protocol=2 ,_skgxp_ant_options=1\" >> /etc/oracle/cell/network-config/cellinit.ora\n\n      b) Execute make -f ins_rdbms.mk ipc_g ioracle at Database Home\n\n      c) Unlock crs and Execute make -f ins_rdbms.mk ipc_g ioracle at GI Home\n\n      d) Update $GI_HOME/bin/oradism file permission and lock crs again\n\n   8) Stop Guests\n\n   9) Define Shared Storage\n\n       9.1) Use targetcli to create backstores/fileio for all guest images\n\n         a) create backstores/fileio\n\n         b) create iscsi client (note target name to be used in later step)\n\n         c) create luns to connect with backstores/fileio to iscsi client\n\n         d) create ACLS for each of the client accessing this client(from step b)\n\n            The ACLS are for iscsi clients from KVM hosts\n            INITIATOR_NAME cat be found in /etc/iscsi/initiatorname.iscsi\n\n            ex: InitiatorName=iqn.1988-12.com.oracle:1fe0e187abf3\n\n       9.2) Expose ISCSI to local drive and for target VM\n\n            a) Using internal/private IP from re0 or re1 to create storage pool (\n               with device path from ISCSI client target name in Step 5.1b\n               using virsh pool-define\n            b) start the storage pool using virsh pool-start\n               iscsi storage should be mount at /dev/disk/by-path/ip*\n\n       9.3) Modify VM's disks to use new ISCSI luns from storage pool above\n            Guest is using local physical disks on compute host,\n            removing current mount points, and attaching to ISCSI storage luns\n\n            a) Detach each of the current storage mount from the guest\n               virsh domblklist scaqap10adm01vm01.us.oracle.com\n\t\t Target   Source\n         \t-------------------------------------------------------------------------------\n              sda      /EXAVMIMAGES/GuestImages/scaqap10adm01vm01.us.oracle.com/System.img\n              sdb      /EXAVMIMAGES/GuestImages/scaqap10adm01vm01.us.oracle.com/grid19.19.0.0.230418.img\n              sdc      /EXAVMIMAGES/GuestImages/scaqap10adm01vm01.us.oracle.com/db19.19.0.0.230418_3.img\n              sdd      /EXAVMIMAGES/GuestImages/scaqap10adm01vm01.us.oracle.com/u01.img\n\n            b) Attach each of the storage mount with luns mount from ISCSI storage\n\t Align them with the proper target (sda, sdb, sdc...) on the guest from virsh domblklist cmd\n\n\t virsh domblklist scaqap10adm01vm01.us.oracle.com\n\n\t Target   Source\n         --------------------------------------------------------------------------------------------------------------------------------\n\t\tsda      /dev/disk/by-path/ip-192.170.216.32:3260-iscsi-iqn.2003-01.org.linux-iscsi.scaqap10adm01.x8664:sn.501a966a6afc-lun-0\n\t\tsdb      /dev/disk/by-path/ip-192.170.216.32:3260-iscsi-iqn.2003-01.org.linux-iscsi.scaqap10adm01.x8664:sn.501a966a6afc-lun-1\n\t\tsdc      /dev/disk/by-path/ip-192.170.216.32:3260-iscsi-iqn.2003-01.org.linux-iscsi.scaqap10adm01.x8664:sn.501a966a6afc-lun-2\n\t\tsdd      /dev/disk/by-path/ip-192.170.216.32:3260-iscsi-iqn.2003-01.org.linux-iscsi.scaqap10adm01.x8664:sn.501a966a6afc-lun-3\n\n\n  10) Startup Guests, and Verify CRS status\n      Verify that guest is reachable and ready for migration\n\n      Completing Step 1 to Step 10, the environment is ready for Live Migration\n\n  11) Perform live migration guest1 from clustersList on compute1 to compute2 and back\n      Repeating the migration back and forth for 10 times.\n\n\n  12) Check CRS cluster status after migration completed\n\n\n   MODIFIED   (MM/DD/YY)\n   dongyzhu    05/08/25 - Add the option to select migrationTool with\n                          oedacli or migratevm\n   dongyzhu    03/25/25 - combining lrg metrics into one output lst file\n   dongyzhu    02/21/25 - Add discover es after 1st live-migration\n   dongyzhu    02/10/25 - remove the MIGRATEVM_CMD due to the change of\n                          migratevm path\n   lknguyen    02/05/25 - Add migratevm\n   lknguyen    02/01/25 - Add create_plugin_var call\n   dongyzhu    11/28/24 - Add save file and run_serial_clusters for live migration temporary\n   lknguyen    08/28/24 - Use oedacli for live-migrate\n   lknguyen    08/02/24 - Use vm_maker for live_migrate\n   lknguyen    09/13/23 - Remove_temp_change_vf\n   suragraw    08/29/23 - Add setup iscsi support\n   lknguyen    08/07/23 - Add workaround disable.fnddqry\n   lknguyen    07/19/23 - Add option for change to UDP\n   lknguyen    07/10/23 - Update config_virtio for 2nd guest\n\t\t  - Move configure shared storage to step10\n   lknguyen    06/26/23 - Add trace file\n   lknguyen    06/07/23 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_livemig_gen_unit_tests.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_livemig_gen_unit_tests.sh - live migration unit test runner file generator",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_livemig_interrupt_func.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_livemig_interrupt_func.sh - unit test for live migration interrupt\n      test main functions\n\nThis is the main functions for unit test for migratevm interrupt handling\n      (Bug 37787446), which runs on 3 clusters\n      back and forth with three different types of signals at different phases\n      of migrations.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_livemig_interrupt_test.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_livemig_interrupt_test.sh - unit test for live migration interrupt unit test\n\nUnit test for migratevm interrupt handling (Bug 37787446), which runs on 3 clusters\n   back and forth with three different types of signals at different phases of migrations.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_logon_util.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_logon_util.sh - utility file for logon storm testing\n\nThis script contains helper functions to run stat collection in the\n     background while logon storm simulation is running.\n     Stat collection currently supported :\n      - CPU\n      - cumulative logons\n      - cell disk open\n      - cell single block physical read: xrmem cache\n      - cell single block physical read: flash cache\n      - cell smart table scan\n      - cell single block physical read\n\nUsage :\n     sh tsagrh_logon_util.sh record <node connection string> <stat name>\n        <collection duration> <stop marker location>\n\n   MODIFIED   (MM/DD/YY)\n   pkuttam     09/02/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_lr_update_oeda_bits.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_lr_update_oeda_bits.sh - Helper script to update OEDA bits on LRE\n\nThis script is written for Long-Runnig ENV and does following:\n\n     1. Updates OEDA Bits from oss/bin of target label on first compute\n     2. Updates System.img from oss/bin of target label on both compute\n\nPass target label during script invocation\n      $ sh tsagrh_lr_update_oeda_bits.sh \"<comma_seperated_computes>\" \"<target_oeda_label>\"\n      Else it will default pick target label from:\n      /net/10.32.19.91/export/exadata_images/dpant/lrg_driver/upgrade_base_labels/base_label_OSS_25\n\n   MODIFIED   (MM/DD/YY)\n   sadwe       07/01/25 - Update OEDA properties rerty rds-ping with interval of 5sec - Bug 37018865\n                        - Correct logic to preserve the previous bits in linux-x64.prev dir\n   sadwe       02/25/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_lrgrunday.sh",
    "setup": null,
    "flags": null,
    "description": "This script is to check the day of the week from the label version matches\n     the day which is designated to run the lrg for the real hardware environment",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_m2_failure_inject.sh",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_m2_failure_restore.sh",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_make_asan_rpm.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_make_asan_rpm.sh - This script is used to create asan RPM on farm\n\nCreate asan RPM on farm , it will change the name of the asan RPM to current_label\n     + 1 to distinguish it from the cell RPM.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_migmon_test.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_migmon_test.sh - Live migration unit test Migmon test.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_migrateids.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_migrateids.sh - testcases for migrate_ids.sh\n\nAdd Testcases for migrate_ids.sh with various parameters\n     such as --uid-file,--gid-file,-uid,-gid",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_notifyplcy.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_notifyplcy.sh - TSAG test suite Real Hardware script to update NOTIFcation PoLiCY\n\nScript to update notification policy on cell node based on content found in\n     /net/10.32.19.91/export/exadata_images/dpant/cell_notification_policy file",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_oeda_label_pick.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_oeda_label_pick.sh - Script to pick Correct OEDA Bits for SE Label",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_oedacli_guest_edv_ops.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_oedacli_guest_edv_ops.sh - RH script for oedacli API for edv\n\nThis script perfrom follwoing steps:\n      1. Change the default password of the guest (using $ passwd, set it to welcome5)\n      2. ALTER MACHINE ACTION=CREATEVOLUME VOLNAME=vol_sadwe WHERE HOSTNAME=scaqaw03adm01vm01\n      3. ALTER MACHINE ACTION=ATTACHVOLUME VOLNAME=vol_sadwe WHERE HOSTNAME=scaqaw03adm01vm01 STEPNAME= ATTACH_VOLUME\n      4. Login to guest and verify the volume attached using lsblk\n      5. ALTER MACHINE ACTION=DETACHVOLUME VOLNAME=vol_sadwe WHERE HOSTNAME=scaqaw03adm01vm01 STEPNAME= DETACH_VOLUME\n      6. Login to guest and verify the volume attached using lsblk are removed\n      7. ALTER MACHINE ACTION=REMOVEVOLUME VOLNAME=vol_sadwe WHERE HOSTNAME=scaqaw03adm01vm01\n      8. Revert the guest password to oeda default (welcome1)\n\nJira: Exadata Exascale Testing Team / EXESL-425: Cloud Use Case: OEDACLI volume attachment with 19c ASM on EDV\n     Wiki: https://confluence.oraclecorp.com/confluence/x/ZrRWhgM\n\n   Usage: sh tsagrh_oedacli_guest_edv_ops.sh <log_suffix> <is_basedb_guest> <is_vhostscsi_disk_hotplug> <attach_disk_count>\n      [log_suffix]:                 Optional. Append to log name for readablity\n      [is_basedb_guest]:            Optional. True for BaseDB or False for Non-BaseDB guest flag (Default: false)\n      [is_vhostscsi_disk_hotplug]:  Optional. True for vhost-scsi hotplug or False for normal disk hotplug flag (Default: false)\n      [attach_disk_count]:          Optional. Number disk hot attachment to specified guest VM (Default: 5)\n      [lrg_name]:                   Optional. Give LRG_NAME if you want to run on specific guest VM from LRG\n\n\n   MODIFIED   (MM/DD/YY)\n   saheranj    08/14/25 - Changes to support both vhost_scsi and non-vhost-scsi\n                          disk hotplug on basedb or non-basedb guest\n                          Test Enhancement provide fix coverage for Bug 38295239\n   sadwe       07/29/25 - Pick OEDA_ZIP from plugin_var.log\n   sadwe       07/07/25 - Increased disk count to 50 - Bug: 38011780\n   sadwe       05/06/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_oedadeplday.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_oedadeplday.sh - TSAG Real Hardware OEDA DEPLoyment DAY check\n\nThis script is to check the day of the week from the label version matches\n     the day which is designated to perform Image + OEDA redeployment for the\n     real hardware environment",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_online_u01disk_expansion.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_online_u01disk_expansion.sh - RH online guest u01 disk expansion\n\nGuest storage expansion in /u01 partition by resizing up the volume\n     Below steps that show how to increase the space in /u01 partition dynamically\n     Note:  VM is using elastic Exascale volumes\n     1. escli chvolume <u01 volId> --attributes size=<new size>\n     2. virsh blockresize <guest> --path <u01 mount> --size <new size>\n     3. On guest: parted <block dev> resizepart <partition no> <partition End>\n     4. On guest: pvresize <phyical vol>\n     5. On guest: lvresize -l +100%FREE <lvpath>\n     6. On guest: xfs_growfs /u01\n     7.Verify new size using 'df -h /u01' on guest\n\nDetailed steps here: https://confluence.oraclecorp.com/confluence/x/rterrQI\n\n   MODIFIED   (MM/DD/YY)\n   sadwe       12/16/24 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_pmem_trace_functions.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_pmem_trace_functions.sh\n\nThis script has pmem logs related functions.\n\nMODIFIED   (MM/DD/YY)\n   sadwe       04/18/25 - Add workaround in get_lgwr_pmem for sid x9cdb1db1\n   saheranj    07/26/24 - Change to use the correct db_trace path after\n                          config change to create 12 cluster\n   mwhwa       02/15/23 - Update get_lgrw_pmem logic\n   mwhwa       02/01/23 - update grep command due to new output behavior\n   sajkansa    05/04/22 - Fixed the way of extracting logs\n   sajkansa    03/06/22 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_poison_x9.sh",
    "setup": null,
    "flags": {
      "-n": "disknum=0"
    },
    "description": "tsagrh_poison_x9.sh - functional tests for poison injection on X9\n\nadds 4 poison injection test :\n    1). Handling of existing badblocks: 2) Handling poison in PMEM celldisk\n        metadata:. 3) Handling poison in PMEM data griddisk.\n        4) Handling poison in PMEM cache metadata.\n\nMODIFIED   (MM/DD/YY)\n   saheranj    10/07/24 - fix for poison test with 12 cluster\n   saheranj    09/27/24 - Add ignore redundancy while cellsrv shutdown\n   suragraw    03/29/23 - Changes in grepping after orion\n   saheranj    02/28/23 - Changes to fix creation of pmemlog\n   saheranj    02/21/23 - Changes to fix poison test failure\n   suragraw    01/24/22 - Disable Part 3, because of WB mode disbaled.\n   suragraw    10/18/22 - Enable Part 3 and 4\n   suragraw    09/13/21 - Disable Part 3 and 4 until Bug 33340215 is resolved\n   dekhatri    02/18/21 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_port_enable_disable.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_port_enable_disable.sh - Roce switch enable/disable\n\nScript to enable/disable ROCE switch",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_post_volume_test.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_post_volume_test.sh - Post Upgrade Volume Tests\n\nThe script contains Post upgrade Volume operations",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_power_outage.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_power_outage.sh - Full-rack-power-off on Exascale Software Stack\n\nTest added as part of \"Full Power Outage\" recent incident\n\n      Steps:\n       1. On a fully deployed system set chassis poweroff on all computes and cells\n       2. Wait for 10 minutes\n       3. Power on all computes and cells. Wait for 15 minutes\n       4. Validate that all services, VM Clusters and DB's are up running",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_pre_volume_test.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_pre_volume_test.sh - Pre Upgrade Volume Tests\n\nThe script contains Post upgrade Volume operations",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_preserve_fw_profile.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_preserve_fw_profile.sh -\n\nScript to downgrade_firmware and generate node_firmware_state.log",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_rdbms_label_pick.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_rdbms_label_pick.sh - Pick latest RDBMS Label from 23.x Series",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_rdma_keep_alive.sh",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_rds_ping_test.sh",
    "setup": null,
    "flags": null,
    "description": "Script to perform rds-ping test given list of KVM/HOST and Guest_nodes\n     between interfaces stre0/stre1\n\ntsagrh_rds_ping_test.sh requires two host_list args for execution\n   example : sh tsagrh_rds_ping_test.sh scaqan15adm03.us.oracle.com,scaqan15adm04.us.oracle.com \\\n                                        scaqan15dv0305.us.oracle.com,scaqan15dv0405.us.oracle.com\n\n   MODIFIED   (MM/DD/YY)\n   lknguyen    05/10/22 - Initial creation, script to perfrom rds_ping test",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_reboot_fndd_check.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_reboot_fndd_check.sh - CHeck logsfrom EGS Leader after rebooting",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_reboot_roce_functions.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_reboot_roce_functions.sh\n\nThis script has functions to extract,reboot roce\n     switch and checking roce switch status functions",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_reboot_roce_switch.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_reboot_roce_switch.sh\n\ntsagrh alerts after 12 min once roce switch is rebooted",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_remove_cluster.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_remove_cluster.sh - Delete The Cluster using OEDA\n\nThe script covers 2 testcases -\n     1. Shutdown the Guest using VM Maker and remove it using OEDA\n     2. Directly Remove the guest using OEDA",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_remove_default_sudo.sh",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_reset_disks.sh",
    "setup": null,
    "flags": null,
    "description": "Script to perform reset_disks given comma separated host lists\n\ntsagrh_reset_disks.sh requires nodelist args for execution\n   example : sh tsagrh_reset_disks.sh -nodelist scaqan15adm03.us.oracle.com,scaqan15adm04.us.oracle.com\n             sh tsagrh_reset_disks.sh -xml /net/10.32.19.91/export/exadata_images/dpant/exascale/exascale_kvm_upgrade.xml\n\n\n   MODIFIED   (MM/DD/YY)\n   lknguyen    03/28/25 - Update tsagimage user connection\n   lknguyen    06/26/23 - Add nodetype in cmd input\n\t          - Use parallel for check_remote_ssh_status\n   dpant       12/15/22 - pass CELLS & COMPUTES var to functions\n   lknguyen    09/30/22 - Add waitfor initialization complete, and -show_init option\n   lknguyen    06/20/22 - Initial creation, script to perfrom reset_disks",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_restore_dbmmgmt_rpm.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_restore_dbmmgmt_rpm.sh - Restore exadata-dbmmgmt RPM and verify dbserver attributes.\n\nThis script restores the exadata-dbmmgmt RPM on guest nodes after uninstall/reinstall\n     procedures. It ensures the dbserver \"comment\" attribute is preserved correctly\n     and validates the installation by checking required RPMs and service states.\n\nMODIFIED   (MM/DD/YY)\n   arassing    08/21/25 - Initial creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_rocesw_patch.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_rocesw_patch.sh - RH ROCESW patch script\n\nThis script runs rocesw patchmgr remotely from kvmhost.\n     Steps performed in order:\n     1. Setup ssh equivalency from remote host to ROCESW'es\n     2. Run Downgrade patch on the ROCESW\n     3. Run Upgrade patch on the ROCESW\n\nswich_patch.zip is used from the $ADE_VIEW_ROOT/oss/bin\n\n   MODIFIED   (MM/DD/YY)\n   muraghav    06/03/25 - Add empty switch list validation\n   sadwe       04/01/25 - Pick up/downgrade rocesw version from tftp loc\n   sadwe       12/27/24 - Update rocesw version after 10.3(6)\n   sadwe       10/24/24 - Change to run the patchmgr remotely from kvmhost\n   sadwe       08/07/24 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_rpm_unins_conf.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_rpm_unins_conf.sh\n\nThis script is used to write a functional\n     test for maa bug 32059080\n\n1.) Extract kvm util rpm names from db nodes\n     2.) Uninstall kvm util rpm from db nodes\n     3.) check presence of /ETC/EXADATA/CONFIG/INITVF.CONF\n\n   MODIFIED   (MM/DD/YY)\n   sajkansa    02/24/22 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_rswitchfail_maa.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_rswitchfail_maa.sh - TSAG Real Hardware Roce SWITCH FAILure injection scenarios\n\nThe script contains RoCE switch related failure injection scenarios to run on X9 QR",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_run_cellrpm_upgrade.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_run_cellrpm_upgrade.sh - Perform Cell RPM upgrade\n\nPerform Cell RPM upgrade after Pre Upgrade Deployment\n This is being done to cover Case for Bug-36197936\n We check if the file exist or not - rhexcupgrade_online_cellrpm_upgrade_label\n If YES we move ahead and do the RPM Rolling upgrade on the Cell nodes.\n If NO we create a suc - skip_cellrpm_upgrade and move ahead with full upgrade",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_run_orion.sh",
    "setup": null,
    "flags": null,
    "description": null,
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_run_orion_in_node.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_run_orion_in_node.sh -\n\nThis script is used to actually run\n     orion workload parallely inside the node\n\nMODIFIED   (MM/DD/YY)\n   sajkansa    05/17/22 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_sav_asan_logs.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_sav_asan_logs.sh - This script is used to save asan logs.\n\nAsan logs copied from storage node to asan.sav in farm results.\n     Then we check whether asan-* files are present in the directory to point it out.\n     Also it clears the RS-700 CELLSRV RESTART time exceeded on the cell nodes.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_save_metadata.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_save_metadata.sh - Backup blockstore metadata from a cell\n               and egs raft logs from cell and compute.\n\nThis script connects to a remote cell, runs the metadata backup,\n    retrieves the backup file and copy egs raft logs from cell and compute.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_send_mail.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_send_mail.sh - Send Email for LRG Start and completion",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_set_deployment_hw.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_set_deployment_hw.sh - Script to pick the correct HW/XML for LRGs\n\nThis Script helps to pick the Correct Scheduling Dir, No of computes,cell\n     XML name and XML Location based on the REAL_HW Variable passed in the LRG\n\n     The LRG Name or Real HW Prefix can be passed to pick other parameters.\n     The script will copy the lrg_deploy_properties file from the TFTP Server\n     And based on the variable passed, it will decide HW, Sched. Dir and XML\n     Example -\n     sh tsagrh_set_deployment_hw.sh scaqan15\n     sh tsagrh_set_deployment_hw.sh lrgrhexcupgrade",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_setup_ssh.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_setup_ssh.sh - setup a ssh connection b/w cell nodes\n\nSetting up ssh connection between cells using setup_ssh_eq_remote\n\n   MODIFIED   (MM/DD/YY)\n   venthiya    02/11/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_simulate_ce_fault.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_simulate_ce_fault.sh - Simulate CE fault to cover kernel page offline\n\nTest to Simulate machine check errors (CE fault) using mce-inject module\n     to a running kernel on compute, cell or guest node's selected processes\n     and verify that kernel handles the fault with page offlining message seen in\n     /var/log/messages with the process running after fault and no DB workload errors\n\n     Cell node - CELLSRV, RS, MS processes used for fault injection\n     Guest node - ORA_PMON process used for fault injection\n\n     For more details on mce-inject module info/usage refer below links:\n       https://github.com/andikleen/mce-inject/tree/master\n       https://linux-git.oraclecorp.com/other/us_linux_systest/-/blob/scheduler/Scheduler/doc/linux_sys_test/mem_tests/ce_howto.txt",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_socket_console_restoration_test.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_socket_console_restoration_test.sh - Test case 4.1 Early stage live migration failure\n\nThis unit test for live migration is to detect socket console\n     restoration upon failed migrations. We use the test to detect if\n     the symbolic link for the socket console subdirectory has been\n     cleaned up successfully after a failed migration.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_special_tc_rule_configuration.sh",
    "setup": null,
    "flags": null,
    "description": "This is a unit tet for live migration called special tc rule configuration. Upon sucessful migration\n     we run the verification of loopback TC rules for storage VLAN and cluster VLAN on the target node,\n     and upon failed migration, we run the verification on the source node.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_stop_start_crs.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_stop_start_crs.sh - This file is used to stop/restart the crs on cluster.\n\nThis file is used to stop crs cluster before restarting the cellsrv on cell\n     to avoid the ORA-221 on the guest nodes and restart crs.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_swtchfail1_maa.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_swtchfail1_maa.sh - Long Running Switch failure Tests\n\nInspired by MAA scripts - added various functions to operate on Switches\n    We can run these for longer duration when DB workload is running.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_ue_dimm_poison.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_ue_dimm_poison.sh - Real Hardware fault injection test for DIMM in cell node\n\nThis test injects failure into cell node's DIMM and failure at key pocesses.\n     After the DIMM failure, we verify the cell node status and check\n     alerts",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_upload_rh_results.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_upload_rh_results.sh - Upload results for integration run on APEX\n\nThis script helps to upload the results for the integration runs on the Apex UI\n      The script first check if the results exist, deletes them and adds new row for same label",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_validate_libndctl_test.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_validate_libndctl_test.sh - Check libndctl.so not loaded on\n          Guest as part of fix from bug34885090\n\nFunctional test added to lrgrhx9amdimonec as part of bug 34885090 fix,\n     to make sure libndctl.so library is not loaded on PMON DB process in\n     guest nodes if pmem hardware not present. This will help to reduce CPU\n     expensive operation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_validate_restsubscriber_files.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_validate_restsubscriber_files.sh - Verify rest subscriber key files pre/post upgrade\n\nSet a REST subscriber before cell upgrade, verify that the key and\n     certificate files exist both before and after cell upgrade",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_verify_esnode_attr.sh",
    "setup": null,
    "flags": null,
    "description": "This scripts is a test for vtambi_clear_ms_esnode_attr_on_ers_disable.\n\n    Steps follwed in order:\n\n    Step 1: Disable AEP\n     Step 1.1: List cluster to check servicePlacement before Disabling AEP\n     Step 1.2: Change AEP servicePlacement from auto to manual\n     Step 1.3: Validate AEP is turned from auto to manual\n\n    Step 2: Run chservice --disable ers for cell2 and verify esnode attr\n     Step 2.1: List esnode on cell2 before disabling ERS\n     Step 2.2: List ERS status before disabling ERS\n     Step 2.3: Disable ERS on cell2\n     Step 2.3: Confirm ERS is disable on cell2\n     Step 2.4: Make sure below ESNODE attributes read as empty on cell2\n           frontEndPriority, restBackend, restBackendServerList, ersip\n\n    Step 3: Disable any non-ERS service (USREDS) on cell3 and verify esnode attr\n     Step 3.1: List USEREDS status before disabling\n     Step 3.2: List esnode on cell3 before disabling USREDS\n     Step 3.3: Disable USEREDS on cell3\n     Step 3.4: ensure that the disable for that service works\n     Step 3.5: verify no ESNODE attribute are cleared on cell3\n\n    Step 4: Re-enable the services back in order\n     Step 4.1: Enable USREDS on cell3\n     Step 4.2: Enable USREDS on cell2\n     Step 4.3: Enable AEP\n\nConnect to ESCLI instance of cell1 using port 8080 (ERS backend)\n\n   MODIFIED   (MM/DD/YY)\n   sadwe      09/10/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_vm_edv_guest_operations.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_vm_edv_guest_operations.sh - Guest operation testcases on VM_EDV\n\nGuest operation functional testcases for VM on voluems",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_vmmaker_bugtests.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_vmmaker_bugtests.sh - Realhw VMMAKER Bugtests\n\nThis file is a part of realhw VMMAKER testsuite which coverage for bug tests\n\nMODIFIED   (MM/DD/YY)\n   sadwe       09/19/24 - Add bug test 36900572\n   sadwe       09/09/24 - Add bug test 36515918 34383808",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_vmmaker_livemig.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_vmmaker_livemig.sh - VM_MAKER utility for Live-Migration\n\nUsage:\n     $ sh tsagrh_vmmaker_livemig.sh <cells> <src_host> <dest_host> <guest> <option>\n\n     Options:\n\n     1. Perform precheck validations before migration event\n        cmd    : vm_maker --precheck-live-migrate-domain\n        oprion : precheck\n        example: $ sh tsagrh_vmmaker_livemig.sh <cells> <src_host> <dest_host> <guest> precheck\n\n     2. Perform EDV resource setup before migration event\n        oprion : setup_resources\n        example: $ sh tsagrh_vmmaker_livemig.sh <cells> <src_host> <dest_host> <guest> setup_resources\n\n     3. Perform Livemigration of domain from src host to destination host\n        cmd    : vm_maker --migrate-domain\n        oprion : live_migrate\n        example: $ sh tsagrh_vmmaker_livemig.sh <cells> <src_host> <dest_host> <guest> live_migrate\n\n     4. List ongoing/completed migration event of domain on src host\n        cmd    : vm_maker --list-migration\n        oprion : list_migration\n        example: $ sh tsagrh_vmmaker_livemig.sh <cells> <src_host> <dest_host> <guest> list_migration\n\n     5. Abort ongoing migration event of  domain on src host\n        cmd    : vm_maker --migrate-domain --domain <domain> --abort\n        oprion : abort_migrate\n        example: $ sh tsagrh_vmmaker_livemig.sh <cells> <src_host> <dest_host> <guest> abort_migrate\n\nThis utility only supports single migration event at a one time\n     For concurrent migration append '&' to run the command in background\n\n   MODIFIED   (MM/DD/YY)\n   sadwe       01/06/25 - Add prereq to create bridges interface on the dest host\n   sadwe       10/09/24 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_vmmaker_livemig_scenarios.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_vmmaker_livemig_scenarios.sh - RH Live Migration Scenarios using VM_MAKER\n\nThis File contins multiple test scenarios for the Live Migrations using\n     VM_MAKER commands, call tsagrh_vmmaker_livemig.sh utility to perform migrations.\n\n     Following scenarios are covered:\n     1. Migrate guest1 from kvmhost1 to kvmhost2 and Migrate it back - 5 times\n     2. Migrate guest1,guest2 from kvmhost1 to kvmhost2 concurrently and Migrate them back concurrently - 5 times (Parallel Migration)\n     3. Migrate guest1 from kvmhost1 and guest2 of same cluster(Cluster-1) from kvmhost2 in parallel and Migrate them back to their original computes concurrently - 5 times (Cross parallel migration of same cluster nodes)\n\nhttps://confluence.oraclecorp.com/confluence/x/dUrqrwI\n\n   MODIFIED   (MM/DD/YY)\n   lknguyen    02/06/25 - Skipping scenario 1, already cover in 2\n   sgangu      01/14/25 - Adding Concurrent parallel live migration scenarios\n   sadwe       10/23/24 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_vmmaker_options.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_vmmaker_options.sh - Script covering options for \"tsagrh_vmmaker_testsuite.sh\"\n\nThis script has all options that are covered as part of VM_MAKER testsuite with:\n     1. Detailed coverage of vm_maker on HOST with vm_maker utility installed.\n     2. Covered various flags with the vm_maker options to test utility end-to-end.\n     3. Should be called/run from driver, details in \"tsagrh_vmmaker_testsuite.sh\".\n     4. Future plan to include edge cases and bug tests as-well.\n\nhttps://confluence.oraclecorp.com/confluence/x/VO40gAI\n\n   MODIFIED   (MM/DD/YY)\n   sadwe       07/31/24 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_vmmaker_testsuite.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_vmmaker_testsuite.sh - Driver for tsagrh_vmmaker_options.sh\n\nThis txn adds vm_maker coverage as a part of dedicated testsuite to RH.\n     Aimed certain abilities in the test framework:\n     1. Improved Coverage of vm_maker commands end-to-end covering all options/flags.\n     2. Ability to run particular set of vm_maker commands from cmd line.\n     3. Flexibility in the framework to be plugged in lrgs or integration(so no oratst).\n     4. Standalone framework that can be run manually/locally outside lrgs/integration.\n\nhttps://confluence.oraclecorp.com/confluence/x/VO40gAI\n\n     How to run the utility?\n     1. Run all vm_maker options:\n     sh tsagrh_vmmaker_testsuite.sh run_all <comma seperated list of hosts>\n     Example: sh tsagrh_vmmaker_testsuite.sh run_all \"scacad02adm01.us.oracle.com,scacad03adm01.us.oracle.com\"\n\n     2. Run list of user specified options:\n     sh tsagrh_vmmaker_testsuite.sh run_list <comma seperated list of options> <comma seperated list of hosts>\n     Example: sh tsagrh_vmmaker_testsuite.sh run_list \"--set --memory,--list --vcpu,--reboot\" \"scacad02adm01.us.oracle.com\"\n\n     3. Run Bug tests: (run_all also calls run_bugtest)\n     sh tsagrh_vmmaker_testsuite.sh run_bugtest <comma seperated list of hosts>\n     Example: sh tsagrh_vmmaker_testsuite.sh run_bugtest \"scacad02adm01.us.oracle.com,\"scacad03adm01.us.oracle.com\"\"\n\n   MODIFIED   (MM/DD/YY)\n   sadwe       07/25/24 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_workload.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_workload.sh - Helper Script to Run DB Functions on DB node\n\nThis script is copied to the DB node to run different workload functions",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_x7_genoedauixml.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_x7_genoedauixml.sh - Generates x7imonec xml using OEDA UI\n\nThis script calls for apptest.ts which builds x7imonec xml from scratch using OEDA UI.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_xsh_functionality_tests.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_xsh_functionality_tests.sh - xsh funtionality tests for users root and a non-root user grid\n\nThis file is reponsible for testing basic xsh commands like ls, touch, lsacl for users root and grid.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhdisablessh.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrhdisablessh.sh - Expect script for test\n\nThis script contains two function :\n     1) Login in cell without password prompt\n     2) Exacli commands execution",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhfnddcompare.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrhfnddcompare.sh - This script compares the alert.logs\n\nFind required traces and find the difference between time stamp in seconds < 15 seconds\n\n./tsagrhfnddcompare.sh \"Currentime\" \"alert.log\"\n   ./tsagrhfnddcompare \"2025-09-09T05:01:28.573-07:00\" alert.log\n\n   MODIFIED   (MM/DD/YY)\n   habanthi    09/09/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhlrghealth.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrhlrghealth.sh - Real HW lrg health report for Exadata\n\nScript for collecting real HW lrgs execution status on scheduling node (slcb02db05)\n\nScript is executed as cronjob ( 9 am/pm ) from /home/animanan/\n    \t\t 00 9 * * * /home/animanan/lrgdetail/tsagrhlrghealth.sh\n\n     Script calls detaillrgreport.sh in <mach_sched, currently slcm02db07>:/root/lrgreport/\n     Script needs tsagprep.sh, push_keys and generate_keys\n\n   MODIFIED   (MM/DD/YY)\n   animanan    04/16/19 - Update scheduling machine detail\n   animanan    05/16/17 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhsnmpv3test_functions.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrhsnmpv3test_functions.sh - remote node side functions\n              for snmp v3 trap delivery detection\n\nThis script is copied to the remote node to run various\n     operations on the remote node.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhvolgrpresource.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrhvolgrpresource.sh - This script contains test for RHW vol group types\n\ntest for new volume group types, with resourceSharing attribute set to aggregate, none, specified.\n    more test details can be found here :\n    https://confluence.oraclecorp.com/confluence/display/EXC/Multiple+Volume+Groups+and+Volume+Group+Type\n\n      test 1\n      create a volume group with resourceSharing=aggregate, iops=0\n      create a volume with iops =200. add this volume to volume group.\n      lsvol and lsvolgrp should show the volume's iops value.\n      change the volume grp type to specified. should alter successfully\n\n      test 2\n      Create a volumegroup with resourceSharing=none.\n      create a volume with 300 iops. add it to volume group.\n      lsvolume should show iops 300 and lsvolumegroup should show iops should be 0\n\n      test 3\n      Create a volume group with resourceSharing=specified, iops=1000\n      Create volume with 500 iops. add this volume to volumegroup\n      lsvolume should show that volume iops is now 1000\n\n      test 4\n      one volume can be added to different vol groups (max 5 groups and only one of them can be sharing.)\n\n      test 5\n      # create a volume group. add few volumes to , check iops of volume group.\n      # create a clone of volumes\n      # then remove some volume groups , check iops of volume group.\n\n      test 6\n      create clones and restored bkps to all 3 different types of volume groups\n      and check the iopsProvisioned attribute of vol grp for expected output\n\n      test 7\n      add volume with invalid id to volumegroups\n      add volume to volumegroup with invalide id\n\n     test 8\n     add a volume to volumeGroups of all type,\n     chvolume alter the volume iops value,\n     check lsvolume and lsvolumegroup",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug38495244.sh",
    "setup": null,
    "flags": null,
    "description": "Test script for security bug 38495244 - DOS attack on cell node.",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_bug_38506948.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_bug_38506948.sh - Test escli --wallet connectivity to ExaCTRL on cell (bug 38506948)\n\nVerifies that escli with --wallet option connects successfully to ExaCTRL server on cell node,\n     without native library errors or connection refused, and reports connection success post-upgrade.\n\nMODIFIED   (MM/DD/YY)\n   kuashen     10/07/25 - Creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrh_labelserver_compute_binaries.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrh_labelserver_compute_binaries.sh - Refresh compute binaries from the label server\n\nThis script is used as part of the ASAN_RUN workflow to address an issue where\n     the dbserver process fails to start on compute nodes during reimaging due to\n     ASAN RPM dependencies.\n\n     It removes any locally generated compute binaries under oss/bin and downloads\n     the corresponding binaries from the current viewâs label server.\n\nMODIFIED   (MM/DD/YY)\n   arassing    10/28/25 - Initial creation",
    "platform": "REAL HARDWARE"
  },
  {
    "test_name": "tsagrhcustomssh.sh",
    "setup": null,
    "flags": null,
    "description": "tsagrhcustomssh.sh - Script to test custom ssh\n\nThis script unzip latest patch manager handling multiple label zip file\n     Performs upgrade and rollback and check permission validation",
    "platform": "REAL HARDWARE"
  }
]